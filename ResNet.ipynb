{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasColas/CNN-VS-ViT/blob/main/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD6I13Z_s6ad"
      },
      "source": [
        "# ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTcoNazCpaIV"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugBidLUTpbaF"
      },
      "source": [
        "### Download packages / dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNX1kWxxuk39",
        "outputId": "2122a07a-455b-496b-e631-7748ab11568b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting medmnist\n",
            "  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from medmnist) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from medmnist) (4.67.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from medmnist) (11.1.0)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.21.0+cu124)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->medmnist) (3.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (1.14.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medmnist) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medmnist) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (4.13.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->medmnist)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->medmnist)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->medmnist)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->medmnist)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->medmnist)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->medmnist)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->medmnist) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->medmnist) (3.0.2)\n",
            "Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=f1040767859abcb90da3f4c792d11cd3171ea786c9ea62eaf4fa2d94985d8d65\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, medmnist\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed fire-0.7.0 medmnist-3.0.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install medmnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGYvDa6_yfVI",
        "outputId": "46a4b1cd-6eec-4a12-8a24-5b4279039cc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au4ree9vpfF7"
      },
      "source": [
        "### Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dNe-jwnUuUwN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from medmnist import OCTMNIST  # Import the OCTMNIST dataset\n",
        "import wandb\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g4wnjaRxqNx",
        "outputId": "de177204-a56e-44a3-dac6-8139b137deeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxroXd-atMKU"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47tSPB0PtFAQ"
      },
      "source": [
        "### ResidualBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jF2GPxJNs4zi"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3,\n",
        "                 activation=nn.ReLU(inplace=True), use_batchnorm=True):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        padding = kernel_size // 2  # To keep spatial dimensions constant\n",
        "\n",
        "        # First convolution layer\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                               padding=padding, bias=not use_batchnorm)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels) if use_batchnorm else nn.Identity()\n",
        "\n",
        "        # Second convolution layer\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size,\n",
        "                               padding=padding, bias=not use_batchnorm)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels) if use_batchnorm else nn.Identity()\n",
        "\n",
        "        # Define the activation function\n",
        "        self.activation = activation\n",
        "\n",
        "        # If input and output channels differ, use a 1x1 conv to match dimensions\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.activation(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += shortcut\n",
        "        out = self.activation(out)\n",
        "        return out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sP9HxoVtO9W"
      },
      "source": [
        "### ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CGjqNFzitOow"
      },
      "outputs": [],
      "source": [
        "class CustomResNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=3,         # Input channels (e.g., 3 for RGB images)\n",
        "                 num_blocks=4,          # Number of residual blocks\n",
        "                 base_channels=64,      # Number of channels for the first block\n",
        "                 kernel_size=3,         # Kernel size for convolutions\n",
        "                 activation=nn.ReLU(inplace=True),  # Activation function\n",
        "                 use_batchnorm=True,    # Whether to use BatchNorm\n",
        "                 num_classes=10         # Number of classes for final output\n",
        "                 ):\n",
        "        super(CustomResNet, self).__init__()\n",
        "\n",
        "        self.initial_conv = nn.Conv2d(in_channels, base_channels, kernel_size=kernel_size,\n",
        "                                      padding=kernel_size//2, bias=not use_batchnorm)\n",
        "        self.initial_bn = nn.BatchNorm2d(base_channels) if use_batchnorm else nn.Identity()\n",
        "        self.activation = activation\n",
        "\n",
        "        # Create a sequential container for residual blocks.\n",
        "        layers = []\n",
        "        # First block: input channels = base_channels, output channels = base_channels\n",
        "        for _ in range(num_blocks):\n",
        "            layers.append(ResidualBlock(base_channels, base_channels,\n",
        "                                        kernel_size=kernel_size,\n",
        "                                        activation=activation,\n",
        "                                        use_batchnorm=use_batchnorm))\n",
        "        self.residual_layers = nn.Sequential(*layers)\n",
        "\n",
        "        # Global average pooling and a final linear classifier.\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(base_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial_conv(x)\n",
        "        x = self.initial_bn(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.residual_layers(x)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvS7mOYitYeA",
        "outputId": "b39d9dc1-956d-4083-dd54-0fd02ea2ff70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "CustomResNet(\n",
            "  (initial_conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (initial_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  (residual_layers): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (4): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "  )\n",
            "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=64, out_features=1000, bias=True)\n",
            ")\n",
            "Output shape: torch.Size([1, 1000])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = CustomResNet(\n",
        "        in_channels=3,\n",
        "        num_blocks=5,               # You can choose how many blocks\n",
        "        base_channels=64,\n",
        "        kernel_size=3,              # Kernel size can be adjusted\n",
        "        activation=nn.LeakyReLU(0.1, inplace=True),  # You can choose any activation\n",
        "        use_batchnorm=True,         # Toggle batch normalization\n",
        "        num_classes=1000            # For example, for ImageNet classification\n",
        "    )\n",
        "\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "\n",
        "\n",
        "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "output = model(dummy_input)\n",
        "print(\"Output shape:\", output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3TY4ytRkcEx"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "KTGP5IGVkenI",
        "outputId": "883d4c47-13ae-4748-d760-c197d8bc5243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 97477\n",
            "Test dataset size: 1000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1])\n"
          ]
        }
      ],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
        "    transforms.RandomRotation(10),      # Randomly rotate images by +/- 10 degrees\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize for a single channel (grayscale)\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "# Download the OCTMNIST dataset.\n",
        "train_dataset = OCTMNIST(split='train', transform=transform_train, download=True)\n",
        "test_dataset  = OCTMNIST(split='test', transform=transform_train, download=True)\n",
        "\n",
        "print(\"Train dataset size:\", len(train_dataset))\n",
        "print(\"Test dataset size:\", len(test_dataset))\n",
        "\n",
        "# Create data loaders.\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Visualize a few training samples\n",
        "def imshow(img, title=None):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "imshow(torchvision.utils.make_grid(images[:1]), title=\"Sample OCTMNIST Images\")\n",
        "print(labels[:1].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YUhZm4xuNUG"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TTT8r-Alz7tg"
      },
      "outputs": [],
      "source": [
        "def train_model(model, config, run_name):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
        "    num_epochs = config[\"num_epochs\"]\n",
        "\n",
        "    # For logging the confusion matrix at the end of training\n",
        "    all_test_targets = []\n",
        "    all_test_preds = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # --------------------\n",
        "        # Training Phase\n",
        "        # --------------------\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        train_loss_epoch = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for i, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            train_loss_epoch += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += targets.size(0)\n",
        "            train_correct += (predicted == targets).sum().item()\n",
        "\n",
        "            if i % 20 == 19:  # Log every 20 mini-batches\n",
        "                avg_loss = running_loss / 20\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {avg_loss:.4f}\")\n",
        "                # Log training loss with epoch as x-axis value\n",
        "                wandb.log({\"epoch\": epoch+1, \"train_loss\": avg_loss})\n",
        "                running_loss = 0.0\n",
        "\n",
        "        avg_train_loss = train_loss_epoch / len(train_loader)\n",
        "        train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "        # --------------------\n",
        "        # Evaluation Phase\n",
        "        # --------------------\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        test_loss_epoch = 0.0\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "        with torch.no_grad():\n",
        "            for i, (inputs, targets) in enumerate(test_loader):\n",
        "                inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                test_loss_epoch += loss.item()\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                test_total += targets.size(0)\n",
        "                test_correct += (predicted == targets).sum().item()\n",
        "\n",
        "                # Accumulate predictions and true labels for confusion matrix\n",
        "                all_test_targets.extend(targets.cpu().numpy())\n",
        "                all_test_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "\n",
        "\n",
        "        avg_test_loss = test_loss_epoch / len(test_loader)\n",
        "        test_accuracy = 100 * test_correct / test_total\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_accuracy\": train_accuracy,\n",
        "            \"test_accuracy\": test_accuracy,\n",
        "        })\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}% | Test Loss: {avg_test_loss:.4f} | Test Acc: {test_accuracy:.2f}%\")\n",
        "\n",
        "    # --------------------\n",
        "    # Confusion Matrix Logging\n",
        "    # --------------------\n",
        "    class_names = [\"CNV\", \"DME\", \"DRUSEN\", \"NORMAL\"]  # Adjust if necessary\n",
        "    cm = confusion_matrix(all_test_targets, all_test_preds)\n",
        "    wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "                probs=None,\n",
        "                y_true=np.array(all_test_targets),\n",
        "                preds=np.array(all_test_preds),\n",
        "                class_names=class_names)})\n",
        "\n",
        "    # Optionally, log a matplotlib figure of the confusion matrix.\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.set(xticks=np.arange(len(class_names)),\n",
        "           yticks=np.arange(len(class_names)),\n",
        "           xticklabels=class_names,\n",
        "           yticklabels=class_names,\n",
        "           title=\"Confusion Matrix\",\n",
        "           ylabel=\"True label\",\n",
        "           xlabel=\"Predicted label\")\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], 'd'),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    wandb.log({\"confusion_matrix_fig\" + run_name: fig})\n",
        "    plt.close(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKnhU1T-0I_w"
      },
      "outputs": [],
      "source": [
        "!wandb online\n",
        "\n",
        "# Configuration dictionary for wandb\n",
        "config = {\n",
        "    \"in_channels\": 1,\n",
        "    \"num_epochs\": 5,            # Adjust number of epochs as necessary\n",
        "    \"lr\": 0.001,                # Learning rate\n",
        "    \"batch_size\": 64,           # Batch size (should match your DataLoader)\n",
        "    \"num_blocks\": 5,            # Number of residual blocks in the network\n",
        "    \"base_channels\": 64,        # Number of filters in the first block\n",
        "    \"kernel_size\": 3,           # Kernel size for convolutions\n",
        "    \"activation\": \"LeakyReLU\",  # Name of the activation function\n",
        "    \"use_batchnorm\": True,      # Whether to use batch normalization\n",
        "    \"num_classes\": 4            # OCTMNIST has 4 classes\n",
        "}\n",
        "\n",
        "# Initialize wandb run and train the model\n",
        "with wandb.init(\n",
        "    config=config,\n",
        "    project='CNN VS ViT',   # Title of your project\n",
        "    group='Test',           # Group name for your run\n",
        "    save_code=True,\n",
        "    name='test',\n",
        "):\n",
        "    # Create the model with custom configuration\n",
        "    model = CustomResNet(\n",
        "        in_channels=config['in_channels'],  # OCTMNIST images are single-channel if you have preprocessed accordingly\n",
        "        num_blocks=config[\"num_blocks\"],\n",
        "        base_channels=config[\"base_channels\"],\n",
        "        kernel_size=config[\"kernel_size\"],\n",
        "        activation=nn.LeakyReLU(0.1, inplace=True),\n",
        "        use_batchnorm=config[\"use_batchnorm\"],\n",
        "        num_classes=config[\"num_classes\"]\n",
        "    )\n",
        "    print(model)\n",
        "\n",
        "    # Train the model (train_loader and test_loader should be defined before)\n",
        "    train_model(model, config, \"test\")\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgXYJQkH0jU_"
      },
      "source": [
        "## Experiments\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVDGdQcqmNhu"
      },
      "source": [
        "### Without data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X8TYaSsYj3oB",
        "outputId": "460d54c1-77b3-46ec-f40e-da5a9e0c3589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250401_134654-ecu3dq4v</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ecu3dq4v' target=\"_blank\">resnet-18</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ecu3dq4v' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ecu3dq4v</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CustomResNet(\n",
            "  (initial_conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (initial_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  (residual_layers): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (4): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (5): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (6): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (7): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (8): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (9): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (10): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (11): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (12): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (13): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (14): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (15): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (16): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (17): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "  )\n",
            "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Epoch [1/20], Step [20/1524], Loss: 1.0445\n",
            "Epoch [1/20], Step [40/1524], Loss: 0.7904\n",
            "Epoch [1/20], Step [60/1524], Loss: 0.7567\n",
            "Epoch [1/20], Step [80/1524], Loss: 0.7216\n",
            "Epoch [1/20], Step [100/1524], Loss: 0.6913\n",
            "Epoch [1/20], Step [120/1524], Loss: 0.6692\n",
            "Epoch [1/20], Step [140/1524], Loss: 0.6825\n",
            "Epoch [1/20], Step [160/1524], Loss: 0.6800\n",
            "Epoch [1/20], Step [180/1524], Loss: 0.6552\n",
            "Epoch [1/20], Step [200/1524], Loss: 0.6643\n",
            "Epoch [1/20], Step [220/1524], Loss: 0.6282\n",
            "Epoch [1/20], Step [240/1524], Loss: 0.5994\n",
            "Epoch [1/20], Step [260/1524], Loss: 0.6591\n",
            "Epoch [1/20], Step [280/1524], Loss: 0.6343\n",
            "Epoch [1/20], Step [300/1524], Loss: 0.5681\n",
            "Epoch [1/20], Step [320/1524], Loss: 0.5701\n",
            "Epoch [1/20], Step [340/1524], Loss: 0.5850\n",
            "Epoch [1/20], Step [360/1524], Loss: 0.5688\n",
            "Epoch [1/20], Step [380/1524], Loss: 0.6099\n",
            "Epoch [1/20], Step [400/1524], Loss: 0.6114\n",
            "Epoch [1/20], Step [420/1524], Loss: 0.5840\n",
            "Epoch [1/20], Step [440/1524], Loss: 0.5549\n",
            "Epoch [1/20], Step [460/1524], Loss: 0.5563\n",
            "Epoch [1/20], Step [480/1524], Loss: 0.5390\n",
            "Epoch [1/20], Step [500/1524], Loss: 0.5589\n",
            "Epoch [1/20], Step [520/1524], Loss: 0.5717\n",
            "Epoch [1/20], Step [540/1524], Loss: 0.5810\n",
            "Epoch [1/20], Step [560/1524], Loss: 0.4853\n",
            "Epoch [1/20], Step [580/1524], Loss: 0.4996\n",
            "Epoch [1/20], Step [600/1524], Loss: 0.5135\n",
            "Epoch [1/20], Step [620/1524], Loss: 0.5555\n",
            "Epoch [1/20], Step [640/1524], Loss: 0.5290\n",
            "Epoch [1/20], Step [660/1524], Loss: 0.5407\n",
            "Epoch [1/20], Step [680/1524], Loss: 0.4976\n",
            "Epoch [1/20], Step [700/1524], Loss: 0.5125\n",
            "Epoch [1/20], Step [720/1524], Loss: 0.5670\n",
            "Epoch [1/20], Step [740/1524], Loss: 0.4773\n",
            "Epoch [1/20], Step [760/1524], Loss: 0.5087\n",
            "Epoch [1/20], Step [780/1524], Loss: 0.4872\n",
            "Epoch [1/20], Step [800/1524], Loss: 0.5371\n",
            "Epoch [1/20], Step [820/1524], Loss: 0.4722\n",
            "Epoch [1/20], Step [840/1524], Loss: 0.5103\n",
            "Epoch [1/20], Step [860/1524], Loss: 0.5352\n",
            "Epoch [1/20], Step [880/1524], Loss: 0.4660\n",
            "Epoch [1/20], Step [900/1524], Loss: 0.4669\n",
            "Epoch [1/20], Step [920/1524], Loss: 0.4337\n",
            "Epoch [1/20], Step [940/1524], Loss: 0.5497\n",
            "Epoch [1/20], Step [960/1524], Loss: 0.4556\n",
            "Epoch [1/20], Step [980/1524], Loss: 0.5053\n",
            "Epoch [1/20], Step [1000/1524], Loss: 0.4239\n",
            "Epoch [1/20], Step [1020/1524], Loss: 0.4835\n",
            "Epoch [1/20], Step [1040/1524], Loss: 0.4871\n",
            "Epoch [1/20], Step [1060/1524], Loss: 0.5214\n",
            "Epoch [1/20], Step [1080/1524], Loss: 0.4825\n",
            "Epoch [1/20], Step [1100/1524], Loss: 0.4712\n",
            "Epoch [1/20], Step [1120/1524], Loss: 0.4712\n",
            "Epoch [1/20], Step [1140/1524], Loss: 0.5223\n",
            "Epoch [1/20], Step [1160/1524], Loss: 0.3877\n",
            "Epoch [1/20], Step [1180/1524], Loss: 0.4439\n",
            "Epoch [1/20], Step [1200/1524], Loss: 0.4740\n",
            "Epoch [1/20], Step [1220/1524], Loss: 0.4368\n",
            "Epoch [1/20], Step [1240/1524], Loss: 0.4455\n",
            "Epoch [1/20], Step [1260/1524], Loss: 0.4672\n",
            "Epoch [1/20], Step [1280/1524], Loss: 0.4628\n",
            "Epoch [1/20], Step [1300/1524], Loss: 0.4782\n",
            "Epoch [1/20], Step [1320/1524], Loss: 0.4360\n",
            "Epoch [1/20], Step [1340/1524], Loss: 0.4298\n",
            "Epoch [1/20], Step [1360/1524], Loss: 0.4188\n",
            "Epoch [1/20], Step [1380/1524], Loss: 0.4750\n",
            "Epoch [1/20], Step [1400/1524], Loss: 0.4580\n",
            "Epoch [1/20], Step [1420/1524], Loss: 0.4050\n",
            "Epoch [1/20], Step [1440/1524], Loss: 0.4451\n",
            "Epoch [1/20], Step [1460/1524], Loss: 0.4409\n",
            "Epoch [1/20], Step [1480/1524], Loss: 0.4480\n",
            "Epoch [1/20], Step [1500/1524], Loss: 0.4444\n",
            "Epoch [1/20], Step [1520/1524], Loss: 0.4048\n",
            "Epoch [1/20] | Train Loss: 0.5353 | Train Acc: 81.43% | Test Loss: 1.1714 | Test Acc: 53.10%\n",
            "Epoch [2/20], Step [20/1524], Loss: 0.4617\n",
            "Epoch [2/20], Step [40/1524], Loss: 0.4617\n",
            "Epoch [2/20], Step [60/1524], Loss: 0.4550\n",
            "Epoch [2/20], Step [80/1524], Loss: 0.4150\n",
            "Epoch [2/20], Step [100/1524], Loss: 0.4260\n",
            "Epoch [2/20], Step [120/1524], Loss: 0.4337\n",
            "Epoch [2/20], Step [140/1524], Loss: 0.4155\n",
            "Epoch [2/20], Step [160/1524], Loss: 0.4468\n",
            "Epoch [2/20], Step [180/1524], Loss: 0.4257\n",
            "Epoch [2/20], Step [200/1524], Loss: 0.4352\n",
            "Epoch [2/20], Step [220/1524], Loss: 0.4134\n",
            "Epoch [2/20], Step [240/1524], Loss: 0.4504\n",
            "Epoch [2/20], Step [260/1524], Loss: 0.3730\n",
            "Epoch [2/20], Step [280/1524], Loss: 0.3915\n",
            "Epoch [2/20], Step [300/1524], Loss: 0.4264\n",
            "Epoch [2/20], Step [320/1524], Loss: 0.4228\n",
            "Epoch [2/20], Step [340/1524], Loss: 0.3996\n",
            "Epoch [2/20], Step [360/1524], Loss: 0.4346\n",
            "Epoch [2/20], Step [380/1524], Loss: 0.4071\n",
            "Epoch [2/20], Step [400/1524], Loss: 0.4120\n",
            "Epoch [2/20], Step [420/1524], Loss: 0.4892\n",
            "Epoch [2/20], Step [440/1524], Loss: 0.4033\n",
            "Epoch [2/20], Step [460/1524], Loss: 0.3992\n",
            "Epoch [2/20], Step [480/1524], Loss: 0.4158\n",
            "Epoch [2/20], Step [500/1524], Loss: 0.4133\n",
            "Epoch [2/20], Step [520/1524], Loss: 0.4477\n",
            "Epoch [2/20], Step [540/1524], Loss: 0.3841\n",
            "Epoch [2/20], Step [560/1524], Loss: 0.4389\n",
            "Epoch [2/20], Step [580/1524], Loss: 0.3902\n",
            "Epoch [2/20], Step [600/1524], Loss: 0.3519\n",
            "Epoch [2/20], Step [620/1524], Loss: 0.4053\n",
            "Epoch [2/20], Step [640/1524], Loss: 0.3873\n",
            "Epoch [2/20], Step [660/1524], Loss: 0.4244\n",
            "Epoch [2/20], Step [680/1524], Loss: 0.4155\n",
            "Epoch [2/20], Step [700/1524], Loss: 0.3916\n",
            "Epoch [2/20], Step [720/1524], Loss: 0.4041\n",
            "Epoch [2/20], Step [740/1524], Loss: 0.3835\n",
            "Epoch [2/20], Step [760/1524], Loss: 0.3897\n",
            "Epoch [2/20], Step [780/1524], Loss: 0.4071\n",
            "Epoch [2/20], Step [800/1524], Loss: 0.4204\n",
            "Epoch [2/20], Step [820/1524], Loss: 0.3855\n",
            "Epoch [2/20], Step [840/1524], Loss: 0.4152\n",
            "Epoch [2/20], Step [860/1524], Loss: 0.4137\n",
            "Epoch [2/20], Step [880/1524], Loss: 0.3604\n",
            "Epoch [2/20], Step [900/1524], Loss: 0.3994\n",
            "Epoch [2/20], Step [920/1524], Loss: 0.3774\n",
            "Epoch [2/20], Step [940/1524], Loss: 0.3746\n",
            "Epoch [2/20], Step [960/1524], Loss: 0.4043\n",
            "Epoch [2/20], Step [980/1524], Loss: 0.3624\n",
            "Epoch [2/20], Step [1000/1524], Loss: 0.4285\n",
            "Epoch [2/20], Step [1020/1524], Loss: 0.3978\n",
            "Epoch [2/20], Step [1040/1524], Loss: 0.3504\n",
            "Epoch [2/20], Step [1060/1524], Loss: 0.3868\n",
            "Epoch [2/20], Step [1080/1524], Loss: 0.3846\n",
            "Epoch [2/20], Step [1100/1524], Loss: 0.4357\n",
            "Epoch [2/20], Step [1120/1524], Loss: 0.3630\n",
            "Epoch [2/20], Step [1140/1524], Loss: 0.4120\n",
            "Epoch [2/20], Step [1160/1524], Loss: 0.3894\n",
            "Epoch [2/20], Step [1180/1524], Loss: 0.4188\n",
            "Epoch [2/20], Step [1200/1524], Loss: 0.3780\n",
            "Epoch [2/20], Step [1220/1524], Loss: 0.3805\n",
            "Epoch [2/20], Step [1240/1524], Loss: 0.3754\n",
            "Epoch [2/20], Step [1260/1524], Loss: 0.3788\n",
            "Epoch [2/20], Step [1280/1524], Loss: 0.3682\n",
            "Epoch [2/20], Step [1300/1524], Loss: 0.3592\n",
            "Epoch [2/20], Step [1320/1524], Loss: 0.3353\n",
            "Epoch [2/20], Step [1340/1524], Loss: 0.3775\n",
            "Epoch [2/20], Step [1360/1524], Loss: 0.3844\n",
            "Epoch [2/20], Step [1380/1524], Loss: 0.4026\n",
            "Epoch [2/20], Step [1400/1524], Loss: 0.3743\n",
            "Epoch [2/20], Step [1420/1524], Loss: 0.3770\n",
            "Epoch [2/20], Step [1440/1524], Loss: 0.3825\n",
            "Epoch [2/20], Step [1460/1524], Loss: 0.3834\n",
            "Epoch [2/20], Step [1480/1524], Loss: 0.3743\n",
            "Epoch [2/20], Step [1500/1524], Loss: 0.3747\n",
            "Epoch [2/20], Step [1520/1524], Loss: 0.3628\n",
            "Epoch [2/20] | Train Loss: 0.4012 | Train Acc: 86.12% | Test Loss: 0.9355 | Test Acc: 68.10%\n",
            "Epoch [3/20], Step [20/1524], Loss: 0.4255\n",
            "Epoch [3/20], Step [40/1524], Loss: 0.3710\n",
            "Epoch [3/20], Step [60/1524], Loss: 0.3541\n",
            "Epoch [3/20], Step [80/1524], Loss: 0.3488\n",
            "Epoch [3/20], Step [100/1524], Loss: 0.3802\n",
            "Epoch [3/20], Step [120/1524], Loss: 0.4504\n",
            "Epoch [3/20], Step [140/1524], Loss: 0.4213\n",
            "Epoch [3/20], Step [160/1524], Loss: 0.3642\n",
            "Epoch [3/20], Step [180/1524], Loss: 0.3667\n",
            "Epoch [3/20], Step [200/1524], Loss: 0.3785\n",
            "Epoch [3/20], Step [220/1524], Loss: 0.3719\n",
            "Epoch [3/20], Step [240/1524], Loss: 0.3930\n",
            "Epoch [3/20], Step [260/1524], Loss: 0.3247\n",
            "Epoch [3/20], Step [280/1524], Loss: 0.3810\n",
            "Epoch [3/20], Step [300/1524], Loss: 0.3682\n",
            "Epoch [3/20], Step [320/1524], Loss: 0.3595\n",
            "Epoch [3/20], Step [340/1524], Loss: 0.3534\n",
            "Epoch [3/20], Step [360/1524], Loss: 0.4039\n",
            "Epoch [3/20], Step [380/1524], Loss: 0.3993\n",
            "Epoch [3/20], Step [400/1524], Loss: 0.3845\n",
            "Epoch [3/20], Step [420/1524], Loss: 0.3408\n",
            "Epoch [3/20], Step [440/1524], Loss: 0.3522\n",
            "Epoch [3/20], Step [460/1524], Loss: 0.3513\n",
            "Epoch [3/20], Step [480/1524], Loss: 0.3839\n",
            "Epoch [3/20], Step [500/1524], Loss: 0.3653\n",
            "Epoch [3/20], Step [520/1524], Loss: 0.3700\n",
            "Epoch [3/20], Step [540/1524], Loss: 0.3918\n",
            "Epoch [3/20], Step [560/1524], Loss: 0.3590\n",
            "Epoch [3/20], Step [580/1524], Loss: 0.3734\n",
            "Epoch [3/20], Step [600/1524], Loss: 0.3872\n",
            "Epoch [3/20], Step [620/1524], Loss: 0.3752\n",
            "Epoch [3/20], Step [640/1524], Loss: 0.3808\n",
            "Epoch [3/20], Step [660/1524], Loss: 0.3682\n",
            "Epoch [3/20], Step [680/1524], Loss: 0.3336\n",
            "Epoch [3/20], Step [700/1524], Loss: 0.3487\n",
            "Epoch [3/20], Step [720/1524], Loss: 0.3703\n",
            "Epoch [3/20], Step [740/1524], Loss: 0.3718\n",
            "Epoch [3/20], Step [760/1524], Loss: 0.3601\n",
            "Epoch [3/20], Step [780/1524], Loss: 0.3868\n",
            "Epoch [3/20], Step [800/1524], Loss: 0.3440\n",
            "Epoch [3/20], Step [820/1524], Loss: 0.3635\n",
            "Epoch [3/20], Step [840/1524], Loss: 0.3754\n",
            "Epoch [3/20], Step [860/1524], Loss: 0.3558\n",
            "Epoch [3/20], Step [880/1524], Loss: 0.3349\n",
            "Epoch [3/20], Step [900/1524], Loss: 0.3574\n",
            "Epoch [3/20], Step [920/1524], Loss: 0.3307\n",
            "Epoch [3/20], Step [940/1524], Loss: 0.3293\n",
            "Epoch [3/20], Step [960/1524], Loss: 0.3855\n",
            "Epoch [3/20], Step [980/1524], Loss: 0.3534\n",
            "Epoch [3/20], Step [1000/1524], Loss: 0.3515\n",
            "Epoch [3/20], Step [1020/1524], Loss: 0.3653\n",
            "Epoch [3/20], Step [1040/1524], Loss: 0.3697\n",
            "Epoch [3/20], Step [1060/1524], Loss: 0.2841\n",
            "Epoch [3/20], Step [1080/1524], Loss: 0.3292\n",
            "Epoch [3/20], Step [1100/1524], Loss: 0.3652\n",
            "Epoch [3/20], Step [1120/1524], Loss: 0.3241\n",
            "Epoch [3/20], Step [1140/1524], Loss: 0.3200\n",
            "Epoch [3/20], Step [1160/1524], Loss: 0.3495\n",
            "Epoch [3/20], Step [1180/1524], Loss: 0.3477\n",
            "Epoch [3/20], Step [1200/1524], Loss: 0.3486\n",
            "Epoch [3/20], Step [1220/1524], Loss: 0.3557\n",
            "Epoch [3/20], Step [1240/1524], Loss: 0.3384\n",
            "Epoch [3/20], Step [1260/1524], Loss: 0.3269\n",
            "Epoch [3/20], Step [1280/1524], Loss: 0.3533\n",
            "Epoch [3/20], Step [1300/1524], Loss: 0.3522\n",
            "Epoch [3/20], Step [1320/1524], Loss: 0.3674\n",
            "Epoch [3/20], Step [1340/1524], Loss: 0.3768\n",
            "Epoch [3/20], Step [1360/1524], Loss: 0.3621\n",
            "Epoch [3/20], Step [1380/1524], Loss: 0.3518\n",
            "Epoch [3/20], Step [1400/1524], Loss: 0.3034\n",
            "Epoch [3/20], Step [1420/1524], Loss: 0.3679\n",
            "Epoch [3/20], Step [1440/1524], Loss: 0.3700\n",
            "Epoch [3/20], Step [1460/1524], Loss: 0.3795\n",
            "Epoch [3/20], Step [1480/1524], Loss: 0.3222\n",
            "Epoch [3/20], Step [1500/1524], Loss: 0.3496\n",
            "Epoch [3/20], Step [1520/1524], Loss: 0.3333\n",
            "Epoch [3/20] | Train Loss: 0.3615 | Train Acc: 87.48% | Test Loss: 0.6059 | Test Acc: 76.70%\n",
            "Epoch [4/20], Step [20/1524], Loss: 0.3902\n",
            "Epoch [4/20], Step [40/1524], Loss: 0.3417\n",
            "Epoch [4/20], Step [60/1524], Loss: 0.3697\n",
            "Epoch [4/20], Step [80/1524], Loss: 0.3948\n",
            "Epoch [4/20], Step [100/1524], Loss: 0.3627\n",
            "Epoch [4/20], Step [120/1524], Loss: 0.3477\n",
            "Epoch [4/20], Step [140/1524], Loss: 0.2917\n",
            "Epoch [4/20], Step [160/1524], Loss: 0.3372\n",
            "Epoch [4/20], Step [180/1524], Loss: 0.3180\n",
            "Epoch [4/20], Step [200/1524], Loss: 0.3140\n",
            "Epoch [4/20], Step [220/1524], Loss: 0.3421\n",
            "Epoch [4/20], Step [240/1524], Loss: 0.3530\n",
            "Epoch [4/20], Step [260/1524], Loss: 0.3283\n",
            "Epoch [4/20], Step [280/1524], Loss: 0.3447\n",
            "Epoch [4/20], Step [300/1524], Loss: 0.3584\n",
            "Epoch [4/20], Step [320/1524], Loss: 0.3407\n",
            "Epoch [4/20], Step [340/1524], Loss: 0.3388\n",
            "Epoch [4/20], Step [360/1524], Loss: 0.3222\n",
            "Epoch [4/20], Step [380/1524], Loss: 0.3609\n",
            "Epoch [4/20], Step [400/1524], Loss: 0.3224\n",
            "Epoch [4/20], Step [420/1524], Loss: 0.3297\n",
            "Epoch [4/20], Step [440/1524], Loss: 0.3601\n",
            "Epoch [4/20], Step [460/1524], Loss: 0.3744\n",
            "Epoch [4/20], Step [480/1524], Loss: 0.3617\n",
            "Epoch [4/20], Step [500/1524], Loss: 0.3518\n",
            "Epoch [4/20], Step [520/1524], Loss: 0.3365\n",
            "Epoch [4/20], Step [540/1524], Loss: 0.3270\n",
            "Epoch [4/20], Step [560/1524], Loss: 0.3262\n",
            "Epoch [4/20], Step [580/1524], Loss: 0.3113\n",
            "Epoch [4/20], Step [600/1524], Loss: 0.2851\n",
            "Epoch [4/20], Step [620/1524], Loss: 0.2774\n",
            "Epoch [4/20], Step [640/1524], Loss: 0.3196\n",
            "Epoch [4/20], Step [660/1524], Loss: 0.3173\n",
            "Epoch [4/20], Step [680/1524], Loss: 0.3264\n",
            "Epoch [4/20], Step [700/1524], Loss: 0.3576\n",
            "Epoch [4/20], Step [720/1524], Loss: 0.2944\n",
            "Epoch [4/20], Step [740/1524], Loss: 0.3597\n",
            "Epoch [4/20], Step [760/1524], Loss: 0.3757\n",
            "Epoch [4/20], Step [780/1524], Loss: 0.3399\n",
            "Epoch [4/20], Step [800/1524], Loss: 0.3425\n",
            "Epoch [4/20], Step [820/1524], Loss: 0.3274\n",
            "Epoch [4/20], Step [840/1524], Loss: 0.3397\n",
            "Epoch [4/20], Step [860/1524], Loss: 0.3017\n",
            "Epoch [4/20], Step [880/1524], Loss: 0.3079\n",
            "Epoch [4/20], Step [900/1524], Loss: 0.3145\n",
            "Epoch [4/20], Step [920/1524], Loss: 0.3379\n",
            "Epoch [4/20], Step [940/1524], Loss: 0.3604\n",
            "Epoch [4/20], Step [960/1524], Loss: 0.3349\n",
            "Epoch [4/20], Step [980/1524], Loss: 0.3719\n",
            "Epoch [4/20], Step [1000/1524], Loss: 0.3486\n",
            "Epoch [4/20], Step [1020/1524], Loss: 0.3673\n",
            "Epoch [4/20], Step [1040/1524], Loss: 0.3369\n",
            "Epoch [4/20], Step [1060/1524], Loss: 0.3511\n",
            "Epoch [4/20], Step [1080/1524], Loss: 0.3142\n",
            "Epoch [4/20], Step [1100/1524], Loss: 0.3043\n",
            "Epoch [4/20], Step [1120/1524], Loss: 0.3426\n",
            "Epoch [4/20], Step [1140/1524], Loss: 0.3033\n",
            "Epoch [4/20], Step [1160/1524], Loss: 0.3347\n",
            "Epoch [4/20], Step [1180/1524], Loss: 0.3448\n",
            "Epoch [4/20], Step [1200/1524], Loss: 0.3527\n",
            "Epoch [4/20], Step [1220/1524], Loss: 0.3295\n",
            "Epoch [4/20], Step [1240/1524], Loss: 0.3472\n",
            "Epoch [4/20], Step [1260/1524], Loss: 0.3683\n",
            "Epoch [4/20], Step [1280/1524], Loss: 0.3451\n",
            "Epoch [4/20], Step [1300/1524], Loss: 0.3543\n",
            "Epoch [4/20], Step [1320/1524], Loss: 0.3445\n",
            "Epoch [4/20], Step [1340/1524], Loss: 0.3079\n",
            "Epoch [4/20], Step [1360/1524], Loss: 0.2786\n",
            "Epoch [4/20], Step [1380/1524], Loss: 0.3165\n",
            "Epoch [4/20], Step [1400/1524], Loss: 0.3481\n",
            "Epoch [4/20], Step [1420/1524], Loss: 0.2926\n",
            "Epoch [4/20], Step [1440/1524], Loss: 0.3319\n",
            "Epoch [4/20], Step [1460/1524], Loss: 0.3292\n",
            "Epoch [4/20], Step [1480/1524], Loss: 0.3113\n",
            "Epoch [4/20], Step [1500/1524], Loss: 0.3057\n",
            "Epoch [4/20], Step [1520/1524], Loss: 0.3015\n",
            "Epoch [4/20] | Train Loss: 0.3347 | Train Acc: 88.42% | Test Loss: 0.8541 | Test Acc: 72.90%\n",
            "Epoch [5/20], Step [20/1524], Loss: 0.3479\n",
            "Epoch [5/20], Step [40/1524], Loss: 0.3026\n",
            "Epoch [5/20], Step [60/1524], Loss: 0.2806\n",
            "Epoch [5/20], Step [80/1524], Loss: 0.3112\n",
            "Epoch [5/20], Step [100/1524], Loss: 0.3324\n",
            "Epoch [5/20], Step [120/1524], Loss: 0.3287\n",
            "Epoch [5/20], Step [140/1524], Loss: 0.3220\n",
            "Epoch [5/20], Step [160/1524], Loss: 0.3062\n",
            "Epoch [5/20], Step [180/1524], Loss: 0.2933\n",
            "Epoch [5/20], Step [200/1524], Loss: 0.2935\n",
            "Epoch [5/20], Step [220/1524], Loss: 0.3280\n",
            "Epoch [5/20], Step [240/1524], Loss: 0.2894\n",
            "Epoch [5/20], Step [260/1524], Loss: 0.2867\n",
            "Epoch [5/20], Step [280/1524], Loss: 0.3013\n",
            "Epoch [5/20], Step [300/1524], Loss: 0.3340\n",
            "Epoch [5/20], Step [320/1524], Loss: 0.3173\n",
            "Epoch [5/20], Step [340/1524], Loss: 0.3420\n",
            "Epoch [5/20], Step [360/1524], Loss: 0.3596\n",
            "Epoch [5/20], Step [380/1524], Loss: 0.3390\n",
            "Epoch [5/20], Step [400/1524], Loss: 0.2903\n",
            "Epoch [5/20], Step [420/1524], Loss: 0.3140\n",
            "Epoch [5/20], Step [440/1524], Loss: 0.3230\n",
            "Epoch [5/20], Step [460/1524], Loss: 0.3129\n",
            "Epoch [5/20], Step [480/1524], Loss: 0.3178\n",
            "Epoch [5/20], Step [500/1524], Loss: 0.3136\n",
            "Epoch [5/20], Step [520/1524], Loss: 0.2880\n",
            "Epoch [5/20], Step [540/1524], Loss: 0.3089\n",
            "Epoch [5/20], Step [560/1524], Loss: 0.3131\n",
            "Epoch [5/20], Step [580/1524], Loss: 0.3112\n",
            "Epoch [5/20], Step [600/1524], Loss: 0.2989\n",
            "Epoch [5/20], Step [620/1524], Loss: 0.3197\n",
            "Epoch [5/20], Step [640/1524], Loss: 0.3488\n",
            "Epoch [5/20], Step [660/1524], Loss: 0.3271\n",
            "Epoch [5/20], Step [680/1524], Loss: 0.3261\n",
            "Epoch [5/20], Step [700/1524], Loss: 0.3186\n",
            "Epoch [5/20], Step [720/1524], Loss: 0.3366\n",
            "Epoch [5/20], Step [740/1524], Loss: 0.3367\n",
            "Epoch [5/20], Step [760/1524], Loss: 0.3064\n",
            "Epoch [5/20], Step [780/1524], Loss: 0.2979\n",
            "Epoch [5/20], Step [800/1524], Loss: 0.3337\n",
            "Epoch [5/20], Step [820/1524], Loss: 0.3646\n",
            "Epoch [5/20], Step [840/1524], Loss: 0.3518\n",
            "Epoch [5/20], Step [860/1524], Loss: 0.3644\n",
            "Epoch [5/20], Step [880/1524], Loss: 0.3460\n",
            "Epoch [5/20], Step [900/1524], Loss: 0.3186\n",
            "Epoch [5/20], Step [920/1524], Loss: 0.3187\n",
            "Epoch [5/20], Step [940/1524], Loss: 0.2896\n",
            "Epoch [5/20], Step [960/1524], Loss: 0.3566\n",
            "Epoch [5/20], Step [980/1524], Loss: 0.3061\n",
            "Epoch [5/20], Step [1000/1524], Loss: 0.3466\n",
            "Epoch [5/20], Step [1020/1524], Loss: 0.2799\n",
            "Epoch [5/20], Step [1040/1524], Loss: 0.2855\n",
            "Epoch [5/20], Step [1060/1524], Loss: 0.3201\n",
            "Epoch [5/20], Step [1080/1524], Loss: 0.3353\n",
            "Epoch [5/20], Step [1100/1524], Loss: 0.3694\n",
            "Epoch [5/20], Step [1120/1524], Loss: 0.3363\n",
            "Epoch [5/20], Step [1140/1524], Loss: 0.3417\n",
            "Epoch [5/20], Step [1160/1524], Loss: 0.3258\n",
            "Epoch [5/20], Step [1180/1524], Loss: 0.3149\n",
            "Epoch [5/20], Step [1200/1524], Loss: 0.2667\n",
            "Epoch [5/20], Step [1220/1524], Loss: 0.3467\n",
            "Epoch [5/20], Step [1240/1524], Loss: 0.3467\n",
            "Epoch [5/20], Step [1260/1524], Loss: 0.2699\n",
            "Epoch [5/20], Step [1280/1524], Loss: 0.3239\n",
            "Epoch [5/20], Step [1300/1524], Loss: 0.3202\n",
            "Epoch [5/20], Step [1320/1524], Loss: 0.3348\n",
            "Epoch [5/20], Step [1340/1524], Loss: 0.3365\n",
            "Epoch [5/20], Step [1360/1524], Loss: 0.2637\n",
            "Epoch [5/20], Step [1380/1524], Loss: 0.3268\n",
            "Epoch [5/20], Step [1400/1524], Loss: 0.3192\n",
            "Epoch [5/20], Step [1420/1524], Loss: 0.2496\n",
            "Epoch [5/20], Step [1440/1524], Loss: 0.3029\n",
            "Epoch [5/20], Step [1460/1524], Loss: 0.3301\n",
            "Epoch [5/20], Step [1480/1524], Loss: 0.2926\n",
            "Epoch [5/20], Step [1500/1524], Loss: 0.3121\n",
            "Epoch [5/20], Step [1520/1524], Loss: 0.2943\n",
            "Epoch [5/20] | Train Loss: 0.3183 | Train Acc: 89.00% | Test Loss: 0.5498 | Test Acc: 77.90%\n",
            "Epoch [6/20], Step [20/1524], Loss: 0.3873\n",
            "Epoch [6/20], Step [40/1524], Loss: 0.3233\n",
            "Epoch [6/20], Step [60/1524], Loss: 0.3569\n",
            "Epoch [6/20], Step [80/1524], Loss: 0.3257\n",
            "Epoch [6/20], Step [100/1524], Loss: 0.3182\n",
            "Epoch [6/20], Step [120/1524], Loss: 0.3453\n",
            "Epoch [6/20], Step [140/1524], Loss: 0.2819\n",
            "Epoch [6/20], Step [160/1524], Loss: 0.3117\n",
            "Epoch [6/20], Step [180/1524], Loss: 0.2738\n",
            "Epoch [6/20], Step [200/1524], Loss: 0.3032\n",
            "Epoch [6/20], Step [220/1524], Loss: 0.2897\n",
            "Epoch [6/20], Step [240/1524], Loss: 0.3690\n",
            "Epoch [6/20], Step [260/1524], Loss: 0.3043\n",
            "Epoch [6/20], Step [280/1524], Loss: 0.2894\n",
            "Epoch [6/20], Step [300/1524], Loss: 0.3163\n",
            "Epoch [6/20], Step [320/1524], Loss: 0.3468\n",
            "Epoch [6/20], Step [340/1524], Loss: 0.3132\n",
            "Epoch [6/20], Step [360/1524], Loss: 0.2971\n",
            "Epoch [6/20], Step [380/1524], Loss: 0.3135\n",
            "Epoch [6/20], Step [400/1524], Loss: 0.3386\n",
            "Epoch [6/20], Step [420/1524], Loss: 0.2978\n",
            "Epoch [6/20], Step [440/1524], Loss: 0.2906\n",
            "Epoch [6/20], Step [460/1524], Loss: 0.3139\n",
            "Epoch [6/20], Step [480/1524], Loss: 0.2802\n",
            "Epoch [6/20], Step [500/1524], Loss: 0.2852\n",
            "Epoch [6/20], Step [520/1524], Loss: 0.2866\n",
            "Epoch [6/20], Step [540/1524], Loss: 0.3236\n",
            "Epoch [6/20], Step [560/1524], Loss: 0.2883\n",
            "Epoch [6/20], Step [580/1524], Loss: 0.2811\n",
            "Epoch [6/20], Step [600/1524], Loss: 0.2879\n",
            "Epoch [6/20], Step [620/1524], Loss: 0.3213\n",
            "Epoch [6/20], Step [640/1524], Loss: 0.3174\n",
            "Epoch [6/20], Step [660/1524], Loss: 0.3410\n",
            "Epoch [6/20], Step [680/1524], Loss: 0.3465\n",
            "Epoch [6/20], Step [700/1524], Loss: 0.2931\n",
            "Epoch [6/20], Step [720/1524], Loss: 0.2706\n",
            "Epoch [6/20], Step [740/1524], Loss: 0.2927\n",
            "Epoch [6/20], Step [760/1524], Loss: 0.3447\n",
            "Epoch [6/20], Step [780/1524], Loss: 0.2881\n",
            "Epoch [6/20], Step [800/1524], Loss: 0.2473\n",
            "Epoch [6/20], Step [820/1524], Loss: 0.3152\n",
            "Epoch [6/20], Step [840/1524], Loss: 0.2825\n",
            "Epoch [6/20], Step [860/1524], Loss: 0.3172\n",
            "Epoch [6/20], Step [880/1524], Loss: 0.3256\n",
            "Epoch [6/20], Step [900/1524], Loss: 0.3105\n",
            "Epoch [6/20], Step [920/1524], Loss: 0.3105\n",
            "Epoch [6/20], Step [940/1524], Loss: 0.3163\n",
            "Epoch [6/20], Step [960/1524], Loss: 0.3060\n",
            "Epoch [6/20], Step [980/1524], Loss: 0.3319\n",
            "Epoch [6/20], Step [1000/1524], Loss: 0.2822\n",
            "Epoch [6/20], Step [1020/1524], Loss: 0.2905\n",
            "Epoch [6/20], Step [1040/1524], Loss: 0.3197\n",
            "Epoch [6/20], Step [1060/1524], Loss: 0.3137\n",
            "Epoch [6/20], Step [1080/1524], Loss: 0.2816\n",
            "Epoch [6/20], Step [1100/1524], Loss: 0.3089\n",
            "Epoch [6/20], Step [1120/1524], Loss: 0.3127\n",
            "Epoch [6/20], Step [1140/1524], Loss: 0.2953\n",
            "Epoch [6/20], Step [1160/1524], Loss: 0.3008\n",
            "Epoch [6/20], Step [1180/1524], Loss: 0.3166\n",
            "Epoch [6/20], Step [1200/1524], Loss: 0.2991\n",
            "Epoch [6/20], Step [1220/1524], Loss: 0.2982\n",
            "Epoch [6/20], Step [1240/1524], Loss: 0.3357\n",
            "Epoch [6/20], Step [1260/1524], Loss: 0.2887\n",
            "Epoch [6/20], Step [1280/1524], Loss: 0.2758\n",
            "Epoch [6/20], Step [1300/1524], Loss: 0.2887\n",
            "Epoch [6/20], Step [1320/1524], Loss: 0.2857\n",
            "Epoch [6/20], Step [1340/1524], Loss: 0.3215\n",
            "Epoch [6/20], Step [1360/1524], Loss: 0.3039\n",
            "Epoch [6/20], Step [1380/1524], Loss: 0.3238\n",
            "Epoch [6/20], Step [1400/1524], Loss: 0.3023\n",
            "Epoch [6/20], Step [1420/1524], Loss: 0.2982\n",
            "Epoch [6/20], Step [1440/1524], Loss: 0.3105\n",
            "Epoch [6/20], Step [1460/1524], Loss: 0.3253\n",
            "Epoch [6/20], Step [1480/1524], Loss: 0.3010\n",
            "Epoch [6/20], Step [1500/1524], Loss: 0.2953\n",
            "Epoch [6/20], Step [1520/1524], Loss: 0.3137\n",
            "Epoch [6/20] | Train Loss: 0.3088 | Train Acc: 89.27% | Test Loss: 0.7924 | Test Acc: 69.70%\n",
            "Epoch [7/20], Step [20/1524], Loss: 0.3615\n",
            "Epoch [7/20], Step [40/1524], Loss: 0.3208\n",
            "Epoch [7/20], Step [60/1524], Loss: 0.2914\n",
            "Epoch [7/20], Step [80/1524], Loss: 0.2901\n",
            "Epoch [7/20], Step [100/1524], Loss: 0.2844\n",
            "Epoch [7/20], Step [120/1524], Loss: 0.3190\n",
            "Epoch [7/20], Step [140/1524], Loss: 0.3154\n",
            "Epoch [7/20], Step [160/1524], Loss: 0.2749\n",
            "Epoch [7/20], Step [180/1524], Loss: 0.3194\n",
            "Epoch [7/20], Step [200/1524], Loss: 0.3219\n",
            "Epoch [7/20], Step [220/1524], Loss: 0.2832\n",
            "Epoch [7/20], Step [240/1524], Loss: 0.2655\n",
            "Epoch [7/20], Step [260/1524], Loss: 0.3179\n",
            "Epoch [7/20], Step [280/1524], Loss: 0.3217\n",
            "Epoch [7/20], Step [300/1524], Loss: 0.2714\n",
            "Epoch [7/20], Step [320/1524], Loss: 0.2569\n",
            "Epoch [7/20], Step [340/1524], Loss: 0.2892\n",
            "Epoch [7/20], Step [360/1524], Loss: 0.3503\n",
            "Epoch [7/20], Step [380/1524], Loss: 0.2649\n",
            "Epoch [7/20], Step [400/1524], Loss: 0.3093\n",
            "Epoch [7/20], Step [420/1524], Loss: 0.2738\n",
            "Epoch [7/20], Step [440/1524], Loss: 0.2801\n",
            "Epoch [7/20], Step [460/1524], Loss: 0.2708\n",
            "Epoch [7/20], Step [480/1524], Loss: 0.3045\n",
            "Epoch [7/20], Step [500/1524], Loss: 0.2875\n",
            "Epoch [7/20], Step [520/1524], Loss: 0.3268\n",
            "Epoch [7/20], Step [540/1524], Loss: 0.3278\n",
            "Epoch [7/20], Step [560/1524], Loss: 0.2572\n",
            "Epoch [7/20], Step [580/1524], Loss: 0.3043\n",
            "Epoch [7/20], Step [600/1524], Loss: 0.2717\n",
            "Epoch [7/20], Step [620/1524], Loss: 0.2969\n",
            "Epoch [7/20], Step [640/1524], Loss: 0.3156\n",
            "Epoch [7/20], Step [660/1524], Loss: 0.2791\n",
            "Epoch [7/20], Step [680/1524], Loss: 0.2974\n",
            "Epoch [7/20], Step [700/1524], Loss: 0.2759\n",
            "Epoch [7/20], Step [720/1524], Loss: 0.2954\n",
            "Epoch [7/20], Step [740/1524], Loss: 0.2688\n",
            "Epoch [7/20], Step [760/1524], Loss: 0.2546\n",
            "Epoch [7/20], Step [780/1524], Loss: 0.2938\n",
            "Epoch [7/20], Step [800/1524], Loss: 0.2645\n",
            "Epoch [7/20], Step [820/1524], Loss: 0.3130\n",
            "Epoch [7/20], Step [840/1524], Loss: 0.2891\n",
            "Epoch [7/20], Step [860/1524], Loss: 0.3211\n",
            "Epoch [7/20], Step [880/1524], Loss: 0.2963\n",
            "Epoch [7/20], Step [900/1524], Loss: 0.2884\n",
            "Epoch [7/20], Step [920/1524], Loss: 0.3208\n",
            "Epoch [7/20], Step [940/1524], Loss: 0.3168\n",
            "Epoch [7/20], Step [960/1524], Loss: 0.2935\n",
            "Epoch [7/20], Step [980/1524], Loss: 0.2959\n",
            "Epoch [7/20], Step [1000/1524], Loss: 0.3173\n",
            "Epoch [7/20], Step [1020/1524], Loss: 0.2948\n",
            "Epoch [7/20], Step [1040/1524], Loss: 0.2668\n",
            "Epoch [7/20], Step [1060/1524], Loss: 0.2973\n",
            "Epoch [7/20], Step [1080/1524], Loss: 0.3026\n",
            "Epoch [7/20], Step [1100/1524], Loss: 0.3217\n",
            "Epoch [7/20], Step [1120/1524], Loss: 0.3040\n",
            "Epoch [7/20], Step [1140/1524], Loss: 0.3023\n",
            "Epoch [7/20], Step [1160/1524], Loss: 0.3175\n",
            "Epoch [7/20], Step [1180/1524], Loss: 0.3120\n",
            "Epoch [7/20], Step [1200/1524], Loss: 0.3144\n",
            "Epoch [7/20], Step [1220/1524], Loss: 0.2824\n",
            "Epoch [7/20], Step [1240/1524], Loss: 0.2762\n",
            "Epoch [7/20], Step [1260/1524], Loss: 0.2698\n",
            "Epoch [7/20], Step [1280/1524], Loss: 0.2579\n",
            "Epoch [7/20], Step [1300/1524], Loss: 0.3020\n",
            "Epoch [7/20], Step [1320/1524], Loss: 0.2707\n",
            "Epoch [7/20], Step [1340/1524], Loss: 0.3056\n",
            "Epoch [7/20], Step [1360/1524], Loss: 0.3216\n",
            "Epoch [7/20], Step [1380/1524], Loss: 0.2905\n",
            "Epoch [7/20], Step [1400/1524], Loss: 0.2845\n",
            "Epoch [7/20], Step [1420/1524], Loss: 0.3243\n",
            "Epoch [7/20], Step [1440/1524], Loss: 0.3258\n",
            "Epoch [7/20], Step [1460/1524], Loss: 0.2786\n",
            "Epoch [7/20], Step [1480/1524], Loss: 0.2933\n",
            "Epoch [7/20], Step [1500/1524], Loss: 0.3240\n",
            "Epoch [7/20], Step [1520/1524], Loss: 0.3088\n",
            "Epoch [7/20] | Train Loss: 0.2974 | Train Acc: 89.76% | Test Loss: 0.7464 | Test Acc: 71.30%\n",
            "Epoch [8/20], Step [20/1524], Loss: 0.2939\n",
            "Epoch [8/20], Step [40/1524], Loss: 0.3348\n",
            "Epoch [8/20], Step [60/1524], Loss: 0.2916\n",
            "Epoch [8/20], Step [80/1524], Loss: 0.2673\n",
            "Epoch [8/20], Step [100/1524], Loss: 0.2612\n",
            "Epoch [8/20], Step [120/1524], Loss: 0.2787\n",
            "Epoch [8/20], Step [140/1524], Loss: 0.2571\n",
            "Epoch [8/20], Step [160/1524], Loss: 0.2796\n",
            "Epoch [8/20], Step [180/1524], Loss: 0.3126\n",
            "Epoch [8/20], Step [200/1524], Loss: 0.3036\n",
            "Epoch [8/20], Step [220/1524], Loss: 0.2989\n",
            "Epoch [8/20], Step [240/1524], Loss: 0.3035\n",
            "Epoch [8/20], Step [260/1524], Loss: 0.3080\n",
            "Epoch [8/20], Step [280/1524], Loss: 0.2799\n",
            "Epoch [8/20], Step [300/1524], Loss: 0.2739\n",
            "Epoch [8/20], Step [320/1524], Loss: 0.2973\n",
            "Epoch [8/20], Step [340/1524], Loss: 0.3109\n",
            "Epoch [8/20], Step [360/1524], Loss: 0.2862\n",
            "Epoch [8/20], Step [380/1524], Loss: 0.2716\n",
            "Epoch [8/20], Step [400/1524], Loss: 0.3129\n",
            "Epoch [8/20], Step [420/1524], Loss: 0.3010\n",
            "Epoch [8/20], Step [440/1524], Loss: 0.2862\n",
            "Epoch [8/20], Step [460/1524], Loss: 0.3048\n",
            "Epoch [8/20], Step [480/1524], Loss: 0.3074\n",
            "Epoch [8/20], Step [500/1524], Loss: 0.2794\n",
            "Epoch [8/20], Step [520/1524], Loss: 0.3646\n",
            "Epoch [8/20], Step [540/1524], Loss: 0.3107\n",
            "Epoch [8/20], Step [560/1524], Loss: 0.2777\n",
            "Epoch [8/20], Step [580/1524], Loss: 0.3286\n",
            "Epoch [8/20], Step [600/1524], Loss: 0.2891\n",
            "Epoch [8/20], Step [620/1524], Loss: 0.2985\n",
            "Epoch [8/20], Step [640/1524], Loss: 0.3135\n",
            "Epoch [8/20], Step [660/1524], Loss: 0.2940\n",
            "Epoch [8/20], Step [680/1524], Loss: 0.3211\n",
            "Epoch [8/20], Step [700/1524], Loss: 0.2738\n",
            "Epoch [8/20], Step [720/1524], Loss: 0.3054\n",
            "Epoch [8/20], Step [740/1524], Loss: 0.2907\n",
            "Epoch [8/20], Step [760/1524], Loss: 0.2569\n",
            "Epoch [8/20], Step [780/1524], Loss: 0.2979\n",
            "Epoch [8/20], Step [800/1524], Loss: 0.2938\n",
            "Epoch [8/20], Step [820/1524], Loss: 0.2748\n",
            "Epoch [8/20], Step [840/1524], Loss: 0.2641\n",
            "Epoch [8/20], Step [860/1524], Loss: 0.3292\n",
            "Epoch [8/20], Step [880/1524], Loss: 0.2641\n",
            "Epoch [8/20], Step [900/1524], Loss: 0.2886\n",
            "Epoch [8/20], Step [920/1524], Loss: 0.2782\n",
            "Epoch [8/20], Step [940/1524], Loss: 0.3065\n",
            "Epoch [8/20], Step [960/1524], Loss: 0.2926\n",
            "Epoch [8/20], Step [980/1524], Loss: 0.2729\n",
            "Epoch [8/20], Step [1000/1524], Loss: 0.2746\n",
            "Epoch [8/20], Step [1020/1524], Loss: 0.2690\n",
            "Epoch [8/20], Step [1040/1524], Loss: 0.2900\n",
            "Epoch [8/20], Step [1060/1524], Loss: 0.2903\n",
            "Epoch [8/20], Step [1080/1524], Loss: 0.2624\n",
            "Epoch [8/20], Step [1100/1524], Loss: 0.2862\n",
            "Epoch [8/20], Step [1120/1524], Loss: 0.2622\n",
            "Epoch [8/20], Step [1140/1524], Loss: 0.2988\n",
            "Epoch [8/20], Step [1160/1524], Loss: 0.2895\n",
            "Epoch [8/20], Step [1180/1524], Loss: 0.2562\n",
            "Epoch [8/20], Step [1200/1524], Loss: 0.2797\n",
            "Epoch [8/20], Step [1220/1524], Loss: 0.3214\n",
            "Epoch [8/20], Step [1240/1524], Loss: 0.2943\n",
            "Epoch [8/20], Step [1260/1524], Loss: 0.2531\n",
            "Epoch [8/20], Step [1280/1524], Loss: 0.2738\n",
            "Epoch [8/20], Step [1300/1524], Loss: 0.2812\n",
            "Epoch [8/20], Step [1320/1524], Loss: 0.3063\n",
            "Epoch [8/20], Step [1340/1524], Loss: 0.3228\n",
            "Epoch [8/20], Step [1360/1524], Loss: 0.2817\n",
            "Epoch [8/20], Step [1380/1524], Loss: 0.2706\n",
            "Epoch [8/20], Step [1400/1524], Loss: 0.2650\n",
            "Epoch [8/20], Step [1420/1524], Loss: 0.3146\n",
            "Epoch [8/20], Step [1440/1524], Loss: 0.2568\n",
            "Epoch [8/20], Step [1460/1524], Loss: 0.2860\n",
            "Epoch [8/20], Step [1480/1524], Loss: 0.2974\n",
            "Epoch [8/20], Step [1500/1524], Loss: 0.2843\n",
            "Epoch [8/20], Step [1520/1524], Loss: 0.2737\n",
            "Epoch [8/20] | Train Loss: 0.2897 | Train Acc: 90.00% | Test Loss: 0.5969 | Test Acc: 78.20%\n",
            "Epoch [9/20], Step [20/1524], Loss: 0.2917\n",
            "Epoch [9/20], Step [40/1524], Loss: 0.2736\n",
            "Epoch [9/20], Step [60/1524], Loss: 0.2778\n",
            "Epoch [9/20], Step [80/1524], Loss: 0.3008\n",
            "Epoch [9/20], Step [100/1524], Loss: 0.2785\n",
            "Epoch [9/20], Step [120/1524], Loss: 0.3138\n",
            "Epoch [9/20], Step [140/1524], Loss: 0.3244\n",
            "Epoch [9/20], Step [160/1524], Loss: 0.2904\n",
            "Epoch [9/20], Step [180/1524], Loss: 0.2638\n",
            "Epoch [9/20], Step [200/1524], Loss: 0.2826\n",
            "Epoch [9/20], Step [220/1524], Loss: 0.2910\n",
            "Epoch [9/20], Step [240/1524], Loss: 0.2687\n",
            "Epoch [9/20], Step [260/1524], Loss: 0.3064\n",
            "Epoch [9/20], Step [280/1524], Loss: 0.2721\n",
            "Epoch [9/20], Step [300/1524], Loss: 0.2807\n",
            "Epoch [9/20], Step [320/1524], Loss: 0.2682\n",
            "Epoch [9/20], Step [340/1524], Loss: 0.2734\n",
            "Epoch [9/20], Step [360/1524], Loss: 0.2656\n",
            "Epoch [9/20], Step [380/1524], Loss: 0.2605\n",
            "Epoch [9/20], Step [400/1524], Loss: 0.2945\n",
            "Epoch [9/20], Step [420/1524], Loss: 0.2689\n",
            "Epoch [9/20], Step [440/1524], Loss: 0.2555\n",
            "Epoch [9/20], Step [460/1524], Loss: 0.2624\n",
            "Epoch [9/20], Step [480/1524], Loss: 0.2735\n",
            "Epoch [9/20], Step [500/1524], Loss: 0.2507\n",
            "Epoch [9/20], Step [520/1524], Loss: 0.3008\n",
            "Epoch [9/20], Step [540/1524], Loss: 0.2792\n",
            "Epoch [9/20], Step [560/1524], Loss: 0.3216\n",
            "Epoch [9/20], Step [580/1524], Loss: 0.2890\n",
            "Epoch [9/20], Step [600/1524], Loss: 0.2699\n",
            "Epoch [9/20], Step [620/1524], Loss: 0.2522\n",
            "Epoch [9/20], Step [640/1524], Loss: 0.3049\n",
            "Epoch [9/20], Step [660/1524], Loss: 0.2903\n",
            "Epoch [9/20], Step [680/1524], Loss: 0.2567\n",
            "Epoch [9/20], Step [700/1524], Loss: 0.2727\n",
            "Epoch [9/20], Step [720/1524], Loss: 0.2707\n",
            "Epoch [9/20], Step [740/1524], Loss: 0.3380\n",
            "Epoch [9/20], Step [760/1524], Loss: 0.2957\n",
            "Epoch [9/20], Step [780/1524], Loss: 0.3090\n",
            "Epoch [9/20], Step [800/1524], Loss: 0.2597\n",
            "Epoch [9/20], Step [820/1524], Loss: 0.2731\n",
            "Epoch [9/20], Step [840/1524], Loss: 0.3106\n",
            "Epoch [9/20], Step [860/1524], Loss: 0.2649\n",
            "Epoch [9/20], Step [880/1524], Loss: 0.3023\n",
            "Epoch [9/20], Step [900/1524], Loss: 0.3210\n",
            "Epoch [9/20], Step [920/1524], Loss: 0.2856\n",
            "Epoch [9/20], Step [940/1524], Loss: 0.2787\n",
            "Epoch [9/20], Step [960/1524], Loss: 0.2441\n",
            "Epoch [9/20], Step [980/1524], Loss: 0.2827\n",
            "Epoch [9/20], Step [1000/1524], Loss: 0.2945\n",
            "Epoch [9/20], Step [1020/1524], Loss: 0.2879\n",
            "Epoch [9/20], Step [1040/1524], Loss: 0.2706\n",
            "Epoch [9/20], Step [1060/1524], Loss: 0.2968\n",
            "Epoch [9/20], Step [1080/1524], Loss: 0.2556\n",
            "Epoch [9/20], Step [1100/1524], Loss: 0.2578\n",
            "Epoch [9/20], Step [1120/1524], Loss: 0.2943\n",
            "Epoch [9/20], Step [1140/1524], Loss: 0.2887\n",
            "Epoch [9/20], Step [1160/1524], Loss: 0.2733\n",
            "Epoch [9/20], Step [1180/1524], Loss: 0.2846\n",
            "Epoch [9/20], Step [1200/1524], Loss: 0.3199\n",
            "Epoch [9/20], Step [1220/1524], Loss: 0.2759\n",
            "Epoch [9/20], Step [1240/1524], Loss: 0.2932\n",
            "Epoch [9/20], Step [1260/1524], Loss: 0.3134\n",
            "Epoch [9/20], Step [1280/1524], Loss: 0.2315\n",
            "Epoch [9/20], Step [1300/1524], Loss: 0.3070\n",
            "Epoch [9/20], Step [1320/1524], Loss: 0.2964\n",
            "Epoch [9/20], Step [1340/1524], Loss: 0.2678\n",
            "Epoch [9/20], Step [1360/1524], Loss: 0.2664\n",
            "Epoch [9/20], Step [1380/1524], Loss: 0.2555\n",
            "Epoch [9/20], Step [1400/1524], Loss: 0.2796\n",
            "Epoch [9/20], Step [1420/1524], Loss: 0.2477\n",
            "Epoch [9/20], Step [1440/1524], Loss: 0.2539\n",
            "Epoch [9/20], Step [1460/1524], Loss: 0.2548\n",
            "Epoch [9/20], Step [1480/1524], Loss: 0.2678\n",
            "Epoch [9/20], Step [1500/1524], Loss: 0.2549\n",
            "Epoch [9/20], Step [1520/1524], Loss: 0.3259\n",
            "Epoch [9/20] | Train Loss: 0.2818 | Train Acc: 90.26% | Test Loss: 1.0943 | Test Acc: 70.40%\n",
            "Epoch [10/20], Step [20/1524], Loss: 0.3201\n",
            "Epoch [10/20], Step [40/1524], Loss: 0.2926\n",
            "Epoch [10/20], Step [60/1524], Loss: 0.2966\n",
            "Epoch [10/20], Step [80/1524], Loss: 0.3246\n",
            "Epoch [10/20], Step [100/1524], Loss: 0.2604\n",
            "Epoch [10/20], Step [120/1524], Loss: 0.3032\n",
            "Epoch [10/20], Step [140/1524], Loss: 0.2649\n",
            "Epoch [10/20], Step [160/1524], Loss: 0.3090\n",
            "Epoch [10/20], Step [180/1524], Loss: 0.2621\n",
            "Epoch [10/20], Step [200/1524], Loss: 0.2454\n",
            "Epoch [10/20], Step [220/1524], Loss: 0.2891\n",
            "Epoch [10/20], Step [240/1524], Loss: 0.2698\n",
            "Epoch [10/20], Step [260/1524], Loss: 0.2422\n",
            "Epoch [10/20], Step [280/1524], Loss: 0.2952\n",
            "Epoch [10/20], Step [300/1524], Loss: 0.2335\n",
            "Epoch [10/20], Step [320/1524], Loss: 0.2747\n",
            "Epoch [10/20], Step [340/1524], Loss: 0.2613\n",
            "Epoch [10/20], Step [360/1524], Loss: 0.2914\n",
            "Epoch [10/20], Step [380/1524], Loss: 0.2732\n",
            "Epoch [10/20], Step [400/1524], Loss: 0.2656\n",
            "Epoch [10/20], Step [420/1524], Loss: 0.2888\n",
            "Epoch [10/20], Step [440/1524], Loss: 0.3066\n",
            "Epoch [10/20], Step [460/1524], Loss: 0.2630\n",
            "Epoch [10/20], Step [480/1524], Loss: 0.2411\n",
            "Epoch [10/20], Step [500/1524], Loss: 0.2459\n",
            "Epoch [10/20], Step [520/1524], Loss: 0.2487\n",
            "Epoch [10/20], Step [540/1524], Loss: 0.2740\n",
            "Epoch [10/20], Step [560/1524], Loss: 0.2642\n",
            "Epoch [10/20], Step [580/1524], Loss: 0.2665\n",
            "Epoch [10/20], Step [600/1524], Loss: 0.2315\n",
            "Epoch [10/20], Step [620/1524], Loss: 0.2967\n",
            "Epoch [10/20], Step [640/1524], Loss: 0.2604\n",
            "Epoch [10/20], Step [660/1524], Loss: 0.2662\n",
            "Epoch [10/20], Step [680/1524], Loss: 0.2781\n",
            "Epoch [10/20], Step [700/1524], Loss: 0.2717\n",
            "Epoch [10/20], Step [720/1524], Loss: 0.2877\n",
            "Epoch [10/20], Step [740/1524], Loss: 0.2810\n",
            "Epoch [10/20], Step [760/1524], Loss: 0.3167\n",
            "Epoch [10/20], Step [780/1524], Loss: 0.2937\n",
            "Epoch [10/20], Step [800/1524], Loss: 0.3026\n",
            "Epoch [10/20], Step [820/1524], Loss: 0.2880\n",
            "Epoch [10/20], Step [840/1524], Loss: 0.3019\n",
            "Epoch [10/20], Step [860/1524], Loss: 0.2868\n",
            "Epoch [10/20], Step [880/1524], Loss: 0.2851\n",
            "Epoch [10/20], Step [900/1524], Loss: 0.2550\n",
            "Epoch [10/20], Step [920/1524], Loss: 0.2675\n",
            "Epoch [10/20], Step [940/1524], Loss: 0.3092\n",
            "Epoch [10/20], Step [960/1524], Loss: 0.2860\n",
            "Epoch [10/20], Step [980/1524], Loss: 0.2720\n",
            "Epoch [10/20], Step [1000/1524], Loss: 0.2736\n",
            "Epoch [10/20], Step [1020/1524], Loss: 0.2661\n",
            "Epoch [10/20], Step [1040/1524], Loss: 0.2552\n",
            "Epoch [10/20], Step [1060/1524], Loss: 0.2344\n",
            "Epoch [10/20], Step [1080/1524], Loss: 0.2735\n",
            "Epoch [10/20], Step [1100/1524], Loss: 0.2803\n",
            "Epoch [10/20], Step [1120/1524], Loss: 0.2727\n",
            "Epoch [10/20], Step [1140/1524], Loss: 0.2753\n",
            "Epoch [10/20], Step [1160/1524], Loss: 0.2938\n",
            "Epoch [10/20], Step [1180/1524], Loss: 0.2666\n",
            "Epoch [10/20], Step [1200/1524], Loss: 0.2716\n",
            "Epoch [10/20], Step [1220/1524], Loss: 0.2747\n",
            "Epoch [10/20], Step [1240/1524], Loss: 0.2943\n",
            "Epoch [10/20], Step [1260/1524], Loss: 0.2783\n",
            "Epoch [10/20], Step [1280/1524], Loss: 0.2484\n",
            "Epoch [10/20], Step [1300/1524], Loss: 0.2753\n",
            "Epoch [10/20], Step [1320/1524], Loss: 0.3244\n",
            "Epoch [10/20], Step [1340/1524], Loss: 0.2462\n",
            "Epoch [10/20], Step [1360/1524], Loss: 0.2796\n",
            "Epoch [10/20], Step [1380/1524], Loss: 0.2393\n",
            "Epoch [10/20], Step [1400/1524], Loss: 0.2258\n",
            "Epoch [10/20], Step [1420/1524], Loss: 0.2926\n",
            "Epoch [10/20], Step [1440/1524], Loss: 0.2841\n",
            "Epoch [10/20], Step [1460/1524], Loss: 0.2913\n",
            "Epoch [10/20], Step [1480/1524], Loss: 0.2988\n",
            "Epoch [10/20], Step [1500/1524], Loss: 0.2885\n",
            "Epoch [10/20], Step [1520/1524], Loss: 0.3041\n",
            "Epoch [10/20] | Train Loss: 0.2776 | Train Acc: 90.47% | Test Loss: 0.6262 | Test Acc: 76.10%\n",
            "Epoch [11/20], Step [20/1524], Loss: 0.3457\n",
            "Epoch [11/20], Step [40/1524], Loss: 0.2550\n",
            "Epoch [11/20], Step [60/1524], Loss: 0.2600\n",
            "Epoch [11/20], Step [80/1524], Loss: 0.2743\n",
            "Epoch [11/20], Step [100/1524], Loss: 0.3014\n",
            "Epoch [11/20], Step [120/1524], Loss: 0.2841\n",
            "Epoch [11/20], Step [140/1524], Loss: 0.3265\n",
            "Epoch [11/20], Step [160/1524], Loss: 0.2629\n",
            "Epoch [11/20], Step [180/1524], Loss: 0.2741\n",
            "Epoch [11/20], Step [200/1524], Loss: 0.2848\n",
            "Epoch [11/20], Step [220/1524], Loss: 0.2621\n",
            "Epoch [11/20], Step [240/1524], Loss: 0.2800\n",
            "Epoch [11/20], Step [260/1524], Loss: 0.2584\n",
            "Epoch [11/20], Step [280/1524], Loss: 0.2829\n",
            "Epoch [11/20], Step [300/1524], Loss: 0.2776\n",
            "Epoch [11/20], Step [320/1524], Loss: 0.2978\n",
            "Epoch [11/20], Step [340/1524], Loss: 0.2466\n",
            "Epoch [11/20], Step [360/1524], Loss: 0.2802\n",
            "Epoch [11/20], Step [380/1524], Loss: 0.3038\n",
            "Epoch [11/20], Step [400/1524], Loss: 0.2758\n",
            "Epoch [11/20], Step [420/1524], Loss: 0.2808\n",
            "Epoch [11/20], Step [440/1524], Loss: 0.2479\n",
            "Epoch [11/20], Step [460/1524], Loss: 0.2465\n",
            "Epoch [11/20], Step [480/1524], Loss: 0.2617\n",
            "Epoch [11/20], Step [500/1524], Loss: 0.2380\n",
            "Epoch [11/20], Step [520/1524], Loss: 0.2668\n",
            "Epoch [11/20], Step [540/1524], Loss: 0.2378\n",
            "Epoch [11/20], Step [560/1524], Loss: 0.2492\n",
            "Epoch [11/20], Step [580/1524], Loss: 0.2624\n",
            "Epoch [11/20], Step [600/1524], Loss: 0.2732\n",
            "Epoch [11/20], Step [620/1524], Loss: 0.2445\n",
            "Epoch [11/20], Step [640/1524], Loss: 0.2770\n",
            "Epoch [11/20], Step [660/1524], Loss: 0.2524\n",
            "Epoch [11/20], Step [680/1524], Loss: 0.2783\n",
            "Epoch [11/20], Step [700/1524], Loss: 0.2595\n",
            "Epoch [11/20], Step [720/1524], Loss: 0.2499\n",
            "Epoch [11/20], Step [740/1524], Loss: 0.2777\n",
            "Epoch [11/20], Step [760/1524], Loss: 0.2660\n",
            "Epoch [11/20], Step [780/1524], Loss: 0.2763\n",
            "Epoch [11/20], Step [800/1524], Loss: 0.2788\n",
            "Epoch [11/20], Step [820/1524], Loss: 0.2434\n",
            "Epoch [11/20], Step [840/1524], Loss: 0.2810\n",
            "Epoch [11/20], Step [860/1524], Loss: 0.2279\n",
            "Epoch [11/20], Step [880/1524], Loss: 0.2920\n",
            "Epoch [11/20], Step [900/1524], Loss: 0.2669\n",
            "Epoch [11/20], Step [920/1524], Loss: 0.2873\n",
            "Epoch [11/20], Step [940/1524], Loss: 0.3256\n",
            "Epoch [11/20], Step [960/1524], Loss: 0.2552\n",
            "Epoch [11/20], Step [980/1524], Loss: 0.2670\n",
            "Epoch [11/20], Step [1000/1524], Loss: 0.2905\n",
            "Epoch [11/20], Step [1020/1524], Loss: 0.2958\n",
            "Epoch [11/20], Step [1040/1524], Loss: 0.2459\n",
            "Epoch [11/20], Step [1060/1524], Loss: 0.2946\n",
            "Epoch [11/20], Step [1080/1524], Loss: 0.2708\n",
            "Epoch [11/20], Step [1100/1524], Loss: 0.2664\n",
            "Epoch [11/20], Step [1120/1524], Loss: 0.2547\n",
            "Epoch [11/20], Step [1140/1524], Loss: 0.3039\n",
            "Epoch [11/20], Step [1160/1524], Loss: 0.2554\n",
            "Epoch [11/20], Step [1180/1524], Loss: 0.2279\n",
            "Epoch [11/20], Step [1200/1524], Loss: 0.2570\n",
            "Epoch [11/20], Step [1220/1524], Loss: 0.2775\n",
            "Epoch [11/20], Step [1240/1524], Loss: 0.2568\n",
            "Epoch [11/20], Step [1260/1524], Loss: 0.2857\n",
            "Epoch [11/20], Step [1280/1524], Loss: 0.2587\n",
            "Epoch [11/20], Step [1300/1524], Loss: 0.2793\n",
            "Epoch [11/20], Step [1320/1524], Loss: 0.2812\n",
            "Epoch [11/20], Step [1340/1524], Loss: 0.2623\n",
            "Epoch [11/20], Step [1360/1524], Loss: 0.2647\n",
            "Epoch [11/20], Step [1380/1524], Loss: 0.2947\n",
            "Epoch [11/20], Step [1400/1524], Loss: 0.2883\n",
            "Epoch [11/20], Step [1420/1524], Loss: 0.2414\n",
            "Epoch [11/20], Step [1440/1524], Loss: 0.2877\n",
            "Epoch [11/20], Step [1460/1524], Loss: 0.2782\n",
            "Epoch [11/20], Step [1480/1524], Loss: 0.2676\n",
            "Epoch [11/20], Step [1500/1524], Loss: 0.3001\n",
            "Epoch [11/20], Step [1520/1524], Loss: 0.2384\n",
            "Epoch [11/20] | Train Loss: 0.2719 | Train Acc: 90.58% | Test Loss: 0.9012 | Test Acc: 70.80%\n",
            "Epoch [12/20], Step [20/1524], Loss: 0.2846\n",
            "Epoch [12/20], Step [40/1524], Loss: 0.2773\n",
            "Epoch [12/20], Step [60/1524], Loss: 0.2870\n",
            "Epoch [12/20], Step [80/1524], Loss: 0.2526\n",
            "Epoch [12/20], Step [100/1524], Loss: 0.2466\n",
            "Epoch [12/20], Step [120/1524], Loss: 0.2666\n",
            "Epoch [12/20], Step [140/1524], Loss: 0.2767\n",
            "Epoch [12/20], Step [160/1524], Loss: 0.2888\n",
            "Epoch [12/20], Step [180/1524], Loss: 0.2719\n",
            "Epoch [12/20], Step [200/1524], Loss: 0.2734\n",
            "Epoch [12/20], Step [220/1524], Loss: 0.2741\n",
            "Epoch [12/20], Step [240/1524], Loss: 0.2336\n",
            "Epoch [12/20], Step [260/1524], Loss: 0.2817\n",
            "Epoch [12/20], Step [280/1524], Loss: 0.3006\n",
            "Epoch [12/20], Step [300/1524], Loss: 0.2548\n",
            "Epoch [12/20], Step [320/1524], Loss: 0.2674\n",
            "Epoch [12/20], Step [340/1524], Loss: 0.2546\n",
            "Epoch [12/20], Step [360/1524], Loss: 0.2950\n",
            "Epoch [12/20], Step [380/1524], Loss: 0.2763\n",
            "Epoch [12/20], Step [400/1524], Loss: 0.2658\n",
            "Epoch [12/20], Step [420/1524], Loss: 0.2654\n",
            "Epoch [12/20], Step [440/1524], Loss: 0.2687\n",
            "Epoch [12/20], Step [460/1524], Loss: 0.2786\n",
            "Epoch [12/20], Step [480/1524], Loss: 0.2701\n",
            "Epoch [12/20], Step [500/1524], Loss: 0.2579\n",
            "Epoch [12/20], Step [520/1524], Loss: 0.2344\n",
            "Epoch [12/20], Step [540/1524], Loss: 0.2665\n",
            "Epoch [12/20], Step [560/1524], Loss: 0.2505\n",
            "Epoch [12/20], Step [580/1524], Loss: 0.2940\n",
            "Epoch [12/20], Step [600/1524], Loss: 0.2601\n",
            "Epoch [12/20], Step [620/1524], Loss: 0.2685\n",
            "Epoch [12/20], Step [640/1524], Loss: 0.2485\n",
            "Epoch [12/20], Step [660/1524], Loss: 0.2650\n",
            "Epoch [12/20], Step [680/1524], Loss: 0.2676\n",
            "Epoch [12/20], Step [700/1524], Loss: 0.2846\n",
            "Epoch [12/20], Step [720/1524], Loss: 0.2531\n",
            "Epoch [12/20], Step [740/1524], Loss: 0.2655\n",
            "Epoch [12/20], Step [760/1524], Loss: 0.2583\n",
            "Epoch [12/20], Step [780/1524], Loss: 0.2540\n",
            "Epoch [12/20], Step [800/1524], Loss: 0.2227\n",
            "Epoch [12/20], Step [820/1524], Loss: 0.2488\n",
            "Epoch [12/20], Step [840/1524], Loss: 0.2771\n",
            "Epoch [12/20], Step [860/1524], Loss: 0.2758\n",
            "Epoch [12/20], Step [880/1524], Loss: 0.2605\n",
            "Epoch [12/20], Step [900/1524], Loss: 0.2683\n",
            "Epoch [12/20], Step [920/1524], Loss: 0.3027\n",
            "Epoch [12/20], Step [940/1524], Loss: 0.2826\n",
            "Epoch [12/20], Step [960/1524], Loss: 0.2508\n",
            "Epoch [12/20], Step [980/1524], Loss: 0.2465\n",
            "Epoch [12/20], Step [1000/1524], Loss: 0.2268\n",
            "Epoch [12/20], Step [1020/1524], Loss: 0.2714\n",
            "Epoch [12/20], Step [1040/1524], Loss: 0.2700\n",
            "Epoch [12/20], Step [1060/1524], Loss: 0.2499\n",
            "Epoch [12/20], Step [1080/1524], Loss: 0.2629\n",
            "Epoch [12/20], Step [1100/1524], Loss: 0.2548\n",
            "Epoch [12/20], Step [1120/1524], Loss: 0.2374\n",
            "Epoch [12/20], Step [1140/1524], Loss: 0.2920\n",
            "Epoch [12/20], Step [1160/1524], Loss: 0.2759\n",
            "Epoch [12/20], Step [1180/1524], Loss: 0.2799\n",
            "Epoch [12/20], Step [1200/1524], Loss: 0.2496\n",
            "Epoch [12/20], Step [1220/1524], Loss: 0.2380\n",
            "Epoch [12/20], Step [1240/1524], Loss: 0.2341\n",
            "Epoch [12/20], Step [1260/1524], Loss: 0.2527\n",
            "Epoch [12/20], Step [1280/1524], Loss: 0.2473\n",
            "Epoch [12/20], Step [1300/1524], Loss: 0.2588\n",
            "Epoch [12/20], Step [1320/1524], Loss: 0.2696\n",
            "Epoch [12/20], Step [1340/1524], Loss: 0.2313\n",
            "Epoch [12/20], Step [1360/1524], Loss: 0.2787\n",
            "Epoch [12/20], Step [1380/1524], Loss: 0.3002\n",
            "Epoch [12/20], Step [1400/1524], Loss: 0.2714\n",
            "Epoch [12/20], Step [1420/1524], Loss: 0.2644\n",
            "Epoch [12/20], Step [1440/1524], Loss: 0.2681\n",
            "Epoch [12/20], Step [1460/1524], Loss: 0.2535\n",
            "Epoch [12/20], Step [1480/1524], Loss: 0.3043\n",
            "Epoch [12/20], Step [1500/1524], Loss: 0.2667\n",
            "Epoch [12/20], Step [1520/1524], Loss: 0.2528\n",
            "Epoch [12/20] | Train Loss: 0.2660 | Train Acc: 90.80% | Test Loss: 0.5971 | Test Acc: 76.80%\n",
            "Epoch [13/20], Step [20/1524], Loss: 0.2965\n",
            "Epoch [13/20], Step [40/1524], Loss: 0.2977\n",
            "Epoch [13/20], Step [60/1524], Loss: 0.2816\n",
            "Epoch [13/20], Step [80/1524], Loss: 0.3192\n",
            "Epoch [13/20], Step [100/1524], Loss: 0.2942\n",
            "Epoch [13/20], Step [120/1524], Loss: 0.2838\n",
            "Epoch [13/20], Step [140/1524], Loss: 0.2436\n",
            "Epoch [13/20], Step [160/1524], Loss: 0.3248\n",
            "Epoch [13/20], Step [180/1524], Loss: 0.2199\n",
            "Epoch [13/20], Step [200/1524], Loss: 0.2484\n",
            "Epoch [13/20], Step [220/1524], Loss: 0.2639\n",
            "Epoch [13/20], Step [240/1524], Loss: 0.2703\n",
            "Epoch [13/20], Step [260/1524], Loss: 0.2631\n",
            "Epoch [13/20], Step [280/1524], Loss: 0.2568\n",
            "Epoch [13/20], Step [300/1524], Loss: 0.2471\n",
            "Epoch [13/20], Step [320/1524], Loss: 0.2996\n",
            "Epoch [13/20], Step [340/1524], Loss: 0.2540\n",
            "Epoch [13/20], Step [360/1524], Loss: 0.2502\n",
            "Epoch [13/20], Step [380/1524], Loss: 0.2530\n",
            "Epoch [13/20], Step [400/1524], Loss: 0.2496\n",
            "Epoch [13/20], Step [420/1524], Loss: 0.2620\n",
            "Epoch [13/20], Step [440/1524], Loss: 0.2619\n",
            "Epoch [13/20], Step [460/1524], Loss: 0.2296\n",
            "Epoch [13/20], Step [480/1524], Loss: 0.2658\n",
            "Epoch [13/20], Step [500/1524], Loss: 0.2595\n",
            "Epoch [13/20], Step [520/1524], Loss: 0.2600\n",
            "Epoch [13/20], Step [540/1524], Loss: 0.2968\n",
            "Epoch [13/20], Step [560/1524], Loss: 0.2743\n",
            "Epoch [13/20], Step [580/1524], Loss: 0.2585\n",
            "Epoch [13/20], Step [600/1524], Loss: 0.2536\n",
            "Epoch [13/20], Step [620/1524], Loss: 0.2770\n",
            "Epoch [13/20], Step [640/1524], Loss: 0.2607\n",
            "Epoch [13/20], Step [660/1524], Loss: 0.2728\n",
            "Epoch [13/20], Step [680/1524], Loss: 0.2512\n",
            "Epoch [13/20], Step [700/1524], Loss: 0.2551\n",
            "Epoch [13/20], Step [720/1524], Loss: 0.2774\n",
            "Epoch [13/20], Step [740/1524], Loss: 0.2425\n",
            "Epoch [13/20], Step [760/1524], Loss: 0.2610\n",
            "Epoch [13/20], Step [780/1524], Loss: 0.2608\n",
            "Epoch [13/20], Step [800/1524], Loss: 0.2603\n",
            "Epoch [13/20], Step [820/1524], Loss: 0.2840\n",
            "Epoch [13/20], Step [840/1524], Loss: 0.2690\n",
            "Epoch [13/20], Step [860/1524], Loss: 0.2462\n",
            "Epoch [13/20], Step [880/1524], Loss: 0.2428\n",
            "Epoch [13/20], Step [900/1524], Loss: 0.2676\n",
            "Epoch [13/20], Step [920/1524], Loss: 0.2536\n",
            "Epoch [13/20], Step [940/1524], Loss: 0.2581\n",
            "Epoch [13/20], Step [960/1524], Loss: 0.2644\n",
            "Epoch [13/20], Step [980/1524], Loss: 0.2640\n",
            "Epoch [13/20], Step [1000/1524], Loss: 0.2541\n",
            "Epoch [13/20], Step [1020/1524], Loss: 0.2400\n",
            "Epoch [13/20], Step [1040/1524], Loss: 0.2293\n",
            "Epoch [13/20], Step [1060/1524], Loss: 0.2537\n",
            "Epoch [13/20], Step [1080/1524], Loss: 0.2839\n",
            "Epoch [13/20], Step [1100/1524], Loss: 0.2490\n",
            "Epoch [13/20], Step [1120/1524], Loss: 0.2755\n",
            "Epoch [13/20], Step [1140/1524], Loss: 0.2588\n",
            "Epoch [13/20], Step [1160/1524], Loss: 0.2648\n",
            "Epoch [13/20], Step [1180/1524], Loss: 0.2312\n",
            "Epoch [13/20], Step [1200/1524], Loss: 0.2649\n",
            "Epoch [13/20], Step [1220/1524], Loss: 0.2868\n",
            "Epoch [13/20], Step [1240/1524], Loss: 0.2342\n",
            "Epoch [13/20], Step [1260/1524], Loss: 0.2360\n",
            "Epoch [13/20], Step [1280/1524], Loss: 0.2631\n",
            "Epoch [13/20], Step [1300/1524], Loss: 0.2453\n",
            "Epoch [13/20], Step [1320/1524], Loss: 0.2718\n",
            "Epoch [13/20], Step [1340/1524], Loss: 0.2565\n",
            "Epoch [13/20], Step [1360/1524], Loss: 0.2607\n",
            "Epoch [13/20], Step [1380/1524], Loss: 0.2502\n",
            "Epoch [13/20], Step [1400/1524], Loss: 0.2643\n",
            "Epoch [13/20], Step [1420/1524], Loss: 0.2758\n",
            "Epoch [13/20], Step [1440/1524], Loss: 0.3150\n",
            "Epoch [13/20], Step [1460/1524], Loss: 0.2649\n",
            "Epoch [13/20], Step [1480/1524], Loss: 0.2703\n",
            "Epoch [13/20], Step [1500/1524], Loss: 0.2553\n",
            "Epoch [13/20], Step [1520/1524], Loss: 0.2600\n",
            "Epoch [13/20] | Train Loss: 0.2637 | Train Acc: 90.83% | Test Loss: 0.5516 | Test Acc: 77.70%\n",
            "Epoch [14/20], Step [20/1524], Loss: 0.2469\n",
            "Epoch [14/20], Step [40/1524], Loss: 0.2567\n",
            "Epoch [14/20], Step [60/1524], Loss: 0.2445\n",
            "Epoch [14/20], Step [80/1524], Loss: 0.2498\n",
            "Epoch [14/20], Step [100/1524], Loss: 0.2360\n",
            "Epoch [14/20], Step [120/1524], Loss: 0.2098\n",
            "Epoch [14/20], Step [140/1524], Loss: 0.2357\n",
            "Epoch [14/20], Step [160/1524], Loss: 0.2813\n",
            "Epoch [14/20], Step [180/1524], Loss: 0.2524\n",
            "Epoch [14/20], Step [200/1524], Loss: 0.2552\n",
            "Epoch [14/20], Step [220/1524], Loss: 0.2440\n",
            "Epoch [14/20], Step [240/1524], Loss: 0.2557\n",
            "Epoch [14/20], Step [260/1524], Loss: 0.2515\n",
            "Epoch [14/20], Step [280/1524], Loss: 0.2624\n",
            "Epoch [14/20], Step [300/1524], Loss: 0.2438\n",
            "Epoch [14/20], Step [320/1524], Loss: 0.2603\n",
            "Epoch [14/20], Step [340/1524], Loss: 0.2539\n",
            "Epoch [14/20], Step [360/1524], Loss: 0.2260\n",
            "Epoch [14/20], Step [380/1524], Loss: 0.2590\n",
            "Epoch [14/20], Step [400/1524], Loss: 0.2490\n",
            "Epoch [14/20], Step [420/1524], Loss: 0.2450\n",
            "Epoch [14/20], Step [440/1524], Loss: 0.3020\n",
            "Epoch [14/20], Step [460/1524], Loss: 0.2999\n",
            "Epoch [14/20], Step [480/1524], Loss: 0.2570\n",
            "Epoch [14/20], Step [500/1524], Loss: 0.2557\n",
            "Epoch [14/20], Step [520/1524], Loss: 0.2615\n",
            "Epoch [14/20], Step [540/1524], Loss: 0.2342\n",
            "Epoch [14/20], Step [560/1524], Loss: 0.2291\n",
            "Epoch [14/20], Step [580/1524], Loss: 0.2719\n",
            "Epoch [14/20], Step [600/1524], Loss: 0.2424\n",
            "Epoch [14/20], Step [620/1524], Loss: 0.2430\n",
            "Epoch [14/20], Step [640/1524], Loss: 0.2723\n",
            "Epoch [14/20], Step [660/1524], Loss: 0.2379\n",
            "Epoch [14/20], Step [680/1524], Loss: 0.2609\n",
            "Epoch [14/20], Step [700/1524], Loss: 0.2762\n",
            "Epoch [14/20], Step [720/1524], Loss: 0.2433\n",
            "Epoch [14/20], Step [740/1524], Loss: 0.2711\n",
            "Epoch [14/20], Step [760/1524], Loss: 0.2490\n",
            "Epoch [14/20], Step [780/1524], Loss: 0.2300\n",
            "Epoch [14/20], Step [800/1524], Loss: 0.2498\n",
            "Epoch [14/20], Step [820/1524], Loss: 0.2691\n",
            "Epoch [14/20], Step [840/1524], Loss: 0.2581\n",
            "Epoch [14/20], Step [860/1524], Loss: 0.2818\n",
            "Epoch [14/20], Step [880/1524], Loss: 0.2485\n",
            "Epoch [14/20], Step [900/1524], Loss: 0.2893\n",
            "Epoch [14/20], Step [920/1524], Loss: 0.2443\n",
            "Epoch [14/20], Step [940/1524], Loss: 0.2631\n",
            "Epoch [14/20], Step [960/1524], Loss: 0.2385\n",
            "Epoch [14/20], Step [980/1524], Loss: 0.2598\n",
            "Epoch [14/20], Step [1000/1524], Loss: 0.3055\n",
            "Epoch [14/20], Step [1020/1524], Loss: 0.2801\n",
            "Epoch [14/20], Step [1040/1524], Loss: 0.2445\n",
            "Epoch [14/20], Step [1060/1524], Loss: 0.2586\n",
            "Epoch [14/20], Step [1080/1524], Loss: 0.2816\n",
            "Epoch [14/20], Step [1100/1524], Loss: 0.2956\n",
            "Epoch [14/20], Step [1120/1524], Loss: 0.2387\n",
            "Epoch [14/20], Step [1140/1524], Loss: 0.2168\n",
            "Epoch [14/20], Step [1160/1524], Loss: 0.2734\n",
            "Epoch [14/20], Step [1180/1524], Loss: 0.2283\n",
            "Epoch [14/20], Step [1200/1524], Loss: 0.2430\n",
            "Epoch [14/20], Step [1220/1524], Loss: 0.2327\n",
            "Epoch [14/20], Step [1240/1524], Loss: 0.2662\n",
            "Epoch [14/20], Step [1260/1524], Loss: 0.2628\n",
            "Epoch [14/20], Step [1280/1524], Loss: 0.2612\n",
            "Epoch [14/20], Step [1300/1524], Loss: 0.2535\n",
            "Epoch [14/20], Step [1320/1524], Loss: 0.2737\n",
            "Epoch [14/20], Step [1340/1524], Loss: 0.2721\n",
            "Epoch [14/20], Step [1360/1524], Loss: 0.2539\n",
            "Epoch [14/20], Step [1380/1524], Loss: 0.2296\n",
            "Epoch [14/20], Step [1400/1524], Loss: 0.2618\n",
            "Epoch [14/20], Step [1420/1524], Loss: 0.2809\n",
            "Epoch [14/20], Step [1440/1524], Loss: 0.2388\n",
            "Epoch [14/20], Step [1460/1524], Loss: 0.2408\n",
            "Epoch [14/20], Step [1480/1524], Loss: 0.2907\n",
            "Epoch [14/20], Step [1500/1524], Loss: 0.2446\n",
            "Epoch [14/20], Step [1520/1524], Loss: 0.2583\n",
            "Epoch [14/20] | Train Loss: 0.2564 | Train Acc: 91.15% | Test Loss: 0.5275 | Test Acc: 80.30%\n",
            "Epoch [15/20], Step [20/1524], Loss: 0.2868\n",
            "Epoch [15/20], Step [40/1524], Loss: 0.2541\n",
            "Epoch [15/20], Step [60/1524], Loss: 0.2661\n",
            "Epoch [15/20], Step [80/1524], Loss: 0.2612\n",
            "Epoch [15/20], Step [100/1524], Loss: 0.2637\n",
            "Epoch [15/20], Step [120/1524], Loss: 0.2488\n",
            "Epoch [15/20], Step [140/1524], Loss: 0.2670\n",
            "Epoch [15/20], Step [160/1524], Loss: 0.2285\n",
            "Epoch [15/20], Step [180/1524], Loss: 0.2300\n",
            "Epoch [15/20], Step [200/1524], Loss: 0.2751\n",
            "Epoch [15/20], Step [220/1524], Loss: 0.2520\n",
            "Epoch [15/20], Step [240/1524], Loss: 0.2555\n",
            "Epoch [15/20], Step [260/1524], Loss: 0.2637\n",
            "Epoch [15/20], Step [280/1524], Loss: 0.2222\n",
            "Epoch [15/20], Step [300/1524], Loss: 0.2256\n",
            "Epoch [15/20], Step [320/1524], Loss: 0.2248\n",
            "Epoch [15/20], Step [340/1524], Loss: 0.2774\n",
            "Epoch [15/20], Step [360/1524], Loss: 0.2325\n",
            "Epoch [15/20], Step [380/1524], Loss: 0.2658\n",
            "Epoch [15/20], Step [400/1524], Loss: 0.2581\n",
            "Epoch [15/20], Step [420/1524], Loss: 0.2821\n",
            "Epoch [15/20], Step [440/1524], Loss: 0.2579\n",
            "Epoch [15/20], Step [460/1524], Loss: 0.2626\n",
            "Epoch [15/20], Step [480/1524], Loss: 0.2570\n",
            "Epoch [15/20], Step [500/1524], Loss: 0.2713\n",
            "Epoch [15/20], Step [520/1524], Loss: 0.2931\n",
            "Epoch [15/20], Step [540/1524], Loss: 0.2516\n",
            "Epoch [15/20], Step [560/1524], Loss: 0.2361\n",
            "Epoch [15/20], Step [580/1524], Loss: 0.2311\n",
            "Epoch [15/20], Step [600/1524], Loss: 0.2427\n",
            "Epoch [15/20], Step [620/1524], Loss: 0.2657\n",
            "Epoch [15/20], Step [640/1524], Loss: 0.2303\n",
            "Epoch [15/20], Step [660/1524], Loss: 0.2407\n",
            "Epoch [15/20], Step [680/1524], Loss: 0.2838\n",
            "Epoch [15/20], Step [700/1524], Loss: 0.2465\n",
            "Epoch [15/20], Step [720/1524], Loss: 0.2294\n",
            "Epoch [15/20], Step [740/1524], Loss: 0.2362\n",
            "Epoch [15/20], Step [760/1524], Loss: 0.2468\n",
            "Epoch [15/20], Step [780/1524], Loss: 0.2325\n",
            "Epoch [15/20], Step [800/1524], Loss: 0.2684\n",
            "Epoch [15/20], Step [820/1524], Loss: 0.2697\n",
            "Epoch [15/20], Step [840/1524], Loss: 0.2362\n",
            "Epoch [15/20], Step [860/1524], Loss: 0.2349\n",
            "Epoch [15/20], Step [880/1524], Loss: 0.2616\n",
            "Epoch [15/20], Step [900/1524], Loss: 0.2721\n",
            "Epoch [15/20], Step [920/1524], Loss: 0.2219\n",
            "Epoch [15/20], Step [940/1524], Loss: 0.2408\n",
            "Epoch [15/20], Step [960/1524], Loss: 0.2626\n",
            "Epoch [15/20], Step [980/1524], Loss: 0.2966\n",
            "Epoch [15/20], Step [1000/1524], Loss: 0.2663\n",
            "Epoch [15/20], Step [1020/1524], Loss: 0.2678\n",
            "Epoch [15/20], Step [1040/1524], Loss: 0.2318\n",
            "Epoch [15/20], Step [1060/1524], Loss: 0.2342\n",
            "Epoch [15/20], Step [1080/1524], Loss: 0.2653\n",
            "Epoch [15/20], Step [1100/1524], Loss: 0.2630\n",
            "Epoch [15/20], Step [1120/1524], Loss: 0.2692\n",
            "Epoch [15/20], Step [1140/1524], Loss: 0.2106\n",
            "Epoch [15/20], Step [1160/1524], Loss: 0.2470\n",
            "Epoch [15/20], Step [1180/1524], Loss: 0.2842\n",
            "Epoch [15/20], Step [1200/1524], Loss: 0.2144\n",
            "Epoch [15/20], Step [1220/1524], Loss: 0.2270\n",
            "Epoch [15/20], Step [1240/1524], Loss: 0.2241\n",
            "Epoch [15/20], Step [1260/1524], Loss: 0.2347\n",
            "Epoch [15/20], Step [1280/1524], Loss: 0.2754\n",
            "Epoch [15/20], Step [1300/1524], Loss: 0.2740\n",
            "Epoch [15/20], Step [1320/1524], Loss: 0.2772\n",
            "Epoch [15/20], Step [1340/1524], Loss: 0.2377\n",
            "Epoch [15/20], Step [1360/1524], Loss: 0.2213\n",
            "Epoch [15/20], Step [1380/1524], Loss: 0.2368\n",
            "Epoch [15/20], Step [1400/1524], Loss: 0.2582\n",
            "Epoch [15/20], Step [1420/1524], Loss: 0.2615\n",
            "Epoch [15/20], Step [1440/1524], Loss: 0.2576\n",
            "Epoch [15/20], Step [1460/1524], Loss: 0.2657\n",
            "Epoch [15/20], Step [1480/1524], Loss: 0.2266\n",
            "Epoch [15/20], Step [1500/1524], Loss: 0.2535\n",
            "Epoch [15/20], Step [1520/1524], Loss: 0.2776\n",
            "Epoch [15/20] | Train Loss: 0.2525 | Train Acc: 91.15% | Test Loss: 0.6458 | Test Acc: 76.20%\n",
            "Epoch [16/20], Step [20/1524], Loss: 0.2394\n",
            "Epoch [16/20], Step [40/1524], Loss: 0.2469\n",
            "Epoch [16/20], Step [60/1524], Loss: 0.2388\n",
            "Epoch [16/20], Step [80/1524], Loss: 0.2414\n",
            "Epoch [16/20], Step [100/1524], Loss: 0.2506\n",
            "Epoch [16/20], Step [120/1524], Loss: 0.2660\n",
            "Epoch [16/20], Step [140/1524], Loss: 0.2441\n",
            "Epoch [16/20], Step [160/1524], Loss: 0.2436\n",
            "Epoch [16/20], Step [180/1524], Loss: 0.2259\n",
            "Epoch [16/20], Step [200/1524], Loss: 0.2469\n",
            "Epoch [16/20], Step [220/1524], Loss: 0.2249\n",
            "Epoch [16/20], Step [240/1524], Loss: 0.2657\n",
            "Epoch [16/20], Step [260/1524], Loss: 0.2567\n",
            "Epoch [16/20], Step [280/1524], Loss: 0.2693\n",
            "Epoch [16/20], Step [300/1524], Loss: 0.2297\n",
            "Epoch [16/20], Step [320/1524], Loss: 0.2865\n",
            "Epoch [16/20], Step [340/1524], Loss: 0.2815\n",
            "Epoch [16/20], Step [360/1524], Loss: 0.2596\n",
            "Epoch [16/20], Step [380/1524], Loss: 0.2327\n",
            "Epoch [16/20], Step [400/1524], Loss: 0.2797\n",
            "Epoch [16/20], Step [420/1524], Loss: 0.2832\n",
            "Epoch [16/20], Step [440/1524], Loss: 0.2595\n",
            "Epoch [16/20], Step [460/1524], Loss: 0.2359\n",
            "Epoch [16/20], Step [480/1524], Loss: 0.2297\n",
            "Epoch [16/20], Step [500/1524], Loss: 0.2621\n",
            "Epoch [16/20], Step [520/1524], Loss: 0.2539\n",
            "Epoch [16/20], Step [540/1524], Loss: 0.2646\n",
            "Epoch [16/20], Step [560/1524], Loss: 0.2402\n",
            "Epoch [16/20], Step [580/1524], Loss: 0.2586\n",
            "Epoch [16/20], Step [600/1524], Loss: 0.2514\n",
            "Epoch [16/20], Step [620/1524], Loss: 0.2646\n",
            "Epoch [16/20], Step [640/1524], Loss: 0.2305\n",
            "Epoch [16/20], Step [660/1524], Loss: 0.2495\n",
            "Epoch [16/20], Step [680/1524], Loss: 0.2560\n",
            "Epoch [16/20], Step [700/1524], Loss: 0.2493\n",
            "Epoch [16/20], Step [720/1524], Loss: 0.2181\n",
            "Epoch [16/20], Step [740/1524], Loss: 0.2395\n",
            "Epoch [16/20], Step [760/1524], Loss: 0.2379\n",
            "Epoch [16/20], Step [780/1524], Loss: 0.2257\n",
            "Epoch [16/20], Step [800/1524], Loss: 0.2097\n",
            "Epoch [16/20], Step [820/1524], Loss: 0.2422\n",
            "Epoch [16/20], Step [840/1524], Loss: 0.2566\n",
            "Epoch [16/20], Step [860/1524], Loss: 0.2462\n",
            "Epoch [16/20], Step [880/1524], Loss: 0.2409\n",
            "Epoch [16/20], Step [900/1524], Loss: 0.2810\n",
            "Epoch [16/20], Step [920/1524], Loss: 0.2398\n",
            "Epoch [16/20], Step [940/1524], Loss: 0.2587\n",
            "Epoch [16/20], Step [960/1524], Loss: 0.2549\n",
            "Epoch [16/20], Step [980/1524], Loss: 0.2514\n",
            "Epoch [16/20], Step [1000/1524], Loss: 0.2316\n",
            "Epoch [16/20], Step [1020/1524], Loss: 0.2610\n",
            "Epoch [16/20], Step [1040/1524], Loss: 0.2223\n",
            "Epoch [16/20], Step [1060/1524], Loss: 0.2567\n",
            "Epoch [16/20], Step [1080/1524], Loss: 0.2447\n",
            "Epoch [16/20], Step [1100/1524], Loss: 0.2407\n",
            "Epoch [16/20], Step [1120/1524], Loss: 0.2458\n",
            "Epoch [16/20], Step [1140/1524], Loss: 0.2338\n",
            "Epoch [16/20], Step [1160/1524], Loss: 0.2668\n",
            "Epoch [16/20], Step [1180/1524], Loss: 0.2275\n",
            "Epoch [16/20], Step [1200/1524], Loss: 0.2083\n",
            "Epoch [16/20], Step [1220/1524], Loss: 0.2359\n",
            "Epoch [16/20], Step [1240/1524], Loss: 0.2394\n",
            "Epoch [16/20], Step [1260/1524], Loss: 0.2564\n",
            "Epoch [16/20], Step [1280/1524], Loss: 0.2344\n",
            "Epoch [16/20], Step [1300/1524], Loss: 0.2471\n",
            "Epoch [16/20], Step [1320/1524], Loss: 0.2506\n",
            "Epoch [16/20], Step [1340/1524], Loss: 0.2425\n",
            "Epoch [16/20], Step [1360/1524], Loss: 0.2321\n",
            "Epoch [16/20], Step [1380/1524], Loss: 0.2637\n",
            "Epoch [16/20], Step [1400/1524], Loss: 0.2494\n",
            "Epoch [16/20], Step [1420/1524], Loss: 0.2433\n",
            "Epoch [16/20], Step [1440/1524], Loss: 0.2663\n",
            "Epoch [16/20], Step [1460/1524], Loss: 0.2823\n",
            "Epoch [16/20], Step [1480/1524], Loss: 0.2560\n",
            "Epoch [16/20], Step [1500/1524], Loss: 0.2421\n",
            "Epoch [16/20], Step [1520/1524], Loss: 0.2563\n",
            "Epoch [16/20] | Train Loss: 0.2490 | Train Acc: 91.29% | Test Loss: 0.7128 | Test Acc: 73.30%\n",
            "Epoch [17/20], Step [20/1524], Loss: 0.3015\n",
            "Epoch [17/20], Step [40/1524], Loss: 0.2910\n",
            "Epoch [17/20], Step [60/1524], Loss: 0.2444\n",
            "Epoch [17/20], Step [80/1524], Loss: 0.2266\n",
            "Epoch [17/20], Step [100/1524], Loss: 0.2258\n",
            "Epoch [17/20], Step [120/1524], Loss: 0.2371\n",
            "Epoch [17/20], Step [140/1524], Loss: 0.2343\n",
            "Epoch [17/20], Step [160/1524], Loss: 0.2398\n",
            "Epoch [17/20], Step [180/1524], Loss: 0.2409\n",
            "Epoch [17/20], Step [200/1524], Loss: 0.2580\n",
            "Epoch [17/20], Step [220/1524], Loss: 0.2647\n",
            "Epoch [17/20], Step [240/1524], Loss: 0.2409\n",
            "Epoch [17/20], Step [260/1524], Loss: 0.2434\n",
            "Epoch [17/20], Step [280/1524], Loss: 0.2460\n",
            "Epoch [17/20], Step [300/1524], Loss: 0.2241\n",
            "Epoch [17/20], Step [320/1524], Loss: 0.2464\n",
            "Epoch [17/20], Step [340/1524], Loss: 0.2609\n",
            "Epoch [17/20], Step [360/1524], Loss: 0.2460\n",
            "Epoch [17/20], Step [380/1524], Loss: 0.2601\n",
            "Epoch [17/20], Step [400/1524], Loss: 0.2378\n",
            "Epoch [17/20], Step [420/1524], Loss: 0.2472\n",
            "Epoch [17/20], Step [440/1524], Loss: 0.2273\n",
            "Epoch [17/20], Step [460/1524], Loss: 0.1950\n",
            "Epoch [17/20], Step [480/1524], Loss: 0.2779\n",
            "Epoch [17/20], Step [500/1524], Loss: 0.2686\n",
            "Epoch [17/20], Step [520/1524], Loss: 0.2388\n",
            "Epoch [17/20], Step [540/1524], Loss: 0.2601\n",
            "Epoch [17/20], Step [560/1524], Loss: 0.2595\n",
            "Epoch [17/20], Step [580/1524], Loss: 0.2450\n",
            "Epoch [17/20], Step [600/1524], Loss: 0.2373\n",
            "Epoch [17/20], Step [620/1524], Loss: 0.2542\n",
            "Epoch [17/20], Step [640/1524], Loss: 0.2288\n",
            "Epoch [17/20], Step [660/1524], Loss: 0.2286\n",
            "Epoch [17/20], Step [680/1524], Loss: 0.2427\n",
            "Epoch [17/20], Step [700/1524], Loss: 0.2812\n",
            "Epoch [17/20], Step [720/1524], Loss: 0.2570\n",
            "Epoch [17/20], Step [740/1524], Loss: 0.2299\n",
            "Epoch [17/20], Step [760/1524], Loss: 0.2328\n",
            "Epoch [17/20], Step [780/1524], Loss: 0.2436\n",
            "Epoch [17/20], Step [800/1524], Loss: 0.2761\n",
            "Epoch [17/20], Step [820/1524], Loss: 0.2282\n",
            "Epoch [17/20], Step [840/1524], Loss: 0.2704\n",
            "Epoch [17/20], Step [860/1524], Loss: 0.2257\n",
            "Epoch [17/20], Step [880/1524], Loss: 0.2408\n",
            "Epoch [17/20], Step [900/1524], Loss: 0.1905\n",
            "Epoch [17/20], Step [920/1524], Loss: 0.2791\n",
            "Epoch [17/20], Step [940/1524], Loss: 0.2258\n",
            "Epoch [17/20], Step [960/1524], Loss: 0.2498\n",
            "Epoch [17/20], Step [980/1524], Loss: 0.2449\n",
            "Epoch [17/20], Step [1000/1524], Loss: 0.2254\n",
            "Epoch [17/20], Step [1020/1524], Loss: 0.2393\n",
            "Epoch [17/20], Step [1040/1524], Loss: 0.2823\n",
            "Epoch [17/20], Step [1060/1524], Loss: 0.2409\n",
            "Epoch [17/20], Step [1080/1524], Loss: 0.2281\n",
            "Epoch [17/20], Step [1100/1524], Loss: 0.2194\n",
            "Epoch [17/20], Step [1120/1524], Loss: 0.2315\n",
            "Epoch [17/20], Step [1140/1524], Loss: 0.2355\n",
            "Epoch [17/20], Step [1160/1524], Loss: 0.2582\n",
            "Epoch [17/20], Step [1180/1524], Loss: 0.2543\n",
            "Epoch [17/20], Step [1200/1524], Loss: 0.2466\n",
            "Epoch [17/20], Step [1220/1524], Loss: 0.2272\n",
            "Epoch [17/20], Step [1240/1524], Loss: 0.2278\n",
            "Epoch [17/20], Step [1260/1524], Loss: 0.2528\n",
            "Epoch [17/20], Step [1280/1524], Loss: 0.2507\n",
            "Epoch [17/20], Step [1300/1524], Loss: 0.2394\n",
            "Epoch [17/20], Step [1320/1524], Loss: 0.2446\n",
            "Epoch [17/20], Step [1340/1524], Loss: 0.2705\n",
            "Epoch [17/20], Step [1360/1524], Loss: 0.2386\n",
            "Epoch [17/20], Step [1380/1524], Loss: 0.2124\n",
            "Epoch [17/20], Step [1400/1524], Loss: 0.2472\n",
            "Epoch [17/20], Step [1420/1524], Loss: 0.2434\n",
            "Epoch [17/20], Step [1440/1524], Loss: 0.2479\n",
            "Epoch [17/20], Step [1460/1524], Loss: 0.2417\n",
            "Epoch [17/20], Step [1480/1524], Loss: 0.2518\n",
            "Epoch [17/20], Step [1500/1524], Loss: 0.2522\n",
            "Epoch [17/20], Step [1520/1524], Loss: 0.2202\n",
            "Epoch [17/20] | Train Loss: 0.2452 | Train Acc: 91.42% | Test Loss: 0.6512 | Test Acc: 77.10%\n",
            "Epoch [18/20], Step [20/1524], Loss: 0.3227\n",
            "Epoch [18/20], Step [40/1524], Loss: 0.2511\n",
            "Epoch [18/20], Step [60/1524], Loss: 0.2629\n",
            "Epoch [18/20], Step [80/1524], Loss: 0.2451\n",
            "Epoch [18/20], Step [100/1524], Loss: 0.2488\n",
            "Epoch [18/20], Step [120/1524], Loss: 0.2237\n",
            "Epoch [18/20], Step [140/1524], Loss: 0.2690\n",
            "Epoch [18/20], Step [160/1524], Loss: 0.2604\n",
            "Epoch [18/20], Step [180/1524], Loss: 0.2433\n",
            "Epoch [18/20], Step [200/1524], Loss: 0.2208\n",
            "Epoch [18/20], Step [220/1524], Loss: 0.2535\n",
            "Epoch [18/20], Step [240/1524], Loss: 0.2196\n",
            "Epoch [18/20], Step [260/1524], Loss: 0.2278\n",
            "Epoch [18/20], Step [280/1524], Loss: 0.2979\n",
            "Epoch [18/20], Step [300/1524], Loss: 0.2294\n",
            "Epoch [18/20], Step [320/1524], Loss: 0.2343\n",
            "Epoch [18/20], Step [340/1524], Loss: 0.1991\n",
            "Epoch [18/20], Step [360/1524], Loss: 0.2248\n",
            "Epoch [18/20], Step [380/1524], Loss: 0.2206\n",
            "Epoch [18/20], Step [400/1524], Loss: 0.2645\n",
            "Epoch [18/20], Step [420/1524], Loss: 0.2401\n",
            "Epoch [18/20], Step [440/1524], Loss: 0.2417\n",
            "Epoch [18/20], Step [460/1524], Loss: 0.2634\n",
            "Epoch [18/20], Step [480/1524], Loss: 0.2522\n",
            "Epoch [18/20], Step [500/1524], Loss: 0.2258\n",
            "Epoch [18/20], Step [520/1524], Loss: 0.2334\n",
            "Epoch [18/20], Step [540/1524], Loss: 0.2612\n",
            "Epoch [18/20], Step [560/1524], Loss: 0.2361\n",
            "Epoch [18/20], Step [580/1524], Loss: 0.2748\n",
            "Epoch [18/20], Step [600/1524], Loss: 0.2446\n",
            "Epoch [18/20], Step [620/1524], Loss: 0.1993\n",
            "Epoch [18/20], Step [640/1524], Loss: 0.2232\n",
            "Epoch [18/20], Step [660/1524], Loss: 0.2180\n",
            "Epoch [18/20], Step [680/1524], Loss: 0.2440\n",
            "Epoch [18/20], Step [700/1524], Loss: 0.2363\n",
            "Epoch [18/20], Step [720/1524], Loss: 0.2228\n",
            "Epoch [18/20], Step [740/1524], Loss: 0.2547\n",
            "Epoch [18/20], Step [760/1524], Loss: 0.2543\n",
            "Epoch [18/20], Step [780/1524], Loss: 0.2639\n",
            "Epoch [18/20], Step [800/1524], Loss: 0.2000\n",
            "Epoch [18/20], Step [820/1524], Loss: 0.2467\n",
            "Epoch [18/20], Step [840/1524], Loss: 0.2236\n",
            "Epoch [18/20], Step [860/1524], Loss: 0.2568\n",
            "Epoch [18/20], Step [880/1524], Loss: 0.2488\n",
            "Epoch [18/20], Step [900/1524], Loss: 0.2171\n",
            "Epoch [18/20], Step [920/1524], Loss: 0.2683\n",
            "Epoch [18/20], Step [940/1524], Loss: 0.2347\n",
            "Epoch [18/20], Step [960/1524], Loss: 0.2769\n",
            "Epoch [18/20], Step [980/1524], Loss: 0.2635\n",
            "Epoch [18/20], Step [1000/1524], Loss: 0.2413\n",
            "Epoch [18/20], Step [1020/1524], Loss: 0.2170\n",
            "Epoch [18/20], Step [1040/1524], Loss: 0.2249\n",
            "Epoch [18/20], Step [1060/1524], Loss: 0.2410\n",
            "Epoch [18/20], Step [1080/1524], Loss: 0.2548\n",
            "Epoch [18/20], Step [1100/1524], Loss: 0.2393\n",
            "Epoch [18/20], Step [1120/1524], Loss: 0.2732\n",
            "Epoch [18/20], Step [1140/1524], Loss: 0.2293\n",
            "Epoch [18/20], Step [1160/1524], Loss: 0.2419\n",
            "Epoch [18/20], Step [1180/1524], Loss: 0.2060\n",
            "Epoch [18/20], Step [1200/1524], Loss: 0.2293\n",
            "Epoch [18/20], Step [1220/1524], Loss: 0.2577\n",
            "Epoch [18/20], Step [1240/1524], Loss: 0.2249\n",
            "Epoch [18/20], Step [1260/1524], Loss: 0.2490\n",
            "Epoch [18/20], Step [1280/1524], Loss: 0.2758\n",
            "Epoch [18/20], Step [1300/1524], Loss: 0.2508\n",
            "Epoch [18/20], Step [1320/1524], Loss: 0.2535\n",
            "Epoch [18/20], Step [1340/1524], Loss: 0.2568\n",
            "Epoch [18/20], Step [1360/1524], Loss: 0.2449\n",
            "Epoch [18/20], Step [1380/1524], Loss: 0.2283\n",
            "Epoch [18/20], Step [1400/1524], Loss: 0.2512\n",
            "Epoch [18/20], Step [1420/1524], Loss: 0.2501\n",
            "Epoch [18/20], Step [1440/1524], Loss: 0.2308\n",
            "Epoch [18/20], Step [1460/1524], Loss: 0.2344\n",
            "Epoch [18/20], Step [1480/1524], Loss: 0.2518\n",
            "Epoch [18/20], Step [1500/1524], Loss: 0.2193\n",
            "Epoch [18/20], Step [1520/1524], Loss: 0.2588\n",
            "Epoch [18/20] | Train Loss: 0.2437 | Train Acc: 91.46% | Test Loss: 0.7843 | Test Acc: 70.20%\n",
            "Epoch [19/20], Step [20/1524], Loss: 0.2574\n",
            "Epoch [19/20], Step [40/1524], Loss: 0.2879\n",
            "Epoch [19/20], Step [60/1524], Loss: 0.2317\n",
            "Epoch [19/20], Step [80/1524], Loss: 0.2363\n",
            "Epoch [19/20], Step [100/1524], Loss: 0.2586\n",
            "Epoch [19/20], Step [120/1524], Loss: 0.2676\n",
            "Epoch [19/20], Step [140/1524], Loss: 0.2403\n",
            "Epoch [19/20], Step [160/1524], Loss: 0.2463\n",
            "Epoch [19/20], Step [180/1524], Loss: 0.2361\n",
            "Epoch [19/20], Step [200/1524], Loss: 0.2341\n",
            "Epoch [19/20], Step [220/1524], Loss: 0.2371\n",
            "Epoch [19/20], Step [240/1524], Loss: 0.2095\n",
            "Epoch [19/20], Step [260/1524], Loss: 0.2400\n",
            "Epoch [19/20], Step [280/1524], Loss: 0.2904\n",
            "Epoch [19/20], Step [300/1524], Loss: 0.2500\n",
            "Epoch [19/20], Step [320/1524], Loss: 0.2449\n",
            "Epoch [19/20], Step [340/1524], Loss: 0.2669\n",
            "Epoch [19/20], Step [360/1524], Loss: 0.2314\n",
            "Epoch [19/20], Step [380/1524], Loss: 0.2259\n",
            "Epoch [19/20], Step [400/1524], Loss: 0.2505\n",
            "Epoch [19/20], Step [420/1524], Loss: 0.2643\n",
            "Epoch [19/20], Step [440/1524], Loss: 0.2426\n",
            "Epoch [19/20], Step [460/1524], Loss: 0.2572\n",
            "Epoch [19/20], Step [480/1524], Loss: 0.2183\n",
            "Epoch [19/20], Step [500/1524], Loss: 0.2181\n",
            "Epoch [19/20], Step [520/1524], Loss: 0.2182\n",
            "Epoch [19/20], Step [540/1524], Loss: 0.2224\n",
            "Epoch [19/20], Step [560/1524], Loss: 0.2591\n",
            "Epoch [19/20], Step [580/1524], Loss: 0.2401\n",
            "Epoch [19/20], Step [600/1524], Loss: 0.2010\n",
            "Epoch [19/20], Step [620/1524], Loss: 0.2276\n",
            "Epoch [19/20], Step [640/1524], Loss: 0.2481\n",
            "Epoch [19/20], Step [660/1524], Loss: 0.2167\n",
            "Epoch [19/20], Step [680/1524], Loss: 0.2258\n",
            "Epoch [19/20], Step [700/1524], Loss: 0.2103\n",
            "Epoch [19/20], Step [720/1524], Loss: 0.2351\n",
            "Epoch [19/20], Step [740/1524], Loss: 0.2431\n",
            "Epoch [19/20], Step [760/1524], Loss: 0.2199\n",
            "Epoch [19/20], Step [780/1524], Loss: 0.2338\n",
            "Epoch [19/20], Step [800/1524], Loss: 0.2467\n",
            "Epoch [19/20], Step [820/1524], Loss: 0.2415\n",
            "Epoch [19/20], Step [840/1524], Loss: 0.2441\n",
            "Epoch [19/20], Step [860/1524], Loss: 0.2026\n",
            "Epoch [19/20], Step [880/1524], Loss: 0.1979\n",
            "Epoch [19/20], Step [900/1524], Loss: 0.2243\n",
            "Epoch [19/20], Step [920/1524], Loss: 0.2448\n",
            "Epoch [19/20], Step [940/1524], Loss: 0.2404\n",
            "Epoch [19/20], Step [960/1524], Loss: 0.2176\n",
            "Epoch [19/20], Step [980/1524], Loss: 0.2208\n",
            "Epoch [19/20], Step [1000/1524], Loss: 0.1983\n",
            "Epoch [19/20], Step [1020/1524], Loss: 0.2305\n",
            "Epoch [19/20], Step [1040/1524], Loss: 0.2054\n",
            "Epoch [19/20], Step [1060/1524], Loss: 0.2303\n",
            "Epoch [19/20], Step [1080/1524], Loss: 0.2392\n",
            "Epoch [19/20], Step [1100/1524], Loss: 0.2217\n",
            "Epoch [19/20], Step [1120/1524], Loss: 0.2327\n",
            "Epoch [19/20], Step [1140/1524], Loss: 0.2421\n",
            "Epoch [19/20], Step [1160/1524], Loss: 0.2384\n",
            "Epoch [19/20], Step [1180/1524], Loss: 0.2553\n",
            "Epoch [19/20], Step [1200/1524], Loss: 0.2706\n",
            "Epoch [19/20], Step [1220/1524], Loss: 0.2509\n",
            "Epoch [19/20], Step [1240/1524], Loss: 0.2114\n",
            "Epoch [19/20], Step [1260/1524], Loss: 0.2116\n",
            "Epoch [19/20], Step [1280/1524], Loss: 0.2279\n",
            "Epoch [19/20], Step [1300/1524], Loss: 0.2222\n",
            "Epoch [19/20], Step [1320/1524], Loss: 0.2761\n",
            "Epoch [19/20], Step [1340/1524], Loss: 0.2356\n",
            "Epoch [19/20], Step [1360/1524], Loss: 0.2391\n",
            "Epoch [19/20], Step [1380/1524], Loss: 0.2249\n",
            "Epoch [19/20], Step [1400/1524], Loss: 0.2437\n",
            "Epoch [19/20], Step [1420/1524], Loss: 0.2282\n",
            "Epoch [19/20], Step [1440/1524], Loss: 0.2369\n",
            "Epoch [19/20], Step [1460/1524], Loss: 0.2456\n",
            "Epoch [19/20], Step [1480/1524], Loss: 0.2292\n",
            "Epoch [19/20], Step [1500/1524], Loss: 0.2647\n",
            "Epoch [19/20], Step [1520/1524], Loss: 0.2427\n",
            "Epoch [19/20] | Train Loss: 0.2366 | Train Acc: 91.83% | Test Loss: 1.2493 | Test Acc: 63.70%\n",
            "Epoch [20/20], Step [20/1524], Loss: 0.2532\n",
            "Epoch [20/20], Step [40/1524], Loss: 0.2377\n",
            "Epoch [20/20], Step [60/1524], Loss: 0.2365\n",
            "Epoch [20/20], Step [80/1524], Loss: 0.2441\n",
            "Epoch [20/20], Step [100/1524], Loss: 0.2312\n",
            "Epoch [20/20], Step [120/1524], Loss: 0.2224\n",
            "Epoch [20/20], Step [140/1524], Loss: 0.2189\n",
            "Epoch [20/20], Step [160/1524], Loss: 0.2406\n",
            "Epoch [20/20], Step [180/1524], Loss: 0.2349\n",
            "Epoch [20/20], Step [200/1524], Loss: 0.2048\n",
            "Epoch [20/20], Step [220/1524], Loss: 0.2415\n",
            "Epoch [20/20], Step [240/1524], Loss: 0.2346\n",
            "Epoch [20/20], Step [260/1524], Loss: 0.2172\n",
            "Epoch [20/20], Step [280/1524], Loss: 0.2255\n",
            "Epoch [20/20], Step [300/1524], Loss: 0.2605\n",
            "Epoch [20/20], Step [320/1524], Loss: 0.2522\n",
            "Epoch [20/20], Step [340/1524], Loss: 0.2261\n",
            "Epoch [20/20], Step [360/1524], Loss: 0.2450\n",
            "Epoch [20/20], Step [380/1524], Loss: 0.2264\n",
            "Epoch [20/20], Step [400/1524], Loss: 0.2202\n",
            "Epoch [20/20], Step [420/1524], Loss: 0.2328\n",
            "Epoch [20/20], Step [440/1524], Loss: 0.2162\n",
            "Epoch [20/20], Step [460/1524], Loss: 0.2036\n",
            "Epoch [20/20], Step [480/1524], Loss: 0.2644\n",
            "Epoch [20/20], Step [500/1524], Loss: 0.2368\n",
            "Epoch [20/20], Step [520/1524], Loss: 0.2014\n",
            "Epoch [20/20], Step [540/1524], Loss: 0.2195\n",
            "Epoch [20/20], Step [560/1524], Loss: 0.2751\n",
            "Epoch [20/20], Step [580/1524], Loss: 0.2225\n",
            "Epoch [20/20], Step [600/1524], Loss: 0.2347\n",
            "Epoch [20/20], Step [620/1524], Loss: 0.2445\n",
            "Epoch [20/20], Step [640/1524], Loss: 0.2365\n",
            "Epoch [20/20], Step [660/1524], Loss: 0.2582\n",
            "Epoch [20/20], Step [680/1524], Loss: 0.2038\n",
            "Epoch [20/20], Step [700/1524], Loss: 0.2193\n",
            "Epoch [20/20], Step [720/1524], Loss: 0.2371\n",
            "Epoch [20/20], Step [740/1524], Loss: 0.2459\n",
            "Epoch [20/20], Step [760/1524], Loss: 0.2436\n",
            "Epoch [20/20], Step [780/1524], Loss: 0.2341\n",
            "Epoch [20/20], Step [800/1524], Loss: 0.2365\n",
            "Epoch [20/20], Step [820/1524], Loss: 0.2048\n",
            "Epoch [20/20], Step [840/1524], Loss: 0.2396\n",
            "Epoch [20/20], Step [860/1524], Loss: 0.2684\n",
            "Epoch [20/20], Step [880/1524], Loss: 0.2578\n",
            "Epoch [20/20], Step [900/1524], Loss: 0.2076\n",
            "Epoch [20/20], Step [920/1524], Loss: 0.2395\n",
            "Epoch [20/20], Step [940/1524], Loss: 0.2280\n",
            "Epoch [20/20], Step [960/1524], Loss: 0.2211\n",
            "Epoch [20/20], Step [980/1524], Loss: 0.2370\n",
            "Epoch [20/20], Step [1000/1524], Loss: 0.2191\n",
            "Epoch [20/20], Step [1020/1524], Loss: 0.2304\n",
            "Epoch [20/20], Step [1040/1524], Loss: 0.2069\n",
            "Epoch [20/20], Step [1060/1524], Loss: 0.2067\n",
            "Epoch [20/20], Step [1080/1524], Loss: 0.2651\n",
            "Epoch [20/20], Step [1100/1524], Loss: 0.2253\n",
            "Epoch [20/20], Step [1120/1524], Loss: 0.2437\n",
            "Epoch [20/20], Step [1140/1524], Loss: 0.2524\n",
            "Epoch [20/20], Step [1160/1524], Loss: 0.2367\n",
            "Epoch [20/20], Step [1180/1524], Loss: 0.2615\n",
            "Epoch [20/20], Step [1200/1524], Loss: 0.2350\n",
            "Epoch [20/20], Step [1220/1524], Loss: 0.2069\n",
            "Epoch [20/20], Step [1240/1524], Loss: 0.2006\n",
            "Epoch [20/20], Step [1260/1524], Loss: 0.2546\n",
            "Epoch [20/20], Step [1280/1524], Loss: 0.2197\n",
            "Epoch [20/20], Step [1300/1524], Loss: 0.2842\n",
            "Epoch [20/20], Step [1320/1524], Loss: 0.2537\n",
            "Epoch [20/20], Step [1340/1524], Loss: 0.2101\n",
            "Epoch [20/20], Step [1360/1524], Loss: 0.2140\n",
            "Epoch [20/20], Step [1380/1524], Loss: 0.2657\n",
            "Epoch [20/20], Step [1400/1524], Loss: 0.2583\n",
            "Epoch [20/20], Step [1420/1524], Loss: 0.2461\n",
            "Epoch [20/20], Step [1440/1524], Loss: 0.2403\n",
            "Epoch [20/20], Step [1460/1524], Loss: 0.2009\n",
            "Epoch [20/20], Step [1480/1524], Loss: 0.2390\n",
            "Epoch [20/20], Step [1500/1524], Loss: 0.2415\n",
            "Epoch [20/20], Step [1520/1524], Loss: 0.2564\n",
            "Epoch [20/20] | Train Loss: 0.2346 | Train Acc: 91.75% | Test Loss: 0.5399 | Test Acc: 78.80%\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇████</td></tr><tr><td>test_accuracy per epoch</td><td>▁▅▇▆▇▅▆▇▅▇▆▇▇█▇▆▇▅▄█</td></tr><tr><td>train_accuracy per epoch</td><td>▁▄▅▆▆▆▇▇▇▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>█▅▅▅▄▂▄▃▄▂▃▃▂▄▃▂▃▃▂▂▃▂▃▂▂▂▂▂▂▁▁▂▂▂▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>test_accuracy per epoch</td><td>78.8</td></tr><tr><td>train_accuracy per epoch</td><td>91.75395</td></tr><tr><td>train_loss</td><td>0.25645</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet-18</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ecu3dq4v' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ecu3dq4v</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT</a><br>Synced 5 W&B file(s), 2 media file(s), 5 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250401_134654-ecu3dq4v/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!wandb online\n",
        "\n",
        "# Configuration dictionary for wandb\n",
        "config = {\n",
        "    \"in_channels\": 1,\n",
        "    \"num_epochs\": 20,            # Adjust number of epochs as necessary\n",
        "    \"lr\": 0.001,                # Learning rate\n",
        "    \"batch_size\": 64,           # Batch size (should match your DataLoader)\n",
        "    \"num_blocks\": 18,            # Number of residual blocks in the network\n",
        "    \"base_channels\": 64,        # Number of filters in the first block\n",
        "    \"kernel_size\": 3,           # Kernel size for convolutions\n",
        "    \"activation\": \"LeakyReLU\",  # Name of the activation function\n",
        "    \"use_batchnorm\": True,      # Whether to use batch normalization\n",
        "    \"num_classes\": 4            # OCTMNIST has 4 classes\n",
        "}\n",
        "\n",
        "# Initialize wandb run and train the model\n",
        "with wandb.init(\n",
        "    config=config,\n",
        "    project='CNN VS ViT',   # Title of your project\n",
        "    group='Test',           # Group name for your run\n",
        "    save_code=True,\n",
        "    name='resnet-18',\n",
        "):\n",
        "    # Create the model with custom configuration\n",
        "    model = CustomResNet(\n",
        "        in_channels=config['in_channels'],  # OCTMNIST images are single-channel if you have preprocessed accordingly\n",
        "        num_blocks=config[\"num_blocks\"],\n",
        "        base_channels=config[\"base_channels\"],\n",
        "        kernel_size=config[\"kernel_size\"],\n",
        "        activation=nn.LeakyReLU(0.1, inplace=True),\n",
        "        use_batchnorm=config[\"use_batchnorm\"],\n",
        "        num_classes=config[\"num_classes\"]\n",
        "    )\n",
        "    print(model)\n",
        "\n",
        "    # Train the model (train_loader and test_loader should be defined before)\n",
        "    train_model(model, config, \"simple resnet\")\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jVPUW1Hm6Pt"
      },
      "source": [
        "### With data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "NQDXosASm83t",
        "outputId": "8dbaa926-7d1c-416b-f237-8cb76225ff5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54.9M/54.9M [00:04<00:00, 13.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Train dataset size: 97477\n",
            "Test dataset size: 1000\n",
            "Augmented Train dataset size: 974770\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: tensor([[3],\n",
            "        [3]])\n"
          ]
        }
      ],
      "source": [
        "# Download the OCTMNIST dataset without transforms to get raw PIL images.\n",
        "raw_train_dataset = OCTMNIST(split='train', transform=None, download=True)\n",
        "test_dataset  = OCTMNIST(split='test', transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "]), download=True)\n",
        "\n",
        "print(\"Raw Train dataset size:\", len(raw_train_dataset))\n",
        "print(\"Test dataset size:\", len(test_dataset))\n",
        "\n",
        "# Define an augmentation transform.\n",
        "augmentation_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # Random horizontal flip.\n",
        "    transforms.RandomRotation(10),        # Random rotation of +/- 10 degrees.\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize for a single channel (grayscale)\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Augment every image in the raw_train_dataset 10 times.\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "n_augmented_images = 10\n",
        "for i in range(len(raw_train_dataset)):\n",
        "    img, label = raw_train_dataset[i]\n",
        "    for _ in range(n_augmented_images):\n",
        "        aug_img = augmentation_transform(img)\n",
        "        augmented_images.append(aug_img)\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "print(\"Augmented Train dataset size:\", len(augmented_images))\n",
        "# Convert lists into a dataset.\n",
        "augmented_train_dataset = TensorDataset(\n",
        "    torch.stack(augmented_images),\n",
        "    torch.from_numpy(np.array(augmented_labels))\n",
        ")\n",
        "\n",
        "# Create data loaders.\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# (Optional) Visualize a few augmented training samples.\n",
        "def imshow(img, title=None):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display the first two images from the augmented training loader.\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "imshow(torchvision.utils.make_grid(images[:2]), title=\"Sample Augmented OCTMNIST Images\")\n",
        "print(\"Labels:\", labels[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Resnet-18"
      ],
      "metadata": {
        "id": "B1jxpGATE4s-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "46XGHko71CDk",
        "outputId": "264a7d4d-1e6c-43e9-e1f9-4b2f84598f56"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250403_113835-iovrpm97</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/iovrpm97' target=\"_blank\">resnet-18 with data aug</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/iovrpm97' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/iovrpm97</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomResNet(\n",
            "  (initial_conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (initial_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  (residual_layers): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (4): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (5): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (6): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (7): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (8): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (9): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (10): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (11): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (12): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (13): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (14): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (15): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (16): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (17): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "  )\n",
            "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Epoch [1/3], Step [20/15231], Loss: 1.0709\n",
            "Epoch [1/3], Step [40/15231], Loss: 0.7821\n",
            "Epoch [1/3], Step [60/15231], Loss: 0.7217\n",
            "Epoch [1/3], Step [80/15231], Loss: 0.6780\n",
            "Epoch [1/3], Step [100/15231], Loss: 0.7268\n",
            "Epoch [1/3], Step [120/15231], Loss: 0.6957\n",
            "Epoch [1/3], Step [140/15231], Loss: 0.6486\n",
            "Epoch [1/3], Step [160/15231], Loss: 0.6874\n",
            "Epoch [1/3], Step [180/15231], Loss: 0.6438\n",
            "Epoch [1/3], Step [200/15231], Loss: 0.6198\n",
            "Epoch [1/3], Step [220/15231], Loss: 0.6489\n",
            "Epoch [1/3], Step [240/15231], Loss: 0.5852\n",
            "Epoch [1/3], Step [260/15231], Loss: 0.6189\n",
            "Epoch [1/3], Step [280/15231], Loss: 0.5892\n",
            "Epoch [1/3], Step [300/15231], Loss: 0.5870\n",
            "Epoch [1/3], Step [320/15231], Loss: 0.6005\n",
            "Epoch [1/3], Step [340/15231], Loss: 0.6163\n",
            "Epoch [1/3], Step [360/15231], Loss: 0.5978\n",
            "Epoch [1/3], Step [380/15231], Loss: 0.5764\n",
            "Epoch [1/3], Step [400/15231], Loss: 0.5512\n",
            "Epoch [1/3], Step [420/15231], Loss: 0.5384\n",
            "Epoch [1/3], Step [440/15231], Loss: 0.5672\n",
            "Epoch [1/3], Step [460/15231], Loss: 0.6057\n",
            "Epoch [1/3], Step [480/15231], Loss: 0.5422\n",
            "Epoch [1/3], Step [500/15231], Loss: 0.5263\n",
            "Epoch [1/3], Step [520/15231], Loss: 0.5356\n",
            "Epoch [1/3], Step [540/15231], Loss: 0.5091\n",
            "Epoch [1/3], Step [560/15231], Loss: 0.4971\n",
            "Epoch [1/3], Step [580/15231], Loss: 0.5432\n",
            "Epoch [1/3], Step [600/15231], Loss: 0.5303\n",
            "Epoch [1/3], Step [620/15231], Loss: 0.5282\n",
            "Epoch [1/3], Step [640/15231], Loss: 0.5059\n",
            "Epoch [1/3], Step [660/15231], Loss: 0.5412\n",
            "Epoch [1/3], Step [680/15231], Loss: 0.5336\n",
            "Epoch [1/3], Step [700/15231], Loss: 0.4960\n",
            "Epoch [1/3], Step [720/15231], Loss: 0.4835\n",
            "Epoch [1/3], Step [740/15231], Loss: 0.4829\n",
            "Epoch [1/3], Step [760/15231], Loss: 0.4973\n",
            "Epoch [1/3], Step [780/15231], Loss: 0.4843\n",
            "Epoch [1/3], Step [800/15231], Loss: 0.4672\n",
            "Epoch [1/3], Step [820/15231], Loss: 0.5251\n",
            "Epoch [1/3], Step [840/15231], Loss: 0.4737\n",
            "Epoch [1/3], Step [860/15231], Loss: 0.4476\n",
            "Epoch [1/3], Step [880/15231], Loss: 0.5140\n",
            "Epoch [1/3], Step [900/15231], Loss: 0.5011\n",
            "Epoch [1/3], Step [920/15231], Loss: 0.4448\n",
            "Epoch [1/3], Step [940/15231], Loss: 0.4604\n",
            "Epoch [1/3], Step [960/15231], Loss: 0.4889\n",
            "Epoch [1/3], Step [980/15231], Loss: 0.4960\n",
            "Epoch [1/3], Step [1000/15231], Loss: 0.4478\n",
            "Epoch [1/3], Step [1020/15231], Loss: 0.4032\n",
            "Epoch [1/3], Step [1040/15231], Loss: 0.4067\n",
            "Epoch [1/3], Step [1060/15231], Loss: 0.4427\n",
            "Epoch [1/3], Step [1080/15231], Loss: 0.4791\n",
            "Epoch [1/3], Step [1100/15231], Loss: 0.4262\n",
            "Epoch [1/3], Step [1120/15231], Loss: 0.4829\n",
            "Epoch [1/3], Step [1140/15231], Loss: 0.4933\n",
            "Epoch [1/3], Step [1160/15231], Loss: 0.4600\n",
            "Epoch [1/3], Step [1180/15231], Loss: 0.4996\n",
            "Epoch [1/3], Step [1200/15231], Loss: 0.3919\n",
            "Epoch [1/3], Step [1220/15231], Loss: 0.4718\n",
            "Epoch [1/3], Step [1240/15231], Loss: 0.3691\n",
            "Epoch [1/3], Step [1260/15231], Loss: 0.4287\n",
            "Epoch [1/3], Step [1280/15231], Loss: 0.4752\n",
            "Epoch [1/3], Step [1300/15231], Loss: 0.4665\n",
            "Epoch [1/3], Step [1320/15231], Loss: 0.4463\n",
            "Epoch [1/3], Step [1340/15231], Loss: 0.4282\n",
            "Epoch [1/3], Step [1360/15231], Loss: 0.4602\n",
            "Epoch [1/3], Step [1380/15231], Loss: 0.4533\n",
            "Epoch [1/3], Step [1400/15231], Loss: 0.4258\n",
            "Epoch [1/3], Step [1420/15231], Loss: 0.4291\n",
            "Epoch [1/3], Step [1440/15231], Loss: 0.4466\n",
            "Epoch [1/3], Step [1460/15231], Loss: 0.4314\n",
            "Epoch [1/3], Step [1480/15231], Loss: 0.4255\n",
            "Epoch [1/3], Step [1500/15231], Loss: 0.4512\n",
            "Epoch [1/3], Step [1520/15231], Loss: 0.4580\n",
            "Epoch [1/3], Step [1540/15231], Loss: 0.4607\n",
            "Epoch [1/3], Step [1560/15231], Loss: 0.4673\n",
            "Epoch [1/3], Step [1580/15231], Loss: 0.4273\n",
            "Epoch [1/3], Step [1600/15231], Loss: 0.4667\n",
            "Epoch [1/3], Step [1620/15231], Loss: 0.3934\n",
            "Epoch [1/3], Step [1640/15231], Loss: 0.3915\n",
            "Epoch [1/3], Step [1660/15231], Loss: 0.4278\n",
            "Epoch [1/3], Step [1680/15231], Loss: 0.4141\n",
            "Epoch [1/3], Step [1700/15231], Loss: 0.4566\n",
            "Epoch [1/3], Step [1720/15231], Loss: 0.4655\n",
            "Epoch [1/3], Step [1740/15231], Loss: 0.4321\n",
            "Epoch [1/3], Step [1760/15231], Loss: 0.4014\n",
            "Epoch [1/3], Step [1780/15231], Loss: 0.4481\n",
            "Epoch [1/3], Step [1800/15231], Loss: 0.4670\n",
            "Epoch [1/3], Step [1820/15231], Loss: 0.4296\n",
            "Epoch [1/3], Step [1840/15231], Loss: 0.4174\n",
            "Epoch [1/3], Step [1860/15231], Loss: 0.4269\n",
            "Epoch [1/3], Step [1880/15231], Loss: 0.3808\n",
            "Epoch [1/3], Step [1900/15231], Loss: 0.3996\n",
            "Epoch [1/3], Step [1920/15231], Loss: 0.4661\n",
            "Epoch [1/3], Step [1940/15231], Loss: 0.4055\n",
            "Epoch [1/3], Step [1960/15231], Loss: 0.4315\n",
            "Epoch [1/3], Step [1980/15231], Loss: 0.4200\n",
            "Epoch [1/3], Step [2000/15231], Loss: 0.4146\n",
            "Epoch [1/3], Step [2020/15231], Loss: 0.4438\n",
            "Epoch [1/3], Step [2040/15231], Loss: 0.4048\n",
            "Epoch [1/3], Step [2060/15231], Loss: 0.3660\n",
            "Epoch [1/3], Step [2080/15231], Loss: 0.3965\n",
            "Epoch [1/3], Step [2100/15231], Loss: 0.4416\n",
            "Epoch [1/3], Step [2120/15231], Loss: 0.4098\n",
            "Epoch [1/3], Step [2140/15231], Loss: 0.4145\n",
            "Epoch [1/3], Step [2160/15231], Loss: 0.4070\n",
            "Epoch [1/3], Step [2180/15231], Loss: 0.4216\n",
            "Epoch [1/3], Step [2200/15231], Loss: 0.4267\n",
            "Epoch [1/3], Step [2220/15231], Loss: 0.4111\n",
            "Epoch [1/3], Step [2240/15231], Loss: 0.3788\n",
            "Epoch [1/3], Step [2260/15231], Loss: 0.3795\n",
            "Epoch [1/3], Step [2280/15231], Loss: 0.4553\n",
            "Epoch [1/3], Step [2300/15231], Loss: 0.3812\n",
            "Epoch [1/3], Step [2320/15231], Loss: 0.3962\n",
            "Epoch [1/3], Step [2340/15231], Loss: 0.3950\n",
            "Epoch [1/3], Step [2360/15231], Loss: 0.3541\n",
            "Epoch [1/3], Step [2380/15231], Loss: 0.3857\n",
            "Epoch [1/3], Step [2400/15231], Loss: 0.3589\n",
            "Epoch [1/3], Step [2420/15231], Loss: 0.4188\n",
            "Epoch [1/3], Step [2440/15231], Loss: 0.3641\n",
            "Epoch [1/3], Step [2460/15231], Loss: 0.3834\n",
            "Epoch [1/3], Step [2480/15231], Loss: 0.4137\n",
            "Epoch [1/3], Step [2500/15231], Loss: 0.3591\n",
            "Epoch [1/3], Step [2520/15231], Loss: 0.3684\n",
            "Epoch [1/3], Step [2540/15231], Loss: 0.4198\n",
            "Epoch [1/3], Step [2560/15231], Loss: 0.4108\n",
            "Epoch [1/3], Step [2580/15231], Loss: 0.3838\n",
            "Epoch [1/3], Step [2600/15231], Loss: 0.3983\n",
            "Epoch [1/3], Step [2620/15231], Loss: 0.3916\n",
            "Epoch [1/3], Step [2640/15231], Loss: 0.4101\n",
            "Epoch [1/3], Step [2660/15231], Loss: 0.3840\n",
            "Epoch [1/3], Step [2680/15231], Loss: 0.4058\n",
            "Epoch [1/3], Step [2700/15231], Loss: 0.3727\n",
            "Epoch [1/3], Step [2720/15231], Loss: 0.3614\n",
            "Epoch [1/3], Step [2740/15231], Loss: 0.3583\n",
            "Epoch [1/3], Step [2760/15231], Loss: 0.3563\n",
            "Epoch [1/3], Step [2780/15231], Loss: 0.3492\n",
            "Epoch [1/3], Step [2800/15231], Loss: 0.4047\n",
            "Epoch [1/3], Step [2820/15231], Loss: 0.3996\n",
            "Epoch [1/3], Step [2840/15231], Loss: 0.3548\n",
            "Epoch [1/3], Step [2860/15231], Loss: 0.3685\n",
            "Epoch [1/3], Step [2880/15231], Loss: 0.3512\n",
            "Epoch [1/3], Step [2900/15231], Loss: 0.3625\n",
            "Epoch [1/3], Step [2920/15231], Loss: 0.3498\n",
            "Epoch [1/3], Step [2940/15231], Loss: 0.4207\n",
            "Epoch [1/3], Step [2960/15231], Loss: 0.3799\n",
            "Epoch [1/3], Step [2980/15231], Loss: 0.3850\n",
            "Epoch [1/3], Step [3000/15231], Loss: 0.3528\n",
            "Epoch [1/3], Step [3020/15231], Loss: 0.3823\n",
            "Epoch [1/3], Step [3040/15231], Loss: 0.3805\n",
            "Epoch [1/3], Step [3060/15231], Loss: 0.3764\n",
            "Epoch [1/3], Step [3080/15231], Loss: 0.3688\n",
            "Epoch [1/3], Step [3100/15231], Loss: 0.3506\n",
            "Epoch [1/3], Step [3120/15231], Loss: 0.3747\n",
            "Epoch [1/3], Step [3140/15231], Loss: 0.3542\n",
            "Epoch [1/3], Step [3160/15231], Loss: 0.3776\n",
            "Epoch [1/3], Step [3180/15231], Loss: 0.3717\n",
            "Epoch [1/3], Step [3200/15231], Loss: 0.3809\n",
            "Epoch [1/3], Step [3220/15231], Loss: 0.3633\n",
            "Epoch [1/3], Step [3240/15231], Loss: 0.3565\n",
            "Epoch [1/3], Step [3260/15231], Loss: 0.3539\n",
            "Epoch [1/3], Step [3280/15231], Loss: 0.4122\n",
            "Epoch [1/3], Step [3300/15231], Loss: 0.4077\n",
            "Epoch [1/3], Step [3320/15231], Loss: 0.3853\n",
            "Epoch [1/3], Step [3340/15231], Loss: 0.3605\n",
            "Epoch [1/3], Step [3360/15231], Loss: 0.3797\n",
            "Epoch [1/3], Step [3380/15231], Loss: 0.3639\n",
            "Epoch [1/3], Step [3400/15231], Loss: 0.3742\n",
            "Epoch [1/3], Step [3420/15231], Loss: 0.3546\n",
            "Epoch [1/3], Step [3440/15231], Loss: 0.3656\n",
            "Epoch [1/3], Step [3460/15231], Loss: 0.4325\n",
            "Epoch [1/3], Step [3480/15231], Loss: 0.3545\n",
            "Epoch [1/3], Step [3500/15231], Loss: 0.3186\n",
            "Epoch [1/3], Step [3520/15231], Loss: 0.3401\n",
            "Epoch [1/3], Step [3540/15231], Loss: 0.3266\n",
            "Epoch [1/3], Step [3560/15231], Loss: 0.3620\n",
            "Epoch [1/3], Step [3580/15231], Loss: 0.3847\n",
            "Epoch [1/3], Step [3600/15231], Loss: 0.3265\n",
            "Epoch [1/3], Step [3620/15231], Loss: 0.3574\n",
            "Epoch [1/3], Step [3640/15231], Loss: 0.3472\n",
            "Epoch [1/3], Step [3660/15231], Loss: 0.3508\n",
            "Epoch [1/3], Step [3680/15231], Loss: 0.3753\n",
            "Epoch [1/3], Step [3700/15231], Loss: 0.3489\n",
            "Epoch [1/3], Step [3720/15231], Loss: 0.3579\n",
            "Epoch [1/3], Step [3740/15231], Loss: 0.3417\n",
            "Epoch [1/3], Step [3760/15231], Loss: 0.3533\n",
            "Epoch [1/3], Step [3780/15231], Loss: 0.3595\n",
            "Epoch [1/3], Step [3800/15231], Loss: 0.4033\n",
            "Epoch [1/3], Step [3820/15231], Loss: 0.3748\n",
            "Epoch [1/3], Step [3840/15231], Loss: 0.3639\n",
            "Epoch [1/3], Step [3860/15231], Loss: 0.3819\n",
            "Epoch [1/3], Step [3880/15231], Loss: 0.3671\n",
            "Epoch [1/3], Step [3900/15231], Loss: 0.3209\n",
            "Epoch [1/3], Step [3920/15231], Loss: 0.3730\n",
            "Epoch [1/3], Step [3940/15231], Loss: 0.3603\n",
            "Epoch [1/3], Step [3960/15231], Loss: 0.3461\n",
            "Epoch [1/3], Step [3980/15231], Loss: 0.3384\n",
            "Epoch [1/3], Step [4000/15231], Loss: 0.3425\n",
            "Epoch [1/3], Step [4020/15231], Loss: 0.3550\n",
            "Epoch [1/3], Step [4040/15231], Loss: 0.3437\n",
            "Epoch [1/3], Step [4060/15231], Loss: 0.3834\n",
            "Epoch [1/3], Step [4080/15231], Loss: 0.3572\n",
            "Epoch [1/3], Step [4100/15231], Loss: 0.3666\n",
            "Epoch [1/3], Step [4120/15231], Loss: 0.3397\n",
            "Epoch [1/3], Step [4140/15231], Loss: 0.3533\n",
            "Epoch [1/3], Step [4160/15231], Loss: 0.3255\n",
            "Epoch [1/3], Step [4180/15231], Loss: 0.3600\n",
            "Epoch [1/3], Step [4200/15231], Loss: 0.3539\n",
            "Epoch [1/3], Step [4220/15231], Loss: 0.3498\n",
            "Epoch [1/3], Step [4240/15231], Loss: 0.3394\n",
            "Epoch [1/3], Step [4260/15231], Loss: 0.3167\n",
            "Epoch [1/3], Step [4280/15231], Loss: 0.3396\n",
            "Epoch [1/3], Step [4300/15231], Loss: 0.3442\n",
            "Epoch [1/3], Step [4320/15231], Loss: 0.3765\n",
            "Epoch [1/3], Step [4340/15231], Loss: 0.3283\n",
            "Epoch [1/3], Step [4360/15231], Loss: 0.3596\n",
            "Epoch [1/3], Step [4380/15231], Loss: 0.3473\n",
            "Epoch [1/3], Step [4400/15231], Loss: 0.3664\n",
            "Epoch [1/3], Step [4420/15231], Loss: 0.3670\n",
            "Epoch [1/3], Step [4440/15231], Loss: 0.3690\n",
            "Epoch [1/3], Step [4460/15231], Loss: 0.3427\n",
            "Epoch [1/3], Step [4480/15231], Loss: 0.3843\n",
            "Epoch [1/3], Step [4500/15231], Loss: 0.3391\n",
            "Epoch [1/3], Step [4520/15231], Loss: 0.3333\n",
            "Epoch [1/3], Step [4540/15231], Loss: 0.3405\n",
            "Epoch [1/3], Step [4560/15231], Loss: 0.3063\n",
            "Epoch [1/3], Step [4580/15231], Loss: 0.3578\n",
            "Epoch [1/3], Step [4600/15231], Loss: 0.3729\n",
            "Epoch [1/3], Step [4620/15231], Loss: 0.3464\n",
            "Epoch [1/3], Step [4640/15231], Loss: 0.3524\n",
            "Epoch [1/3], Step [4660/15231], Loss: 0.3085\n",
            "Epoch [1/3], Step [4680/15231], Loss: 0.3529\n",
            "Epoch [1/3], Step [4700/15231], Loss: 0.3351\n",
            "Epoch [1/3], Step [4720/15231], Loss: 0.3424\n",
            "Epoch [1/3], Step [4740/15231], Loss: 0.3418\n",
            "Epoch [1/3], Step [4760/15231], Loss: 0.3736\n",
            "Epoch [1/3], Step [4780/15231], Loss: 0.3365\n",
            "Epoch [1/3], Step [4800/15231], Loss: 0.3549\n",
            "Epoch [1/3], Step [4820/15231], Loss: 0.3551\n",
            "Epoch [1/3], Step [4840/15231], Loss: 0.3091\n",
            "Epoch [1/3], Step [4860/15231], Loss: 0.3057\n",
            "Epoch [1/3], Step [4880/15231], Loss: 0.3633\n",
            "Epoch [1/3], Step [4900/15231], Loss: 0.3569\n",
            "Epoch [1/3], Step [4920/15231], Loss: 0.3373\n",
            "Epoch [1/3], Step [4940/15231], Loss: 0.3216\n",
            "Epoch [1/3], Step [4960/15231], Loss: 0.3063\n",
            "Epoch [1/3], Step [4980/15231], Loss: 0.3797\n",
            "Epoch [1/3], Step [5000/15231], Loss: 0.3148\n",
            "Epoch [1/3], Step [5020/15231], Loss: 0.3365\n",
            "Epoch [1/3], Step [5040/15231], Loss: 0.3422\n",
            "Epoch [1/3], Step [5060/15231], Loss: 0.3267\n",
            "Epoch [1/3], Step [5080/15231], Loss: 0.3235\n",
            "Epoch [1/3], Step [5100/15231], Loss: 0.3678\n",
            "Epoch [1/3], Step [5120/15231], Loss: 0.3516\n",
            "Epoch [1/3], Step [5140/15231], Loss: 0.3378\n",
            "Epoch [1/3], Step [5160/15231], Loss: 0.3148\n",
            "Epoch [1/3], Step [5180/15231], Loss: 0.3238\n",
            "Epoch [1/3], Step [5200/15231], Loss: 0.3258\n",
            "Epoch [1/3], Step [5220/15231], Loss: 0.3209\n",
            "Epoch [1/3], Step [5240/15231], Loss: 0.3264\n",
            "Epoch [1/3], Step [5260/15231], Loss: 0.2918\n",
            "Epoch [1/3], Step [5280/15231], Loss: 0.3118\n",
            "Epoch [1/3], Step [5300/15231], Loss: 0.3461\n",
            "Epoch [1/3], Step [5320/15231], Loss: 0.2967\n",
            "Epoch [1/3], Step [5340/15231], Loss: 0.3365\n",
            "Epoch [1/3], Step [5360/15231], Loss: 0.3066\n",
            "Epoch [1/3], Step [5380/15231], Loss: 0.3187\n",
            "Epoch [1/3], Step [5400/15231], Loss: 0.3137\n",
            "Epoch [1/3], Step [5420/15231], Loss: 0.3199\n",
            "Epoch [1/3], Step [5440/15231], Loss: 0.3127\n",
            "Epoch [1/3], Step [5460/15231], Loss: 0.3710\n",
            "Epoch [1/3], Step [5480/15231], Loss: 0.3340\n",
            "Epoch [1/3], Step [5500/15231], Loss: 0.3262\n",
            "Epoch [1/3], Step [5520/15231], Loss: 0.3345\n",
            "Epoch [1/3], Step [5540/15231], Loss: 0.3291\n",
            "Epoch [1/3], Step [5560/15231], Loss: 0.3719\n",
            "Epoch [1/3], Step [5580/15231], Loss: 0.3251\n",
            "Epoch [1/3], Step [5600/15231], Loss: 0.3641\n",
            "Epoch [1/3], Step [5620/15231], Loss: 0.3382\n",
            "Epoch [1/3], Step [5640/15231], Loss: 0.3356\n",
            "Epoch [1/3], Step [5660/15231], Loss: 0.3386\n",
            "Epoch [1/3], Step [5680/15231], Loss: 0.3071\n",
            "Epoch [1/3], Step [5700/15231], Loss: 0.3293\n",
            "Epoch [1/3], Step [5720/15231], Loss: 0.3533\n",
            "Epoch [1/3], Step [5740/15231], Loss: 0.3153\n",
            "Epoch [1/3], Step [5760/15231], Loss: 0.3366\n",
            "Epoch [1/3], Step [5780/15231], Loss: 0.3388\n",
            "Epoch [1/3], Step [5800/15231], Loss: 0.3084\n",
            "Epoch [1/3], Step [5820/15231], Loss: 0.2865\n",
            "Epoch [1/3], Step [5840/15231], Loss: 0.3525\n",
            "Epoch [1/3], Step [5860/15231], Loss: 0.3293\n",
            "Epoch [1/3], Step [5880/15231], Loss: 0.3357\n",
            "Epoch [1/3], Step [5900/15231], Loss: 0.3504\n",
            "Epoch [1/3], Step [5920/15231], Loss: 0.3634\n",
            "Epoch [1/3], Step [5940/15231], Loss: 0.3197\n",
            "Epoch [1/3], Step [5960/15231], Loss: 0.3317\n",
            "Epoch [1/3], Step [5980/15231], Loss: 0.3215\n",
            "Epoch [1/3], Step [6000/15231], Loss: 0.3432\n",
            "Epoch [1/3], Step [6020/15231], Loss: 0.3148\n",
            "Epoch [1/3], Step [6040/15231], Loss: 0.3269\n",
            "Epoch [1/3], Step [6060/15231], Loss: 0.3166\n",
            "Epoch [1/3], Step [6080/15231], Loss: 0.3329\n",
            "Epoch [1/3], Step [6100/15231], Loss: 0.3036\n",
            "Epoch [1/3], Step [6120/15231], Loss: 0.3057\n",
            "Epoch [1/3], Step [6140/15231], Loss: 0.3292\n",
            "Epoch [1/3], Step [6160/15231], Loss: 0.3383\n",
            "Epoch [1/3], Step [6180/15231], Loss: 0.3192\n",
            "Epoch [1/3], Step [6200/15231], Loss: 0.2732\n",
            "Epoch [1/3], Step [6220/15231], Loss: 0.3090\n",
            "Epoch [1/3], Step [6240/15231], Loss: 0.3264\n",
            "Epoch [1/3], Step [6260/15231], Loss: 0.2988\n",
            "Epoch [1/3], Step [6280/15231], Loss: 0.2869\n",
            "Epoch [1/3], Step [6300/15231], Loss: 0.2850\n",
            "Epoch [1/3], Step [6320/15231], Loss: 0.3503\n",
            "Epoch [1/3], Step [6340/15231], Loss: 0.3286\n",
            "Epoch [1/3], Step [6360/15231], Loss: 0.3008\n",
            "Epoch [1/3], Step [6380/15231], Loss: 0.3330\n",
            "Epoch [1/3], Step [6400/15231], Loss: 0.3116\n",
            "Epoch [1/3], Step [6420/15231], Loss: 0.3131\n",
            "Epoch [1/3], Step [6440/15231], Loss: 0.3446\n",
            "Epoch [1/3], Step [6460/15231], Loss: 0.3502\n",
            "Epoch [1/3], Step [6480/15231], Loss: 0.3693\n",
            "Epoch [1/3], Step [6500/15231], Loss: 0.3183\n",
            "Epoch [1/3], Step [6520/15231], Loss: 0.3515\n",
            "Epoch [1/3], Step [6540/15231], Loss: 0.3282\n",
            "Epoch [1/3], Step [6560/15231], Loss: 0.3197\n",
            "Epoch [1/3], Step [6580/15231], Loss: 0.3002\n",
            "Epoch [1/3], Step [6600/15231], Loss: 0.3153\n",
            "Epoch [1/3], Step [6620/15231], Loss: 0.3018\n",
            "Epoch [1/3], Step [6640/15231], Loss: 0.2815\n",
            "Epoch [1/3], Step [6660/15231], Loss: 0.2993\n",
            "Epoch [1/3], Step [6680/15231], Loss: 0.3179\n",
            "Epoch [1/3], Step [6700/15231], Loss: 0.3399\n",
            "Epoch [1/3], Step [6720/15231], Loss: 0.3018\n",
            "Epoch [1/3], Step [6740/15231], Loss: 0.2744\n",
            "Epoch [1/3], Step [6760/15231], Loss: 0.3274\n",
            "Epoch [1/3], Step [6780/15231], Loss: 0.3082\n",
            "Epoch [1/3], Step [6800/15231], Loss: 0.3075\n",
            "Epoch [1/3], Step [6820/15231], Loss: 0.3279\n",
            "Epoch [1/3], Step [6840/15231], Loss: 0.3380\n",
            "Epoch [1/3], Step [6860/15231], Loss: 0.3072\n",
            "Epoch [1/3], Step [6880/15231], Loss: 0.3080\n",
            "Epoch [1/3], Step [6900/15231], Loss: 0.3379\n",
            "Epoch [1/3], Step [6920/15231], Loss: 0.3363\n",
            "Epoch [1/3], Step [6940/15231], Loss: 0.3118\n",
            "Epoch [1/3], Step [6960/15231], Loss: 0.3263\n",
            "Epoch [1/3], Step [6980/15231], Loss: 0.2885\n",
            "Epoch [1/3], Step [7000/15231], Loss: 0.3312\n",
            "Epoch [1/3], Step [7020/15231], Loss: 0.3311\n",
            "Epoch [1/3], Step [7040/15231], Loss: 0.2995\n",
            "Epoch [1/3], Step [7060/15231], Loss: 0.3169\n",
            "Epoch [1/3], Step [7080/15231], Loss: 0.3213\n",
            "Epoch [1/3], Step [7100/15231], Loss: 0.3129\n",
            "Epoch [1/3], Step [7120/15231], Loss: 0.2713\n",
            "Epoch [1/3], Step [7140/15231], Loss: 0.3084\n",
            "Epoch [1/3], Step [7160/15231], Loss: 0.3019\n",
            "Epoch [1/3], Step [7180/15231], Loss: 0.3178\n",
            "Epoch [1/3], Step [7200/15231], Loss: 0.3072\n",
            "Epoch [1/3], Step [7220/15231], Loss: 0.3246\n",
            "Epoch [1/3], Step [7240/15231], Loss: 0.2843\n",
            "Epoch [1/3], Step [7260/15231], Loss: 0.3292\n",
            "Epoch [1/3], Step [7280/15231], Loss: 0.3190\n",
            "Epoch [1/3], Step [7300/15231], Loss: 0.2962\n",
            "Epoch [1/3], Step [7320/15231], Loss: 0.3213\n",
            "Epoch [1/3], Step [7340/15231], Loss: 0.2848\n",
            "Epoch [1/3], Step [7360/15231], Loss: 0.2630\n",
            "Epoch [1/3], Step [7380/15231], Loss: 0.3363\n",
            "Epoch [1/3], Step [7400/15231], Loss: 0.3083\n",
            "Epoch [1/3], Step [7420/15231], Loss: 0.3265\n",
            "Epoch [1/3], Step [7440/15231], Loss: 0.2933\n",
            "Epoch [1/3], Step [7460/15231], Loss: 0.3699\n",
            "Epoch [1/3], Step [7480/15231], Loss: 0.3037\n",
            "Epoch [1/3], Step [7500/15231], Loss: 0.3145\n",
            "Epoch [1/3], Step [7520/15231], Loss: 0.3224\n",
            "Epoch [1/3], Step [7540/15231], Loss: 0.3211\n",
            "Epoch [1/3], Step [7560/15231], Loss: 0.3063\n",
            "Epoch [1/3], Step [7580/15231], Loss: 0.3099\n",
            "Epoch [1/3], Step [7600/15231], Loss: 0.3298\n",
            "Epoch [1/3], Step [7620/15231], Loss: 0.3375\n",
            "Epoch [1/3], Step [7640/15231], Loss: 0.3149\n",
            "Epoch [1/3], Step [7660/15231], Loss: 0.2779\n",
            "Epoch [1/3], Step [7680/15231], Loss: 0.3121\n",
            "Epoch [1/3], Step [7700/15231], Loss: 0.2859\n",
            "Epoch [1/3], Step [7720/15231], Loss: 0.3217\n",
            "Epoch [1/3], Step [7740/15231], Loss: 0.3094\n",
            "Epoch [1/3], Step [7760/15231], Loss: 0.3026\n",
            "Epoch [1/3], Step [7780/15231], Loss: 0.3204\n",
            "Epoch [1/3], Step [7800/15231], Loss: 0.2872\n",
            "Epoch [1/3], Step [7820/15231], Loss: 0.3182\n",
            "Epoch [1/3], Step [7840/15231], Loss: 0.3018\n",
            "Epoch [1/3], Step [7860/15231], Loss: 0.3120\n",
            "Epoch [1/3], Step [7880/15231], Loss: 0.3161\n",
            "Epoch [1/3], Step [7900/15231], Loss: 0.3154\n",
            "Epoch [1/3], Step [7920/15231], Loss: 0.3196\n",
            "Epoch [1/3], Step [7940/15231], Loss: 0.3170\n",
            "Epoch [1/3], Step [7960/15231], Loss: 0.3253\n",
            "Epoch [1/3], Step [7980/15231], Loss: 0.3319\n",
            "Epoch [1/3], Step [8000/15231], Loss: 0.2670\n",
            "Epoch [1/3], Step [8020/15231], Loss: 0.3022\n",
            "Epoch [1/3], Step [8040/15231], Loss: 0.3272\n",
            "Epoch [1/3], Step [8060/15231], Loss: 0.3306\n",
            "Epoch [1/3], Step [8080/15231], Loss: 0.2974\n",
            "Epoch [1/3], Step [8100/15231], Loss: 0.2994\n",
            "Epoch [1/3], Step [8120/15231], Loss: 0.3509\n",
            "Epoch [1/3], Step [8140/15231], Loss: 0.2978\n",
            "Epoch [1/3], Step [8160/15231], Loss: 0.3044\n",
            "Epoch [1/3], Step [8180/15231], Loss: 0.3256\n",
            "Epoch [1/3], Step [8200/15231], Loss: 0.2886\n",
            "Epoch [1/3], Step [8220/15231], Loss: 0.3078\n",
            "Epoch [1/3], Step [8240/15231], Loss: 0.3065\n",
            "Epoch [1/3], Step [8260/15231], Loss: 0.3446\n",
            "Epoch [1/3], Step [8280/15231], Loss: 0.3053\n",
            "Epoch [1/3], Step [8300/15231], Loss: 0.3113\n",
            "Epoch [1/3], Step [8320/15231], Loss: 0.3170\n",
            "Epoch [1/3], Step [8340/15231], Loss: 0.2912\n",
            "Epoch [1/3], Step [8360/15231], Loss: 0.3022\n",
            "Epoch [1/3], Step [8380/15231], Loss: 0.2551\n",
            "Epoch [1/3], Step [8400/15231], Loss: 0.3468\n",
            "Epoch [1/3], Step [8420/15231], Loss: 0.2796\n",
            "Epoch [1/3], Step [8440/15231], Loss: 0.3126\n",
            "Epoch [1/3], Step [8460/15231], Loss: 0.3270\n",
            "Epoch [1/3], Step [8480/15231], Loss: 0.3027\n",
            "Epoch [1/3], Step [8500/15231], Loss: 0.2955\n",
            "Epoch [1/3], Step [8520/15231], Loss: 0.3147\n",
            "Epoch [1/3], Step [8540/15231], Loss: 0.3157\n",
            "Epoch [1/3], Step [8560/15231], Loss: 0.3086\n",
            "Epoch [1/3], Step [8580/15231], Loss: 0.2775\n",
            "Epoch [1/3], Step [8600/15231], Loss: 0.3233\n",
            "Epoch [1/3], Step [8620/15231], Loss: 0.3172\n",
            "Epoch [1/3], Step [8640/15231], Loss: 0.3230\n",
            "Epoch [1/3], Step [8660/15231], Loss: 0.3026\n",
            "Epoch [1/3], Step [8680/15231], Loss: 0.2948\n",
            "Epoch [1/3], Step [8700/15231], Loss: 0.3008\n",
            "Epoch [1/3], Step [8720/15231], Loss: 0.2983\n",
            "Epoch [1/3], Step [8740/15231], Loss: 0.3538\n",
            "Epoch [1/3], Step [8760/15231], Loss: 0.2622\n",
            "Epoch [1/3], Step [8780/15231], Loss: 0.2838\n",
            "Epoch [1/3], Step [8800/15231], Loss: 0.3240\n",
            "Epoch [1/3], Step [8820/15231], Loss: 0.2981\n",
            "Epoch [1/3], Step [8840/15231], Loss: 0.3009\n",
            "Epoch [1/3], Step [8860/15231], Loss: 0.3352\n",
            "Epoch [1/3], Step [8880/15231], Loss: 0.2957\n",
            "Epoch [1/3], Step [8900/15231], Loss: 0.3024\n",
            "Epoch [1/3], Step [8920/15231], Loss: 0.3044\n",
            "Epoch [1/3], Step [8940/15231], Loss: 0.2842\n",
            "Epoch [1/3], Step [8960/15231], Loss: 0.3330\n",
            "Epoch [1/3], Step [8980/15231], Loss: 0.2981\n",
            "Epoch [1/3], Step [9000/15231], Loss: 0.3162\n",
            "Epoch [1/3], Step [9020/15231], Loss: 0.2749\n",
            "Epoch [1/3], Step [9040/15231], Loss: 0.2738\n",
            "Epoch [1/3], Step [9060/15231], Loss: 0.3363\n",
            "Epoch [1/3], Step [9080/15231], Loss: 0.2901\n",
            "Epoch [1/3], Step [9100/15231], Loss: 0.3095\n",
            "Epoch [1/3], Step [9120/15231], Loss: 0.2857\n",
            "Epoch [1/3], Step [9140/15231], Loss: 0.3227\n",
            "Epoch [1/3], Step [9160/15231], Loss: 0.2883\n",
            "Epoch [1/3], Step [9180/15231], Loss: 0.2785\n",
            "Epoch [1/3], Step [9200/15231], Loss: 0.2895\n",
            "Epoch [1/3], Step [9220/15231], Loss: 0.2665\n",
            "Epoch [1/3], Step [9240/15231], Loss: 0.3369\n",
            "Epoch [1/3], Step [9260/15231], Loss: 0.2889\n",
            "Epoch [1/3], Step [9280/15231], Loss: 0.2826\n",
            "Epoch [1/3], Step [9300/15231], Loss: 0.3183\n",
            "Epoch [1/3], Step [9320/15231], Loss: 0.2704\n",
            "Epoch [1/3], Step [9340/15231], Loss: 0.2882\n",
            "Epoch [1/3], Step [9360/15231], Loss: 0.2879\n",
            "Epoch [1/3], Step [9380/15231], Loss: 0.3185\n",
            "Epoch [1/3], Step [9400/15231], Loss: 0.3019\n",
            "Epoch [1/3], Step [9420/15231], Loss: 0.2851\n",
            "Epoch [1/3], Step [9440/15231], Loss: 0.2957\n",
            "Epoch [1/3], Step [9460/15231], Loss: 0.2643\n",
            "Epoch [1/3], Step [9480/15231], Loss: 0.3114\n",
            "Epoch [1/3], Step [9500/15231], Loss: 0.3051\n",
            "Epoch [1/3], Step [9520/15231], Loss: 0.2992\n",
            "Epoch [1/3], Step [9540/15231], Loss: 0.3194\n",
            "Epoch [1/3], Step [9560/15231], Loss: 0.3598\n",
            "Epoch [1/3], Step [9580/15231], Loss: 0.2804\n",
            "Epoch [1/3], Step [9600/15231], Loss: 0.3052\n",
            "Epoch [1/3], Step [9620/15231], Loss: 0.3128\n",
            "Epoch [1/3], Step [9640/15231], Loss: 0.2873\n",
            "Epoch [1/3], Step [9660/15231], Loss: 0.2837\n",
            "Epoch [1/3], Step [9680/15231], Loss: 0.2937\n",
            "Epoch [1/3], Step [9700/15231], Loss: 0.2831\n",
            "Epoch [1/3], Step [9720/15231], Loss: 0.3060\n",
            "Epoch [1/3], Step [9740/15231], Loss: 0.2921\n",
            "Epoch [1/3], Step [9760/15231], Loss: 0.3095\n",
            "Epoch [1/3], Step [9780/15231], Loss: 0.2834\n",
            "Epoch [1/3], Step [9800/15231], Loss: 0.2651\n",
            "Epoch [1/3], Step [9820/15231], Loss: 0.2854\n",
            "Epoch [1/3], Step [9840/15231], Loss: 0.3020\n",
            "Epoch [1/3], Step [9860/15231], Loss: 0.2790\n",
            "Epoch [1/3], Step [9880/15231], Loss: 0.2926\n",
            "Epoch [1/3], Step [9900/15231], Loss: 0.2694\n",
            "Epoch [1/3], Step [9920/15231], Loss: 0.2599\n",
            "Epoch [1/3], Step [9940/15231], Loss: 0.2913\n",
            "Epoch [1/3], Step [9960/15231], Loss: 0.3186\n",
            "Epoch [1/3], Step [9980/15231], Loss: 0.3209\n",
            "Epoch [1/3], Step [10000/15231], Loss: 0.2811\n",
            "Epoch [1/3], Step [10020/15231], Loss: 0.2916\n",
            "Epoch [1/3], Step [10040/15231], Loss: 0.2998\n",
            "Epoch [1/3], Step [10060/15231], Loss: 0.3150\n",
            "Epoch [1/3], Step [10080/15231], Loss: 0.2881\n",
            "Epoch [1/3], Step [10100/15231], Loss: 0.2923\n",
            "Epoch [1/3], Step [10120/15231], Loss: 0.3186\n",
            "Epoch [1/3], Step [10140/15231], Loss: 0.2959\n",
            "Epoch [1/3], Step [10160/15231], Loss: 0.3098\n",
            "Epoch [1/3], Step [10180/15231], Loss: 0.2774\n",
            "Epoch [1/3], Step [10200/15231], Loss: 0.2968\n",
            "Epoch [1/3], Step [10220/15231], Loss: 0.2740\n",
            "Epoch [1/3], Step [10240/15231], Loss: 0.2809\n",
            "Epoch [1/3], Step [10260/15231], Loss: 0.3085\n",
            "Epoch [1/3], Step [10280/15231], Loss: 0.3043\n",
            "Epoch [1/3], Step [10300/15231], Loss: 0.2887\n",
            "Epoch [1/3], Step [10320/15231], Loss: 0.2567\n",
            "Epoch [1/3], Step [10340/15231], Loss: 0.2859\n",
            "Epoch [1/3], Step [10360/15231], Loss: 0.3011\n",
            "Epoch [1/3], Step [10380/15231], Loss: 0.2886\n",
            "Epoch [1/3], Step [10400/15231], Loss: 0.2890\n",
            "Epoch [1/3], Step [10420/15231], Loss: 0.2513\n",
            "Epoch [1/3], Step [10440/15231], Loss: 0.3057\n",
            "Epoch [1/3], Step [10460/15231], Loss: 0.2979\n",
            "Epoch [1/3], Step [10480/15231], Loss: 0.2750\n",
            "Epoch [1/3], Step [10500/15231], Loss: 0.2384\n",
            "Epoch [1/3], Step [10520/15231], Loss: 0.2729\n",
            "Epoch [1/3], Step [10540/15231], Loss: 0.2444\n",
            "Epoch [1/3], Step [10560/15231], Loss: 0.2946\n",
            "Epoch [1/3], Step [10580/15231], Loss: 0.3043\n",
            "Epoch [1/3], Step [10600/15231], Loss: 0.2842\n",
            "Epoch [1/3], Step [10620/15231], Loss: 0.2897\n",
            "Epoch [1/3], Step [10640/15231], Loss: 0.2844\n",
            "Epoch [1/3], Step [10660/15231], Loss: 0.3000\n",
            "Epoch [1/3], Step [10680/15231], Loss: 0.2810\n",
            "Epoch [1/3], Step [10700/15231], Loss: 0.2820\n",
            "Epoch [1/3], Step [10720/15231], Loss: 0.2906\n",
            "Epoch [1/3], Step [10740/15231], Loss: 0.2725\n",
            "Epoch [1/3], Step [10760/15231], Loss: 0.3066\n",
            "Epoch [1/3], Step [10780/15231], Loss: 0.2900\n",
            "Epoch [1/3], Step [10800/15231], Loss: 0.3123\n",
            "Epoch [1/3], Step [10820/15231], Loss: 0.2675\n",
            "Epoch [1/3], Step [10840/15231], Loss: 0.2703\n",
            "Epoch [1/3], Step [10860/15231], Loss: 0.2885\n",
            "Epoch [1/3], Step [10880/15231], Loss: 0.3119\n",
            "Epoch [1/3], Step [10900/15231], Loss: 0.2913\n",
            "Epoch [1/3], Step [10920/15231], Loss: 0.3103\n",
            "Epoch [1/3], Step [10940/15231], Loss: 0.2556\n",
            "Epoch [1/3], Step [10960/15231], Loss: 0.3299\n",
            "Epoch [1/3], Step [10980/15231], Loss: 0.2799\n",
            "Epoch [1/3], Step [11000/15231], Loss: 0.2882\n",
            "Epoch [1/3], Step [11020/15231], Loss: 0.2547\n",
            "Epoch [1/3], Step [11040/15231], Loss: 0.2981\n",
            "Epoch [1/3], Step [11060/15231], Loss: 0.2849\n",
            "Epoch [1/3], Step [11080/15231], Loss: 0.2777\n",
            "Epoch [1/3], Step [11100/15231], Loss: 0.2874\n",
            "Epoch [1/3], Step [11120/15231], Loss: 0.3017\n",
            "Epoch [1/3], Step [11140/15231], Loss: 0.2617\n",
            "Epoch [1/3], Step [11160/15231], Loss: 0.2760\n",
            "Epoch [1/3], Step [11180/15231], Loss: 0.3078\n",
            "Epoch [1/3], Step [11200/15231], Loss: 0.2737\n",
            "Epoch [1/3], Step [11220/15231], Loss: 0.2935\n",
            "Epoch [1/3], Step [11240/15231], Loss: 0.3037\n",
            "Epoch [1/3], Step [11260/15231], Loss: 0.3185\n",
            "Epoch [1/3], Step [11280/15231], Loss: 0.2634\n",
            "Epoch [1/3], Step [11300/15231], Loss: 0.2980\n",
            "Epoch [1/3], Step [11320/15231], Loss: 0.3031\n",
            "Epoch [1/3], Step [11340/15231], Loss: 0.2643\n",
            "Epoch [1/3], Step [11360/15231], Loss: 0.2730\n",
            "Epoch [1/3], Step [11380/15231], Loss: 0.2896\n",
            "Epoch [1/3], Step [11400/15231], Loss: 0.2908\n",
            "Epoch [1/3], Step [11420/15231], Loss: 0.2535\n",
            "Epoch [1/3], Step [11440/15231], Loss: 0.2911\n",
            "Epoch [1/3], Step [11460/15231], Loss: 0.2829\n",
            "Epoch [1/3], Step [11480/15231], Loss: 0.3127\n",
            "Epoch [1/3], Step [11500/15231], Loss: 0.3088\n",
            "Epoch [1/3], Step [11520/15231], Loss: 0.3133\n",
            "Epoch [1/3], Step [11540/15231], Loss: 0.2750\n",
            "Epoch [1/3], Step [11560/15231], Loss: 0.2630\n",
            "Epoch [1/3], Step [11580/15231], Loss: 0.3241\n",
            "Epoch [1/3], Step [11600/15231], Loss: 0.2797\n",
            "Epoch [1/3], Step [11620/15231], Loss: 0.2956\n",
            "Epoch [1/3], Step [11640/15231], Loss: 0.2451\n",
            "Epoch [1/3], Step [11660/15231], Loss: 0.2697\n",
            "Epoch [1/3], Step [11680/15231], Loss: 0.2894\n",
            "Epoch [1/3], Step [11700/15231], Loss: 0.2793\n",
            "Epoch [1/3], Step [11720/15231], Loss: 0.2867\n",
            "Epoch [1/3], Step [11740/15231], Loss: 0.3000\n",
            "Epoch [1/3], Step [11760/15231], Loss: 0.2575\n",
            "Epoch [1/3], Step [11780/15231], Loss: 0.2858\n",
            "Epoch [1/3], Step [11800/15231], Loss: 0.2715\n",
            "Epoch [1/3], Step [11820/15231], Loss: 0.2493\n",
            "Epoch [1/3], Step [11840/15231], Loss: 0.2830\n",
            "Epoch [1/3], Step [11860/15231], Loss: 0.2711\n",
            "Epoch [1/3], Step [11880/15231], Loss: 0.2906\n",
            "Epoch [1/3], Step [11900/15231], Loss: 0.3021\n",
            "Epoch [1/3], Step [11920/15231], Loss: 0.3067\n",
            "Epoch [1/3], Step [11940/15231], Loss: 0.2835\n",
            "Epoch [1/3], Step [11960/15231], Loss: 0.2871\n",
            "Epoch [1/3], Step [11980/15231], Loss: 0.2659\n",
            "Epoch [1/3], Step [12000/15231], Loss: 0.2632\n",
            "Epoch [1/3], Step [12020/15231], Loss: 0.2809\n",
            "Epoch [1/3], Step [12040/15231], Loss: 0.2734\n",
            "Epoch [1/3], Step [12060/15231], Loss: 0.2788\n",
            "Epoch [1/3], Step [12080/15231], Loss: 0.2984\n",
            "Epoch [1/3], Step [12100/15231], Loss: 0.2694\n",
            "Epoch [1/3], Step [12120/15231], Loss: 0.2575\n",
            "Epoch [1/3], Step [12140/15231], Loss: 0.2832\n",
            "Epoch [1/3], Step [12160/15231], Loss: 0.3143\n",
            "Epoch [1/3], Step [12180/15231], Loss: 0.2745\n",
            "Epoch [1/3], Step [12200/15231], Loss: 0.2790\n",
            "Epoch [1/3], Step [12220/15231], Loss: 0.2565\n",
            "Epoch [1/3], Step [12240/15231], Loss: 0.2865\n",
            "Epoch [1/3], Step [12260/15231], Loss: 0.2962\n",
            "Epoch [1/3], Step [12280/15231], Loss: 0.2517\n",
            "Epoch [1/3], Step [12300/15231], Loss: 0.2982\n",
            "Epoch [1/3], Step [12320/15231], Loss: 0.2870\n",
            "Epoch [1/3], Step [12340/15231], Loss: 0.2584\n",
            "Epoch [1/3], Step [12360/15231], Loss: 0.2553\n",
            "Epoch [1/3], Step [12380/15231], Loss: 0.2735\n",
            "Epoch [1/3], Step [12400/15231], Loss: 0.2809\n",
            "Epoch [1/3], Step [12420/15231], Loss: 0.2833\n",
            "Epoch [1/3], Step [12440/15231], Loss: 0.2476\n",
            "Epoch [1/3], Step [12460/15231], Loss: 0.2756\n",
            "Epoch [1/3], Step [12480/15231], Loss: 0.2723\n",
            "Epoch [1/3], Step [12500/15231], Loss: 0.2930\n",
            "Epoch [1/3], Step [12520/15231], Loss: 0.3375\n",
            "Epoch [1/3], Step [12540/15231], Loss: 0.3061\n",
            "Epoch [1/3], Step [12560/15231], Loss: 0.3066\n",
            "Epoch [1/3], Step [12580/15231], Loss: 0.2558\n",
            "Epoch [1/3], Step [12600/15231], Loss: 0.2491\n",
            "Epoch [1/3], Step [12620/15231], Loss: 0.2580\n",
            "Epoch [1/3], Step [12640/15231], Loss: 0.2760\n",
            "Epoch [1/3], Step [12660/15231], Loss: 0.2515\n",
            "Epoch [1/3], Step [12680/15231], Loss: 0.2777\n",
            "Epoch [1/3], Step [12700/15231], Loss: 0.2753\n",
            "Epoch [1/3], Step [12720/15231], Loss: 0.3048\n",
            "Epoch [1/3], Step [12740/15231], Loss: 0.3107\n",
            "Epoch [1/3], Step [12760/15231], Loss: 0.2966\n",
            "Epoch [1/3], Step [12780/15231], Loss: 0.2771\n",
            "Epoch [1/3], Step [12800/15231], Loss: 0.2531\n",
            "Epoch [1/3], Step [12820/15231], Loss: 0.2533\n",
            "Epoch [1/3], Step [12840/15231], Loss: 0.2361\n",
            "Epoch [1/3], Step [12860/15231], Loss: 0.2719\n",
            "Epoch [1/3], Step [12880/15231], Loss: 0.2735\n",
            "Epoch [1/3], Step [12900/15231], Loss: 0.2706\n",
            "Epoch [1/3], Step [12920/15231], Loss: 0.2764\n",
            "Epoch [1/3], Step [12940/15231], Loss: 0.2881\n",
            "Epoch [1/3], Step [12960/15231], Loss: 0.2679\n",
            "Epoch [1/3], Step [12980/15231], Loss: 0.2616\n",
            "Epoch [1/3], Step [13000/15231], Loss: 0.2512\n",
            "Epoch [1/3], Step [13020/15231], Loss: 0.2934\n",
            "Epoch [1/3], Step [13040/15231], Loss: 0.2771\n",
            "Epoch [1/3], Step [13060/15231], Loss: 0.2659\n",
            "Epoch [1/3], Step [13080/15231], Loss: 0.2861\n",
            "Epoch [1/3], Step [13100/15231], Loss: 0.3255\n",
            "Epoch [1/3], Step [13120/15231], Loss: 0.3069\n",
            "Epoch [1/3], Step [13140/15231], Loss: 0.2494\n",
            "Epoch [1/3], Step [13160/15231], Loss: 0.2662\n",
            "Epoch [1/3], Step [13180/15231], Loss: 0.2589\n",
            "Epoch [1/3], Step [13200/15231], Loss: 0.2677\n",
            "Epoch [1/3], Step [13220/15231], Loss: 0.2617\n",
            "Epoch [1/3], Step [13240/15231], Loss: 0.2917\n",
            "Epoch [1/3], Step [13260/15231], Loss: 0.2875\n",
            "Epoch [1/3], Step [13280/15231], Loss: 0.2680\n",
            "Epoch [1/3], Step [13300/15231], Loss: 0.2862\n",
            "Epoch [1/3], Step [13320/15231], Loss: 0.3001\n",
            "Epoch [1/3], Step [13340/15231], Loss: 0.2357\n",
            "Epoch [1/3], Step [13360/15231], Loss: 0.3097\n",
            "Epoch [1/3], Step [13380/15231], Loss: 0.2867\n",
            "Epoch [1/3], Step [13400/15231], Loss: 0.2726\n",
            "Epoch [1/3], Step [13420/15231], Loss: 0.2947\n",
            "Epoch [1/3], Step [13440/15231], Loss: 0.2814\n",
            "Epoch [1/3], Step [13460/15231], Loss: 0.2947\n",
            "Epoch [1/3], Step [13480/15231], Loss: 0.2935\n",
            "Epoch [1/3], Step [13500/15231], Loss: 0.2951\n",
            "Epoch [1/3], Step [13520/15231], Loss: 0.2825\n",
            "Epoch [1/3], Step [13540/15231], Loss: 0.2863\n",
            "Epoch [1/3], Step [13560/15231], Loss: 0.2906\n",
            "Epoch [1/3], Step [13580/15231], Loss: 0.3065\n",
            "Epoch [1/3], Step [13600/15231], Loss: 0.2812\n",
            "Epoch [1/3], Step [13620/15231], Loss: 0.2747\n",
            "Epoch [1/3], Step [13640/15231], Loss: 0.2710\n",
            "Epoch [1/3], Step [13660/15231], Loss: 0.2673\n",
            "Epoch [1/3], Step [13680/15231], Loss: 0.2678\n",
            "Epoch [1/3], Step [13700/15231], Loss: 0.2923\n",
            "Epoch [1/3], Step [13720/15231], Loss: 0.2529\n",
            "Epoch [1/3], Step [13740/15231], Loss: 0.2582\n",
            "Epoch [1/3], Step [13760/15231], Loss: 0.2929\n",
            "Epoch [1/3], Step [13780/15231], Loss: 0.2492\n",
            "Epoch [1/3], Step [13800/15231], Loss: 0.2464\n",
            "Epoch [1/3], Step [13820/15231], Loss: 0.2629\n",
            "Epoch [1/3], Step [13840/15231], Loss: 0.2579\n",
            "Epoch [1/3], Step [13860/15231], Loss: 0.2803\n",
            "Epoch [1/3], Step [13880/15231], Loss: 0.2610\n",
            "Epoch [1/3], Step [13900/15231], Loss: 0.2761\n",
            "Epoch [1/3], Step [13920/15231], Loss: 0.2799\n",
            "Epoch [1/3], Step [13940/15231], Loss: 0.2640\n",
            "Epoch [1/3], Step [13960/15231], Loss: 0.2502\n",
            "Epoch [1/3], Step [13980/15231], Loss: 0.3106\n",
            "Epoch [1/3], Step [14000/15231], Loss: 0.2692\n",
            "Epoch [1/3], Step [14020/15231], Loss: 0.2685\n",
            "Epoch [1/3], Step [14040/15231], Loss: 0.3087\n",
            "Epoch [1/3], Step [14060/15231], Loss: 0.2643\n",
            "Epoch [1/3], Step [14080/15231], Loss: 0.2921\n",
            "Epoch [1/3], Step [14100/15231], Loss: 0.2599\n",
            "Epoch [1/3], Step [14120/15231], Loss: 0.2595\n",
            "Epoch [1/3], Step [14140/15231], Loss: 0.2773\n",
            "Epoch [1/3], Step [14160/15231], Loss: 0.2833\n",
            "Epoch [1/3], Step [14180/15231], Loss: 0.2535\n",
            "Epoch [1/3], Step [14200/15231], Loss: 0.2662\n",
            "Epoch [1/3], Step [14220/15231], Loss: 0.2587\n",
            "Epoch [1/3], Step [14240/15231], Loss: 0.2872\n",
            "Epoch [1/3], Step [14260/15231], Loss: 0.2602\n",
            "Epoch [1/3], Step [14280/15231], Loss: 0.2898\n",
            "Epoch [1/3], Step [14300/15231], Loss: 0.2555\n",
            "Epoch [1/3], Step [14320/15231], Loss: 0.2693\n",
            "Epoch [1/3], Step [14340/15231], Loss: 0.2726\n",
            "Epoch [1/3], Step [14360/15231], Loss: 0.3156\n",
            "Epoch [1/3], Step [14380/15231], Loss: 0.2706\n",
            "Epoch [1/3], Step [14400/15231], Loss: 0.2772\n",
            "Epoch [1/3], Step [14420/15231], Loss: 0.2472\n",
            "Epoch [1/3], Step [14440/15231], Loss: 0.2764\n",
            "Epoch [1/3], Step [14460/15231], Loss: 0.2544\n",
            "Epoch [1/3], Step [14480/15231], Loss: 0.3162\n",
            "Epoch [1/3], Step [14500/15231], Loss: 0.2543\n",
            "Epoch [1/3], Step [14520/15231], Loss: 0.2767\n",
            "Epoch [1/3], Step [14540/15231], Loss: 0.2356\n",
            "Epoch [1/3], Step [14560/15231], Loss: 0.2873\n",
            "Epoch [1/3], Step [14580/15231], Loss: 0.2644\n",
            "Epoch [1/3], Step [14600/15231], Loss: 0.3082\n",
            "Epoch [1/3], Step [14620/15231], Loss: 0.3000\n",
            "Epoch [1/3], Step [14640/15231], Loss: 0.2844\n",
            "Epoch [1/3], Step [14660/15231], Loss: 0.3132\n",
            "Epoch [1/3], Step [14680/15231], Loss: 0.3001\n",
            "Epoch [1/3], Step [14700/15231], Loss: 0.2598\n",
            "Epoch [1/3], Step [14720/15231], Loss: 0.2708\n",
            "Epoch [1/3], Step [14740/15231], Loss: 0.2337\n",
            "Epoch [1/3], Step [14760/15231], Loss: 0.2540\n",
            "Epoch [1/3], Step [14780/15231], Loss: 0.3130\n",
            "Epoch [1/3], Step [14800/15231], Loss: 0.2815\n",
            "Epoch [1/3], Step [14820/15231], Loss: 0.3031\n",
            "Epoch [1/3], Step [14840/15231], Loss: 0.2805\n",
            "Epoch [1/3], Step [14860/15231], Loss: 0.2626\n",
            "Epoch [1/3], Step [14880/15231], Loss: 0.3162\n",
            "Epoch [1/3], Step [14900/15231], Loss: 0.3035\n",
            "Epoch [1/3], Step [14920/15231], Loss: 0.3268\n",
            "Epoch [1/3], Step [14940/15231], Loss: 0.2553\n",
            "Epoch [1/3], Step [14960/15231], Loss: 0.2453\n",
            "Epoch [1/3], Step [14980/15231], Loss: 0.2730\n",
            "Epoch [1/3], Step [15000/15231], Loss: 0.2672\n",
            "Epoch [1/3], Step [15020/15231], Loss: 0.2728\n",
            "Epoch [1/3], Step [15040/15231], Loss: 0.2736\n",
            "Epoch [1/3], Step [15060/15231], Loss: 0.2770\n",
            "Epoch [1/3], Step [15080/15231], Loss: 0.2630\n",
            "Epoch [1/3], Step [15100/15231], Loss: 0.2817\n",
            "Epoch [1/3], Step [15120/15231], Loss: 0.2705\n",
            "Epoch [1/3], Step [15140/15231], Loss: 0.2456\n",
            "Epoch [1/3], Step [15160/15231], Loss: 0.2855\n",
            "Epoch [1/3], Step [15180/15231], Loss: 0.2675\n",
            "Epoch [1/3], Step [15200/15231], Loss: 0.2656\n",
            "Epoch [1/3], Step [15220/15231], Loss: 0.2823\n",
            "Epoch [1/3] | Train Loss: 0.3371 | Train Acc: 88.30% | Test Loss: 0.6285 | Test Acc: 79.10%\n",
            "Epoch [2/3], Step [20/15231], Loss: 0.2739\n",
            "Epoch [2/3], Step [40/15231], Loss: 0.2761\n",
            "Epoch [2/3], Step [60/15231], Loss: 0.2758\n",
            "Epoch [2/3], Step [80/15231], Loss: 0.2799\n",
            "Epoch [2/3], Step [100/15231], Loss: 0.2696\n",
            "Epoch [2/3], Step [120/15231], Loss: 0.2445\n",
            "Epoch [2/3], Step [140/15231], Loss: 0.2541\n",
            "Epoch [2/3], Step [160/15231], Loss: 0.2743\n",
            "Epoch [2/3], Step [180/15231], Loss: 0.2694\n",
            "Epoch [2/3], Step [200/15231], Loss: 0.2760\n",
            "Epoch [2/3], Step [220/15231], Loss: 0.2586\n",
            "Epoch [2/3], Step [240/15231], Loss: 0.3038\n",
            "Epoch [2/3], Step [260/15231], Loss: 0.2668\n",
            "Epoch [2/3], Step [280/15231], Loss: 0.2650\n",
            "Epoch [2/3], Step [300/15231], Loss: 0.2750\n",
            "Epoch [2/3], Step [320/15231], Loss: 0.2813\n",
            "Epoch [2/3], Step [340/15231], Loss: 0.2650\n",
            "Epoch [2/3], Step [360/15231], Loss: 0.2844\n",
            "Epoch [2/3], Step [380/15231], Loss: 0.2579\n",
            "Epoch [2/3], Step [400/15231], Loss: 0.2479\n",
            "Epoch [2/3], Step [420/15231], Loss: 0.2635\n",
            "Epoch [2/3], Step [440/15231], Loss: 0.2953\n",
            "Epoch [2/3], Step [460/15231], Loss: 0.2628\n",
            "Epoch [2/3], Step [480/15231], Loss: 0.2788\n",
            "Epoch [2/3], Step [500/15231], Loss: 0.2632\n",
            "Epoch [2/3], Step [520/15231], Loss: 0.3080\n",
            "Epoch [2/3], Step [540/15231], Loss: 0.2810\n",
            "Epoch [2/3], Step [560/15231], Loss: 0.2795\n",
            "Epoch [2/3], Step [580/15231], Loss: 0.2557\n",
            "Epoch [2/3], Step [600/15231], Loss: 0.2500\n",
            "Epoch [2/3], Step [620/15231], Loss: 0.2858\n",
            "Epoch [2/3], Step [640/15231], Loss: 0.2989\n",
            "Epoch [2/3], Step [660/15231], Loss: 0.2581\n",
            "Epoch [2/3], Step [680/15231], Loss: 0.2723\n",
            "Epoch [2/3], Step [700/15231], Loss: 0.2644\n",
            "Epoch [2/3], Step [720/15231], Loss: 0.2170\n",
            "Epoch [2/3], Step [740/15231], Loss: 0.3035\n",
            "Epoch [2/3], Step [760/15231], Loss: 0.2640\n",
            "Epoch [2/3], Step [780/15231], Loss: 0.2298\n",
            "Epoch [2/3], Step [800/15231], Loss: 0.2766\n",
            "Epoch [2/3], Step [820/15231], Loss: 0.2967\n",
            "Epoch [2/3], Step [840/15231], Loss: 0.3038\n",
            "Epoch [2/3], Step [860/15231], Loss: 0.2904\n",
            "Epoch [2/3], Step [880/15231], Loss: 0.3163\n",
            "Epoch [2/3], Step [900/15231], Loss: 0.2714\n",
            "Epoch [2/3], Step [920/15231], Loss: 0.2744\n",
            "Epoch [2/3], Step [940/15231], Loss: 0.2691\n",
            "Epoch [2/3], Step [960/15231], Loss: 0.2953\n",
            "Epoch [2/3], Step [980/15231], Loss: 0.2503\n",
            "Epoch [2/3], Step [1000/15231], Loss: 0.2758\n",
            "Epoch [2/3], Step [1020/15231], Loss: 0.2794\n",
            "Epoch [2/3], Step [1040/15231], Loss: 0.2138\n",
            "Epoch [2/3], Step [1060/15231], Loss: 0.3039\n",
            "Epoch [2/3], Step [1080/15231], Loss: 0.2768\n",
            "Epoch [2/3], Step [1100/15231], Loss: 0.3292\n",
            "Epoch [2/3], Step [1120/15231], Loss: 0.2343\n",
            "Epoch [2/3], Step [1140/15231], Loss: 0.2842\n",
            "Epoch [2/3], Step [1160/15231], Loss: 0.2542\n",
            "Epoch [2/3], Step [1180/15231], Loss: 0.3088\n",
            "Epoch [2/3], Step [1200/15231], Loss: 0.2718\n",
            "Epoch [2/3], Step [1220/15231], Loss: 0.2864\n",
            "Epoch [2/3], Step [1240/15231], Loss: 0.2366\n",
            "Epoch [2/3], Step [1260/15231], Loss: 0.2469\n",
            "Epoch [2/3], Step [1280/15231], Loss: 0.2272\n",
            "Epoch [2/3], Step [1300/15231], Loss: 0.2372\n",
            "Epoch [2/3], Step [1320/15231], Loss: 0.2656\n",
            "Epoch [2/3], Step [1340/15231], Loss: 0.2394\n",
            "Epoch [2/3], Step [1360/15231], Loss: 0.2808\n",
            "Epoch [2/3], Step [1380/15231], Loss: 0.2294\n",
            "Epoch [2/3], Step [1400/15231], Loss: 0.2700\n",
            "Epoch [2/3], Step [1420/15231], Loss: 0.2705\n",
            "Epoch [2/3], Step [1440/15231], Loss: 0.2514\n",
            "Epoch [2/3], Step [1460/15231], Loss: 0.3066\n",
            "Epoch [2/3], Step [1480/15231], Loss: 0.2851\n",
            "Epoch [2/3], Step [1500/15231], Loss: 0.2913\n",
            "Epoch [2/3], Step [1520/15231], Loss: 0.2461\n",
            "Epoch [2/3], Step [1540/15231], Loss: 0.2475\n",
            "Epoch [2/3], Step [1560/15231], Loss: 0.2795\n",
            "Epoch [2/3], Step [1580/15231], Loss: 0.2495\n",
            "Epoch [2/3], Step [1600/15231], Loss: 0.2831\n",
            "Epoch [2/3], Step [1620/15231], Loss: 0.2569\n",
            "Epoch [2/3], Step [1640/15231], Loss: 0.2618\n",
            "Epoch [2/3], Step [1660/15231], Loss: 0.2223\n",
            "Epoch [2/3], Step [1680/15231], Loss: 0.2591\n",
            "Epoch [2/3], Step [1700/15231], Loss: 0.2358\n",
            "Epoch [2/3], Step [1720/15231], Loss: 0.2352\n",
            "Epoch [2/3], Step [1740/15231], Loss: 0.2750\n",
            "Epoch [2/3], Step [1760/15231], Loss: 0.2568\n",
            "Epoch [2/3], Step [1780/15231], Loss: 0.2455\n",
            "Epoch [2/3], Step [1800/15231], Loss: 0.2572\n",
            "Epoch [2/3], Step [1820/15231], Loss: 0.2567\n",
            "Epoch [2/3], Step [1840/15231], Loss: 0.2553\n",
            "Epoch [2/3], Step [1860/15231], Loss: 0.2716\n",
            "Epoch [2/3], Step [1880/15231], Loss: 0.2625\n",
            "Epoch [2/3], Step [1900/15231], Loss: 0.2852\n",
            "Epoch [2/3], Step [1920/15231], Loss: 0.2708\n",
            "Epoch [2/3], Step [1940/15231], Loss: 0.2643\n",
            "Epoch [2/3], Step [1960/15231], Loss: 0.2904\n",
            "Epoch [2/3], Step [1980/15231], Loss: 0.2654\n",
            "Epoch [2/3], Step [2000/15231], Loss: 0.2782\n",
            "Epoch [2/3], Step [2020/15231], Loss: 0.2414\n",
            "Epoch [2/3], Step [2040/15231], Loss: 0.2891\n",
            "Epoch [2/3], Step [2060/15231], Loss: 0.2796\n",
            "Epoch [2/3], Step [2080/15231], Loss: 0.2439\n",
            "Epoch [2/3], Step [2100/15231], Loss: 0.2543\n",
            "Epoch [2/3], Step [2120/15231], Loss: 0.2688\n",
            "Epoch [2/3], Step [2140/15231], Loss: 0.2471\n",
            "Epoch [2/3], Step [2160/15231], Loss: 0.3141\n",
            "Epoch [2/3], Step [2180/15231], Loss: 0.2683\n",
            "Epoch [2/3], Step [2200/15231], Loss: 0.2484\n",
            "Epoch [2/3], Step [2220/15231], Loss: 0.2583\n",
            "Epoch [2/3], Step [2240/15231], Loss: 0.2402\n",
            "Epoch [2/3], Step [2260/15231], Loss: 0.2657\n",
            "Epoch [2/3], Step [2280/15231], Loss: 0.2256\n",
            "Epoch [2/3], Step [2300/15231], Loss: 0.2870\n",
            "Epoch [2/3], Step [2320/15231], Loss: 0.2523\n",
            "Epoch [2/3], Step [2340/15231], Loss: 0.2459\n",
            "Epoch [2/3], Step [2360/15231], Loss: 0.2463\n",
            "Epoch [2/3], Step [2380/15231], Loss: 0.2495\n",
            "Epoch [2/3], Step [2400/15231], Loss: 0.2722\n",
            "Epoch [2/3], Step [2420/15231], Loss: 0.2648\n",
            "Epoch [2/3], Step [2440/15231], Loss: 0.2440\n",
            "Epoch [2/3], Step [2460/15231], Loss: 0.2527\n",
            "Epoch [2/3], Step [2480/15231], Loss: 0.2742\n",
            "Epoch [2/3], Step [2500/15231], Loss: 0.2445\n",
            "Epoch [2/3], Step [2520/15231], Loss: 0.2924\n",
            "Epoch [2/3], Step [2540/15231], Loss: 0.2553\n",
            "Epoch [2/3], Step [2560/15231], Loss: 0.2720\n",
            "Epoch [2/3], Step [2580/15231], Loss: 0.2378\n",
            "Epoch [2/3], Step [2600/15231], Loss: 0.2830\n",
            "Epoch [2/3], Step [2620/15231], Loss: 0.2552\n",
            "Epoch [2/3], Step [2640/15231], Loss: 0.2356\n",
            "Epoch [2/3], Step [2660/15231], Loss: 0.2539\n",
            "Epoch [2/3], Step [2680/15231], Loss: 0.2726\n",
            "Epoch [2/3], Step [2700/15231], Loss: 0.2664\n",
            "Epoch [2/3], Step [2720/15231], Loss: 0.2238\n",
            "Epoch [2/3], Step [2740/15231], Loss: 0.2431\n",
            "Epoch [2/3], Step [2760/15231], Loss: 0.2391\n",
            "Epoch [2/3], Step [2780/15231], Loss: 0.2380\n",
            "Epoch [2/3], Step [2800/15231], Loss: 0.2105\n",
            "Epoch [2/3], Step [2820/15231], Loss: 0.2480\n",
            "Epoch [2/3], Step [2840/15231], Loss: 0.2360\n",
            "Epoch [2/3], Step [2860/15231], Loss: 0.2500\n",
            "Epoch [2/3], Step [2880/15231], Loss: 0.2562\n",
            "Epoch [2/3], Step [2900/15231], Loss: 0.2536\n",
            "Epoch [2/3], Step [2920/15231], Loss: 0.3028\n",
            "Epoch [2/3], Step [2940/15231], Loss: 0.2775\n",
            "Epoch [2/3], Step [2960/15231], Loss: 0.2453\n",
            "Epoch [2/3], Step [2980/15231], Loss: 0.2837\n",
            "Epoch [2/3], Step [3000/15231], Loss: 0.2404\n",
            "Epoch [2/3], Step [3020/15231], Loss: 0.2801\n",
            "Epoch [2/3], Step [3040/15231], Loss: 0.2153\n",
            "Epoch [2/3], Step [3060/15231], Loss: 0.2591\n",
            "Epoch [2/3], Step [3080/15231], Loss: 0.2828\n",
            "Epoch [2/3], Step [3100/15231], Loss: 0.2472\n",
            "Epoch [2/3], Step [3120/15231], Loss: 0.2669\n",
            "Epoch [2/3], Step [3140/15231], Loss: 0.2372\n",
            "Epoch [2/3], Step [3160/15231], Loss: 0.2747\n",
            "Epoch [2/3], Step [3180/15231], Loss: 0.2367\n",
            "Epoch [2/3], Step [3200/15231], Loss: 0.2907\n",
            "Epoch [2/3], Step [3220/15231], Loss: 0.2697\n",
            "Epoch [2/3], Step [3240/15231], Loss: 0.2271\n",
            "Epoch [2/3], Step [3260/15231], Loss: 0.2829\n",
            "Epoch [2/3], Step [3280/15231], Loss: 0.2824\n",
            "Epoch [2/3], Step [3300/15231], Loss: 0.2804\n",
            "Epoch [2/3], Step [3320/15231], Loss: 0.2590\n",
            "Epoch [2/3], Step [3340/15231], Loss: 0.2674\n",
            "Epoch [2/3], Step [3360/15231], Loss: 0.2747\n",
            "Epoch [2/3], Step [3380/15231], Loss: 0.2298\n",
            "Epoch [2/3], Step [3400/15231], Loss: 0.2415\n",
            "Epoch [2/3], Step [3420/15231], Loss: 0.2668\n",
            "Epoch [2/3], Step [3440/15231], Loss: 0.2668\n",
            "Epoch [2/3], Step [3460/15231], Loss: 0.2491\n",
            "Epoch [2/3], Step [3480/15231], Loss: 0.2267\n",
            "Epoch [2/3], Step [3500/15231], Loss: 0.2305\n",
            "Epoch [2/3], Step [3520/15231], Loss: 0.2585\n",
            "Epoch [2/3], Step [3540/15231], Loss: 0.2227\n",
            "Epoch [2/3], Step [3560/15231], Loss: 0.2435\n",
            "Epoch [2/3], Step [3580/15231], Loss: 0.2371\n",
            "Epoch [2/3], Step [3600/15231], Loss: 0.2571\n",
            "Epoch [2/3], Step [3620/15231], Loss: 0.2653\n",
            "Epoch [2/3], Step [3640/15231], Loss: 0.2544\n",
            "Epoch [2/3], Step [3660/15231], Loss: 0.2475\n",
            "Epoch [2/3], Step [3680/15231], Loss: 0.2418\n",
            "Epoch [2/3], Step [3700/15231], Loss: 0.2211\n",
            "Epoch [2/3], Step [3720/15231], Loss: 0.2268\n",
            "Epoch [2/3], Step [3740/15231], Loss: 0.2784\n",
            "Epoch [2/3], Step [3760/15231], Loss: 0.2533\n",
            "Epoch [2/3], Step [3780/15231], Loss: 0.2489\n",
            "Epoch [2/3], Step [3800/15231], Loss: 0.2801\n",
            "Epoch [2/3], Step [3820/15231], Loss: 0.2721\n",
            "Epoch [2/3], Step [3840/15231], Loss: 0.2308\n",
            "Epoch [2/3], Step [3860/15231], Loss: 0.2826\n",
            "Epoch [2/3], Step [3880/15231], Loss: 0.2877\n",
            "Epoch [2/3], Step [3900/15231], Loss: 0.2685\n",
            "Epoch [2/3], Step [3920/15231], Loss: 0.2289\n",
            "Epoch [2/3], Step [3940/15231], Loss: 0.2416\n",
            "Epoch [2/3], Step [3960/15231], Loss: 0.2278\n",
            "Epoch [2/3], Step [3980/15231], Loss: 0.2800\n",
            "Epoch [2/3], Step [4000/15231], Loss: 0.2353\n",
            "Epoch [2/3], Step [4020/15231], Loss: 0.2501\n",
            "Epoch [2/3], Step [4040/15231], Loss: 0.2638\n",
            "Epoch [2/3], Step [4060/15231], Loss: 0.2608\n",
            "Epoch [2/3], Step [4080/15231], Loss: 0.2571\n",
            "Epoch [2/3], Step [4100/15231], Loss: 0.2982\n",
            "Epoch [2/3], Step [4120/15231], Loss: 0.2429\n",
            "Epoch [2/3], Step [4140/15231], Loss: 0.3092\n",
            "Epoch [2/3], Step [4160/15231], Loss: 0.2615\n",
            "Epoch [2/3], Step [4180/15231], Loss: 0.2416\n",
            "Epoch [2/3], Step [4200/15231], Loss: 0.2478\n",
            "Epoch [2/3], Step [4220/15231], Loss: 0.2538\n",
            "Epoch [2/3], Step [4240/15231], Loss: 0.2520\n",
            "Epoch [2/3], Step [4260/15231], Loss: 0.2254\n",
            "Epoch [2/3], Step [4280/15231], Loss: 0.2600\n",
            "Epoch [2/3], Step [4300/15231], Loss: 0.2666\n",
            "Epoch [2/3], Step [4320/15231], Loss: 0.2270\n",
            "Epoch [2/3], Step [4340/15231], Loss: 0.2337\n",
            "Epoch [2/3], Step [4360/15231], Loss: 0.2813\n",
            "Epoch [2/3], Step [4380/15231], Loss: 0.2659\n",
            "Epoch [2/3], Step [4400/15231], Loss: 0.2461\n",
            "Epoch [2/3], Step [4420/15231], Loss: 0.2915\n",
            "Epoch [2/3], Step [4440/15231], Loss: 0.2464\n",
            "Epoch [2/3], Step [4460/15231], Loss: 0.2723\n",
            "Epoch [2/3], Step [4480/15231], Loss: 0.2417\n",
            "Epoch [2/3], Step [4500/15231], Loss: 0.2505\n",
            "Epoch [2/3], Step [4520/15231], Loss: 0.2638\n",
            "Epoch [2/3], Step [4540/15231], Loss: 0.2837\n",
            "Epoch [2/3], Step [4560/15231], Loss: 0.2543\n",
            "Epoch [2/3], Step [4580/15231], Loss: 0.2473\n",
            "Epoch [2/3], Step [4600/15231], Loss: 0.2215\n",
            "Epoch [2/3], Step [4620/15231], Loss: 0.2623\n",
            "Epoch [2/3], Step [4640/15231], Loss: 0.2459\n",
            "Epoch [2/3], Step [4660/15231], Loss: 0.2255\n",
            "Epoch [2/3], Step [4680/15231], Loss: 0.2476\n",
            "Epoch [2/3], Step [4700/15231], Loss: 0.2905\n",
            "Epoch [2/3], Step [4720/15231], Loss: 0.2439\n",
            "Epoch [2/3], Step [4740/15231], Loss: 0.2579\n",
            "Epoch [2/3], Step [4760/15231], Loss: 0.2506\n",
            "Epoch [2/3], Step [4780/15231], Loss: 0.2694\n",
            "Epoch [2/3], Step [4800/15231], Loss: 0.2547\n",
            "Epoch [2/3], Step [4820/15231], Loss: 0.2561\n",
            "Epoch [2/3], Step [4840/15231], Loss: 0.2639\n",
            "Epoch [2/3], Step [4860/15231], Loss: 0.2562\n",
            "Epoch [2/3], Step [4880/15231], Loss: 0.2695\n",
            "Epoch [2/3], Step [4900/15231], Loss: 0.2278\n",
            "Epoch [2/3], Step [4920/15231], Loss: 0.2600\n",
            "Epoch [2/3], Step [4940/15231], Loss: 0.2512\n",
            "Epoch [2/3], Step [4960/15231], Loss: 0.2538\n",
            "Epoch [2/3], Step [4980/15231], Loss: 0.2494\n",
            "Epoch [2/3], Step [5000/15231], Loss: 0.2718\n",
            "Epoch [2/3], Step [5020/15231], Loss: 0.2620\n",
            "Epoch [2/3], Step [5040/15231], Loss: 0.2505\n",
            "Epoch [2/3], Step [5060/15231], Loss: 0.2614\n",
            "Epoch [2/3], Step [5080/15231], Loss: 0.2422\n",
            "Epoch [2/3], Step [5100/15231], Loss: 0.2676\n",
            "Epoch [2/3], Step [5120/15231], Loss: 0.2442\n",
            "Epoch [2/3], Step [5140/15231], Loss: 0.2728\n",
            "Epoch [2/3], Step [5160/15231], Loss: 0.2421\n",
            "Epoch [2/3], Step [5180/15231], Loss: 0.2398\n",
            "Epoch [2/3], Step [5200/15231], Loss: 0.2297\n",
            "Epoch [2/3], Step [5220/15231], Loss: 0.3199\n",
            "Epoch [2/3], Step [5240/15231], Loss: 0.2626\n",
            "Epoch [2/3], Step [5260/15231], Loss: 0.2676\n",
            "Epoch [2/3], Step [5280/15231], Loss: 0.2590\n",
            "Epoch [2/3], Step [5300/15231], Loss: 0.2302\n",
            "Epoch [2/3], Step [5320/15231], Loss: 0.2430\n",
            "Epoch [2/3], Step [5340/15231], Loss: 0.2412\n",
            "Epoch [2/3], Step [5360/15231], Loss: 0.2626\n",
            "Epoch [2/3], Step [5380/15231], Loss: 0.2621\n",
            "Epoch [2/3], Step [5400/15231], Loss: 0.2218\n",
            "Epoch [2/3], Step [5420/15231], Loss: 0.2767\n",
            "Epoch [2/3], Step [5440/15231], Loss: 0.2515\n",
            "Epoch [2/3], Step [5460/15231], Loss: 0.2400\n",
            "Epoch [2/3], Step [5480/15231], Loss: 0.2766\n",
            "Epoch [2/3], Step [5500/15231], Loss: 0.2282\n",
            "Epoch [2/3], Step [5520/15231], Loss: 0.2419\n",
            "Epoch [2/3], Step [5540/15231], Loss: 0.2795\n",
            "Epoch [2/3], Step [5560/15231], Loss: 0.2536\n",
            "Epoch [2/3], Step [5580/15231], Loss: 0.2584\n",
            "Epoch [2/3], Step [5600/15231], Loss: 0.2255\n",
            "Epoch [2/3], Step [5620/15231], Loss: 0.2620\n",
            "Epoch [2/3], Step [5640/15231], Loss: 0.2411\n",
            "Epoch [2/3], Step [5660/15231], Loss: 0.2719\n",
            "Epoch [2/3], Step [5680/15231], Loss: 0.2620\n",
            "Epoch [2/3], Step [5700/15231], Loss: 0.2542\n",
            "Epoch [2/3], Step [5720/15231], Loss: 0.2376\n",
            "Epoch [2/3], Step [5740/15231], Loss: 0.2724\n",
            "Epoch [2/3], Step [5760/15231], Loss: 0.2539\n",
            "Epoch [2/3], Step [5780/15231], Loss: 0.2486\n",
            "Epoch [2/3], Step [5800/15231], Loss: 0.2345\n",
            "Epoch [2/3], Step [5820/15231], Loss: 0.2787\n",
            "Epoch [2/3], Step [5840/15231], Loss: 0.2500\n",
            "Epoch [2/3], Step [5860/15231], Loss: 0.2473\n",
            "Epoch [2/3], Step [5880/15231], Loss: 0.2376\n",
            "Epoch [2/3], Step [5900/15231], Loss: 0.2443\n",
            "Epoch [2/3], Step [5920/15231], Loss: 0.2613\n",
            "Epoch [2/3], Step [5940/15231], Loss: 0.2759\n",
            "Epoch [2/3], Step [5960/15231], Loss: 0.2354\n",
            "Epoch [2/3], Step [5980/15231], Loss: 0.2466\n",
            "Epoch [2/3], Step [6000/15231], Loss: 0.2574\n",
            "Epoch [2/3], Step [6020/15231], Loss: 0.2573\n",
            "Epoch [2/3], Step [6040/15231], Loss: 0.2588\n",
            "Epoch [2/3], Step [6060/15231], Loss: 0.2520\n",
            "Epoch [2/3], Step [6080/15231], Loss: 0.2419\n",
            "Epoch [2/3], Step [6100/15231], Loss: 0.2200\n",
            "Epoch [2/3], Step [6120/15231], Loss: 0.2370\n",
            "Epoch [2/3], Step [6140/15231], Loss: 0.2485\n",
            "Epoch [2/3], Step [6160/15231], Loss: 0.2401\n",
            "Epoch [2/3], Step [6180/15231], Loss: 0.2697\n",
            "Epoch [2/3], Step [6200/15231], Loss: 0.2348\n",
            "Epoch [2/3], Step [6220/15231], Loss: 0.2694\n",
            "Epoch [2/3], Step [6240/15231], Loss: 0.2606\n",
            "Epoch [2/3], Step [6260/15231], Loss: 0.2459\n",
            "Epoch [2/3], Step [6280/15231], Loss: 0.2638\n",
            "Epoch [2/3], Step [6300/15231], Loss: 0.2377\n",
            "Epoch [2/3], Step [6320/15231], Loss: 0.2329\n",
            "Epoch [2/3], Step [6340/15231], Loss: 0.2705\n",
            "Epoch [2/3], Step [6360/15231], Loss: 0.2385\n",
            "Epoch [2/3], Step [6380/15231], Loss: 0.2279\n",
            "Epoch [2/3], Step [6400/15231], Loss: 0.2487\n",
            "Epoch [2/3], Step [6420/15231], Loss: 0.2300\n",
            "Epoch [2/3], Step [6440/15231], Loss: 0.2542\n",
            "Epoch [2/3], Step [6460/15231], Loss: 0.2974\n",
            "Epoch [2/3], Step [6480/15231], Loss: 0.2454\n",
            "Epoch [2/3], Step [6500/15231], Loss: 0.2440\n",
            "Epoch [2/3], Step [6520/15231], Loss: 0.2448\n",
            "Epoch [2/3], Step [6540/15231], Loss: 0.2232\n",
            "Epoch [2/3], Step [6560/15231], Loss: 0.2452\n",
            "Epoch [2/3], Step [6580/15231], Loss: 0.2594\n",
            "Epoch [2/3], Step [6600/15231], Loss: 0.2671\n",
            "Epoch [2/3], Step [6620/15231], Loss: 0.2380\n",
            "Epoch [2/3], Step [6640/15231], Loss: 0.2387\n",
            "Epoch [2/3], Step [6660/15231], Loss: 0.2858\n",
            "Epoch [2/3], Step [6680/15231], Loss: 0.2522\n",
            "Epoch [2/3], Step [6700/15231], Loss: 0.2893\n",
            "Epoch [2/3], Step [6720/15231], Loss: 0.2288\n",
            "Epoch [2/3], Step [6740/15231], Loss: 0.2619\n",
            "Epoch [2/3], Step [6760/15231], Loss: 0.2707\n",
            "Epoch [2/3], Step [6780/15231], Loss: 0.2396\n",
            "Epoch [2/3], Step [6800/15231], Loss: 0.2417\n",
            "Epoch [2/3], Step [6820/15231], Loss: 0.2722\n",
            "Epoch [2/3], Step [6840/15231], Loss: 0.2594\n",
            "Epoch [2/3], Step [6860/15231], Loss: 0.2339\n",
            "Epoch [2/3], Step [6880/15231], Loss: 0.2589\n",
            "Epoch [2/3], Step [6900/15231], Loss: 0.2508\n",
            "Epoch [2/3], Step [6920/15231], Loss: 0.2095\n",
            "Epoch [2/3], Step [6940/15231], Loss: 0.2587\n",
            "Epoch [2/3], Step [6960/15231], Loss: 0.2574\n",
            "Epoch [2/3], Step [6980/15231], Loss: 0.2349\n",
            "Epoch [2/3], Step [7000/15231], Loss: 0.2519\n",
            "Epoch [2/3], Step [7020/15231], Loss: 0.2602\n",
            "Epoch [2/3], Step [7040/15231], Loss: 0.2352\n",
            "Epoch [2/3], Step [7060/15231], Loss: 0.2405\n",
            "Epoch [2/3], Step [7080/15231], Loss: 0.2624\n",
            "Epoch [2/3], Step [7100/15231], Loss: 0.2373\n",
            "Epoch [2/3], Step [7120/15231], Loss: 0.2330\n",
            "Epoch [2/3], Step [7140/15231], Loss: 0.2367\n",
            "Epoch [2/3], Step [7160/15231], Loss: 0.2413\n",
            "Epoch [2/3], Step [7180/15231], Loss: 0.2575\n",
            "Epoch [2/3], Step [7200/15231], Loss: 0.2478\n",
            "Epoch [2/3], Step [7220/15231], Loss: 0.2587\n",
            "Epoch [2/3], Step [7240/15231], Loss: 0.2473\n",
            "Epoch [2/3], Step [7260/15231], Loss: 0.2430\n",
            "Epoch [2/3], Step [7280/15231], Loss: 0.2646\n",
            "Epoch [2/3], Step [7300/15231], Loss: 0.2496\n",
            "Epoch [2/3], Step [7320/15231], Loss: 0.2779\n",
            "Epoch [2/3], Step [7340/15231], Loss: 0.2118\n",
            "Epoch [2/3], Step [7360/15231], Loss: 0.2316\n",
            "Epoch [2/3], Step [7380/15231], Loss: 0.2103\n",
            "Epoch [2/3], Step [7400/15231], Loss: 0.2403\n",
            "Epoch [2/3], Step [7420/15231], Loss: 0.2598\n",
            "Epoch [2/3], Step [7440/15231], Loss: 0.2663\n",
            "Epoch [2/3], Step [7460/15231], Loss: 0.2451\n",
            "Epoch [2/3], Step [7480/15231], Loss: 0.2520\n",
            "Epoch [2/3], Step [7500/15231], Loss: 0.2209\n",
            "Epoch [2/3], Step [7520/15231], Loss: 0.2885\n",
            "Epoch [2/3], Step [7540/15231], Loss: 0.2576\n",
            "Epoch [2/3], Step [7560/15231], Loss: 0.2692\n",
            "Epoch [2/3], Step [7580/15231], Loss: 0.2338\n",
            "Epoch [2/3], Step [7600/15231], Loss: 0.2500\n",
            "Epoch [2/3], Step [7620/15231], Loss: 0.2352\n",
            "Epoch [2/3], Step [7640/15231], Loss: 0.2231\n",
            "Epoch [2/3], Step [7660/15231], Loss: 0.2569\n",
            "Epoch [2/3], Step [7680/15231], Loss: 0.2362\n",
            "Epoch [2/3], Step [7700/15231], Loss: 0.2330\n",
            "Epoch [2/3], Step [7720/15231], Loss: 0.2221\n",
            "Epoch [2/3], Step [7740/15231], Loss: 0.2332\n",
            "Epoch [2/3], Step [7760/15231], Loss: 0.2335\n",
            "Epoch [2/3], Step [7780/15231], Loss: 0.2344\n",
            "Epoch [2/3], Step [7800/15231], Loss: 0.2282\n",
            "Epoch [2/3], Step [7820/15231], Loss: 0.2049\n",
            "Epoch [2/3], Step [7840/15231], Loss: 0.2738\n",
            "Epoch [2/3], Step [7860/15231], Loss: 0.2672\n",
            "Epoch [2/3], Step [7880/15231], Loss: 0.2501\n",
            "Epoch [2/3], Step [7900/15231], Loss: 0.2734\n",
            "Epoch [2/3], Step [7920/15231], Loss: 0.2207\n",
            "Epoch [2/3], Step [7940/15231], Loss: 0.2442\n",
            "Epoch [2/3], Step [7960/15231], Loss: 0.2878\n",
            "Epoch [2/3], Step [7980/15231], Loss: 0.2915\n",
            "Epoch [2/3], Step [8000/15231], Loss: 0.2362\n",
            "Epoch [2/3], Step [8020/15231], Loss: 0.2354\n",
            "Epoch [2/3], Step [8040/15231], Loss: 0.2688\n",
            "Epoch [2/3], Step [8060/15231], Loss: 0.2350\n",
            "Epoch [2/3], Step [8080/15231], Loss: 0.2384\n",
            "Epoch [2/3], Step [8100/15231], Loss: 0.2465\n",
            "Epoch [2/3], Step [8120/15231], Loss: 0.2768\n",
            "Epoch [2/3], Step [8140/15231], Loss: 0.2606\n",
            "Epoch [2/3], Step [8160/15231], Loss: 0.2211\n",
            "Epoch [2/3], Step [8180/15231], Loss: 0.2856\n",
            "Epoch [2/3], Step [8200/15231], Loss: 0.2492\n",
            "Epoch [2/3], Step [8220/15231], Loss: 0.2220\n",
            "Epoch [2/3], Step [8240/15231], Loss: 0.2330\n",
            "Epoch [2/3], Step [8260/15231], Loss: 0.2785\n",
            "Epoch [2/3], Step [8280/15231], Loss: 0.2595\n",
            "Epoch [2/3], Step [8300/15231], Loss: 0.2647\n",
            "Epoch [2/3], Step [8320/15231], Loss: 0.2549\n",
            "Epoch [2/3], Step [8340/15231], Loss: 0.2514\n",
            "Epoch [2/3], Step [8360/15231], Loss: 0.2118\n",
            "Epoch [2/3], Step [8380/15231], Loss: 0.2233\n",
            "Epoch [2/3], Step [8400/15231], Loss: 0.2580\n",
            "Epoch [2/3], Step [8420/15231], Loss: 0.2303\n",
            "Epoch [2/3], Step [8440/15231], Loss: 0.2110\n",
            "Epoch [2/3], Step [8460/15231], Loss: 0.2558\n",
            "Epoch [2/3], Step [8480/15231], Loss: 0.2443\n",
            "Epoch [2/3], Step [8500/15231], Loss: 0.2700\n",
            "Epoch [2/3], Step [8520/15231], Loss: 0.2431\n",
            "Epoch [2/3], Step [8540/15231], Loss: 0.2361\n",
            "Epoch [2/3], Step [8560/15231], Loss: 0.2529\n",
            "Epoch [2/3], Step [8580/15231], Loss: 0.2259\n",
            "Epoch [2/3], Step [8600/15231], Loss: 0.1990\n",
            "Epoch [2/3], Step [8620/15231], Loss: 0.2744\n",
            "Epoch [2/3], Step [8640/15231], Loss: 0.2525\n",
            "Epoch [2/3], Step [8660/15231], Loss: 0.2346\n",
            "Epoch [2/3], Step [8680/15231], Loss: 0.2355\n",
            "Epoch [2/3], Step [8700/15231], Loss: 0.2423\n",
            "Epoch [2/3], Step [8720/15231], Loss: 0.2302\n",
            "Epoch [2/3], Step [8740/15231], Loss: 0.2153\n",
            "Epoch [2/3], Step [8760/15231], Loss: 0.2440\n",
            "Epoch [2/3], Step [8780/15231], Loss: 0.2644\n",
            "Epoch [2/3], Step [8800/15231], Loss: 0.2430\n",
            "Epoch [2/3], Step [8820/15231], Loss: 0.2493\n",
            "Epoch [2/3], Step [8840/15231], Loss: 0.2496\n",
            "Epoch [2/3], Step [8860/15231], Loss: 0.2674\n",
            "Epoch [2/3], Step [8880/15231], Loss: 0.2398\n",
            "Epoch [2/3], Step [8900/15231], Loss: 0.2158\n",
            "Epoch [2/3], Step [8920/15231], Loss: 0.2250\n",
            "Epoch [2/3], Step [8940/15231], Loss: 0.2494\n",
            "Epoch [2/3], Step [8960/15231], Loss: 0.2310\n",
            "Epoch [2/3], Step [8980/15231], Loss: 0.2402\n",
            "Epoch [2/3], Step [9000/15231], Loss: 0.2313\n",
            "Epoch [2/3], Step [9020/15231], Loss: 0.2344\n",
            "Epoch [2/3], Step [9040/15231], Loss: 0.2423\n",
            "Epoch [2/3], Step [9060/15231], Loss: 0.2154\n",
            "Epoch [2/3], Step [9080/15231], Loss: 0.2735\n",
            "Epoch [2/3], Step [9100/15231], Loss: 0.2375\n",
            "Epoch [2/3], Step [9120/15231], Loss: 0.2558\n",
            "Epoch [2/3], Step [9140/15231], Loss: 0.2678\n",
            "Epoch [2/3], Step [9160/15231], Loss: 0.2614\n",
            "Epoch [2/3], Step [9180/15231], Loss: 0.2481\n",
            "Epoch [2/3], Step [9200/15231], Loss: 0.2825\n",
            "Epoch [2/3], Step [9220/15231], Loss: 0.2204\n",
            "Epoch [2/3], Step [9240/15231], Loss: 0.2328\n",
            "Epoch [2/3], Step [9260/15231], Loss: 0.2295\n",
            "Epoch [2/3], Step [9280/15231], Loss: 0.2251\n",
            "Epoch [2/3], Step [9300/15231], Loss: 0.2522\n",
            "Epoch [2/3], Step [9320/15231], Loss: 0.2491\n",
            "Epoch [2/3], Step [9340/15231], Loss: 0.2515\n",
            "Epoch [2/3], Step [9360/15231], Loss: 0.2563\n",
            "Epoch [2/3], Step [9380/15231], Loss: 0.2101\n",
            "Epoch [2/3], Step [9400/15231], Loss: 0.1919\n",
            "Epoch [2/3], Step [9420/15231], Loss: 0.2258\n",
            "Epoch [2/3], Step [9440/15231], Loss: 0.2773\n",
            "Epoch [2/3], Step [9460/15231], Loss: 0.2401\n",
            "Epoch [2/3], Step [9480/15231], Loss: 0.2349\n",
            "Epoch [2/3], Step [9500/15231], Loss: 0.2485\n",
            "Epoch [2/3], Step [9520/15231], Loss: 0.2428\n",
            "Epoch [2/3], Step [9540/15231], Loss: 0.2394\n",
            "Epoch [2/3], Step [9560/15231], Loss: 0.2376\n",
            "Epoch [2/3], Step [9580/15231], Loss: 0.2573\n",
            "Epoch [2/3], Step [9600/15231], Loss: 0.2337\n",
            "Epoch [2/3], Step [9620/15231], Loss: 0.2194\n",
            "Epoch [2/3], Step [9640/15231], Loss: 0.2169\n",
            "Epoch [2/3], Step [9660/15231], Loss: 0.2732\n",
            "Epoch [2/3], Step [9680/15231], Loss: 0.2744\n",
            "Epoch [2/3], Step [9700/15231], Loss: 0.2370\n",
            "Epoch [2/3], Step [9720/15231], Loss: 0.2542\n",
            "Epoch [2/3], Step [9740/15231], Loss: 0.2457\n",
            "Epoch [2/3], Step [9760/15231], Loss: 0.2463\n",
            "Epoch [2/3], Step [9780/15231], Loss: 0.2450\n",
            "Epoch [2/3], Step [9800/15231], Loss: 0.2263\n",
            "Epoch [2/3], Step [9820/15231], Loss: 0.2637\n",
            "Epoch [2/3], Step [9840/15231], Loss: 0.2858\n",
            "Epoch [2/3], Step [9860/15231], Loss: 0.2618\n",
            "Epoch [2/3], Step [9880/15231], Loss: 0.2368\n",
            "Epoch [2/3], Step [9900/15231], Loss: 0.2544\n",
            "Epoch [2/3], Step [9920/15231], Loss: 0.2298\n",
            "Epoch [2/3], Step [9940/15231], Loss: 0.2414\n",
            "Epoch [2/3], Step [9960/15231], Loss: 0.1986\n",
            "Epoch [2/3], Step [9980/15231], Loss: 0.2368\n",
            "Epoch [2/3], Step [10000/15231], Loss: 0.2303\n",
            "Epoch [2/3], Step [10020/15231], Loss: 0.2577\n",
            "Epoch [2/3], Step [10040/15231], Loss: 0.2381\n",
            "Epoch [2/3], Step [10060/15231], Loss: 0.2282\n",
            "Epoch [2/3], Step [10080/15231], Loss: 0.2515\n",
            "Epoch [2/3], Step [10100/15231], Loss: 0.2252\n",
            "Epoch [2/3], Step [10120/15231], Loss: 0.2242\n",
            "Epoch [2/3], Step [10140/15231], Loss: 0.2702\n",
            "Epoch [2/3], Step [10160/15231], Loss: 0.2584\n",
            "Epoch [2/3], Step [10180/15231], Loss: 0.2365\n",
            "Epoch [2/3], Step [10200/15231], Loss: 0.2400\n",
            "Epoch [2/3], Step [10220/15231], Loss: 0.2578\n",
            "Epoch [2/3], Step [10240/15231], Loss: 0.2178\n",
            "Epoch [2/3], Step [10260/15231], Loss: 0.2052\n",
            "Epoch [2/3], Step [10280/15231], Loss: 0.2575\n",
            "Epoch [2/3], Step [10300/15231], Loss: 0.2569\n",
            "Epoch [2/3], Step [10320/15231], Loss: 0.2304\n",
            "Epoch [2/3], Step [10340/15231], Loss: 0.2476\n",
            "Epoch [2/3], Step [10360/15231], Loss: 0.2258\n",
            "Epoch [2/3], Step [10380/15231], Loss: 0.2285\n",
            "Epoch [2/3], Step [10400/15231], Loss: 0.2634\n",
            "Epoch [2/3], Step [10420/15231], Loss: 0.2376\n",
            "Epoch [2/3], Step [10440/15231], Loss: 0.2726\n",
            "Epoch [2/3], Step [10460/15231], Loss: 0.2486\n",
            "Epoch [2/3], Step [10480/15231], Loss: 0.2181\n",
            "Epoch [2/3], Step [10500/15231], Loss: 0.2555\n",
            "Epoch [2/3], Step [10520/15231], Loss: 0.2300\n",
            "Epoch [2/3], Step [10540/15231], Loss: 0.2596\n",
            "Epoch [2/3], Step [10560/15231], Loss: 0.2849\n",
            "Epoch [2/3], Step [10580/15231], Loss: 0.2114\n",
            "Epoch [2/3], Step [10600/15231], Loss: 0.2318\n",
            "Epoch [2/3], Step [10620/15231], Loss: 0.2408\n",
            "Epoch [2/3], Step [10640/15231], Loss: 0.2433\n",
            "Epoch [2/3], Step [10660/15231], Loss: 0.2176\n",
            "Epoch [2/3], Step [10680/15231], Loss: 0.2385\n",
            "Epoch [2/3], Step [10700/15231], Loss: 0.2519\n",
            "Epoch [2/3], Step [10720/15231], Loss: 0.2284\n",
            "Epoch [2/3], Step [10740/15231], Loss: 0.2499\n",
            "Epoch [2/3], Step [10760/15231], Loss: 0.2328\n",
            "Epoch [2/3], Step [10780/15231], Loss: 0.2408\n",
            "Epoch [2/3], Step [10800/15231], Loss: 0.2289\n",
            "Epoch [2/3], Step [10820/15231], Loss: 0.2389\n",
            "Epoch [2/3], Step [10840/15231], Loss: 0.2319\n",
            "Epoch [2/3], Step [10860/15231], Loss: 0.2384\n",
            "Epoch [2/3], Step [10880/15231], Loss: 0.2479\n",
            "Epoch [2/3], Step [10900/15231], Loss: 0.2241\n",
            "Epoch [2/3], Step [10920/15231], Loss: 0.2472\n",
            "Epoch [2/3], Step [10940/15231], Loss: 0.2259\n",
            "Epoch [2/3], Step [10960/15231], Loss: 0.2643\n",
            "Epoch [2/3], Step [10980/15231], Loss: 0.2331\n",
            "Epoch [2/3], Step [11000/15231], Loss: 0.2151\n",
            "Epoch [2/3], Step [11020/15231], Loss: 0.2250\n",
            "Epoch [2/3], Step [11040/15231], Loss: 0.2390\n",
            "Epoch [2/3], Step [11060/15231], Loss: 0.2282\n",
            "Epoch [2/3], Step [11080/15231], Loss: 0.2387\n",
            "Epoch [2/3], Step [11100/15231], Loss: 0.2118\n",
            "Epoch [2/3], Step [11120/15231], Loss: 0.2525\n",
            "Epoch [2/3], Step [11140/15231], Loss: 0.2656\n",
            "Epoch [2/3], Step [11160/15231], Loss: 0.2205\n",
            "Epoch [2/3], Step [11180/15231], Loss: 0.2330\n",
            "Epoch [2/3], Step [11200/15231], Loss: 0.2473\n",
            "Epoch [2/3], Step [11220/15231], Loss: 0.2401\n",
            "Epoch [2/3], Step [11240/15231], Loss: 0.2128\n",
            "Epoch [2/3], Step [11260/15231], Loss: 0.2206\n",
            "Epoch [2/3], Step [11280/15231], Loss: 0.2427\n",
            "Epoch [2/3], Step [11300/15231], Loss: 0.2187\n",
            "Epoch [2/3], Step [11320/15231], Loss: 0.2367\n",
            "Epoch [2/3], Step [11340/15231], Loss: 0.2387\n",
            "Epoch [2/3], Step [11360/15231], Loss: 0.2216\n",
            "Epoch [2/3], Step [11380/15231], Loss: 0.2551\n",
            "Epoch [2/3], Step [11400/15231], Loss: 0.2314\n",
            "Epoch [2/3], Step [11420/15231], Loss: 0.2408\n",
            "Epoch [2/3], Step [11440/15231], Loss: 0.2432\n",
            "Epoch [2/3], Step [11460/15231], Loss: 0.2130\n",
            "Epoch [2/3], Step [11480/15231], Loss: 0.2362\n",
            "Epoch [2/3], Step [11500/15231], Loss: 0.1881\n",
            "Epoch [2/3], Step [11520/15231], Loss: 0.2188\n",
            "Epoch [2/3], Step [11540/15231], Loss: 0.2651\n",
            "Epoch [2/3], Step [11560/15231], Loss: 0.2650\n",
            "Epoch [2/3], Step [11580/15231], Loss: 0.2294\n",
            "Epoch [2/3], Step [11600/15231], Loss: 0.2554\n",
            "Epoch [2/3], Step [11620/15231], Loss: 0.2482\n",
            "Epoch [2/3], Step [11640/15231], Loss: 0.2506\n",
            "Epoch [2/3], Step [11660/15231], Loss: 0.2616\n",
            "Epoch [2/3], Step [11680/15231], Loss: 0.2445\n",
            "Epoch [2/3], Step [11700/15231], Loss: 0.2084\n",
            "Epoch [2/3], Step [11720/15231], Loss: 0.2500\n",
            "Epoch [2/3], Step [11740/15231], Loss: 0.2154\n",
            "Epoch [2/3], Step [11760/15231], Loss: 0.2737\n",
            "Epoch [2/3], Step [11780/15231], Loss: 0.2239\n",
            "Epoch [2/3], Step [11800/15231], Loss: 0.2459\n",
            "Epoch [2/3], Step [11820/15231], Loss: 0.2275\n",
            "Epoch [2/3], Step [11840/15231], Loss: 0.2414\n",
            "Epoch [2/3], Step [11860/15231], Loss: 0.2418\n",
            "Epoch [2/3], Step [11880/15231], Loss: 0.2443\n",
            "Epoch [2/3], Step [11900/15231], Loss: 0.2472\n",
            "Epoch [2/3], Step [11920/15231], Loss: 0.2199\n",
            "Epoch [2/3], Step [11940/15231], Loss: 0.2253\n",
            "Epoch [2/3], Step [11960/15231], Loss: 0.2183\n",
            "Epoch [2/3], Step [11980/15231], Loss: 0.2293\n",
            "Epoch [2/3], Step [12000/15231], Loss: 0.2374\n",
            "Epoch [2/3], Step [12020/15231], Loss: 0.2506\n",
            "Epoch [2/3], Step [12040/15231], Loss: 0.2252\n",
            "Epoch [2/3], Step [12060/15231], Loss: 0.2216\n",
            "Epoch [2/3], Step [12080/15231], Loss: 0.2546\n",
            "Epoch [2/3], Step [12100/15231], Loss: 0.2353\n",
            "Epoch [2/3], Step [12120/15231], Loss: 0.2153\n",
            "Epoch [2/3], Step [12140/15231], Loss: 0.2295\n",
            "Epoch [2/3], Step [12160/15231], Loss: 0.2120\n",
            "Epoch [2/3], Step [12180/15231], Loss: 0.2067\n",
            "Epoch [2/3], Step [12200/15231], Loss: 0.2275\n",
            "Epoch [2/3], Step [12220/15231], Loss: 0.2290\n",
            "Epoch [2/3], Step [12240/15231], Loss: 0.2567\n",
            "Epoch [2/3], Step [12260/15231], Loss: 0.2409\n",
            "Epoch [2/3], Step [12280/15231], Loss: 0.2485\n",
            "Epoch [2/3], Step [12300/15231], Loss: 0.2365\n",
            "Epoch [2/3], Step [12320/15231], Loss: 0.2611\n",
            "Epoch [2/3], Step [12340/15231], Loss: 0.2492\n",
            "Epoch [2/3], Step [12360/15231], Loss: 0.2474\n",
            "Epoch [2/3], Step [12380/15231], Loss: 0.2316\n",
            "Epoch [2/3], Step [12400/15231], Loss: 0.2118\n",
            "Epoch [2/3], Step [12420/15231], Loss: 0.2145\n",
            "Epoch [2/3], Step [12440/15231], Loss: 0.2296\n",
            "Epoch [2/3], Step [12460/15231], Loss: 0.2117\n",
            "Epoch [2/3], Step [12480/15231], Loss: 0.2225\n",
            "Epoch [2/3], Step [12500/15231], Loss: 0.2488\n",
            "Epoch [2/3], Step [12520/15231], Loss: 0.2155\n",
            "Epoch [2/3], Step [12540/15231], Loss: 0.2321\n",
            "Epoch [2/3], Step [12560/15231], Loss: 0.2056\n",
            "Epoch [2/3], Step [12580/15231], Loss: 0.2172\n",
            "Epoch [2/3], Step [12600/15231], Loss: 0.2488\n",
            "Epoch [2/3], Step [12620/15231], Loss: 0.2471\n",
            "Epoch [2/3], Step [12640/15231], Loss: 0.2475\n",
            "Epoch [2/3], Step [12660/15231], Loss: 0.2445\n",
            "Epoch [2/3], Step [12680/15231], Loss: 0.2510\n",
            "Epoch [2/3], Step [12700/15231], Loss: 0.2452\n",
            "Epoch [2/3], Step [12720/15231], Loss: 0.2361\n",
            "Epoch [2/3], Step [12740/15231], Loss: 0.2619\n",
            "Epoch [2/3], Step [12760/15231], Loss: 0.2084\n",
            "Epoch [2/3], Step [12780/15231], Loss: 0.2333\n",
            "Epoch [2/3], Step [12800/15231], Loss: 0.2285\n",
            "Epoch [2/3], Step [12820/15231], Loss: 0.2253\n",
            "Epoch [2/3], Step [12840/15231], Loss: 0.2487\n",
            "Epoch [2/3], Step [12860/15231], Loss: 0.2179\n",
            "Epoch [2/3], Step [12880/15231], Loss: 0.2479\n",
            "Epoch [2/3], Step [12900/15231], Loss: 0.2210\n",
            "Epoch [2/3], Step [12920/15231], Loss: 0.2059\n",
            "Epoch [2/3], Step [12940/15231], Loss: 0.2054\n",
            "Epoch [2/3], Step [12960/15231], Loss: 0.2599\n",
            "Epoch [2/3], Step [12980/15231], Loss: 0.2546\n",
            "Epoch [2/3], Step [13000/15231], Loss: 0.2412\n",
            "Epoch [2/3], Step [13020/15231], Loss: 0.2445\n",
            "Epoch [2/3], Step [13040/15231], Loss: 0.2139\n",
            "Epoch [2/3], Step [13060/15231], Loss: 0.2134\n",
            "Epoch [2/3], Step [13080/15231], Loss: 0.2402\n",
            "Epoch [2/3], Step [13100/15231], Loss: 0.2468\n",
            "Epoch [2/3], Step [13120/15231], Loss: 0.2333\n",
            "Epoch [2/3], Step [13140/15231], Loss: 0.1976\n",
            "Epoch [2/3], Step [13160/15231], Loss: 0.2396\n",
            "Epoch [2/3], Step [13180/15231], Loss: 0.2410\n",
            "Epoch [2/3], Step [13200/15231], Loss: 0.2027\n",
            "Epoch [2/3], Step [13220/15231], Loss: 0.2419\n",
            "Epoch [2/3], Step [13240/15231], Loss: 0.2012\n",
            "Epoch [2/3], Step [13260/15231], Loss: 0.2255\n",
            "Epoch [2/3], Step [13280/15231], Loss: 0.2412\n",
            "Epoch [2/3], Step [13300/15231], Loss: 0.2309\n",
            "Epoch [2/3], Step [13320/15231], Loss: 0.2464\n",
            "Epoch [2/3], Step [13340/15231], Loss: 0.2482\n",
            "Epoch [2/3], Step [13360/15231], Loss: 0.2542\n",
            "Epoch [2/3], Step [13380/15231], Loss: 0.2392\n",
            "Epoch [2/3], Step [13400/15231], Loss: 0.2143\n",
            "Epoch [2/3], Step [13420/15231], Loss: 0.2585\n",
            "Epoch [2/3], Step [13440/15231], Loss: 0.2441\n",
            "Epoch [2/3], Step [13460/15231], Loss: 0.2029\n",
            "Epoch [2/3], Step [13480/15231], Loss: 0.2324\n",
            "Epoch [2/3], Step [13500/15231], Loss: 0.2311\n",
            "Epoch [2/3], Step [13520/15231], Loss: 0.2632\n",
            "Epoch [2/3], Step [13540/15231], Loss: 0.2318\n",
            "Epoch [2/3], Step [13560/15231], Loss: 0.2539\n",
            "Epoch [2/3], Step [13580/15231], Loss: 0.2508\n",
            "Epoch [2/3], Step [13600/15231], Loss: 0.2435\n",
            "Epoch [2/3], Step [13620/15231], Loss: 0.2174\n",
            "Epoch [2/3], Step [13640/15231], Loss: 0.2138\n",
            "Epoch [2/3], Step [13660/15231], Loss: 0.2445\n",
            "Epoch [2/3], Step [13680/15231], Loss: 0.2298\n",
            "Epoch [2/3], Step [13700/15231], Loss: 0.2209\n",
            "Epoch [2/3], Step [13720/15231], Loss: 0.2119\n",
            "Epoch [2/3], Step [13740/15231], Loss: 0.2135\n",
            "Epoch [2/3], Step [13760/15231], Loss: 0.2112\n",
            "Epoch [2/3], Step [13780/15231], Loss: 0.2278\n",
            "Epoch [2/3], Step [13800/15231], Loss: 0.2597\n",
            "Epoch [2/3], Step [13820/15231], Loss: 0.2224\n",
            "Epoch [2/3], Step [13840/15231], Loss: 0.2276\n",
            "Epoch [2/3], Step [13860/15231], Loss: 0.2439\n",
            "Epoch [2/3], Step [13880/15231], Loss: 0.2479\n",
            "Epoch [2/3], Step [13900/15231], Loss: 0.2518\n",
            "Epoch [2/3], Step [13920/15231], Loss: 0.2165\n",
            "Epoch [2/3], Step [13940/15231], Loss: 0.2373\n",
            "Epoch [2/3], Step [13960/15231], Loss: 0.2382\n",
            "Epoch [2/3], Step [13980/15231], Loss: 0.2172\n",
            "Epoch [2/3], Step [14000/15231], Loss: 0.2112\n",
            "Epoch [2/3], Step [14020/15231], Loss: 0.2746\n",
            "Epoch [2/3], Step [14040/15231], Loss: 0.1993\n",
            "Epoch [2/3], Step [14060/15231], Loss: 0.2265\n",
            "Epoch [2/3], Step [14080/15231], Loss: 0.2552\n",
            "Epoch [2/3], Step [14100/15231], Loss: 0.2429\n",
            "Epoch [2/3], Step [14120/15231], Loss: 0.2196\n",
            "Epoch [2/3], Step [14140/15231], Loss: 0.2382\n",
            "Epoch [2/3], Step [14160/15231], Loss: 0.2163\n",
            "Epoch [2/3], Step [14180/15231], Loss: 0.2200\n",
            "Epoch [2/3], Step [14200/15231], Loss: 0.2300\n",
            "Epoch [2/3], Step [14220/15231], Loss: 0.2233\n",
            "Epoch [2/3], Step [14240/15231], Loss: 0.2157\n",
            "Epoch [2/3], Step [14260/15231], Loss: 0.2199\n",
            "Epoch [2/3], Step [14280/15231], Loss: 0.2152\n",
            "Epoch [2/3], Step [14300/15231], Loss: 0.2615\n",
            "Epoch [2/3], Step [14320/15231], Loss: 0.2272\n",
            "Epoch [2/3], Step [14340/15231], Loss: 0.2191\n",
            "Epoch [2/3], Step [14360/15231], Loss: 0.2402\n",
            "Epoch [2/3], Step [14380/15231], Loss: 0.2194\n",
            "Epoch [2/3], Step [14400/15231], Loss: 0.2110\n",
            "Epoch [2/3], Step [14420/15231], Loss: 0.2227\n",
            "Epoch [2/3], Step [14440/15231], Loss: 0.2698\n",
            "Epoch [2/3], Step [14460/15231], Loss: 0.2291\n",
            "Epoch [2/3], Step [14480/15231], Loss: 0.2382\n",
            "Epoch [2/3], Step [14500/15231], Loss: 0.2335\n",
            "Epoch [2/3], Step [14520/15231], Loss: 0.2235\n",
            "Epoch [2/3], Step [14540/15231], Loss: 0.2444\n",
            "Epoch [2/3], Step [14560/15231], Loss: 0.2370\n",
            "Epoch [2/3], Step [14580/15231], Loss: 0.2229\n",
            "Epoch [2/3], Step [14600/15231], Loss: 0.2555\n",
            "Epoch [2/3], Step [14620/15231], Loss: 0.2346\n",
            "Epoch [2/3], Step [14640/15231], Loss: 0.2222\n",
            "Epoch [2/3], Step [14660/15231], Loss: 0.2294\n",
            "Epoch [2/3], Step [14680/15231], Loss: 0.2140\n",
            "Epoch [2/3], Step [14700/15231], Loss: 0.2363\n",
            "Epoch [2/3], Step [14720/15231], Loss: 0.2613\n",
            "Epoch [2/3], Step [14740/15231], Loss: 0.2325\n",
            "Epoch [2/3], Step [14760/15231], Loss: 0.2609\n",
            "Epoch [2/3], Step [14780/15231], Loss: 0.2615\n",
            "Epoch [2/3], Step [14800/15231], Loss: 0.2461\n",
            "Epoch [2/3], Step [14820/15231], Loss: 0.2388\n",
            "Epoch [2/3], Step [14840/15231], Loss: 0.2181\n",
            "Epoch [2/3], Step [14860/15231], Loss: 0.2035\n",
            "Epoch [2/3], Step [14880/15231], Loss: 0.2463\n",
            "Epoch [2/3], Step [14900/15231], Loss: 0.2072\n",
            "Epoch [2/3], Step [14920/15231], Loss: 0.2258\n",
            "Epoch [2/3], Step [14940/15231], Loss: 0.2207\n",
            "Epoch [2/3], Step [14960/15231], Loss: 0.2262\n",
            "Epoch [2/3], Step [14980/15231], Loss: 0.2137\n",
            "Epoch [2/3], Step [15000/15231], Loss: 0.2445\n",
            "Epoch [2/3], Step [15020/15231], Loss: 0.2152\n",
            "Epoch [2/3], Step [15040/15231], Loss: 0.1990\n",
            "Epoch [2/3], Step [15060/15231], Loss: 0.2124\n",
            "Epoch [2/3], Step [15080/15231], Loss: 0.2181\n",
            "Epoch [2/3], Step [15100/15231], Loss: 0.2166\n",
            "Epoch [2/3], Step [15120/15231], Loss: 0.2222\n",
            "Epoch [2/3], Step [15140/15231], Loss: 0.2292\n",
            "Epoch [2/3], Step [15160/15231], Loss: 0.2311\n",
            "Epoch [2/3], Step [15180/15231], Loss: 0.2002\n",
            "Epoch [2/3], Step [15200/15231], Loss: 0.2159\n",
            "Epoch [2/3], Step [15220/15231], Loss: 0.2082\n",
            "Epoch [2/3] | Train Loss: 0.2471 | Train Acc: 91.36% | Test Loss: 0.7825 | Test Acc: 76.20%\n",
            "Epoch [3/3], Step [20/15231], Loss: 0.2268\n",
            "Epoch [3/3], Step [40/15231], Loss: 0.2290\n",
            "Epoch [3/3], Step [60/15231], Loss: 0.2394\n",
            "Epoch [3/3], Step [80/15231], Loss: 0.2245\n",
            "Epoch [3/3], Step [100/15231], Loss: 0.2288\n",
            "Epoch [3/3], Step [120/15231], Loss: 0.2769\n",
            "Epoch [3/3], Step [140/15231], Loss: 0.2326\n",
            "Epoch [3/3], Step [160/15231], Loss: 0.1995\n",
            "Epoch [3/3], Step [180/15231], Loss: 0.2270\n",
            "Epoch [3/3], Step [200/15231], Loss: 0.2089\n",
            "Epoch [3/3], Step [220/15231], Loss: 0.2048\n",
            "Epoch [3/3], Step [240/15231], Loss: 0.2019\n",
            "Epoch [3/3], Step [260/15231], Loss: 0.2457\n",
            "Epoch [3/3], Step [280/15231], Loss: 0.2351\n",
            "Epoch [3/3], Step [300/15231], Loss: 0.1927\n",
            "Epoch [3/3], Step [320/15231], Loss: 0.2260\n",
            "Epoch [3/3], Step [340/15231], Loss: 0.2168\n",
            "Epoch [3/3], Step [360/15231], Loss: 0.2231\n",
            "Epoch [3/3], Step [380/15231], Loss: 0.2250\n",
            "Epoch [3/3], Step [400/15231], Loss: 0.2152\n",
            "Epoch [3/3], Step [420/15231], Loss: 0.1753\n",
            "Epoch [3/3], Step [440/15231], Loss: 0.2458\n",
            "Epoch [3/3], Step [460/15231], Loss: 0.2317\n",
            "Epoch [3/3], Step [480/15231], Loss: 0.2170\n",
            "Epoch [3/3], Step [500/15231], Loss: 0.2137\n",
            "Epoch [3/3], Step [520/15231], Loss: 0.2797\n",
            "Epoch [3/3], Step [540/15231], Loss: 0.2315\n",
            "Epoch [3/3], Step [560/15231], Loss: 0.2092\n",
            "Epoch [3/3], Step [580/15231], Loss: 0.2323\n",
            "Epoch [3/3], Step [600/15231], Loss: 0.2223\n",
            "Epoch [3/3], Step [620/15231], Loss: 0.2353\n",
            "Epoch [3/3], Step [640/15231], Loss: 0.2160\n",
            "Epoch [3/3], Step [660/15231], Loss: 0.2408\n",
            "Epoch [3/3], Step [680/15231], Loss: 0.2285\n",
            "Epoch [3/3], Step [700/15231], Loss: 0.2340\n",
            "Epoch [3/3], Step [720/15231], Loss: 0.2104\n",
            "Epoch [3/3], Step [740/15231], Loss: 0.2200\n",
            "Epoch [3/3], Step [760/15231], Loss: 0.2061\n",
            "Epoch [3/3], Step [780/15231], Loss: 0.2186\n",
            "Epoch [3/3], Step [800/15231], Loss: 0.2396\n",
            "Epoch [3/3], Step [820/15231], Loss: 0.2310\n",
            "Epoch [3/3], Step [840/15231], Loss: 0.2344\n",
            "Epoch [3/3], Step [860/15231], Loss: 0.2021\n",
            "Epoch [3/3], Step [880/15231], Loss: 0.2056\n",
            "Epoch [3/3], Step [900/15231], Loss: 0.1920\n",
            "Epoch [3/3], Step [920/15231], Loss: 0.2188\n",
            "Epoch [3/3], Step [940/15231], Loss: 0.2153\n",
            "Epoch [3/3], Step [960/15231], Loss: 0.1996\n",
            "Epoch [3/3], Step [980/15231], Loss: 0.1865\n",
            "Epoch [3/3], Step [1000/15231], Loss: 0.2032\n",
            "Epoch [3/3], Step [1020/15231], Loss: 0.1933\n",
            "Epoch [3/3], Step [1040/15231], Loss: 0.2262\n",
            "Epoch [3/3], Step [1060/15231], Loss: 0.2376\n",
            "Epoch [3/3], Step [1080/15231], Loss: 0.2362\n",
            "Epoch [3/3], Step [1100/15231], Loss: 0.2217\n",
            "Epoch [3/3], Step [1120/15231], Loss: 0.2074\n",
            "Epoch [3/3], Step [1140/15231], Loss: 0.2157\n",
            "Epoch [3/3], Step [1160/15231], Loss: 0.2575\n",
            "Epoch [3/3], Step [1180/15231], Loss: 0.2376\n",
            "Epoch [3/3], Step [1200/15231], Loss: 0.1909\n",
            "Epoch [3/3], Step [1220/15231], Loss: 0.1906\n",
            "Epoch [3/3], Step [1240/15231], Loss: 0.2115\n",
            "Epoch [3/3], Step [1260/15231], Loss: 0.1928\n",
            "Epoch [3/3], Step [1280/15231], Loss: 0.2137\n",
            "Epoch [3/3], Step [1300/15231], Loss: 0.2204\n",
            "Epoch [3/3], Step [1320/15231], Loss: 0.2193\n",
            "Epoch [3/3], Step [1340/15231], Loss: 0.2116\n",
            "Epoch [3/3], Step [1360/15231], Loss: 0.1965\n",
            "Epoch [3/3], Step [1380/15231], Loss: 0.2058\n",
            "Epoch [3/3], Step [1400/15231], Loss: 0.2183\n",
            "Epoch [3/3], Step [1420/15231], Loss: 0.2280\n",
            "Epoch [3/3], Step [1440/15231], Loss: 0.2197\n",
            "Epoch [3/3], Step [1460/15231], Loss: 0.2571\n",
            "Epoch [3/3], Step [1480/15231], Loss: 0.2164\n",
            "Epoch [3/3], Step [1500/15231], Loss: 0.2384\n",
            "Epoch [3/3], Step [1520/15231], Loss: 0.2149\n",
            "Epoch [3/3], Step [1540/15231], Loss: 0.2065\n",
            "Epoch [3/3], Step [1560/15231], Loss: 0.1906\n",
            "Epoch [3/3], Step [1580/15231], Loss: 0.2166\n",
            "Epoch [3/3], Step [1600/15231], Loss: 0.2118\n",
            "Epoch [3/3], Step [1620/15231], Loss: 0.2469\n",
            "Epoch [3/3], Step [1640/15231], Loss: 0.2122\n",
            "Epoch [3/3], Step [1660/15231], Loss: 0.2187\n",
            "Epoch [3/3], Step [1680/15231], Loss: 0.2046\n",
            "Epoch [3/3], Step [1700/15231], Loss: 0.2114\n",
            "Epoch [3/3], Step [1720/15231], Loss: 0.2291\n",
            "Epoch [3/3], Step [1740/15231], Loss: 0.2099\n",
            "Epoch [3/3], Step [1760/15231], Loss: 0.1881\n",
            "Epoch [3/3], Step [1780/15231], Loss: 0.2187\n",
            "Epoch [3/3], Step [1800/15231], Loss: 0.2201\n",
            "Epoch [3/3], Step [1820/15231], Loss: 0.2235\n",
            "Epoch [3/3], Step [1840/15231], Loss: 0.1986\n",
            "Epoch [3/3], Step [1860/15231], Loss: 0.2385\n",
            "Epoch [3/3], Step [1880/15231], Loss: 0.2368\n",
            "Epoch [3/3], Step [1900/15231], Loss: 0.2081\n",
            "Epoch [3/3], Step [1920/15231], Loss: 0.2135\n",
            "Epoch [3/3], Step [1940/15231], Loss: 0.2512\n",
            "Epoch [3/3], Step [1960/15231], Loss: 0.2352\n",
            "Epoch [3/3], Step [1980/15231], Loss: 0.2246\n",
            "Epoch [3/3], Step [2000/15231], Loss: 0.2541\n",
            "Epoch [3/3], Step [2020/15231], Loss: 0.2298\n",
            "Epoch [3/3], Step [2040/15231], Loss: 0.2045\n",
            "Epoch [3/3], Step [2060/15231], Loss: 0.1855\n",
            "Epoch [3/3], Step [2080/15231], Loss: 0.2403\n",
            "Epoch [3/3], Step [2100/15231], Loss: 0.2200\n",
            "Epoch [3/3], Step [2120/15231], Loss: 0.2087\n",
            "Epoch [3/3], Step [2140/15231], Loss: 0.2004\n",
            "Epoch [3/3], Step [2160/15231], Loss: 0.2017\n",
            "Epoch [3/3], Step [2180/15231], Loss: 0.2674\n",
            "Epoch [3/3], Step [2200/15231], Loss: 0.2317\n",
            "Epoch [3/3], Step [2220/15231], Loss: 0.2366\n",
            "Epoch [3/3], Step [2240/15231], Loss: 0.2097\n",
            "Epoch [3/3], Step [2260/15231], Loss: 0.2236\n",
            "Epoch [3/3], Step [2280/15231], Loss: 0.2270\n",
            "Epoch [3/3], Step [2300/15231], Loss: 0.2149\n",
            "Epoch [3/3], Step [2320/15231], Loss: 0.2167\n",
            "Epoch [3/3], Step [2340/15231], Loss: 0.2197\n",
            "Epoch [3/3], Step [2360/15231], Loss: 0.2635\n",
            "Epoch [3/3], Step [2380/15231], Loss: 0.2103\n",
            "Epoch [3/3], Step [2400/15231], Loss: 0.2177\n",
            "Epoch [3/3], Step [2420/15231], Loss: 0.2096\n",
            "Epoch [3/3], Step [2440/15231], Loss: 0.2393\n",
            "Epoch [3/3], Step [2460/15231], Loss: 0.2396\n",
            "Epoch [3/3], Step [2480/15231], Loss: 0.2218\n",
            "Epoch [3/3], Step [2500/15231], Loss: 0.2522\n",
            "Epoch [3/3], Step [2520/15231], Loss: 0.2497\n",
            "Epoch [3/3], Step [2540/15231], Loss: 0.2214\n",
            "Epoch [3/3], Step [2560/15231], Loss: 0.2148\n",
            "Epoch [3/3], Step [2580/15231], Loss: 0.2140\n",
            "Epoch [3/3], Step [2600/15231], Loss: 0.2171\n",
            "Epoch [3/3], Step [2620/15231], Loss: 0.2300\n",
            "Epoch [3/3], Step [2640/15231], Loss: 0.2167\n",
            "Epoch [3/3], Step [2660/15231], Loss: 0.1970\n",
            "Epoch [3/3], Step [2680/15231], Loss: 0.2350\n",
            "Epoch [3/3], Step [2700/15231], Loss: 0.2102\n",
            "Epoch [3/3], Step [2720/15231], Loss: 0.2280\n",
            "Epoch [3/3], Step [2740/15231], Loss: 0.2188\n",
            "Epoch [3/3], Step [2760/15231], Loss: 0.2155\n",
            "Epoch [3/3], Step [2780/15231], Loss: 0.2269\n",
            "Epoch [3/3], Step [2800/15231], Loss: 0.2527\n",
            "Epoch [3/3], Step [2820/15231], Loss: 0.1981\n",
            "Epoch [3/3], Step [2840/15231], Loss: 0.2012\n",
            "Epoch [3/3], Step [2860/15231], Loss: 0.2160\n",
            "Epoch [3/3], Step [2880/15231], Loss: 0.2094\n",
            "Epoch [3/3], Step [2900/15231], Loss: 0.2428\n",
            "Epoch [3/3], Step [2920/15231], Loss: 0.2236\n",
            "Epoch [3/3], Step [2940/15231], Loss: 0.1959\n",
            "Epoch [3/3], Step [2960/15231], Loss: 0.2004\n",
            "Epoch [3/3], Step [2980/15231], Loss: 0.2153\n",
            "Epoch [3/3], Step [3000/15231], Loss: 0.2488\n",
            "Epoch [3/3], Step [3020/15231], Loss: 0.2391\n",
            "Epoch [3/3], Step [3040/15231], Loss: 0.2409\n",
            "Epoch [3/3], Step [3060/15231], Loss: 0.1880\n",
            "Epoch [3/3], Step [3080/15231], Loss: 0.2153\n",
            "Epoch [3/3], Step [3100/15231], Loss: 0.2241\n",
            "Epoch [3/3], Step [3120/15231], Loss: 0.2107\n",
            "Epoch [3/3], Step [3140/15231], Loss: 0.2314\n",
            "Epoch [3/3], Step [3160/15231], Loss: 0.1974\n",
            "Epoch [3/3], Step [3180/15231], Loss: 0.2166\n",
            "Epoch [3/3], Step [3200/15231], Loss: 0.2396\n",
            "Epoch [3/3], Step [3220/15231], Loss: 0.2275\n",
            "Epoch [3/3], Step [3240/15231], Loss: 0.2152\n",
            "Epoch [3/3], Step [3260/15231], Loss: 0.2064\n",
            "Epoch [3/3], Step [3280/15231], Loss: 0.2159\n",
            "Epoch [3/3], Step [3300/15231], Loss: 0.2288\n",
            "Epoch [3/3], Step [3320/15231], Loss: 0.2348\n",
            "Epoch [3/3], Step [3340/15231], Loss: 0.2263\n",
            "Epoch [3/3], Step [3360/15231], Loss: 0.2251\n",
            "Epoch [3/3], Step [3380/15231], Loss: 0.2166\n",
            "Epoch [3/3], Step [3400/15231], Loss: 0.2359\n",
            "Epoch [3/3], Step [3420/15231], Loss: 0.2031\n",
            "Epoch [3/3], Step [3440/15231], Loss: 0.2590\n",
            "Epoch [3/3], Step [3460/15231], Loss: 0.1856\n",
            "Epoch [3/3], Step [3480/15231], Loss: 0.2274\n",
            "Epoch [3/3], Step [3500/15231], Loss: 0.2323\n",
            "Epoch [3/3], Step [3520/15231], Loss: 0.1731\n",
            "Epoch [3/3], Step [3540/15231], Loss: 0.2072\n",
            "Epoch [3/3], Step [3560/15231], Loss: 0.2681\n",
            "Epoch [3/3], Step [3580/15231], Loss: 0.2302\n",
            "Epoch [3/3], Step [3600/15231], Loss: 0.2427\n",
            "Epoch [3/3], Step [3620/15231], Loss: 0.2134\n",
            "Epoch [3/3], Step [3640/15231], Loss: 0.2144\n",
            "Epoch [3/3], Step [3660/15231], Loss: 0.2175\n",
            "Epoch [3/3], Step [3680/15231], Loss: 0.2278\n",
            "Epoch [3/3], Step [3700/15231], Loss: 0.1898\n",
            "Epoch [3/3], Step [3720/15231], Loss: 0.2195\n",
            "Epoch [3/3], Step [3740/15231], Loss: 0.2170\n",
            "Epoch [3/3], Step [3760/15231], Loss: 0.2012\n",
            "Epoch [3/3], Step [3780/15231], Loss: 0.2070\n",
            "Epoch [3/3], Step [3800/15231], Loss: 0.2146\n",
            "Epoch [3/3], Step [3820/15231], Loss: 0.2063\n",
            "Epoch [3/3], Step [3840/15231], Loss: 0.1965\n",
            "Epoch [3/3], Step [3860/15231], Loss: 0.2095\n",
            "Epoch [3/3], Step [3880/15231], Loss: 0.2172\n",
            "Epoch [3/3], Step [3900/15231], Loss: 0.2205\n",
            "Epoch [3/3], Step [3920/15231], Loss: 0.2155\n",
            "Epoch [3/3], Step [3940/15231], Loss: 0.2341\n",
            "Epoch [3/3], Step [3960/15231], Loss: 0.2136\n",
            "Epoch [3/3], Step [3980/15231], Loss: 0.2286\n",
            "Epoch [3/3], Step [4000/15231], Loss: 0.2207\n",
            "Epoch [3/3], Step [4020/15231], Loss: 0.2267\n",
            "Epoch [3/3], Step [4040/15231], Loss: 0.2372\n",
            "Epoch [3/3], Step [4060/15231], Loss: 0.2074\n",
            "Epoch [3/3], Step [4080/15231], Loss: 0.2024\n",
            "Epoch [3/3], Step [4100/15231], Loss: 0.2146\n",
            "Epoch [3/3], Step [4120/15231], Loss: 0.2011\n",
            "Epoch [3/3], Step [4140/15231], Loss: 0.2467\n",
            "Epoch [3/3], Step [4160/15231], Loss: 0.2092\n",
            "Epoch [3/3], Step [4180/15231], Loss: 0.2192\n",
            "Epoch [3/3], Step [4200/15231], Loss: 0.2225\n",
            "Epoch [3/3], Step [4220/15231], Loss: 0.2148\n",
            "Epoch [3/3], Step [4240/15231], Loss: 0.2193\n",
            "Epoch [3/3], Step [4260/15231], Loss: 0.2551\n",
            "Epoch [3/3], Step [4280/15231], Loss: 0.2484\n",
            "Epoch [3/3], Step [4300/15231], Loss: 0.2196\n",
            "Epoch [3/3], Step [4320/15231], Loss: 0.2125\n",
            "Epoch [3/3], Step [4340/15231], Loss: 0.1995\n",
            "Epoch [3/3], Step [4360/15231], Loss: 0.2003\n",
            "Epoch [3/3], Step [4380/15231], Loss: 0.2021\n",
            "Epoch [3/3], Step [4400/15231], Loss: 0.2127\n",
            "Epoch [3/3], Step [4420/15231], Loss: 0.2583\n",
            "Epoch [3/3], Step [4440/15231], Loss: 0.2121\n",
            "Epoch [3/3], Step [4460/15231], Loss: 0.1993\n",
            "Epoch [3/3], Step [4480/15231], Loss: 0.2253\n",
            "Epoch [3/3], Step [4500/15231], Loss: 0.2538\n",
            "Epoch [3/3], Step [4520/15231], Loss: 0.2191\n",
            "Epoch [3/3], Step [4540/15231], Loss: 0.2262\n",
            "Epoch [3/3], Step [4560/15231], Loss: 0.2252\n",
            "Epoch [3/3], Step [4580/15231], Loss: 0.2318\n",
            "Epoch [3/3], Step [4600/15231], Loss: 0.2187\n",
            "Epoch [3/3], Step [4620/15231], Loss: 0.2247\n",
            "Epoch [3/3], Step [4640/15231], Loss: 0.1946\n",
            "Epoch [3/3], Step [4660/15231], Loss: 0.1967\n",
            "Epoch [3/3], Step [4680/15231], Loss: 0.2041\n",
            "Epoch [3/3], Step [4700/15231], Loss: 0.2278\n",
            "Epoch [3/3], Step [4720/15231], Loss: 0.2038\n",
            "Epoch [3/3], Step [4740/15231], Loss: 0.2239\n",
            "Epoch [3/3], Step [4760/15231], Loss: 0.2222\n",
            "Epoch [3/3], Step [4780/15231], Loss: 0.2297\n",
            "Epoch [3/3], Step [4800/15231], Loss: 0.2143\n",
            "Epoch [3/3], Step [4820/15231], Loss: 0.2300\n",
            "Epoch [3/3], Step [4840/15231], Loss: 0.2388\n",
            "Epoch [3/3], Step [4860/15231], Loss: 0.2135\n",
            "Epoch [3/3], Step [4880/15231], Loss: 0.2358\n",
            "Epoch [3/3], Step [4900/15231], Loss: 0.2117\n",
            "Epoch [3/3], Step [4920/15231], Loss: 0.2165\n",
            "Epoch [3/3], Step [4940/15231], Loss: 0.2492\n",
            "Epoch [3/3], Step [4960/15231], Loss: 0.1835\n",
            "Epoch [3/3], Step [4980/15231], Loss: 0.2180\n",
            "Epoch [3/3], Step [5000/15231], Loss: 0.2130\n",
            "Epoch [3/3], Step [5020/15231], Loss: 0.2170\n",
            "Epoch [3/3], Step [5040/15231], Loss: 0.2430\n",
            "Epoch [3/3], Step [5060/15231], Loss: 0.2102\n",
            "Epoch [3/3], Step [5080/15231], Loss: 0.2078\n",
            "Epoch [3/3], Step [5100/15231], Loss: 0.1991\n",
            "Epoch [3/3], Step [5120/15231], Loss: 0.1996\n",
            "Epoch [3/3], Step [5140/15231], Loss: 0.2595\n",
            "Epoch [3/3], Step [5160/15231], Loss: 0.2222\n",
            "Epoch [3/3], Step [5180/15231], Loss: 0.2257\n",
            "Epoch [3/3], Step [5200/15231], Loss: 0.2236\n",
            "Epoch [3/3], Step [5220/15231], Loss: 0.2077\n",
            "Epoch [3/3], Step [5240/15231], Loss: 0.2375\n",
            "Epoch [3/3], Step [5260/15231], Loss: 0.2253\n",
            "Epoch [3/3], Step [5280/15231], Loss: 0.2269\n",
            "Epoch [3/3], Step [5300/15231], Loss: 0.2299\n",
            "Epoch [3/3], Step [5320/15231], Loss: 0.2291\n",
            "Epoch [3/3], Step [5340/15231], Loss: 0.2353\n",
            "Epoch [3/3], Step [5360/15231], Loss: 0.2174\n",
            "Epoch [3/3], Step [5380/15231], Loss: 0.2273\n",
            "Epoch [3/3], Step [5400/15231], Loss: 0.2140\n",
            "Epoch [3/3], Step [5420/15231], Loss: 0.1989\n",
            "Epoch [3/3], Step [5440/15231], Loss: 0.2117\n",
            "Epoch [3/3], Step [5460/15231], Loss: 0.2429\n",
            "Epoch [3/3], Step [5480/15231], Loss: 0.2165\n",
            "Epoch [3/3], Step [5500/15231], Loss: 0.2276\n",
            "Epoch [3/3], Step [5520/15231], Loss: 0.2174\n",
            "Epoch [3/3], Step [5540/15231], Loss: 0.2258\n",
            "Epoch [3/3], Step [5560/15231], Loss: 0.2279\n",
            "Epoch [3/3], Step [5580/15231], Loss: 0.2076\n",
            "Epoch [3/3], Step [5600/15231], Loss: 0.2494\n",
            "Epoch [3/3], Step [5620/15231], Loss: 0.2432\n",
            "Epoch [3/3], Step [5640/15231], Loss: 0.1945\n",
            "Epoch [3/3], Step [5660/15231], Loss: 0.2456\n",
            "Epoch [3/3], Step [5680/15231], Loss: 0.2224\n",
            "Epoch [3/3], Step [5700/15231], Loss: 0.2187\n",
            "Epoch [3/3], Step [5720/15231], Loss: 0.2394\n",
            "Epoch [3/3], Step [5740/15231], Loss: 0.1899\n",
            "Epoch [3/3], Step [5760/15231], Loss: 0.2333\n",
            "Epoch [3/3], Step [5780/15231], Loss: 0.2269\n",
            "Epoch [3/3], Step [5800/15231], Loss: 0.2232\n",
            "Epoch [3/3], Step [5820/15231], Loss: 0.2079\n",
            "Epoch [3/3], Step [5840/15231], Loss: 0.2497\n",
            "Epoch [3/3], Step [5860/15231], Loss: 0.1981\n",
            "Epoch [3/3], Step [5880/15231], Loss: 0.1973\n",
            "Epoch [3/3], Step [5900/15231], Loss: 0.2346\n",
            "Epoch [3/3], Step [5920/15231], Loss: 0.2127\n",
            "Epoch [3/3], Step [5940/15231], Loss: 0.2247\n",
            "Epoch [3/3], Step [5960/15231], Loss: 0.2221\n",
            "Epoch [3/3], Step [5980/15231], Loss: 0.2218\n",
            "Epoch [3/3], Step [6000/15231], Loss: 0.1906\n",
            "Epoch [3/3], Step [6020/15231], Loss: 0.2051\n",
            "Epoch [3/3], Step [6040/15231], Loss: 0.2053\n",
            "Epoch [3/3], Step [6060/15231], Loss: 0.2229\n",
            "Epoch [3/3], Step [6080/15231], Loss: 0.2059\n",
            "Epoch [3/3], Step [6100/15231], Loss: 0.2157\n",
            "Epoch [3/3], Step [6120/15231], Loss: 0.2321\n",
            "Epoch [3/3], Step [6140/15231], Loss: 0.2471\n",
            "Epoch [3/3], Step [6160/15231], Loss: 0.2077\n",
            "Epoch [3/3], Step [6180/15231], Loss: 0.2265\n",
            "Epoch [3/3], Step [6200/15231], Loss: 0.2332\n",
            "Epoch [3/3], Step [6220/15231], Loss: 0.2203\n",
            "Epoch [3/3], Step [6240/15231], Loss: 0.2114\n",
            "Epoch [3/3], Step [6260/15231], Loss: 0.2048\n",
            "Epoch [3/3], Step [6280/15231], Loss: 0.2084\n",
            "Epoch [3/3], Step [6300/15231], Loss: 0.2292\n",
            "Epoch [3/3], Step [6320/15231], Loss: 0.2026\n",
            "Epoch [3/3], Step [6340/15231], Loss: 0.2201\n",
            "Epoch [3/3], Step [6360/15231], Loss: 0.2173\n",
            "Epoch [3/3], Step [6380/15231], Loss: 0.2151\n",
            "Epoch [3/3], Step [6400/15231], Loss: 0.1981\n",
            "Epoch [3/3], Step [6420/15231], Loss: 0.2116\n",
            "Epoch [3/3], Step [6440/15231], Loss: 0.2190\n",
            "Epoch [3/3], Step [6460/15231], Loss: 0.2376\n",
            "Epoch [3/3], Step [6480/15231], Loss: 0.2159\n",
            "Epoch [3/3], Step [6500/15231], Loss: 0.2311\n",
            "Epoch [3/3], Step [6520/15231], Loss: 0.2144\n",
            "Epoch [3/3], Step [6540/15231], Loss: 0.2263\n",
            "Epoch [3/3], Step [6560/15231], Loss: 0.2534\n",
            "Epoch [3/3], Step [6580/15231], Loss: 0.2342\n",
            "Epoch [3/3], Step [6600/15231], Loss: 0.2295\n",
            "Epoch [3/3], Step [6620/15231], Loss: 0.2247\n",
            "Epoch [3/3], Step [6640/15231], Loss: 0.2109\n",
            "Epoch [3/3], Step [6660/15231], Loss: 0.2254\n",
            "Epoch [3/3], Step [6680/15231], Loss: 0.2221\n",
            "Epoch [3/3], Step [6700/15231], Loss: 0.2307\n",
            "Epoch [3/3], Step [6720/15231], Loss: 0.1870\n",
            "Epoch [3/3], Step [6740/15231], Loss: 0.2064\n",
            "Epoch [3/3], Step [6760/15231], Loss: 0.2333\n",
            "Epoch [3/3], Step [6780/15231], Loss: 0.1964\n",
            "Epoch [3/3], Step [6800/15231], Loss: 0.2105\n",
            "Epoch [3/3], Step [6820/15231], Loss: 0.2172\n",
            "Epoch [3/3], Step [6840/15231], Loss: 0.2664\n",
            "Epoch [3/3], Step [6860/15231], Loss: 0.2266\n",
            "Epoch [3/3], Step [6880/15231], Loss: 0.2025\n",
            "Epoch [3/3], Step [6900/15231], Loss: 0.2264\n",
            "Epoch [3/3], Step [6920/15231], Loss: 0.2438\n",
            "Epoch [3/3], Step [6940/15231], Loss: 0.1842\n",
            "Epoch [3/3], Step [6960/15231], Loss: 0.1999\n",
            "Epoch [3/3], Step [6980/15231], Loss: 0.1994\n",
            "Epoch [3/3], Step [7000/15231], Loss: 0.2243\n",
            "Epoch [3/3], Step [7020/15231], Loss: 0.2254\n",
            "Epoch [3/3], Step [7040/15231], Loss: 0.2070\n",
            "Epoch [3/3], Step [7060/15231], Loss: 0.2115\n",
            "Epoch [3/3], Step [7080/15231], Loss: 0.2559\n",
            "Epoch [3/3], Step [7100/15231], Loss: 0.2159\n",
            "Epoch [3/3], Step [7120/15231], Loss: 0.1848\n",
            "Epoch [3/3], Step [7140/15231], Loss: 0.2218\n",
            "Epoch [3/3], Step [7160/15231], Loss: 0.2267\n",
            "Epoch [3/3], Step [7180/15231], Loss: 0.2013\n",
            "Epoch [3/3], Step [7200/15231], Loss: 0.2076\n",
            "Epoch [3/3], Step [7220/15231], Loss: 0.2169\n",
            "Epoch [3/3], Step [7240/15231], Loss: 0.2607\n",
            "Epoch [3/3], Step [7260/15231], Loss: 0.2294\n",
            "Epoch [3/3], Step [7280/15231], Loss: 0.2232\n",
            "Epoch [3/3], Step [7300/15231], Loss: 0.2305\n",
            "Epoch [3/3], Step [7320/15231], Loss: 0.1877\n",
            "Epoch [3/3], Step [7340/15231], Loss: 0.1832\n",
            "Epoch [3/3], Step [7360/15231], Loss: 0.2179\n",
            "Epoch [3/3], Step [7380/15231], Loss: 0.2005\n",
            "Epoch [3/3], Step [7400/15231], Loss: 0.2058\n",
            "Epoch [3/3], Step [7420/15231], Loss: 0.1994\n",
            "Epoch [3/3], Step [7440/15231], Loss: 0.2109\n",
            "Epoch [3/3], Step [7460/15231], Loss: 0.2387\n",
            "Epoch [3/3], Step [7480/15231], Loss: 0.2134\n",
            "Epoch [3/3], Step [7500/15231], Loss: 0.2450\n",
            "Epoch [3/3], Step [7520/15231], Loss: 0.2144\n",
            "Epoch [3/3], Step [7540/15231], Loss: 0.1944\n",
            "Epoch [3/3], Step [7560/15231], Loss: 0.1938\n",
            "Epoch [3/3], Step [7580/15231], Loss: 0.1953\n",
            "Epoch [3/3], Step [7600/15231], Loss: 0.2277\n",
            "Epoch [3/3], Step [7620/15231], Loss: 0.2228\n",
            "Epoch [3/3], Step [7640/15231], Loss: 0.2014\n",
            "Epoch [3/3], Step [7660/15231], Loss: 0.2144\n",
            "Epoch [3/3], Step [7680/15231], Loss: 0.2127\n",
            "Epoch [3/3], Step [7700/15231], Loss: 0.2097\n",
            "Epoch [3/3], Step [7720/15231], Loss: 0.1873\n",
            "Epoch [3/3], Step [7740/15231], Loss: 0.1964\n",
            "Epoch [3/3], Step [7760/15231], Loss: 0.2054\n",
            "Epoch [3/3], Step [7780/15231], Loss: 0.1893\n",
            "Epoch [3/3], Step [7800/15231], Loss: 0.2185\n",
            "Epoch [3/3], Step [7820/15231], Loss: 0.2200\n",
            "Epoch [3/3], Step [7840/15231], Loss: 0.2211\n",
            "Epoch [3/3], Step [7860/15231], Loss: 0.2081\n",
            "Epoch [3/3], Step [7880/15231], Loss: 0.1909\n",
            "Epoch [3/3], Step [7900/15231], Loss: 0.1969\n",
            "Epoch [3/3], Step [7920/15231], Loss: 0.2057\n",
            "Epoch [3/3], Step [7940/15231], Loss: 0.2228\n",
            "Epoch [3/3], Step [7960/15231], Loss: 0.2280\n",
            "Epoch [3/3], Step [7980/15231], Loss: 0.1948\n",
            "Epoch [3/3], Step [8000/15231], Loss: 0.2332\n",
            "Epoch [3/3], Step [8020/15231], Loss: 0.2208\n",
            "Epoch [3/3], Step [8040/15231], Loss: 0.2313\n",
            "Epoch [3/3], Step [8060/15231], Loss: 0.1919\n",
            "Epoch [3/3], Step [8080/15231], Loss: 0.2229\n",
            "Epoch [3/3], Step [8100/15231], Loss: 0.1885\n",
            "Epoch [3/3], Step [8120/15231], Loss: 0.2105\n",
            "Epoch [3/3], Step [8140/15231], Loss: 0.1982\n",
            "Epoch [3/3], Step [8160/15231], Loss: 0.2304\n",
            "Epoch [3/3], Step [8180/15231], Loss: 0.2154\n",
            "Epoch [3/3], Step [8200/15231], Loss: 0.2241\n",
            "Epoch [3/3], Step [8220/15231], Loss: 0.2219\n",
            "Epoch [3/3], Step [8240/15231], Loss: 0.2419\n",
            "Epoch [3/3], Step [8260/15231], Loss: 0.1929\n",
            "Epoch [3/3], Step [8280/15231], Loss: 0.2236\n",
            "Epoch [3/3], Step [8300/15231], Loss: 0.2222\n",
            "Epoch [3/3], Step [8320/15231], Loss: 0.2025\n",
            "Epoch [3/3], Step [8340/15231], Loss: 0.1831\n",
            "Epoch [3/3], Step [8360/15231], Loss: 0.2412\n",
            "Epoch [3/3], Step [8380/15231], Loss: 0.2072\n",
            "Epoch [3/3], Step [8400/15231], Loss: 0.2084\n",
            "Epoch [3/3], Step [8420/15231], Loss: 0.2415\n",
            "Epoch [3/3], Step [8440/15231], Loss: 0.2365\n",
            "Epoch [3/3], Step [8460/15231], Loss: 0.2006\n",
            "Epoch [3/3], Step [8480/15231], Loss: 0.2120\n",
            "Epoch [3/3], Step [8500/15231], Loss: 0.2207\n",
            "Epoch [3/3], Step [8520/15231], Loss: 0.2103\n",
            "Epoch [3/3], Step [8540/15231], Loss: 0.2086\n",
            "Epoch [3/3], Step [8560/15231], Loss: 0.2103\n",
            "Epoch [3/3], Step [8580/15231], Loss: 0.2293\n",
            "Epoch [3/3], Step [8600/15231], Loss: 0.2134\n",
            "Epoch [3/3], Step [8620/15231], Loss: 0.2265\n",
            "Epoch [3/3], Step [8640/15231], Loss: 0.2145\n",
            "Epoch [3/3], Step [8660/15231], Loss: 0.2016\n",
            "Epoch [3/3], Step [8680/15231], Loss: 0.1883\n",
            "Epoch [3/3], Step [8700/15231], Loss: 0.2125\n",
            "Epoch [3/3], Step [8720/15231], Loss: 0.2377\n",
            "Epoch [3/3], Step [8740/15231], Loss: 0.1845\n",
            "Epoch [3/3], Step [8760/15231], Loss: 0.2004\n",
            "Epoch [3/3], Step [8780/15231], Loss: 0.2271\n",
            "Epoch [3/3], Step [8800/15231], Loss: 0.2190\n",
            "Epoch [3/3], Step [8820/15231], Loss: 0.2280\n",
            "Epoch [3/3], Step [8840/15231], Loss: 0.2174\n",
            "Epoch [3/3], Step [8860/15231], Loss: 0.2256\n",
            "Epoch [3/3], Step [8880/15231], Loss: 0.2052\n",
            "Epoch [3/3], Step [8900/15231], Loss: 0.2115\n",
            "Epoch [3/3], Step [8920/15231], Loss: 0.1618\n",
            "Epoch [3/3], Step [8940/15231], Loss: 0.1961\n",
            "Epoch [3/3], Step [8960/15231], Loss: 0.2307\n",
            "Epoch [3/3], Step [8980/15231], Loss: 0.2137\n",
            "Epoch [3/3], Step [9000/15231], Loss: 0.2267\n",
            "Epoch [3/3], Step [9020/15231], Loss: 0.2356\n",
            "Epoch [3/3], Step [9040/15231], Loss: 0.2177\n",
            "Epoch [3/3], Step [9060/15231], Loss: 0.1933\n",
            "Epoch [3/3], Step [9080/15231], Loss: 0.2374\n",
            "Epoch [3/3], Step [9100/15231], Loss: 0.1963\n",
            "Epoch [3/3], Step [9120/15231], Loss: 0.2037\n",
            "Epoch [3/3], Step [9140/15231], Loss: 0.2065\n",
            "Epoch [3/3], Step [9160/15231], Loss: 0.2084\n",
            "Epoch [3/3], Step [9180/15231], Loss: 0.2118\n",
            "Epoch [3/3], Step [9200/15231], Loss: 0.1754\n",
            "Epoch [3/3], Step [9220/15231], Loss: 0.2211\n",
            "Epoch [3/3], Step [9240/15231], Loss: 0.2166\n",
            "Epoch [3/3], Step [9260/15231], Loss: 0.2058\n",
            "Epoch [3/3], Step [9280/15231], Loss: 0.2134\n",
            "Epoch [3/3], Step [9300/15231], Loss: 0.2144\n",
            "Epoch [3/3], Step [9320/15231], Loss: 0.1962\n",
            "Epoch [3/3], Step [9340/15231], Loss: 0.2005\n",
            "Epoch [3/3], Step [9360/15231], Loss: 0.2427\n",
            "Epoch [3/3], Step [9380/15231], Loss: 0.2269\n",
            "Epoch [3/3], Step [9400/15231], Loss: 0.1944\n",
            "Epoch [3/3], Step [9420/15231], Loss: 0.2309\n",
            "Epoch [3/3], Step [9440/15231], Loss: 0.2387\n",
            "Epoch [3/3], Step [9460/15231], Loss: 0.1866\n",
            "Epoch [3/3], Step [9480/15231], Loss: 0.1882\n",
            "Epoch [3/3], Step [9500/15231], Loss: 0.2036\n",
            "Epoch [3/3], Step [9520/15231], Loss: 0.2255\n",
            "Epoch [3/3], Step [9540/15231], Loss: 0.1774\n",
            "Epoch [3/3], Step [9560/15231], Loss: 0.2202\n",
            "Epoch [3/3], Step [9580/15231], Loss: 0.1969\n",
            "Epoch [3/3], Step [9600/15231], Loss: 0.2181\n",
            "Epoch [3/3], Step [9620/15231], Loss: 0.1924\n",
            "Epoch [3/3], Step [9640/15231], Loss: 0.2268\n",
            "Epoch [3/3], Step [9660/15231], Loss: 0.2173\n",
            "Epoch [3/3], Step [9680/15231], Loss: 0.2132\n",
            "Epoch [3/3], Step [9700/15231], Loss: 0.2102\n",
            "Epoch [3/3], Step [9720/15231], Loss: 0.2388\n",
            "Epoch [3/3], Step [9740/15231], Loss: 0.2029\n",
            "Epoch [3/3], Step [9760/15231], Loss: 0.2052\n",
            "Epoch [3/3], Step [9780/15231], Loss: 0.1785\n",
            "Epoch [3/3], Step [9800/15231], Loss: 0.2156\n",
            "Epoch [3/3], Step [9820/15231], Loss: 0.1958\n",
            "Epoch [3/3], Step [9840/15231], Loss: 0.2106\n",
            "Epoch [3/3], Step [9860/15231], Loss: 0.1638\n",
            "Epoch [3/3], Step [9880/15231], Loss: 0.2386\n",
            "Epoch [3/3], Step [9900/15231], Loss: 0.2043\n",
            "Epoch [3/3], Step [9920/15231], Loss: 0.1961\n",
            "Epoch [3/3], Step [9940/15231], Loss: 0.2064\n",
            "Epoch [3/3], Step [9960/15231], Loss: 0.1939\n",
            "Epoch [3/3], Step [9980/15231], Loss: 0.2206\n",
            "Epoch [3/3], Step [10000/15231], Loss: 0.2207\n",
            "Epoch [3/3], Step [10020/15231], Loss: 0.2193\n",
            "Epoch [3/3], Step [10040/15231], Loss: 0.2115\n",
            "Epoch [3/3], Step [10060/15231], Loss: 0.2010\n",
            "Epoch [3/3], Step [10080/15231], Loss: 0.1933\n",
            "Epoch [3/3], Step [10100/15231], Loss: 0.1760\n",
            "Epoch [3/3], Step [10120/15231], Loss: 0.2052\n",
            "Epoch [3/3], Step [10140/15231], Loss: 0.1806\n",
            "Epoch [3/3], Step [10160/15231], Loss: 0.1829\n",
            "Epoch [3/3], Step [10180/15231], Loss: 0.2081\n",
            "Epoch [3/3], Step [10200/15231], Loss: 0.2050\n",
            "Epoch [3/3], Step [10220/15231], Loss: 0.2161\n",
            "Epoch [3/3], Step [10240/15231], Loss: 0.2232\n",
            "Epoch [3/3], Step [10260/15231], Loss: 0.1913\n",
            "Epoch [3/3], Step [10280/15231], Loss: 0.2276\n",
            "Epoch [3/3], Step [10300/15231], Loss: 0.2225\n",
            "Epoch [3/3], Step [10320/15231], Loss: 0.2218\n",
            "Epoch [3/3], Step [10340/15231], Loss: 0.2129\n",
            "Epoch [3/3], Step [10360/15231], Loss: 0.1924\n",
            "Epoch [3/3], Step [10380/15231], Loss: 0.2111\n",
            "Epoch [3/3], Step [10400/15231], Loss: 0.2377\n",
            "Epoch [3/3], Step [10420/15231], Loss: 0.2157\n",
            "Epoch [3/3], Step [10440/15231], Loss: 0.2147\n",
            "Epoch [3/3], Step [10460/15231], Loss: 0.2247\n",
            "Epoch [3/3], Step [10480/15231], Loss: 0.2312\n",
            "Epoch [3/3], Step [10500/15231], Loss: 0.1962\n",
            "Epoch [3/3], Step [10520/15231], Loss: 0.2400\n",
            "Epoch [3/3], Step [10540/15231], Loss: 0.2465\n",
            "Epoch [3/3], Step [10560/15231], Loss: 0.1931\n",
            "Epoch [3/3], Step [10580/15231], Loss: 0.2020\n",
            "Epoch [3/3], Step [10600/15231], Loss: 0.2238\n",
            "Epoch [3/3], Step [10620/15231], Loss: 0.2015\n",
            "Epoch [3/3], Step [10640/15231], Loss: 0.2074\n",
            "Epoch [3/3], Step [10660/15231], Loss: 0.1806\n",
            "Epoch [3/3], Step [10680/15231], Loss: 0.2366\n",
            "Epoch [3/3], Step [10700/15231], Loss: 0.2321\n",
            "Epoch [3/3], Step [10720/15231], Loss: 0.2046\n",
            "Epoch [3/3], Step [10740/15231], Loss: 0.1977\n",
            "Epoch [3/3], Step [10760/15231], Loss: 0.1984\n",
            "Epoch [3/3], Step [10780/15231], Loss: 0.2258\n",
            "Epoch [3/3], Step [10800/15231], Loss: 0.1859\n",
            "Epoch [3/3], Step [10820/15231], Loss: 0.2177\n",
            "Epoch [3/3], Step [10840/15231], Loss: 0.2239\n",
            "Epoch [3/3], Step [10860/15231], Loss: 0.1876\n",
            "Epoch [3/3], Step [10880/15231], Loss: 0.2087\n",
            "Epoch [3/3], Step [10900/15231], Loss: 0.2307\n",
            "Epoch [3/3], Step [10920/15231], Loss: 0.2087\n",
            "Epoch [3/3], Step [10940/15231], Loss: 0.2051\n",
            "Epoch [3/3], Step [10960/15231], Loss: 0.2127\n",
            "Epoch [3/3], Step [10980/15231], Loss: 0.2065\n",
            "Epoch [3/3], Step [11000/15231], Loss: 0.2186\n",
            "Epoch [3/3], Step [11020/15231], Loss: 0.2082\n",
            "Epoch [3/3], Step [11040/15231], Loss: 0.1906\n",
            "Epoch [3/3], Step [11060/15231], Loss: 0.2288\n",
            "Epoch [3/3], Step [11080/15231], Loss: 0.2276\n",
            "Epoch [3/3], Step [11100/15231], Loss: 0.2235\n",
            "Epoch [3/3], Step [11120/15231], Loss: 0.2045\n",
            "Epoch [3/3], Step [11140/15231], Loss: 0.2278\n",
            "Epoch [3/3], Step [11160/15231], Loss: 0.2101\n",
            "Epoch [3/3], Step [11180/15231], Loss: 0.2291\n",
            "Epoch [3/3], Step [11200/15231], Loss: 0.2194\n",
            "Epoch [3/3], Step [11220/15231], Loss: 0.1872\n",
            "Epoch [3/3], Step [11240/15231], Loss: 0.1937\n",
            "Epoch [3/3], Step [11260/15231], Loss: 0.1679\n",
            "Epoch [3/3], Step [11280/15231], Loss: 0.2046\n",
            "Epoch [3/3], Step [11300/15231], Loss: 0.2192\n",
            "Epoch [3/3], Step [11320/15231], Loss: 0.2286\n",
            "Epoch [3/3], Step [11340/15231], Loss: 0.2039\n",
            "Epoch [3/3], Step [11360/15231], Loss: 0.2154\n",
            "Epoch [3/3], Step [11380/15231], Loss: 0.2112\n",
            "Epoch [3/3], Step [11400/15231], Loss: 0.1773\n",
            "Epoch [3/3], Step [11420/15231], Loss: 0.2028\n",
            "Epoch [3/3], Step [11440/15231], Loss: 0.1777\n",
            "Epoch [3/3], Step [11460/15231], Loss: 0.2301\n",
            "Epoch [3/3], Step [11480/15231], Loss: 0.1959\n",
            "Epoch [3/3], Step [11500/15231], Loss: 0.2006\n",
            "Epoch [3/3], Step [11520/15231], Loss: 0.2186\n",
            "Epoch [3/3], Step [11540/15231], Loss: 0.2492\n",
            "Epoch [3/3], Step [11560/15231], Loss: 0.2023\n",
            "Epoch [3/3], Step [11580/15231], Loss: 0.2175\n",
            "Epoch [3/3], Step [11600/15231], Loss: 0.2144\n",
            "Epoch [3/3], Step [11620/15231], Loss: 0.1943\n",
            "Epoch [3/3], Step [11640/15231], Loss: 0.2156\n",
            "Epoch [3/3], Step [11660/15231], Loss: 0.2041\n",
            "Epoch [3/3], Step [11680/15231], Loss: 0.1922\n",
            "Epoch [3/3], Step [11700/15231], Loss: 0.1983\n",
            "Epoch [3/3], Step [11720/15231], Loss: 0.2323\n",
            "Epoch [3/3], Step [11740/15231], Loss: 0.2040\n",
            "Epoch [3/3], Step [11760/15231], Loss: 0.1727\n",
            "Epoch [3/3], Step [11780/15231], Loss: 0.2246\n",
            "Epoch [3/3], Step [11800/15231], Loss: 0.1782\n",
            "Epoch [3/3], Step [11820/15231], Loss: 0.1888\n",
            "Epoch [3/3], Step [11840/15231], Loss: 0.2325\n",
            "Epoch [3/3], Step [11860/15231], Loss: 0.1979\n",
            "Epoch [3/3], Step [11880/15231], Loss: 0.1707\n",
            "Epoch [3/3], Step [11900/15231], Loss: 0.2235\n",
            "Epoch [3/3], Step [11920/15231], Loss: 0.2224\n",
            "Epoch [3/3], Step [11940/15231], Loss: 0.1814\n",
            "Epoch [3/3], Step [11960/15231], Loss: 0.2096\n",
            "Epoch [3/3], Step [11980/15231], Loss: 0.1787\n",
            "Epoch [3/3], Step [12000/15231], Loss: 0.2010\n",
            "Epoch [3/3], Step [12020/15231], Loss: 0.2023\n",
            "Epoch [3/3], Step [12040/15231], Loss: 0.1707\n",
            "Epoch [3/3], Step [12060/15231], Loss: 0.1978\n",
            "Epoch [3/3], Step [12080/15231], Loss: 0.1909\n",
            "Epoch [3/3], Step [12100/15231], Loss: 0.2136\n",
            "Epoch [3/3], Step [12120/15231], Loss: 0.2293\n",
            "Epoch [3/3], Step [12140/15231], Loss: 0.1804\n",
            "Epoch [3/3], Step [12160/15231], Loss: 0.2102\n",
            "Epoch [3/3], Step [12180/15231], Loss: 0.1889\n",
            "Epoch [3/3], Step [12200/15231], Loss: 0.1899\n",
            "Epoch [3/3], Step [12220/15231], Loss: 0.1968\n",
            "Epoch [3/3], Step [12240/15231], Loss: 0.1698\n",
            "Epoch [3/3], Step [12260/15231], Loss: 0.2288\n",
            "Epoch [3/3], Step [12280/15231], Loss: 0.1870\n",
            "Epoch [3/3], Step [12300/15231], Loss: 0.2156\n",
            "Epoch [3/3], Step [12320/15231], Loss: 0.1977\n",
            "Epoch [3/3], Step [12340/15231], Loss: 0.2463\n",
            "Epoch [3/3], Step [12360/15231], Loss: 0.2231\n",
            "Epoch [3/3], Step [12380/15231], Loss: 0.1910\n",
            "Epoch [3/3], Step [12400/15231], Loss: 0.2015\n",
            "Epoch [3/3], Step [12420/15231], Loss: 0.1999\n",
            "Epoch [3/3], Step [12440/15231], Loss: 0.1885\n",
            "Epoch [3/3], Step [12460/15231], Loss: 0.2463\n",
            "Epoch [3/3], Step [12480/15231], Loss: 0.2436\n",
            "Epoch [3/3], Step [12500/15231], Loss: 0.2175\n",
            "Epoch [3/3], Step [12520/15231], Loss: 0.2174\n",
            "Epoch [3/3], Step [12540/15231], Loss: 0.2401\n",
            "Epoch [3/3], Step [12560/15231], Loss: 0.2287\n",
            "Epoch [3/3], Step [12580/15231], Loss: 0.1967\n",
            "Epoch [3/3], Step [12600/15231], Loss: 0.1779\n",
            "Epoch [3/3], Step [12620/15231], Loss: 0.2392\n",
            "Epoch [3/3], Step [12640/15231], Loss: 0.1948\n",
            "Epoch [3/3], Step [12660/15231], Loss: 0.2266\n",
            "Epoch [3/3], Step [12680/15231], Loss: 0.2317\n",
            "Epoch [3/3], Step [12700/15231], Loss: 0.2197\n",
            "Epoch [3/3], Step [12720/15231], Loss: 0.2079\n",
            "Epoch [3/3], Step [12740/15231], Loss: 0.1984\n",
            "Epoch [3/3], Step [12760/15231], Loss: 0.2238\n",
            "Epoch [3/3], Step [12780/15231], Loss: 0.2006\n",
            "Epoch [3/3], Step [12800/15231], Loss: 0.2210\n",
            "Epoch [3/3], Step [12820/15231], Loss: 0.1994\n",
            "Epoch [3/3], Step [12840/15231], Loss: 0.2030\n",
            "Epoch [3/3], Step [12860/15231], Loss: 0.1898\n",
            "Epoch [3/3], Step [12880/15231], Loss: 0.2163\n",
            "Epoch [3/3], Step [12900/15231], Loss: 0.2454\n",
            "Epoch [3/3], Step [12920/15231], Loss: 0.2248\n",
            "Epoch [3/3], Step [12940/15231], Loss: 0.1933\n",
            "Epoch [3/3], Step [12960/15231], Loss: 0.1935\n",
            "Epoch [3/3], Step [12980/15231], Loss: 0.2157\n",
            "Epoch [3/3], Step [13000/15231], Loss: 0.2236\n",
            "Epoch [3/3], Step [13020/15231], Loss: 0.1858\n",
            "Epoch [3/3], Step [13040/15231], Loss: 0.1940\n",
            "Epoch [3/3], Step [13060/15231], Loss: 0.1876\n",
            "Epoch [3/3], Step [13080/15231], Loss: 0.2078\n",
            "Epoch [3/3], Step [13100/15231], Loss: 0.1878\n",
            "Epoch [3/3], Step [13120/15231], Loss: 0.2156\n",
            "Epoch [3/3], Step [13140/15231], Loss: 0.2182\n",
            "Epoch [3/3], Step [13160/15231], Loss: 0.2024\n",
            "Epoch [3/3], Step [13180/15231], Loss: 0.1927\n",
            "Epoch [3/3], Step [13200/15231], Loss: 0.2059\n",
            "Epoch [3/3], Step [13220/15231], Loss: 0.2298\n",
            "Epoch [3/3], Step [13240/15231], Loss: 0.1956\n",
            "Epoch [3/3], Step [13260/15231], Loss: 0.2003\n",
            "Epoch [3/3], Step [13280/15231], Loss: 0.2119\n",
            "Epoch [3/3], Step [13300/15231], Loss: 0.1701\n",
            "Epoch [3/3], Step [13320/15231], Loss: 0.2108\n",
            "Epoch [3/3], Step [13340/15231], Loss: 0.2103\n",
            "Epoch [3/3], Step [13360/15231], Loss: 0.2390\n",
            "Epoch [3/3], Step [13380/15231], Loss: 0.1748\n",
            "Epoch [3/3], Step [13400/15231], Loss: 0.1991\n",
            "Epoch [3/3], Step [13420/15231], Loss: 0.1902\n",
            "Epoch [3/3], Step [13440/15231], Loss: 0.2149\n",
            "Epoch [3/3], Step [13460/15231], Loss: 0.2334\n",
            "Epoch [3/3], Step [13480/15231], Loss: 0.2080\n",
            "Epoch [3/3], Step [13500/15231], Loss: 0.2063\n",
            "Epoch [3/3], Step [13520/15231], Loss: 0.2010\n",
            "Epoch [3/3], Step [13540/15231], Loss: 0.1888\n",
            "Epoch [3/3], Step [13560/15231], Loss: 0.1840\n",
            "Epoch [3/3], Step [13580/15231], Loss: 0.1765\n",
            "Epoch [3/3], Step [13600/15231], Loss: 0.2214\n",
            "Epoch [3/3], Step [13620/15231], Loss: 0.1905\n",
            "Epoch [3/3], Step [13640/15231], Loss: 0.2091\n",
            "Epoch [3/3], Step [13660/15231], Loss: 0.1988\n",
            "Epoch [3/3], Step [13680/15231], Loss: 0.2494\n",
            "Epoch [3/3], Step [13700/15231], Loss: 0.2069\n",
            "Epoch [3/3], Step [13720/15231], Loss: 0.1716\n",
            "Epoch [3/3], Step [13740/15231], Loss: 0.2126\n",
            "Epoch [3/3], Step [13760/15231], Loss: 0.2302\n",
            "Epoch [3/3], Step [13780/15231], Loss: 0.2114\n",
            "Epoch [3/3], Step [13800/15231], Loss: 0.2142\n",
            "Epoch [3/3], Step [13820/15231], Loss: 0.1945\n",
            "Epoch [3/3], Step [13840/15231], Loss: 0.2272\n",
            "Epoch [3/3], Step [13860/15231], Loss: 0.2098\n",
            "Epoch [3/3], Step [13880/15231], Loss: 0.2032\n",
            "Epoch [3/3], Step [13900/15231], Loss: 0.1865\n",
            "Epoch [3/3], Step [13920/15231], Loss: 0.1900\n",
            "Epoch [3/3], Step [13940/15231], Loss: 0.2140\n",
            "Epoch [3/3], Step [13960/15231], Loss: 0.1988\n",
            "Epoch [3/3], Step [13980/15231], Loss: 0.1607\n",
            "Epoch [3/3], Step [14000/15231], Loss: 0.1803\n",
            "Epoch [3/3], Step [14020/15231], Loss: 0.2149\n",
            "Epoch [3/3], Step [14040/15231], Loss: 0.2197\n",
            "Epoch [3/3], Step [14060/15231], Loss: 0.1876\n",
            "Epoch [3/3], Step [14080/15231], Loss: 0.1816\n",
            "Epoch [3/3], Step [14100/15231], Loss: 0.2014\n",
            "Epoch [3/3], Step [14120/15231], Loss: 0.1885\n",
            "Epoch [3/3], Step [14140/15231], Loss: 0.1883\n",
            "Epoch [3/3], Step [14160/15231], Loss: 0.1968\n",
            "Epoch [3/3], Step [14180/15231], Loss: 0.2178\n",
            "Epoch [3/3], Step [14200/15231], Loss: 0.2003\n",
            "Epoch [3/3], Step [14220/15231], Loss: 0.2305\n",
            "Epoch [3/3], Step [14240/15231], Loss: 0.1779\n",
            "Epoch [3/3], Step [14260/15231], Loss: 0.2165\n",
            "Epoch [3/3], Step [14280/15231], Loss: 0.2363\n",
            "Epoch [3/3], Step [14300/15231], Loss: 0.1805\n",
            "Epoch [3/3], Step [14320/15231], Loss: 0.1866\n",
            "Epoch [3/3], Step [14340/15231], Loss: 0.2175\n",
            "Epoch [3/3], Step [14360/15231], Loss: 0.2208\n",
            "Epoch [3/3], Step [14380/15231], Loss: 0.2340\n",
            "Epoch [3/3], Step [14400/15231], Loss: 0.1861\n",
            "Epoch [3/3], Step [14420/15231], Loss: 0.2132\n",
            "Epoch [3/3], Step [14440/15231], Loss: 0.2194\n",
            "Epoch [3/3], Step [14460/15231], Loss: 0.1831\n",
            "Epoch [3/3], Step [14480/15231], Loss: 0.2001\n",
            "Epoch [3/3], Step [14500/15231], Loss: 0.1955\n",
            "Epoch [3/3], Step [14520/15231], Loss: 0.1847\n",
            "Epoch [3/3], Step [14540/15231], Loss: 0.2021\n",
            "Epoch [3/3], Step [14560/15231], Loss: 0.1881\n",
            "Epoch [3/3], Step [14580/15231], Loss: 0.2188\n",
            "Epoch [3/3], Step [14600/15231], Loss: 0.1912\n",
            "Epoch [3/3], Step [14620/15231], Loss: 0.2030\n",
            "Epoch [3/3], Step [14640/15231], Loss: 0.1992\n",
            "Epoch [3/3], Step [14660/15231], Loss: 0.2051\n",
            "Epoch [3/3], Step [14680/15231], Loss: 0.2184\n",
            "Epoch [3/3], Step [14700/15231], Loss: 0.2447\n",
            "Epoch [3/3], Step [14720/15231], Loss: 0.2103\n",
            "Epoch [3/3], Step [14740/15231], Loss: 0.1971\n",
            "Epoch [3/3], Step [14760/15231], Loss: 0.2081\n",
            "Epoch [3/3], Step [14780/15231], Loss: 0.1955\n",
            "Epoch [3/3], Step [14800/15231], Loss: 0.2047\n",
            "Epoch [3/3], Step [14820/15231], Loss: 0.1848\n",
            "Epoch [3/3], Step [14840/15231], Loss: 0.1872\n",
            "Epoch [3/3], Step [14860/15231], Loss: 0.2000\n",
            "Epoch [3/3], Step [14880/15231], Loss: 0.2080\n",
            "Epoch [3/3], Step [14900/15231], Loss: 0.1975\n",
            "Epoch [3/3], Step [14920/15231], Loss: 0.2229\n",
            "Epoch [3/3], Step [14940/15231], Loss: 0.2037\n",
            "Epoch [3/3], Step [14960/15231], Loss: 0.1954\n",
            "Epoch [3/3], Step [14980/15231], Loss: 0.2522\n",
            "Epoch [3/3], Step [15000/15231], Loss: 0.2347\n",
            "Epoch [3/3], Step [15020/15231], Loss: 0.2146\n",
            "Epoch [3/3], Step [15040/15231], Loss: 0.2135\n",
            "Epoch [3/3], Step [15060/15231], Loss: 0.1995\n",
            "Epoch [3/3], Step [15080/15231], Loss: 0.1930\n",
            "Epoch [3/3], Step [15100/15231], Loss: 0.2248\n",
            "Epoch [3/3], Step [15120/15231], Loss: 0.2063\n",
            "Epoch [3/3], Step [15140/15231], Loss: 0.2042\n",
            "Epoch [3/3], Step [15160/15231], Loss: 0.2247\n",
            "Epoch [3/3], Step [15180/15231], Loss: 0.1875\n",
            "Epoch [3/3], Step [15200/15231], Loss: 0.2271\n",
            "Epoch [3/3], Step [15220/15231], Loss: 0.1827\n",
            "Epoch [3/3] | Train Loss: 0.2139 | Train Acc: 92.49% | Test Loss: 0.8061 | Test Acc: 74.10%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅██████████████</td></tr><tr><td>test_accuracy per epoch</td><td>█▄▁</td></tr><tr><td>train_accuracy per epoch</td><td>▁▆█</td></tr><tr><td>train_loss</td><td>█▄▄▄▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>test_accuracy per epoch</td><td>74.1</td></tr><tr><td>train_accuracy per epoch</td><td>92.48838</td></tr><tr><td>train_loss</td><td>0.18269</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet-18 with data aug</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/iovrpm97' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/iovrpm97</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT</a><br>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250403_113835-iovrpm97/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "!wandb online\n",
        "\n",
        "name = \"resnet-18 with data aug\"\n",
        "\n",
        "# Configuration dictionary for wandb\n",
        "config = {\n",
        "    \"in_channels\": 1,\n",
        "    \"num_epochs\": 3,            # Adjust number of epochs as necessary\n",
        "    \"lr\": 0.001,                # Learning rate\n",
        "    \"batch_size\": 64,           # Batch size (should match your DataLoader)\n",
        "    \"num_blocks\": 18,            # Number of residual blocks in the network\n",
        "    \"base_channels\": 64,        # Number of filters in the first block\n",
        "    \"kernel_size\": 3,           # Kernel size for convolutions\n",
        "    \"activation\": \"LeakyReLU\",  # Name of the activation function\n",
        "    \"use_batchnorm\": True,      # Whether to use batch normalization\n",
        "    \"num_classes\": 4            # OCTMNIST has 4 classes\n",
        "}\n",
        "\n",
        "# Initialize wandb run and train the model\n",
        "with wandb.init(\n",
        "    config=config,\n",
        "    project='CNN VS ViT',   # Title of your project\n",
        "    group='Test',           # Group name for your run\n",
        "    save_code=True,\n",
        "    name=name,\n",
        "):\n",
        "    # Create the model with custom configuration\n",
        "    model = CustomResNet(\n",
        "        in_channels=config['in_channels'],  # OCTMNIST images are single-channel if you have preprocessed accordingly\n",
        "        num_blocks=config[\"num_blocks\"],\n",
        "        base_channels=config[\"base_channels\"],\n",
        "        kernel_size=config[\"kernel_size\"],\n",
        "        activation=nn.LeakyReLU(0.1, inplace=True),\n",
        "        use_batchnorm=config[\"use_batchnorm\"],\n",
        "        num_classes=config[\"num_classes\"]\n",
        "    )\n",
        "    print(model)\n",
        "\n",
        "    # Train the model (train_loader and test_loader should be defined before)\n",
        "    train_model(model, config, name)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Resnet-18 with 1 epoch"
      ],
      "metadata": {
        "id": "y0BgAVIg63PA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "\n",
        "name = \"resnet-18 with data aug (1 epoch)\"\n",
        "\n",
        "# Configuration dictionary for wandb\n",
        "config = {\n",
        "    \"in_channels\": 1,\n",
        "    \"num_epochs\": 1,            # Adjust number of epochs as necessary\n",
        "    \"lr\": 0.001,                # Learning rate\n",
        "    \"batch_size\": 64,           # Batch size (should match your DataLoader)\n",
        "    \"num_blocks\": 18,            # Number of residual blocks in the network\n",
        "    \"base_channels\": 64,        # Number of filters in the first block\n",
        "    \"kernel_size\": 3,           # Kernel size for convolutions\n",
        "    \"activation\": \"LeakyReLU\",  # Name of the activation function\n",
        "    \"use_batchnorm\": True,      # Whether to use batch normalization\n",
        "    \"num_classes\": 4            # OCTMNIST has 4 classes\n",
        "}\n",
        "\n",
        "# Initialize wandb run and train the model\n",
        "with wandb.init(\n",
        "    config=config,\n",
        "    project='CNN VS ViT',   # Title of your project\n",
        "    group='Test',           # Group name for your run\n",
        "    save_code=True,\n",
        "    name=name,\n",
        "):\n",
        "    # Create the model with custom configuration\n",
        "    model = CustomResNet(\n",
        "        in_channels=config['in_channels'],  # OCTMNIST images are single-channel if you have preprocessed accordingly\n",
        "        num_blocks=config[\"num_blocks\"],\n",
        "        base_channels=config[\"base_channels\"],\n",
        "        kernel_size=config[\"kernel_size\"],\n",
        "        activation=nn.LeakyReLU(0.1, inplace=True),\n",
        "        use_batchnorm=config[\"use_batchnorm\"],\n",
        "        num_classes=config[\"num_classes\"]\n",
        "    )\n",
        "    print(model)\n",
        "\n",
        "    # Train the model (train_loader and test_loader should be defined before)\n",
        "    train_model(model, config, name)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ok32PoGd65Yp",
        "outputId": "74cf5377-8b2f-4639-df26-05fc68ed0c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250403_183521-drm4su0v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/drm4su0v' target=\"_blank\">resnet-18 with data aug (1 epoch)</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/drm4su0v' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/drm4su0v</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomResNet(\n",
            "  (initial_conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (initial_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  (residual_layers): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (4): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (5): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (6): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (7): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (8): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (9): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (10): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (11): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (12): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (13): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (14): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (15): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (16): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (17): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "  )\n",
            "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Epoch [1/1], Step [20/15231], Loss: 1.1196\n",
            "Epoch [1/1], Step [40/15231], Loss: 0.8361\n",
            "Epoch [1/1], Step [60/15231], Loss: 0.7870\n",
            "Epoch [1/1], Step [80/15231], Loss: 0.6974\n",
            "Epoch [1/1], Step [100/15231], Loss: 0.6632\n",
            "Epoch [1/1], Step [120/15231], Loss: 0.6318\n",
            "Epoch [1/1], Step [140/15231], Loss: 0.6770\n",
            "Epoch [1/1], Step [160/15231], Loss: 0.6561\n",
            "Epoch [1/1], Step [180/15231], Loss: 0.5905\n",
            "Epoch [1/1], Step [200/15231], Loss: 0.6645\n",
            "Epoch [1/1], Step [220/15231], Loss: 0.6202\n",
            "Epoch [1/1], Step [240/15231], Loss: 0.6022\n",
            "Epoch [1/1], Step [260/15231], Loss: 0.6317\n",
            "Epoch [1/1], Step [280/15231], Loss: 0.6222\n",
            "Epoch [1/1], Step [300/15231], Loss: 0.5351\n",
            "Epoch [1/1], Step [320/15231], Loss: 0.6150\n",
            "Epoch [1/1], Step [340/15231], Loss: 0.5974\n",
            "Epoch [1/1], Step [360/15231], Loss: 0.6222\n",
            "Epoch [1/1], Step [380/15231], Loss: 0.5929\n",
            "Epoch [1/1], Step [400/15231], Loss: 0.5661\n",
            "Epoch [1/1], Step [420/15231], Loss: 0.5429\n",
            "Epoch [1/1], Step [440/15231], Loss: 0.5504\n",
            "Epoch [1/1], Step [460/15231], Loss: 0.5621\n",
            "Epoch [1/1], Step [480/15231], Loss: 0.5267\n",
            "Epoch [1/1], Step [500/15231], Loss: 0.5217\n",
            "Epoch [1/1], Step [520/15231], Loss: 0.5658\n",
            "Epoch [1/1], Step [540/15231], Loss: 0.6110\n",
            "Epoch [1/1], Step [560/15231], Loss: 0.5332\n",
            "Epoch [1/1], Step [580/15231], Loss: 0.5422\n",
            "Epoch [1/1], Step [600/15231], Loss: 0.5433\n",
            "Epoch [1/1], Step [620/15231], Loss: 0.5333\n",
            "Epoch [1/1], Step [640/15231], Loss: 0.4914\n",
            "Epoch [1/1], Step [660/15231], Loss: 0.5479\n",
            "Epoch [1/1], Step [680/15231], Loss: 0.4954\n",
            "Epoch [1/1], Step [700/15231], Loss: 0.5126\n",
            "Epoch [1/1], Step [720/15231], Loss: 0.5063\n",
            "Epoch [1/1], Step [740/15231], Loss: 0.4816\n",
            "Epoch [1/1], Step [760/15231], Loss: 0.4824\n",
            "Epoch [1/1], Step [780/15231], Loss: 0.5246\n",
            "Epoch [1/1], Step [800/15231], Loss: 0.4949\n",
            "Epoch [1/1], Step [820/15231], Loss: 0.5184\n",
            "Epoch [1/1], Step [840/15231], Loss: 0.4823\n",
            "Epoch [1/1], Step [860/15231], Loss: 0.5285\n",
            "Epoch [1/1], Step [880/15231], Loss: 0.4993\n",
            "Epoch [1/1], Step [900/15231], Loss: 0.5100\n",
            "Epoch [1/1], Step [920/15231], Loss: 0.4771\n",
            "Epoch [1/1], Step [940/15231], Loss: 0.4570\n",
            "Epoch [1/1], Step [960/15231], Loss: 0.4785\n",
            "Epoch [1/1], Step [980/15231], Loss: 0.4945\n",
            "Epoch [1/1], Step [1000/15231], Loss: 0.4512\n",
            "Epoch [1/1], Step [1020/15231], Loss: 0.4302\n",
            "Epoch [1/1], Step [1040/15231], Loss: 0.4676\n",
            "Epoch [1/1], Step [1060/15231], Loss: 0.4993\n",
            "Epoch [1/1], Step [1080/15231], Loss: 0.4513\n",
            "Epoch [1/1], Step [1100/15231], Loss: 0.4666\n",
            "Epoch [1/1], Step [1120/15231], Loss: 0.4683\n",
            "Epoch [1/1], Step [1140/15231], Loss: 0.4474\n",
            "Epoch [1/1], Step [1160/15231], Loss: 0.4189\n",
            "Epoch [1/1], Step [1180/15231], Loss: 0.4355\n",
            "Epoch [1/1], Step [1200/15231], Loss: 0.4455\n",
            "Epoch [1/1], Step [1220/15231], Loss: 0.4737\n",
            "Epoch [1/1], Step [1240/15231], Loss: 0.4327\n",
            "Epoch [1/1], Step [1260/15231], Loss: 0.5067\n",
            "Epoch [1/1], Step [1280/15231], Loss: 0.4340\n",
            "Epoch [1/1], Step [1300/15231], Loss: 0.4628\n",
            "Epoch [1/1], Step [1320/15231], Loss: 0.4072\n",
            "Epoch [1/1], Step [1340/15231], Loss: 0.4366\n",
            "Epoch [1/1], Step [1360/15231], Loss: 0.4256\n",
            "Epoch [1/1], Step [1380/15231], Loss: 0.4553\n",
            "Epoch [1/1], Step [1400/15231], Loss: 0.4871\n",
            "Epoch [1/1], Step [1420/15231], Loss: 0.4565\n",
            "Epoch [1/1], Step [1440/15231], Loss: 0.4443\n",
            "Epoch [1/1], Step [1460/15231], Loss: 0.4754\n",
            "Epoch [1/1], Step [1480/15231], Loss: 0.4906\n",
            "Epoch [1/1], Step [1500/15231], Loss: 0.4369\n",
            "Epoch [1/1], Step [1520/15231], Loss: 0.4144\n",
            "Epoch [1/1], Step [1540/15231], Loss: 0.4490\n",
            "Epoch [1/1], Step [1560/15231], Loss: 0.4538\n",
            "Epoch [1/1], Step [1580/15231], Loss: 0.4239\n",
            "Epoch [1/1], Step [1600/15231], Loss: 0.4130\n",
            "Epoch [1/1], Step [1620/15231], Loss: 0.4325\n",
            "Epoch [1/1], Step [1640/15231], Loss: 0.4214\n",
            "Epoch [1/1], Step [1660/15231], Loss: 0.4668\n",
            "Epoch [1/1], Step [1680/15231], Loss: 0.4357\n",
            "Epoch [1/1], Step [1700/15231], Loss: 0.4009\n",
            "Epoch [1/1], Step [1720/15231], Loss: 0.4152\n",
            "Epoch [1/1], Step [1740/15231], Loss: 0.4201\n",
            "Epoch [1/1], Step [1760/15231], Loss: 0.4473\n",
            "Epoch [1/1], Step [1780/15231], Loss: 0.4218\n",
            "Epoch [1/1], Step [1800/15231], Loss: 0.4233\n",
            "Epoch [1/1], Step [1820/15231], Loss: 0.3874\n",
            "Epoch [1/1], Step [1840/15231], Loss: 0.4406\n",
            "Epoch [1/1], Step [1860/15231], Loss: 0.4220\n",
            "Epoch [1/1], Step [1880/15231], Loss: 0.4128\n",
            "Epoch [1/1], Step [1900/15231], Loss: 0.4041\n",
            "Epoch [1/1], Step [1920/15231], Loss: 0.3941\n",
            "Epoch [1/1], Step [1940/15231], Loss: 0.4141\n",
            "Epoch [1/1], Step [1960/15231], Loss: 0.4054\n",
            "Epoch [1/1], Step [1980/15231], Loss: 0.3819\n",
            "Epoch [1/1], Step [2000/15231], Loss: 0.3823\n",
            "Epoch [1/1], Step [2020/15231], Loss: 0.4311\n",
            "Epoch [1/1], Step [2040/15231], Loss: 0.4598\n",
            "Epoch [1/1], Step [2060/15231], Loss: 0.4043\n",
            "Epoch [1/1], Step [2080/15231], Loss: 0.3934\n",
            "Epoch [1/1], Step [2100/15231], Loss: 0.4096\n",
            "Epoch [1/1], Step [2120/15231], Loss: 0.4227\n",
            "Epoch [1/1], Step [2140/15231], Loss: 0.4013\n",
            "Epoch [1/1], Step [2160/15231], Loss: 0.3667\n",
            "Epoch [1/1], Step [2180/15231], Loss: 0.3730\n",
            "Epoch [1/1], Step [2200/15231], Loss: 0.3866\n",
            "Epoch [1/1], Step [2220/15231], Loss: 0.4186\n",
            "Epoch [1/1], Step [2240/15231], Loss: 0.3739\n",
            "Epoch [1/1], Step [2260/15231], Loss: 0.3549\n",
            "Epoch [1/1], Step [2280/15231], Loss: 0.3670\n",
            "Epoch [1/1], Step [2300/15231], Loss: 0.4075\n",
            "Epoch [1/1], Step [2320/15231], Loss: 0.3889\n",
            "Epoch [1/1], Step [2340/15231], Loss: 0.3901\n",
            "Epoch [1/1], Step [2360/15231], Loss: 0.4023\n",
            "Epoch [1/1], Step [2380/15231], Loss: 0.3681\n",
            "Epoch [1/1], Step [2400/15231], Loss: 0.4238\n",
            "Epoch [1/1], Step [2420/15231], Loss: 0.4046\n",
            "Epoch [1/1], Step [2440/15231], Loss: 0.4188\n",
            "Epoch [1/1], Step [2460/15231], Loss: 0.3798\n",
            "Epoch [1/1], Step [2480/15231], Loss: 0.3665\n",
            "Epoch [1/1], Step [2500/15231], Loss: 0.4039\n",
            "Epoch [1/1], Step [2520/15231], Loss: 0.3784\n",
            "Epoch [1/1], Step [2540/15231], Loss: 0.3981\n",
            "Epoch [1/1], Step [2560/15231], Loss: 0.3919\n",
            "Epoch [1/1], Step [2580/15231], Loss: 0.3940\n",
            "Epoch [1/1], Step [2600/15231], Loss: 0.4170\n",
            "Epoch [1/1], Step [2620/15231], Loss: 0.4149\n",
            "Epoch [1/1], Step [2640/15231], Loss: 0.3947\n",
            "Epoch [1/1], Step [2660/15231], Loss: 0.3851\n",
            "Epoch [1/1], Step [2680/15231], Loss: 0.4088\n",
            "Epoch [1/1], Step [2700/15231], Loss: 0.3597\n",
            "Epoch [1/1], Step [2720/15231], Loss: 0.3874\n",
            "Epoch [1/1], Step [2740/15231], Loss: 0.4311\n",
            "Epoch [1/1], Step [2760/15231], Loss: 0.3742\n",
            "Epoch [1/1], Step [2780/15231], Loss: 0.3600\n",
            "Epoch [1/1], Step [2800/15231], Loss: 0.3971\n",
            "Epoch [1/1], Step [2820/15231], Loss: 0.4084\n",
            "Epoch [1/1], Step [2840/15231], Loss: 0.3579\n",
            "Epoch [1/1], Step [2860/15231], Loss: 0.3869\n",
            "Epoch [1/1], Step [2880/15231], Loss: 0.4043\n",
            "Epoch [1/1], Step [2900/15231], Loss: 0.3618\n",
            "Epoch [1/1], Step [2920/15231], Loss: 0.3811\n",
            "Epoch [1/1], Step [2940/15231], Loss: 0.3475\n",
            "Epoch [1/1], Step [2960/15231], Loss: 0.3865\n",
            "Epoch [1/1], Step [2980/15231], Loss: 0.3753\n",
            "Epoch [1/1], Step [3000/15231], Loss: 0.4028\n",
            "Epoch [1/1], Step [3020/15231], Loss: 0.3708\n",
            "Epoch [1/1], Step [3040/15231], Loss: 0.3890\n",
            "Epoch [1/1], Step [3060/15231], Loss: 0.3679\n",
            "Epoch [1/1], Step [3080/15231], Loss: 0.3334\n",
            "Epoch [1/1], Step [3100/15231], Loss: 0.3644\n",
            "Epoch [1/1], Step [3120/15231], Loss: 0.3553\n",
            "Epoch [1/1], Step [3140/15231], Loss: 0.3575\n",
            "Epoch [1/1], Step [3160/15231], Loss: 0.3821\n",
            "Epoch [1/1], Step [3180/15231], Loss: 0.3446\n",
            "Epoch [1/1], Step [3200/15231], Loss: 0.3466\n",
            "Epoch [1/1], Step [3220/15231], Loss: 0.3568\n",
            "Epoch [1/1], Step [3240/15231], Loss: 0.3823\n",
            "Epoch [1/1], Step [3260/15231], Loss: 0.3383\n",
            "Epoch [1/1], Step [3280/15231], Loss: 0.3936\n",
            "Epoch [1/1], Step [3300/15231], Loss: 0.3627\n",
            "Epoch [1/1], Step [3320/15231], Loss: 0.3284\n",
            "Epoch [1/1], Step [3340/15231], Loss: 0.3377\n",
            "Epoch [1/1], Step [3360/15231], Loss: 0.3846\n",
            "Epoch [1/1], Step [3380/15231], Loss: 0.3449\n",
            "Epoch [1/1], Step [3400/15231], Loss: 0.3352\n",
            "Epoch [1/1], Step [3420/15231], Loss: 0.3202\n",
            "Epoch [1/1], Step [3440/15231], Loss: 0.3548\n",
            "Epoch [1/1], Step [3460/15231], Loss: 0.3414\n",
            "Epoch [1/1], Step [3480/15231], Loss: 0.4132\n",
            "Epoch [1/1], Step [3500/15231], Loss: 0.3886\n",
            "Epoch [1/1], Step [3520/15231], Loss: 0.4129\n",
            "Epoch [1/1], Step [3540/15231], Loss: 0.3225\n",
            "Epoch [1/1], Step [3560/15231], Loss: 0.3548\n",
            "Epoch [1/1], Step [3580/15231], Loss: 0.3873\n",
            "Epoch [1/1], Step [3600/15231], Loss: 0.3538\n",
            "Epoch [1/1], Step [3620/15231], Loss: 0.3414\n",
            "Epoch [1/1], Step [3640/15231], Loss: 0.3541\n",
            "Epoch [1/1], Step [3660/15231], Loss: 0.3502\n",
            "Epoch [1/1], Step [3680/15231], Loss: 0.3559\n",
            "Epoch [1/1], Step [3700/15231], Loss: 0.3589\n",
            "Epoch [1/1], Step [3720/15231], Loss: 0.3491\n",
            "Epoch [1/1], Step [3740/15231], Loss: 0.3541\n",
            "Epoch [1/1], Step [3760/15231], Loss: 0.3446\n",
            "Epoch [1/1], Step [3780/15231], Loss: 0.3434\n",
            "Epoch [1/1], Step [3800/15231], Loss: 0.3465\n",
            "Epoch [1/1], Step [3820/15231], Loss: 0.3364\n",
            "Epoch [1/1], Step [3840/15231], Loss: 0.3736\n",
            "Epoch [1/1], Step [3860/15231], Loss: 0.3406\n",
            "Epoch [1/1], Step [3880/15231], Loss: 0.3275\n",
            "Epoch [1/1], Step [3900/15231], Loss: 0.3407\n",
            "Epoch [1/1], Step [3920/15231], Loss: 0.3629\n",
            "Epoch [1/1], Step [3940/15231], Loss: 0.3585\n",
            "Epoch [1/1], Step [3960/15231], Loss: 0.3334\n",
            "Epoch [1/1], Step [3980/15231], Loss: 0.3475\n",
            "Epoch [1/1], Step [4000/15231], Loss: 0.3680\n",
            "Epoch [1/1], Step [4020/15231], Loss: 0.3867\n",
            "Epoch [1/1], Step [4040/15231], Loss: 0.3690\n",
            "Epoch [1/1], Step [4060/15231], Loss: 0.3489\n",
            "Epoch [1/1], Step [4080/15231], Loss: 0.3193\n",
            "Epoch [1/1], Step [4100/15231], Loss: 0.3737\n",
            "Epoch [1/1], Step [4120/15231], Loss: 0.3243\n",
            "Epoch [1/1], Step [4140/15231], Loss: 0.3483\n",
            "Epoch [1/1], Step [4160/15231], Loss: 0.3803\n",
            "Epoch [1/1], Step [4180/15231], Loss: 0.3269\n",
            "Epoch [1/1], Step [4200/15231], Loss: 0.3874\n",
            "Epoch [1/1], Step [4220/15231], Loss: 0.2888\n",
            "Epoch [1/1], Step [4240/15231], Loss: 0.3309\n",
            "Epoch [1/1], Step [4260/15231], Loss: 0.3516\n",
            "Epoch [1/1], Step [4280/15231], Loss: 0.3506\n",
            "Epoch [1/1], Step [4300/15231], Loss: 0.3591\n",
            "Epoch [1/1], Step [4320/15231], Loss: 0.3351\n",
            "Epoch [1/1], Step [4340/15231], Loss: 0.3223\n",
            "Epoch [1/1], Step [4360/15231], Loss: 0.3475\n",
            "Epoch [1/1], Step [4380/15231], Loss: 0.3850\n",
            "Epoch [1/1], Step [4400/15231], Loss: 0.3487\n",
            "Epoch [1/1], Step [4420/15231], Loss: 0.3246\n",
            "Epoch [1/1], Step [4440/15231], Loss: 0.3596\n",
            "Epoch [1/1], Step [4460/15231], Loss: 0.3313\n",
            "Epoch [1/1], Step [4480/15231], Loss: 0.3712\n",
            "Epoch [1/1], Step [4500/15231], Loss: 0.3201\n",
            "Epoch [1/1], Step [4520/15231], Loss: 0.2979\n",
            "Epoch [1/1], Step [4540/15231], Loss: 0.3775\n",
            "Epoch [1/1], Step [4560/15231], Loss: 0.3438\n",
            "Epoch [1/1], Step [4580/15231], Loss: 0.3803\n",
            "Epoch [1/1], Step [4600/15231], Loss: 0.3249\n",
            "Epoch [1/1], Step [4620/15231], Loss: 0.3857\n",
            "Epoch [1/1], Step [4640/15231], Loss: 0.3585\n",
            "Epoch [1/1], Step [4660/15231], Loss: 0.3390\n",
            "Epoch [1/1], Step [4680/15231], Loss: 0.3190\n",
            "Epoch [1/1], Step [4700/15231], Loss: 0.3475\n",
            "Epoch [1/1], Step [4720/15231], Loss: 0.3192\n",
            "Epoch [1/1], Step [4740/15231], Loss: 0.3471\n",
            "Epoch [1/1], Step [4760/15231], Loss: 0.3160\n",
            "Epoch [1/1], Step [4780/15231], Loss: 0.3214\n",
            "Epoch [1/1], Step [4800/15231], Loss: 0.3421\n",
            "Epoch [1/1], Step [4820/15231], Loss: 0.3239\n",
            "Epoch [1/1], Step [4840/15231], Loss: 0.3412\n",
            "Epoch [1/1], Step [4860/15231], Loss: 0.3027\n",
            "Epoch [1/1], Step [4880/15231], Loss: 0.3254\n",
            "Epoch [1/1], Step [4900/15231], Loss: 0.3581\n",
            "Epoch [1/1], Step [4920/15231], Loss: 0.3042\n",
            "Epoch [1/1], Step [4940/15231], Loss: 0.3482\n",
            "Epoch [1/1], Step [4960/15231], Loss: 0.3218\n",
            "Epoch [1/1], Step [4980/15231], Loss: 0.3336\n",
            "Epoch [1/1], Step [5000/15231], Loss: 0.2937\n",
            "Epoch [1/1], Step [5020/15231], Loss: 0.3345\n",
            "Epoch [1/1], Step [5040/15231], Loss: 0.3136\n",
            "Epoch [1/1], Step [5060/15231], Loss: 0.3480\n",
            "Epoch [1/1], Step [5080/15231], Loss: 0.3071\n",
            "Epoch [1/1], Step [5100/15231], Loss: 0.3416\n",
            "Epoch [1/1], Step [5120/15231], Loss: 0.3167\n",
            "Epoch [1/1], Step [5140/15231], Loss: 0.2895\n",
            "Epoch [1/1], Step [5160/15231], Loss: 0.3611\n",
            "Epoch [1/1], Step [5180/15231], Loss: 0.3557\n",
            "Epoch [1/1], Step [5200/15231], Loss: 0.3769\n",
            "Epoch [1/1], Step [5220/15231], Loss: 0.3668\n",
            "Epoch [1/1], Step [5240/15231], Loss: 0.3401\n",
            "Epoch [1/1], Step [5260/15231], Loss: 0.3528\n",
            "Epoch [1/1], Step [5280/15231], Loss: 0.3275\n",
            "Epoch [1/1], Step [5300/15231], Loss: 0.3117\n",
            "Epoch [1/1], Step [5320/15231], Loss: 0.3295\n",
            "Epoch [1/1], Step [5340/15231], Loss: 0.3444\n",
            "Epoch [1/1], Step [5360/15231], Loss: 0.3289\n",
            "Epoch [1/1], Step [5380/15231], Loss: 0.3452\n",
            "Epoch [1/1], Step [5400/15231], Loss: 0.3399\n",
            "Epoch [1/1], Step [5420/15231], Loss: 0.3160\n",
            "Epoch [1/1], Step [5440/15231], Loss: 0.3219\n",
            "Epoch [1/1], Step [5460/15231], Loss: 0.3336\n",
            "Epoch [1/1], Step [5480/15231], Loss: 0.3621\n",
            "Epoch [1/1], Step [5500/15231], Loss: 0.3400\n",
            "Epoch [1/1], Step [5520/15231], Loss: 0.2985\n",
            "Epoch [1/1], Step [5540/15231], Loss: 0.3584\n",
            "Epoch [1/1], Step [5560/15231], Loss: 0.3129\n",
            "Epoch [1/1], Step [5580/15231], Loss: 0.3091\n",
            "Epoch [1/1], Step [5600/15231], Loss: 0.3151\n",
            "Epoch [1/1], Step [5620/15231], Loss: 0.3051\n",
            "Epoch [1/1], Step [5640/15231], Loss: 0.3013\n",
            "Epoch [1/1], Step [5660/15231], Loss: 0.3099\n",
            "Epoch [1/1], Step [5680/15231], Loss: 0.3436\n",
            "Epoch [1/1], Step [5700/15231], Loss: 0.3563\n",
            "Epoch [1/1], Step [5720/15231], Loss: 0.3064\n",
            "Epoch [1/1], Step [5740/15231], Loss: 0.3340\n",
            "Epoch [1/1], Step [5760/15231], Loss: 0.3771\n",
            "Epoch [1/1], Step [5780/15231], Loss: 0.3285\n",
            "Epoch [1/1], Step [5800/15231], Loss: 0.3308\n",
            "Epoch [1/1], Step [5820/15231], Loss: 0.3351\n",
            "Epoch [1/1], Step [5840/15231], Loss: 0.2922\n",
            "Epoch [1/1], Step [5860/15231], Loss: 0.3503\n",
            "Epoch [1/1], Step [5880/15231], Loss: 0.3159\n",
            "Epoch [1/1], Step [5900/15231], Loss: 0.3675\n",
            "Epoch [1/1], Step [5920/15231], Loss: 0.3174\n",
            "Epoch [1/1], Step [5940/15231], Loss: 0.3147\n",
            "Epoch [1/1], Step [5960/15231], Loss: 0.3459\n",
            "Epoch [1/1], Step [5980/15231], Loss: 0.3281\n",
            "Epoch [1/1], Step [6000/15231], Loss: 0.2980\n",
            "Epoch [1/1], Step [6020/15231], Loss: 0.3197\n",
            "Epoch [1/1], Step [6040/15231], Loss: 0.3456\n",
            "Epoch [1/1], Step [6060/15231], Loss: 0.3490\n",
            "Epoch [1/1], Step [6080/15231], Loss: 0.2890\n",
            "Epoch [1/1], Step [6100/15231], Loss: 0.3342\n",
            "Epoch [1/1], Step [6120/15231], Loss: 0.2948\n",
            "Epoch [1/1], Step [6140/15231], Loss: 0.2904\n",
            "Epoch [1/1], Step [6160/15231], Loss: 0.3616\n",
            "Epoch [1/1], Step [6180/15231], Loss: 0.3379\n",
            "Epoch [1/1], Step [6200/15231], Loss: 0.3190\n",
            "Epoch [1/1], Step [6220/15231], Loss: 0.3164\n",
            "Epoch [1/1], Step [6240/15231], Loss: 0.2905\n",
            "Epoch [1/1], Step [6260/15231], Loss: 0.3004\n",
            "Epoch [1/1], Step [6280/15231], Loss: 0.3198\n",
            "Epoch [1/1], Step [6300/15231], Loss: 0.3134\n",
            "Epoch [1/1], Step [6320/15231], Loss: 0.3162\n",
            "Epoch [1/1], Step [6340/15231], Loss: 0.3558\n",
            "Epoch [1/1], Step [6360/15231], Loss: 0.3296\n",
            "Epoch [1/1], Step [6380/15231], Loss: 0.3186\n",
            "Epoch [1/1], Step [6400/15231], Loss: 0.2736\n",
            "Epoch [1/1], Step [6420/15231], Loss: 0.3287\n",
            "Epoch [1/1], Step [6440/15231], Loss: 0.3024\n",
            "Epoch [1/1], Step [6460/15231], Loss: 0.2986\n",
            "Epoch [1/1], Step [6480/15231], Loss: 0.2957\n",
            "Epoch [1/1], Step [6500/15231], Loss: 0.2847\n",
            "Epoch [1/1], Step [6520/15231], Loss: 0.3568\n",
            "Epoch [1/1], Step [6540/15231], Loss: 0.3086\n",
            "Epoch [1/1], Step [6560/15231], Loss: 0.3058\n",
            "Epoch [1/1], Step [6580/15231], Loss: 0.3281\n",
            "Epoch [1/1], Step [6600/15231], Loss: 0.3499\n",
            "Epoch [1/1], Step [6620/15231], Loss: 0.2733\n",
            "Epoch [1/1], Step [6640/15231], Loss: 0.3162\n",
            "Epoch [1/1], Step [6660/15231], Loss: 0.3195\n",
            "Epoch [1/1], Step [6680/15231], Loss: 0.3212\n",
            "Epoch [1/1], Step [6700/15231], Loss: 0.3337\n",
            "Epoch [1/1], Step [6720/15231], Loss: 0.3096\n",
            "Epoch [1/1], Step [6740/15231], Loss: 0.3188\n",
            "Epoch [1/1], Step [6760/15231], Loss: 0.3033\n",
            "Epoch [1/1], Step [6780/15231], Loss: 0.3039\n",
            "Epoch [1/1], Step [6800/15231], Loss: 0.3210\n",
            "Epoch [1/1], Step [6820/15231], Loss: 0.3101\n",
            "Epoch [1/1], Step [6840/15231], Loss: 0.3258\n",
            "Epoch [1/1], Step [6860/15231], Loss: 0.3139\n",
            "Epoch [1/1], Step [6880/15231], Loss: 0.3104\n",
            "Epoch [1/1], Step [6900/15231], Loss: 0.3194\n",
            "Epoch [1/1], Step [6920/15231], Loss: 0.3242\n",
            "Epoch [1/1], Step [6940/15231], Loss: 0.2972\n",
            "Epoch [1/1], Step [6960/15231], Loss: 0.3720\n",
            "Epoch [1/1], Step [6980/15231], Loss: 0.3139\n",
            "Epoch [1/1], Step [7000/15231], Loss: 0.3241\n",
            "Epoch [1/1], Step [7020/15231], Loss: 0.3345\n",
            "Epoch [1/1], Step [7040/15231], Loss: 0.3364\n",
            "Epoch [1/1], Step [7060/15231], Loss: 0.3361\n",
            "Epoch [1/1], Step [7080/15231], Loss: 0.2906\n",
            "Epoch [1/1], Step [7100/15231], Loss: 0.2937\n",
            "Epoch [1/1], Step [7120/15231], Loss: 0.2930\n",
            "Epoch [1/1], Step [7140/15231], Loss: 0.3227\n",
            "Epoch [1/1], Step [7160/15231], Loss: 0.3401\n",
            "Epoch [1/1], Step [7180/15231], Loss: 0.3227\n",
            "Epoch [1/1], Step [7200/15231], Loss: 0.3003\n",
            "Epoch [1/1], Step [7220/15231], Loss: 0.3196\n",
            "Epoch [1/1], Step [7240/15231], Loss: 0.3218\n",
            "Epoch [1/1], Step [7260/15231], Loss: 0.2931\n",
            "Epoch [1/1], Step [7280/15231], Loss: 0.2948\n",
            "Epoch [1/1], Step [7300/15231], Loss: 0.2987\n",
            "Epoch [1/1], Step [7320/15231], Loss: 0.2870\n",
            "Epoch [1/1], Step [7340/15231], Loss: 0.2973\n",
            "Epoch [1/1], Step [7360/15231], Loss: 0.3016\n",
            "Epoch [1/1], Step [7380/15231], Loss: 0.2810\n",
            "Epoch [1/1], Step [7400/15231], Loss: 0.3034\n",
            "Epoch [1/1], Step [7420/15231], Loss: 0.3112\n",
            "Epoch [1/1], Step [7440/15231], Loss: 0.3122\n",
            "Epoch [1/1], Step [7460/15231], Loss: 0.2974\n",
            "Epoch [1/1], Step [7480/15231], Loss: 0.2880\n",
            "Epoch [1/1], Step [7500/15231], Loss: 0.3123\n",
            "Epoch [1/1], Step [7520/15231], Loss: 0.2930\n",
            "Epoch [1/1], Step [7540/15231], Loss: 0.3022\n",
            "Epoch [1/1], Step [7560/15231], Loss: 0.3053\n",
            "Epoch [1/1], Step [7580/15231], Loss: 0.3102\n",
            "Epoch [1/1], Step [7600/15231], Loss: 0.3100\n",
            "Epoch [1/1], Step [7620/15231], Loss: 0.3312\n",
            "Epoch [1/1], Step [7640/15231], Loss: 0.3025\n",
            "Epoch [1/1], Step [7660/15231], Loss: 0.3322\n",
            "Epoch [1/1], Step [7680/15231], Loss: 0.3222\n",
            "Epoch [1/1], Step [7700/15231], Loss: 0.2998\n",
            "Epoch [1/1], Step [7720/15231], Loss: 0.3883\n",
            "Epoch [1/1], Step [7740/15231], Loss: 0.3117\n",
            "Epoch [1/1], Step [7760/15231], Loss: 0.3206\n",
            "Epoch [1/1], Step [7780/15231], Loss: 0.3226\n",
            "Epoch [1/1], Step [7800/15231], Loss: 0.2940\n",
            "Epoch [1/1], Step [7820/15231], Loss: 0.3192\n",
            "Epoch [1/1], Step [7840/15231], Loss: 0.3075\n",
            "Epoch [1/1], Step [7860/15231], Loss: 0.2851\n",
            "Epoch [1/1], Step [7880/15231], Loss: 0.3292\n",
            "Epoch [1/1], Step [7900/15231], Loss: 0.2956\n",
            "Epoch [1/1], Step [7920/15231], Loss: 0.2639\n",
            "Epoch [1/1], Step [7940/15231], Loss: 0.3206\n",
            "Epoch [1/1], Step [7960/15231], Loss: 0.3166\n",
            "Epoch [1/1], Step [7980/15231], Loss: 0.2911\n",
            "Epoch [1/1], Step [8000/15231], Loss: 0.2939\n",
            "Epoch [1/1], Step [8020/15231], Loss: 0.2821\n",
            "Epoch [1/1], Step [8040/15231], Loss: 0.3272\n",
            "Epoch [1/1], Step [8060/15231], Loss: 0.2815\n",
            "Epoch [1/1], Step [8080/15231], Loss: 0.3366\n",
            "Epoch [1/1], Step [8100/15231], Loss: 0.2830\n",
            "Epoch [1/1], Step [8120/15231], Loss: 0.3201\n",
            "Epoch [1/1], Step [8140/15231], Loss: 0.2993\n",
            "Epoch [1/1], Step [8160/15231], Loss: 0.3333\n",
            "Epoch [1/1], Step [8180/15231], Loss: 0.3108\n",
            "Epoch [1/1], Step [8200/15231], Loss: 0.2864\n",
            "Epoch [1/1], Step [8220/15231], Loss: 0.2948\n",
            "Epoch [1/1], Step [8240/15231], Loss: 0.2934\n",
            "Epoch [1/1], Step [8260/15231], Loss: 0.2799\n",
            "Epoch [1/1], Step [8280/15231], Loss: 0.2757\n",
            "Epoch [1/1], Step [8300/15231], Loss: 0.2922\n",
            "Epoch [1/1], Step [8320/15231], Loss: 0.3158\n",
            "Epoch [1/1], Step [8340/15231], Loss: 0.2929\n",
            "Epoch [1/1], Step [8360/15231], Loss: 0.3197\n",
            "Epoch [1/1], Step [8380/15231], Loss: 0.2853\n",
            "Epoch [1/1], Step [8400/15231], Loss: 0.3007\n",
            "Epoch [1/1], Step [8420/15231], Loss: 0.2892\n",
            "Epoch [1/1], Step [8440/15231], Loss: 0.3042\n",
            "Epoch [1/1], Step [8460/15231], Loss: 0.3059\n",
            "Epoch [1/1], Step [8480/15231], Loss: 0.3363\n",
            "Epoch [1/1], Step [8500/15231], Loss: 0.3362\n",
            "Epoch [1/1], Step [8520/15231], Loss: 0.3280\n",
            "Epoch [1/1], Step [8540/15231], Loss: 0.2787\n",
            "Epoch [1/1], Step [8560/15231], Loss: 0.3077\n",
            "Epoch [1/1], Step [8580/15231], Loss: 0.2947\n",
            "Epoch [1/1], Step [8600/15231], Loss: 0.3124\n",
            "Epoch [1/1], Step [8620/15231], Loss: 0.3191\n",
            "Epoch [1/1], Step [8640/15231], Loss: 0.2718\n",
            "Epoch [1/1], Step [8660/15231], Loss: 0.2972\n",
            "Epoch [1/1], Step [8680/15231], Loss: 0.3131\n",
            "Epoch [1/1], Step [8700/15231], Loss: 0.3138\n",
            "Epoch [1/1], Step [8720/15231], Loss: 0.2741\n",
            "Epoch [1/1], Step [8740/15231], Loss: 0.3071\n",
            "Epoch [1/1], Step [8760/15231], Loss: 0.3075\n",
            "Epoch [1/1], Step [8780/15231], Loss: 0.3040\n",
            "Epoch [1/1], Step [8800/15231], Loss: 0.2681\n",
            "Epoch [1/1], Step [8820/15231], Loss: 0.2750\n",
            "Epoch [1/1], Step [8840/15231], Loss: 0.2986\n",
            "Epoch [1/1], Step [8860/15231], Loss: 0.3056\n",
            "Epoch [1/1], Step [8880/15231], Loss: 0.2953\n",
            "Epoch [1/1], Step [8900/15231], Loss: 0.3149\n",
            "Epoch [1/1], Step [8920/15231], Loss: 0.3154\n",
            "Epoch [1/1], Step [8940/15231], Loss: 0.2828\n",
            "Epoch [1/1], Step [8960/15231], Loss: 0.3252\n",
            "Epoch [1/1], Step [8980/15231], Loss: 0.3206\n",
            "Epoch [1/1], Step [9000/15231], Loss: 0.2843\n",
            "Epoch [1/1], Step [9020/15231], Loss: 0.3162\n",
            "Epoch [1/1], Step [9040/15231], Loss: 0.3097\n",
            "Epoch [1/1], Step [9060/15231], Loss: 0.3277\n",
            "Epoch [1/1], Step [9080/15231], Loss: 0.3063\n",
            "Epoch [1/1], Step [9100/15231], Loss: 0.2922\n",
            "Epoch [1/1], Step [9120/15231], Loss: 0.2936\n",
            "Epoch [1/1], Step [9140/15231], Loss: 0.3401\n",
            "Epoch [1/1], Step [9160/15231], Loss: 0.3098\n",
            "Epoch [1/1], Step [9180/15231], Loss: 0.2960\n",
            "Epoch [1/1], Step [9200/15231], Loss: 0.3065\n",
            "Epoch [1/1], Step [9220/15231], Loss: 0.3300\n",
            "Epoch [1/1], Step [9240/15231], Loss: 0.2606\n",
            "Epoch [1/1], Step [9260/15231], Loss: 0.3118\n",
            "Epoch [1/1], Step [9280/15231], Loss: 0.2702\n",
            "Epoch [1/1], Step [9300/15231], Loss: 0.3240\n",
            "Epoch [1/1], Step [9320/15231], Loss: 0.3244\n",
            "Epoch [1/1], Step [9340/15231], Loss: 0.3186\n",
            "Epoch [1/1], Step [9360/15231], Loss: 0.2890\n",
            "Epoch [1/1], Step [9380/15231], Loss: 0.2923\n",
            "Epoch [1/1], Step [9400/15231], Loss: 0.2905\n",
            "Epoch [1/1], Step [9420/15231], Loss: 0.2921\n",
            "Epoch [1/1], Step [9440/15231], Loss: 0.3159\n",
            "Epoch [1/1], Step [9460/15231], Loss: 0.2701\n",
            "Epoch [1/1], Step [9480/15231], Loss: 0.2872\n",
            "Epoch [1/1], Step [9500/15231], Loss: 0.2926\n",
            "Epoch [1/1], Step [9520/15231], Loss: 0.3071\n",
            "Epoch [1/1], Step [9540/15231], Loss: 0.2678\n",
            "Epoch [1/1], Step [9560/15231], Loss: 0.2854\n",
            "Epoch [1/1], Step [9580/15231], Loss: 0.3053\n",
            "Epoch [1/1], Step [9600/15231], Loss: 0.3118\n",
            "Epoch [1/1], Step [9620/15231], Loss: 0.2986\n",
            "Epoch [1/1], Step [9640/15231], Loss: 0.3029\n",
            "Epoch [1/1], Step [9660/15231], Loss: 0.2909\n",
            "Epoch [1/1], Step [9680/15231], Loss: 0.3075\n",
            "Epoch [1/1], Step [9700/15231], Loss: 0.3070\n",
            "Epoch [1/1], Step [9720/15231], Loss: 0.2798\n",
            "Epoch [1/1], Step [9740/15231], Loss: 0.3011\n",
            "Epoch [1/1], Step [9760/15231], Loss: 0.3028\n",
            "Epoch [1/1], Step [9780/15231], Loss: 0.3120\n",
            "Epoch [1/1], Step [9800/15231], Loss: 0.2940\n",
            "Epoch [1/1], Step [9820/15231], Loss: 0.3021\n",
            "Epoch [1/1], Step [9840/15231], Loss: 0.3072\n",
            "Epoch [1/1], Step [9860/15231], Loss: 0.2984\n",
            "Epoch [1/1], Step [9880/15231], Loss: 0.2863\n",
            "Epoch [1/1], Step [9900/15231], Loss: 0.3064\n",
            "Epoch [1/1], Step [9920/15231], Loss: 0.2935\n",
            "Epoch [1/1], Step [9940/15231], Loss: 0.2920\n",
            "Epoch [1/1], Step [9960/15231], Loss: 0.3206\n",
            "Epoch [1/1], Step [9980/15231], Loss: 0.3219\n",
            "Epoch [1/1], Step [10000/15231], Loss: 0.3025\n",
            "Epoch [1/1], Step [10020/15231], Loss: 0.2899\n",
            "Epoch [1/1], Step [10040/15231], Loss: 0.3064\n",
            "Epoch [1/1], Step [10060/15231], Loss: 0.3007\n",
            "Epoch [1/1], Step [10080/15231], Loss: 0.3120\n",
            "Epoch [1/1], Step [10100/15231], Loss: 0.3094\n",
            "Epoch [1/1], Step [10120/15231], Loss: 0.2606\n",
            "Epoch [1/1], Step [10140/15231], Loss: 0.3206\n",
            "Epoch [1/1], Step [10160/15231], Loss: 0.3105\n",
            "Epoch [1/1], Step [10180/15231], Loss: 0.2933\n",
            "Epoch [1/1], Step [10200/15231], Loss: 0.2725\n",
            "Epoch [1/1], Step [10220/15231], Loss: 0.2989\n",
            "Epoch [1/1], Step [10240/15231], Loss: 0.3001\n",
            "Epoch [1/1], Step [10260/15231], Loss: 0.3109\n",
            "Epoch [1/1], Step [10280/15231], Loss: 0.2602\n",
            "Epoch [1/1], Step [10300/15231], Loss: 0.3076\n",
            "Epoch [1/1], Step [10320/15231], Loss: 0.2961\n",
            "Epoch [1/1], Step [10340/15231], Loss: 0.2515\n",
            "Epoch [1/1], Step [10360/15231], Loss: 0.3094\n",
            "Epoch [1/1], Step [10380/15231], Loss: 0.2551\n",
            "Epoch [1/1], Step [10400/15231], Loss: 0.2862\n",
            "Epoch [1/1], Step [10420/15231], Loss: 0.2924\n",
            "Epoch [1/1], Step [10440/15231], Loss: 0.3188\n",
            "Epoch [1/1], Step [10460/15231], Loss: 0.3012\n",
            "Epoch [1/1], Step [10480/15231], Loss: 0.2875\n",
            "Epoch [1/1], Step [10500/15231], Loss: 0.2903\n",
            "Epoch [1/1], Step [10520/15231], Loss: 0.2900\n",
            "Epoch [1/1], Step [10540/15231], Loss: 0.3044\n",
            "Epoch [1/1], Step [10560/15231], Loss: 0.2992\n",
            "Epoch [1/1], Step [10580/15231], Loss: 0.3015\n",
            "Epoch [1/1], Step [10600/15231], Loss: 0.2949\n",
            "Epoch [1/1], Step [10620/15231], Loss: 0.2771\n",
            "Epoch [1/1], Step [10640/15231], Loss: 0.2781\n",
            "Epoch [1/1], Step [10660/15231], Loss: 0.3108\n",
            "Epoch [1/1], Step [10680/15231], Loss: 0.2632\n",
            "Epoch [1/1], Step [10700/15231], Loss: 0.3009\n",
            "Epoch [1/1], Step [10720/15231], Loss: 0.2832\n",
            "Epoch [1/1], Step [10740/15231], Loss: 0.2968\n",
            "Epoch [1/1], Step [10760/15231], Loss: 0.2721\n",
            "Epoch [1/1], Step [10780/15231], Loss: 0.3111\n",
            "Epoch [1/1], Step [10800/15231], Loss: 0.2828\n",
            "Epoch [1/1], Step [10820/15231], Loss: 0.3043\n",
            "Epoch [1/1], Step [10840/15231], Loss: 0.3098\n",
            "Epoch [1/1], Step [10860/15231], Loss: 0.2563\n",
            "Epoch [1/1], Step [10880/15231], Loss: 0.2947\n",
            "Epoch [1/1], Step [10900/15231], Loss: 0.3039\n",
            "Epoch [1/1], Step [10920/15231], Loss: 0.2882\n",
            "Epoch [1/1], Step [10940/15231], Loss: 0.3218\n",
            "Epoch [1/1], Step [10960/15231], Loss: 0.2667\n",
            "Epoch [1/1], Step [10980/15231], Loss: 0.3051\n",
            "Epoch [1/1], Step [11000/15231], Loss: 0.3359\n",
            "Epoch [1/1], Step [11020/15231], Loss: 0.2831\n",
            "Epoch [1/1], Step [11040/15231], Loss: 0.2731\n",
            "Epoch [1/1], Step [11060/15231], Loss: 0.3045\n",
            "Epoch [1/1], Step [11080/15231], Loss: 0.2777\n",
            "Epoch [1/1], Step [11100/15231], Loss: 0.3022\n",
            "Epoch [1/1], Step [11120/15231], Loss: 0.2827\n",
            "Epoch [1/1], Step [11140/15231], Loss: 0.2740\n",
            "Epoch [1/1], Step [11160/15231], Loss: 0.3034\n",
            "Epoch [1/1], Step [11180/15231], Loss: 0.2758\n",
            "Epoch [1/1], Step [11200/15231], Loss: 0.3117\n",
            "Epoch [1/1], Step [11220/15231], Loss: 0.2752\n",
            "Epoch [1/1], Step [11240/15231], Loss: 0.3268\n",
            "Epoch [1/1], Step [11260/15231], Loss: 0.3250\n",
            "Epoch [1/1], Step [11280/15231], Loss: 0.2960\n",
            "Epoch [1/1], Step [11300/15231], Loss: 0.2922\n",
            "Epoch [1/1], Step [11320/15231], Loss: 0.2863\n",
            "Epoch [1/1], Step [11340/15231], Loss: 0.2639\n",
            "Epoch [1/1], Step [11360/15231], Loss: 0.3408\n",
            "Epoch [1/1], Step [11380/15231], Loss: 0.2900\n",
            "Epoch [1/1], Step [11400/15231], Loss: 0.2969\n",
            "Epoch [1/1], Step [11420/15231], Loss: 0.2825\n",
            "Epoch [1/1], Step [11440/15231], Loss: 0.2658\n",
            "Epoch [1/1], Step [11460/15231], Loss: 0.2724\n",
            "Epoch [1/1], Step [11480/15231], Loss: 0.2985\n",
            "Epoch [1/1], Step [11500/15231], Loss: 0.3047\n",
            "Epoch [1/1], Step [11520/15231], Loss: 0.2775\n",
            "Epoch [1/1], Step [11540/15231], Loss: 0.2952\n",
            "Epoch [1/1], Step [11560/15231], Loss: 0.2747\n",
            "Epoch [1/1], Step [11580/15231], Loss: 0.2895\n",
            "Epoch [1/1], Step [11600/15231], Loss: 0.3250\n",
            "Epoch [1/1], Step [11620/15231], Loss: 0.2700\n",
            "Epoch [1/1], Step [11640/15231], Loss: 0.3110\n",
            "Epoch [1/1], Step [11660/15231], Loss: 0.2696\n",
            "Epoch [1/1], Step [11680/15231], Loss: 0.2959\n",
            "Epoch [1/1], Step [11700/15231], Loss: 0.2736\n",
            "Epoch [1/1], Step [11720/15231], Loss: 0.2684\n",
            "Epoch [1/1], Step [11740/15231], Loss: 0.2626\n",
            "Epoch [1/1], Step [11760/15231], Loss: 0.3037\n",
            "Epoch [1/1], Step [11780/15231], Loss: 0.2790\n",
            "Epoch [1/1], Step [11800/15231], Loss: 0.2860\n",
            "Epoch [1/1], Step [11820/15231], Loss: 0.2405\n",
            "Epoch [1/1], Step [11840/15231], Loss: 0.3061\n",
            "Epoch [1/1], Step [11860/15231], Loss: 0.2862\n",
            "Epoch [1/1], Step [11880/15231], Loss: 0.2742\n",
            "Epoch [1/1], Step [11900/15231], Loss: 0.3183\n",
            "Epoch [1/1], Step [11920/15231], Loss: 0.3013\n",
            "Epoch [1/1], Step [11940/15231], Loss: 0.3026\n",
            "Epoch [1/1], Step [11960/15231], Loss: 0.2746\n",
            "Epoch [1/1], Step [11980/15231], Loss: 0.2716\n",
            "Epoch [1/1], Step [12000/15231], Loss: 0.2601\n",
            "Epoch [1/1], Step [12020/15231], Loss: 0.2650\n",
            "Epoch [1/1], Step [12040/15231], Loss: 0.2821\n",
            "Epoch [1/1], Step [12060/15231], Loss: 0.3114\n",
            "Epoch [1/1], Step [12080/15231], Loss: 0.2848\n",
            "Epoch [1/1], Step [12100/15231], Loss: 0.3390\n",
            "Epoch [1/1], Step [12120/15231], Loss: 0.2681\n",
            "Epoch [1/1], Step [12140/15231], Loss: 0.2999\n",
            "Epoch [1/1], Step [12160/15231], Loss: 0.2929\n",
            "Epoch [1/1], Step [12180/15231], Loss: 0.3063\n",
            "Epoch [1/1], Step [12200/15231], Loss: 0.2938\n",
            "Epoch [1/1], Step [12220/15231], Loss: 0.3046\n",
            "Epoch [1/1], Step [12240/15231], Loss: 0.3016\n",
            "Epoch [1/1], Step [12260/15231], Loss: 0.2912\n",
            "Epoch [1/1], Step [12280/15231], Loss: 0.2582\n",
            "Epoch [1/1], Step [12300/15231], Loss: 0.2651\n",
            "Epoch [1/1], Step [12320/15231], Loss: 0.2397\n",
            "Epoch [1/1], Step [12340/15231], Loss: 0.2690\n",
            "Epoch [1/1], Step [12360/15231], Loss: 0.2828\n",
            "Epoch [1/1], Step [12380/15231], Loss: 0.2647\n",
            "Epoch [1/1], Step [12400/15231], Loss: 0.2913\n",
            "Epoch [1/1], Step [12420/15231], Loss: 0.3033\n",
            "Epoch [1/1], Step [12440/15231], Loss: 0.2752\n",
            "Epoch [1/1], Step [12460/15231], Loss: 0.2963\n",
            "Epoch [1/1], Step [12480/15231], Loss: 0.2814\n",
            "Epoch [1/1], Step [12500/15231], Loss: 0.2962\n",
            "Epoch [1/1], Step [12520/15231], Loss: 0.2855\n",
            "Epoch [1/1], Step [12540/15231], Loss: 0.2650\n",
            "Epoch [1/1], Step [12560/15231], Loss: 0.2658\n",
            "Epoch [1/1], Step [12580/15231], Loss: 0.3006\n",
            "Epoch [1/1], Step [12600/15231], Loss: 0.2924\n",
            "Epoch [1/1], Step [12620/15231], Loss: 0.2575\n",
            "Epoch [1/1], Step [12640/15231], Loss: 0.2507\n",
            "Epoch [1/1], Step [12660/15231], Loss: 0.2695\n",
            "Epoch [1/1], Step [12680/15231], Loss: 0.3007\n",
            "Epoch [1/1], Step [12700/15231], Loss: 0.2972\n",
            "Epoch [1/1], Step [12720/15231], Loss: 0.2581\n",
            "Epoch [1/1], Step [12740/15231], Loss: 0.3098\n",
            "Epoch [1/1], Step [12760/15231], Loss: 0.2691\n",
            "Epoch [1/1], Step [12780/15231], Loss: 0.2741\n",
            "Epoch [1/1], Step [12800/15231], Loss: 0.2844\n",
            "Epoch [1/1], Step [12820/15231], Loss: 0.2859\n",
            "Epoch [1/1], Step [12840/15231], Loss: 0.2920\n",
            "Epoch [1/1], Step [12860/15231], Loss: 0.2780\n",
            "Epoch [1/1], Step [12880/15231], Loss: 0.2882\n",
            "Epoch [1/1], Step [12900/15231], Loss: 0.2764\n",
            "Epoch [1/1], Step [12920/15231], Loss: 0.2833\n",
            "Epoch [1/1], Step [12940/15231], Loss: 0.2946\n",
            "Epoch [1/1], Step [12960/15231], Loss: 0.2958\n",
            "Epoch [1/1], Step [12980/15231], Loss: 0.3147\n",
            "Epoch [1/1], Step [13000/15231], Loss: 0.2452\n",
            "Epoch [1/1], Step [13020/15231], Loss: 0.3288\n",
            "Epoch [1/1], Step [13040/15231], Loss: 0.2745\n",
            "Epoch [1/1], Step [13060/15231], Loss: 0.2722\n",
            "Epoch [1/1], Step [13080/15231], Loss: 0.2918\n",
            "Epoch [1/1], Step [13100/15231], Loss: 0.2740\n",
            "Epoch [1/1], Step [13120/15231], Loss: 0.2725\n",
            "Epoch [1/1], Step [13140/15231], Loss: 0.2597\n",
            "Epoch [1/1], Step [13160/15231], Loss: 0.2590\n",
            "Epoch [1/1], Step [13180/15231], Loss: 0.2569\n",
            "Epoch [1/1], Step [13200/15231], Loss: 0.2478\n",
            "Epoch [1/1], Step [13220/15231], Loss: 0.3007\n",
            "Epoch [1/1], Step [13240/15231], Loss: 0.3064\n",
            "Epoch [1/1], Step [13260/15231], Loss: 0.2991\n",
            "Epoch [1/1], Step [13280/15231], Loss: 0.2644\n",
            "Epoch [1/1], Step [13300/15231], Loss: 0.2619\n",
            "Epoch [1/1], Step [13320/15231], Loss: 0.2774\n",
            "Epoch [1/1], Step [13340/15231], Loss: 0.2881\n",
            "Epoch [1/1], Step [13360/15231], Loss: 0.2672\n",
            "Epoch [1/1], Step [13380/15231], Loss: 0.2816\n",
            "Epoch [1/1], Step [13400/15231], Loss: 0.2968\n",
            "Epoch [1/1], Step [13420/15231], Loss: 0.3047\n",
            "Epoch [1/1], Step [13440/15231], Loss: 0.2905\n",
            "Epoch [1/1], Step [13460/15231], Loss: 0.3414\n",
            "Epoch [1/1], Step [13480/15231], Loss: 0.2657\n",
            "Epoch [1/1], Step [13500/15231], Loss: 0.2659\n",
            "Epoch [1/1], Step [13520/15231], Loss: 0.2802\n",
            "Epoch [1/1], Step [13540/15231], Loss: 0.2529\n",
            "Epoch [1/1], Step [13560/15231], Loss: 0.2901\n",
            "Epoch [1/1], Step [13580/15231], Loss: 0.2584\n",
            "Epoch [1/1], Step [13600/15231], Loss: 0.2993\n",
            "Epoch [1/1], Step [13620/15231], Loss: 0.2612\n",
            "Epoch [1/1], Step [13640/15231], Loss: 0.2800\n",
            "Epoch [1/1], Step [13660/15231], Loss: 0.2642\n",
            "Epoch [1/1], Step [13680/15231], Loss: 0.2363\n",
            "Epoch [1/1], Step [13700/15231], Loss: 0.2416\n",
            "Epoch [1/1], Step [13720/15231], Loss: 0.2967\n",
            "Epoch [1/1], Step [13740/15231], Loss: 0.3057\n",
            "Epoch [1/1], Step [13760/15231], Loss: 0.2724\n",
            "Epoch [1/1], Step [13780/15231], Loss: 0.2635\n",
            "Epoch [1/1], Step [13800/15231], Loss: 0.3001\n",
            "Epoch [1/1], Step [13820/15231], Loss: 0.2706\n",
            "Epoch [1/1], Step [13840/15231], Loss: 0.2930\n",
            "Epoch [1/1], Step [13860/15231], Loss: 0.2875\n",
            "Epoch [1/1], Step [13880/15231], Loss: 0.2680\n",
            "Epoch [1/1], Step [13900/15231], Loss: 0.3024\n",
            "Epoch [1/1], Step [13920/15231], Loss: 0.2728\n",
            "Epoch [1/1], Step [13940/15231], Loss: 0.2814\n",
            "Epoch [1/1], Step [13960/15231], Loss: 0.2655\n",
            "Epoch [1/1], Step [13980/15231], Loss: 0.2923\n",
            "Epoch [1/1], Step [14000/15231], Loss: 0.2831\n",
            "Epoch [1/1], Step [14020/15231], Loss: 0.2898\n",
            "Epoch [1/1], Step [14040/15231], Loss: 0.3034\n",
            "Epoch [1/1], Step [14060/15231], Loss: 0.2485\n",
            "Epoch [1/1], Step [14080/15231], Loss: 0.2582\n",
            "Epoch [1/1], Step [14100/15231], Loss: 0.2937\n",
            "Epoch [1/1], Step [14120/15231], Loss: 0.3310\n",
            "Epoch [1/1], Step [14140/15231], Loss: 0.2555\n",
            "Epoch [1/1], Step [14160/15231], Loss: 0.2594\n",
            "Epoch [1/1], Step [14180/15231], Loss: 0.2953\n",
            "Epoch [1/1], Step [14200/15231], Loss: 0.2549\n",
            "Epoch [1/1], Step [14220/15231], Loss: 0.2907\n",
            "Epoch [1/1], Step [14240/15231], Loss: 0.2907\n",
            "Epoch [1/1], Step [14260/15231], Loss: 0.2849\n",
            "Epoch [1/1], Step [14280/15231], Loss: 0.2822\n",
            "Epoch [1/1], Step [14300/15231], Loss: 0.2479\n",
            "Epoch [1/1], Step [14320/15231], Loss: 0.2664\n",
            "Epoch [1/1], Step [14340/15231], Loss: 0.2527\n",
            "Epoch [1/1], Step [14360/15231], Loss: 0.2802\n",
            "Epoch [1/1], Step [14380/15231], Loss: 0.2564\n",
            "Epoch [1/1], Step [14400/15231], Loss: 0.2604\n",
            "Epoch [1/1], Step [14420/15231], Loss: 0.2453\n",
            "Epoch [1/1], Step [14440/15231], Loss: 0.2683\n",
            "Epoch [1/1], Step [14460/15231], Loss: 0.2604\n",
            "Epoch [1/1], Step [14480/15231], Loss: 0.2558\n",
            "Epoch [1/1], Step [14500/15231], Loss: 0.2659\n",
            "Epoch [1/1], Step [14520/15231], Loss: 0.2850\n",
            "Epoch [1/1], Step [14540/15231], Loss: 0.2920\n",
            "Epoch [1/1], Step [14560/15231], Loss: 0.2900\n",
            "Epoch [1/1], Step [14580/15231], Loss: 0.2532\n",
            "Epoch [1/1], Step [14600/15231], Loss: 0.3194\n",
            "Epoch [1/1], Step [14620/15231], Loss: 0.2830\n",
            "Epoch [1/1], Step [14640/15231], Loss: 0.2919\n",
            "Epoch [1/1], Step [14660/15231], Loss: 0.3001\n",
            "Epoch [1/1], Step [14680/15231], Loss: 0.2700\n",
            "Epoch [1/1], Step [14700/15231], Loss: 0.2740\n",
            "Epoch [1/1], Step [14720/15231], Loss: 0.2430\n",
            "Epoch [1/1], Step [14740/15231], Loss: 0.2847\n",
            "Epoch [1/1], Step [14760/15231], Loss: 0.2708\n",
            "Epoch [1/1], Step [14780/15231], Loss: 0.2757\n",
            "Epoch [1/1], Step [14800/15231], Loss: 0.2657\n",
            "Epoch [1/1], Step [14820/15231], Loss: 0.2868\n",
            "Epoch [1/1], Step [14840/15231], Loss: 0.2649\n",
            "Epoch [1/1], Step [14860/15231], Loss: 0.2552\n",
            "Epoch [1/1], Step [14880/15231], Loss: 0.2397\n",
            "Epoch [1/1], Step [14900/15231], Loss: 0.2742\n",
            "Epoch [1/1], Step [14920/15231], Loss: 0.2804\n",
            "Epoch [1/1], Step [14940/15231], Loss: 0.2849\n",
            "Epoch [1/1], Step [14960/15231], Loss: 0.2321\n",
            "Epoch [1/1], Step [14980/15231], Loss: 0.2564\n",
            "Epoch [1/1], Step [15000/15231], Loss: 0.2377\n",
            "Epoch [1/1], Step [15020/15231], Loss: 0.2933\n",
            "Epoch [1/1], Step [15040/15231], Loss: 0.2909\n",
            "Epoch [1/1], Step [15060/15231], Loss: 0.2737\n",
            "Epoch [1/1], Step [15080/15231], Loss: 0.2559\n",
            "Epoch [1/1], Step [15100/15231], Loss: 0.2702\n",
            "Epoch [1/1], Step [15120/15231], Loss: 0.3075\n",
            "Epoch [1/1], Step [15140/15231], Loss: 0.2503\n",
            "Epoch [1/1], Step [15160/15231], Loss: 0.2724\n",
            "Epoch [1/1], Step [15180/15231], Loss: 0.2895\n",
            "Epoch [1/1], Step [15200/15231], Loss: 0.2886\n",
            "Epoch [1/1], Step [15220/15231], Loss: 0.2752\n",
            "Epoch [1/1] | Train Loss: 0.3379 | Train Acc: 88.30% | Test Loss: 0.5986 | Test Acc: 77.80%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy per epoch</td><td>▁</td></tr><tr><td>train_accuracy per epoch</td><td>▁</td></tr><tr><td>train_loss</td><td>█▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>test_accuracy per epoch</td><td>77.8</td></tr><tr><td>train_accuracy per epoch</td><td>88.2956</td></tr><tr><td>train_loss</td><td>0.27521</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet-18 with data aug (1 epoch)</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/drm4su0v' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/drm4su0v</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT</a><br>Synced 5 W&B file(s), 2 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250403_183521-drm4su0v/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Resnet-3"
      ],
      "metadata": {
        "id": "h-jiL1fPEy3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "\n",
        "name = \"resnet-3 with data aug\"\n",
        "\n",
        "# Configuration dictionary for wandb\n",
        "config = {\n",
        "    \"in_channels\": 1,\n",
        "    \"num_epochs\": 5,            # Adjust number of epochs as necessary\n",
        "    \"lr\": 0.001,                # Learning rate\n",
        "    \"batch_size\": 64,           # Batch size (should match your DataLoader)\n",
        "    \"num_blocks\": 3,            # Number of residual blocks in the network\n",
        "    \"base_channels\": 64,        # Number of filters in the first block\n",
        "    \"kernel_size\": 3,           # Kernel size for convolutions\n",
        "    \"activation\": \"LeakyReLU\",  # Name of the activation function\n",
        "    \"use_batchnorm\": True,      # Whether to use batch normalization\n",
        "    \"num_classes\": 4            # OCTMNIST has 4 classes\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUXMAR-HD2a3",
        "outputId": "9209dc2b-064c-4412-f58e-2ab1d99dd61c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model with custom configuration\n",
        "model = CustomResNet(\n",
        "        in_channels=config['in_channels'],  # OCTMNIST images are single-channel if you have preprocessed accordingly\n",
        "        num_blocks=config[\"num_blocks\"],\n",
        "        base_channels=config[\"base_channels\"],\n",
        "        kernel_size=config[\"kernel_size\"],\n",
        "        activation=nn.LeakyReLU(0.1, inplace=True),\n",
        "        use_batchnorm=config[\"use_batchnorm\"],\n",
        "        num_classes=config[\"num_classes\"]\n",
        "    )\n",
        "print(model)\n",
        "\n",
        "print(\"number of parameters : \")\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcRoDbhkEaPC",
        "outputId": "963573dd-7560-4f68-bd87-e3aed8c5299f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomResNet(\n",
            "  (initial_conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (initial_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  (residual_layers): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "  )\n",
            "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "number of parameters : \n",
            "222916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb run and train the model\n",
        "with wandb.init(\n",
        "    config=config,\n",
        "    project='CNN VS ViT',   # Title of your project\n",
        "    group='Test',           # Group name for your run\n",
        "    save_code=True,\n",
        "    name=name,\n",
        "):\n",
        "\n",
        "    # Train the model (train_loader and test_loader should be defined before)\n",
        "    train_model(model, config, name)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Id-iYRClEXNY",
        "outputId": "2591f731-81b7-406f-cfb8-d637ffb70646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250404_134801-ckvbfm2w</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ckvbfm2w' target=\"_blank\">resnet-3 with data aug</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ckvbfm2w' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ckvbfm2w</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [20/15231], Loss: 1.0101\n",
            "Epoch [1/5], Step [40/15231], Loss: 0.8249\n",
            "Epoch [1/5], Step [60/15231], Loss: 0.7395\n",
            "Epoch [1/5], Step [80/15231], Loss: 0.6796\n",
            "Epoch [1/5], Step [100/15231], Loss: 0.7375\n",
            "Epoch [1/5], Step [120/15231], Loss: 0.7404\n",
            "Epoch [1/5], Step [140/15231], Loss: 0.6529\n",
            "Epoch [1/5], Step [160/15231], Loss: 0.6288\n",
            "Epoch [1/5], Step [180/15231], Loss: 0.6157\n",
            "Epoch [1/5], Step [200/15231], Loss: 0.6088\n",
            "Epoch [1/5], Step [220/15231], Loss: 0.6501\n",
            "Epoch [1/5], Step [240/15231], Loss: 0.6224\n",
            "Epoch [1/5], Step [260/15231], Loss: 0.6017\n",
            "Epoch [1/5], Step [280/15231], Loss: 0.6473\n",
            "Epoch [1/5], Step [300/15231], Loss: 0.5842\n",
            "Epoch [1/5], Step [320/15231], Loss: 0.5249\n",
            "Epoch [1/5], Step [340/15231], Loss: 0.6389\n",
            "Epoch [1/5], Step [360/15231], Loss: 0.5811\n",
            "Epoch [1/5], Step [380/15231], Loss: 0.5950\n",
            "Epoch [1/5], Step [400/15231], Loss: 0.5955\n",
            "Epoch [1/5], Step [420/15231], Loss: 0.5359\n",
            "Epoch [1/5], Step [440/15231], Loss: 0.5849\n",
            "Epoch [1/5], Step [460/15231], Loss: 0.5780\n",
            "Epoch [1/5], Step [480/15231], Loss: 0.5528\n",
            "Epoch [1/5], Step [500/15231], Loss: 0.5365\n",
            "Epoch [1/5], Step [520/15231], Loss: 0.5557\n",
            "Epoch [1/5], Step [540/15231], Loss: 0.5533\n",
            "Epoch [1/5], Step [560/15231], Loss: 0.5390\n",
            "Epoch [1/5], Step [580/15231], Loss: 0.5164\n",
            "Epoch [1/5], Step [600/15231], Loss: 0.5002\n",
            "Epoch [1/5], Step [620/15231], Loss: 0.4975\n",
            "Epoch [1/5], Step [640/15231], Loss: 0.5232\n",
            "Epoch [1/5], Step [660/15231], Loss: 0.4863\n",
            "Epoch [1/5], Step [680/15231], Loss: 0.4989\n",
            "Epoch [1/5], Step [700/15231], Loss: 0.5197\n",
            "Epoch [1/5], Step [720/15231], Loss: 0.4801\n",
            "Epoch [1/5], Step [740/15231], Loss: 0.5119\n",
            "Epoch [1/5], Step [760/15231], Loss: 0.5141\n",
            "Epoch [1/5], Step [780/15231], Loss: 0.4869\n",
            "Epoch [1/5], Step [800/15231], Loss: 0.5433\n",
            "Epoch [1/5], Step [820/15231], Loss: 0.5389\n",
            "Epoch [1/5], Step [840/15231], Loss: 0.5259\n",
            "Epoch [1/5], Step [860/15231], Loss: 0.5053\n",
            "Epoch [1/5], Step [880/15231], Loss: 0.4837\n",
            "Epoch [1/5], Step [900/15231], Loss: 0.5300\n",
            "Epoch [1/5], Step [920/15231], Loss: 0.4555\n",
            "Epoch [1/5], Step [940/15231], Loss: 0.5357\n",
            "Epoch [1/5], Step [960/15231], Loss: 0.4593\n",
            "Epoch [1/5], Step [980/15231], Loss: 0.4677\n",
            "Epoch [1/5], Step [1000/15231], Loss: 0.4481\n",
            "Epoch [1/5], Step [1020/15231], Loss: 0.4435\n",
            "Epoch [1/5], Step [1040/15231], Loss: 0.5002\n",
            "Epoch [1/5], Step [1060/15231], Loss: 0.5273\n",
            "Epoch [1/5], Step [1080/15231], Loss: 0.5058\n",
            "Epoch [1/5], Step [1100/15231], Loss: 0.4367\n",
            "Epoch [1/5], Step [1120/15231], Loss: 0.4700\n",
            "Epoch [1/5], Step [1140/15231], Loss: 0.4366\n",
            "Epoch [1/5], Step [1160/15231], Loss: 0.5110\n",
            "Epoch [1/5], Step [1180/15231], Loss: 0.4254\n",
            "Epoch [1/5], Step [1200/15231], Loss: 0.4977\n",
            "Epoch [1/5], Step [1220/15231], Loss: 0.4429\n",
            "Epoch [1/5], Step [1240/15231], Loss: 0.4611\n",
            "Epoch [1/5], Step [1260/15231], Loss: 0.4327\n",
            "Epoch [1/5], Step [1280/15231], Loss: 0.4451\n",
            "Epoch [1/5], Step [1300/15231], Loss: 0.4668\n",
            "Epoch [1/5], Step [1320/15231], Loss: 0.4599\n",
            "Epoch [1/5], Step [1340/15231], Loss: 0.4639\n",
            "Epoch [1/5], Step [1360/15231], Loss: 0.4307\n",
            "Epoch [1/5], Step [1380/15231], Loss: 0.4906\n",
            "Epoch [1/5], Step [1400/15231], Loss: 0.4282\n",
            "Epoch [1/5], Step [1420/15231], Loss: 0.4079\n",
            "Epoch [1/5], Step [1440/15231], Loss: 0.4373\n",
            "Epoch [1/5], Step [1460/15231], Loss: 0.4643\n",
            "Epoch [1/5], Step [1480/15231], Loss: 0.4397\n",
            "Epoch [1/5], Step [1500/15231], Loss: 0.4215\n",
            "Epoch [1/5], Step [1520/15231], Loss: 0.4750\n",
            "Epoch [1/5], Step [1540/15231], Loss: 0.4496\n",
            "Epoch [1/5], Step [1560/15231], Loss: 0.4442\n",
            "Epoch [1/5], Step [1580/15231], Loss: 0.4438\n",
            "Epoch [1/5], Step [1600/15231], Loss: 0.4055\n",
            "Epoch [1/5], Step [1620/15231], Loss: 0.4129\n",
            "Epoch [1/5], Step [1640/15231], Loss: 0.4411\n",
            "Epoch [1/5], Step [1660/15231], Loss: 0.4707\n",
            "Epoch [1/5], Step [1680/15231], Loss: 0.4604\n",
            "Epoch [1/5], Step [1700/15231], Loss: 0.4819\n",
            "Epoch [1/5], Step [1720/15231], Loss: 0.4535\n",
            "Epoch [1/5], Step [1740/15231], Loss: 0.4344\n",
            "Epoch [1/5], Step [1760/15231], Loss: 0.4217\n",
            "Epoch [1/5], Step [1780/15231], Loss: 0.4170\n",
            "Epoch [1/5], Step [1800/15231], Loss: 0.4327\n",
            "Epoch [1/5], Step [1820/15231], Loss: 0.4368\n",
            "Epoch [1/5], Step [1840/15231], Loss: 0.4095\n",
            "Epoch [1/5], Step [1860/15231], Loss: 0.4274\n",
            "Epoch [1/5], Step [1880/15231], Loss: 0.4309\n",
            "Epoch [1/5], Step [1900/15231], Loss: 0.3855\n",
            "Epoch [1/5], Step [1920/15231], Loss: 0.3972\n",
            "Epoch [1/5], Step [1940/15231], Loss: 0.4071\n",
            "Epoch [1/5], Step [1960/15231], Loss: 0.4186\n",
            "Epoch [1/5], Step [1980/15231], Loss: 0.4133\n",
            "Epoch [1/5], Step [2000/15231], Loss: 0.3843\n",
            "Epoch [1/5], Step [2020/15231], Loss: 0.3882\n",
            "Epoch [1/5], Step [2040/15231], Loss: 0.4481\n",
            "Epoch [1/5], Step [2060/15231], Loss: 0.4116\n",
            "Epoch [1/5], Step [2080/15231], Loss: 0.4020\n",
            "Epoch [1/5], Step [2100/15231], Loss: 0.4023\n",
            "Epoch [1/5], Step [2120/15231], Loss: 0.3892\n",
            "Epoch [1/5], Step [2140/15231], Loss: 0.4044\n",
            "Epoch [1/5], Step [2160/15231], Loss: 0.4698\n",
            "Epoch [1/5], Step [2180/15231], Loss: 0.4234\n",
            "Epoch [1/5], Step [2200/15231], Loss: 0.4326\n",
            "Epoch [1/5], Step [2220/15231], Loss: 0.4156\n",
            "Epoch [1/5], Step [2240/15231], Loss: 0.4417\n",
            "Epoch [1/5], Step [2260/15231], Loss: 0.4158\n",
            "Epoch [1/5], Step [2280/15231], Loss: 0.3934\n",
            "Epoch [1/5], Step [2300/15231], Loss: 0.4164\n",
            "Epoch [1/5], Step [2320/15231], Loss: 0.4143\n",
            "Epoch [1/5], Step [2340/15231], Loss: 0.4295\n",
            "Epoch [1/5], Step [2360/15231], Loss: 0.4051\n",
            "Epoch [1/5], Step [2380/15231], Loss: 0.3743\n",
            "Epoch [1/5], Step [2400/15231], Loss: 0.4092\n",
            "Epoch [1/5], Step [2420/15231], Loss: 0.3809\n",
            "Epoch [1/5], Step [2440/15231], Loss: 0.4277\n",
            "Epoch [1/5], Step [2460/15231], Loss: 0.3849\n",
            "Epoch [1/5], Step [2480/15231], Loss: 0.4200\n",
            "Epoch [1/5], Step [2500/15231], Loss: 0.4159\n",
            "Epoch [1/5], Step [2520/15231], Loss: 0.4024\n",
            "Epoch [1/5], Step [2540/15231], Loss: 0.3762\n",
            "Epoch [1/5], Step [2560/15231], Loss: 0.3681\n",
            "Epoch [1/5], Step [2580/15231], Loss: 0.3896\n",
            "Epoch [1/5], Step [2600/15231], Loss: 0.3806\n",
            "Epoch [1/5], Step [2620/15231], Loss: 0.3912\n",
            "Epoch [1/5], Step [2640/15231], Loss: 0.3973\n",
            "Epoch [1/5], Step [2660/15231], Loss: 0.4173\n",
            "Epoch [1/5], Step [2680/15231], Loss: 0.4035\n",
            "Epoch [1/5], Step [2700/15231], Loss: 0.3848\n",
            "Epoch [1/5], Step [2720/15231], Loss: 0.4091\n",
            "Epoch [1/5], Step [2740/15231], Loss: 0.3732\n",
            "Epoch [1/5], Step [2760/15231], Loss: 0.4338\n",
            "Epoch [1/5], Step [2780/15231], Loss: 0.3842\n",
            "Epoch [1/5], Step [2800/15231], Loss: 0.4198\n",
            "Epoch [1/5], Step [2820/15231], Loss: 0.3889\n",
            "Epoch [1/5], Step [2840/15231], Loss: 0.3635\n",
            "Epoch [1/5], Step [2860/15231], Loss: 0.3837\n",
            "Epoch [1/5], Step [2880/15231], Loss: 0.3750\n",
            "Epoch [1/5], Step [2900/15231], Loss: 0.4208\n",
            "Epoch [1/5], Step [2920/15231], Loss: 0.4049\n",
            "Epoch [1/5], Step [2940/15231], Loss: 0.3792\n",
            "Epoch [1/5], Step [2960/15231], Loss: 0.3892\n",
            "Epoch [1/5], Step [2980/15231], Loss: 0.3893\n",
            "Epoch [1/5], Step [3000/15231], Loss: 0.3714\n",
            "Epoch [1/5], Step [3020/15231], Loss: 0.4073\n",
            "Epoch [1/5], Step [3040/15231], Loss: 0.3850\n",
            "Epoch [1/5], Step [3060/15231], Loss: 0.3946\n",
            "Epoch [1/5], Step [3080/15231], Loss: 0.3677\n",
            "Epoch [1/5], Step [3100/15231], Loss: 0.4108\n",
            "Epoch [1/5], Step [3120/15231], Loss: 0.3908\n",
            "Epoch [1/5], Step [3140/15231], Loss: 0.3790\n",
            "Epoch [1/5], Step [3160/15231], Loss: 0.3968\n",
            "Epoch [1/5], Step [3180/15231], Loss: 0.3773\n",
            "Epoch [1/5], Step [3200/15231], Loss: 0.3824\n",
            "Epoch [1/5], Step [3220/15231], Loss: 0.3732\n",
            "Epoch [1/5], Step [3240/15231], Loss: 0.4105\n",
            "Epoch [1/5], Step [3260/15231], Loss: 0.3657\n",
            "Epoch [1/5], Step [3280/15231], Loss: 0.3670\n",
            "Epoch [1/5], Step [3300/15231], Loss: 0.3342\n",
            "Epoch [1/5], Step [3320/15231], Loss: 0.3769\n",
            "Epoch [1/5], Step [3340/15231], Loss: 0.3897\n",
            "Epoch [1/5], Step [3360/15231], Loss: 0.3673\n",
            "Epoch [1/5], Step [3380/15231], Loss: 0.3667\n",
            "Epoch [1/5], Step [3400/15231], Loss: 0.3587\n",
            "Epoch [1/5], Step [3420/15231], Loss: 0.3824\n",
            "Epoch [1/5], Step [3440/15231], Loss: 0.3812\n",
            "Epoch [1/5], Step [3460/15231], Loss: 0.3852\n",
            "Epoch [1/5], Step [3480/15231], Loss: 0.3781\n",
            "Epoch [1/5], Step [3500/15231], Loss: 0.4056\n",
            "Epoch [1/5], Step [3520/15231], Loss: 0.4046\n",
            "Epoch [1/5], Step [3540/15231], Loss: 0.4035\n",
            "Epoch [1/5], Step [3560/15231], Loss: 0.3874\n",
            "Epoch [1/5], Step [3580/15231], Loss: 0.3935\n",
            "Epoch [1/5], Step [3600/15231], Loss: 0.3602\n",
            "Epoch [1/5], Step [3620/15231], Loss: 0.3794\n",
            "Epoch [1/5], Step [3640/15231], Loss: 0.3885\n",
            "Epoch [1/5], Step [3660/15231], Loss: 0.3723\n",
            "Epoch [1/5], Step [3680/15231], Loss: 0.3977\n",
            "Epoch [1/5], Step [3700/15231], Loss: 0.3867\n",
            "Epoch [1/5], Step [3720/15231], Loss: 0.3674\n",
            "Epoch [1/5], Step [3740/15231], Loss: 0.3680\n",
            "Epoch [1/5], Step [3760/15231], Loss: 0.3548\n",
            "Epoch [1/5], Step [3780/15231], Loss: 0.3395\n",
            "Epoch [1/5], Step [3800/15231], Loss: 0.3990\n",
            "Epoch [1/5], Step [3820/15231], Loss: 0.3897\n",
            "Epoch [1/5], Step [3840/15231], Loss: 0.3457\n",
            "Epoch [1/5], Step [3860/15231], Loss: 0.3919\n",
            "Epoch [1/5], Step [3880/15231], Loss: 0.3921\n",
            "Epoch [1/5], Step [3900/15231], Loss: 0.3992\n",
            "Epoch [1/5], Step [3920/15231], Loss: 0.3587\n",
            "Epoch [1/5], Step [3940/15231], Loss: 0.3502\n",
            "Epoch [1/5], Step [3960/15231], Loss: 0.3661\n",
            "Epoch [1/5], Step [3980/15231], Loss: 0.3527\n",
            "Epoch [1/5], Step [4000/15231], Loss: 0.4187\n",
            "Epoch [1/5], Step [4020/15231], Loss: 0.3870\n",
            "Epoch [1/5], Step [4040/15231], Loss: 0.3541\n",
            "Epoch [1/5], Step [4060/15231], Loss: 0.3470\n",
            "Epoch [1/5], Step [4080/15231], Loss: 0.3457\n",
            "Epoch [1/5], Step [4100/15231], Loss: 0.3572\n",
            "Epoch [1/5], Step [4120/15231], Loss: 0.3710\n",
            "Epoch [1/5], Step [4140/15231], Loss: 0.3767\n",
            "Epoch [1/5], Step [4160/15231], Loss: 0.3479\n",
            "Epoch [1/5], Step [4180/15231], Loss: 0.3385\n",
            "Epoch [1/5], Step [4200/15231], Loss: 0.3461\n",
            "Epoch [1/5], Step [4220/15231], Loss: 0.3713\n",
            "Epoch [1/5], Step [4240/15231], Loss: 0.3890\n",
            "Epoch [1/5], Step [4260/15231], Loss: 0.3559\n",
            "Epoch [1/5], Step [4280/15231], Loss: 0.3773\n",
            "Epoch [1/5], Step [4300/15231], Loss: 0.3455\n",
            "Epoch [1/5], Step [4320/15231], Loss: 0.3716\n",
            "Epoch [1/5], Step [4340/15231], Loss: 0.3717\n",
            "Epoch [1/5], Step [4360/15231], Loss: 0.3889\n",
            "Epoch [1/5], Step [4380/15231], Loss: 0.3519\n",
            "Epoch [1/5], Step [4400/15231], Loss: 0.3647\n",
            "Epoch [1/5], Step [4420/15231], Loss: 0.3371\n",
            "Epoch [1/5], Step [4440/15231], Loss: 0.3434\n",
            "Epoch [1/5], Step [4460/15231], Loss: 0.3497\n",
            "Epoch [1/5], Step [4480/15231], Loss: 0.3356\n",
            "Epoch [1/5], Step [4500/15231], Loss: 0.3592\n",
            "Epoch [1/5], Step [4520/15231], Loss: 0.3846\n",
            "Epoch [1/5], Step [4540/15231], Loss: 0.3774\n",
            "Epoch [1/5], Step [4560/15231], Loss: 0.3570\n",
            "Epoch [1/5], Step [4580/15231], Loss: 0.3405\n",
            "Epoch [1/5], Step [4600/15231], Loss: 0.3297\n",
            "Epoch [1/5], Step [4620/15231], Loss: 0.3510\n",
            "Epoch [1/5], Step [4640/15231], Loss: 0.3990\n",
            "Epoch [1/5], Step [4660/15231], Loss: 0.3534\n",
            "Epoch [1/5], Step [4680/15231], Loss: 0.3344\n",
            "Epoch [1/5], Step [4700/15231], Loss: 0.3499\n",
            "Epoch [1/5], Step [4720/15231], Loss: 0.3553\n",
            "Epoch [1/5], Step [4740/15231], Loss: 0.3530\n",
            "Epoch [1/5], Step [4760/15231], Loss: 0.3317\n",
            "Epoch [1/5], Step [4780/15231], Loss: 0.3454\n",
            "Epoch [1/5], Step [4800/15231], Loss: 0.3582\n",
            "Epoch [1/5], Step [4820/15231], Loss: 0.3756\n",
            "Epoch [1/5], Step [4840/15231], Loss: 0.3672\n",
            "Epoch [1/5], Step [4860/15231], Loss: 0.3594\n",
            "Epoch [1/5], Step [4880/15231], Loss: 0.3443\n",
            "Epoch [1/5], Step [4900/15231], Loss: 0.3181\n",
            "Epoch [1/5], Step [4920/15231], Loss: 0.3955\n",
            "Epoch [1/5], Step [4940/15231], Loss: 0.3330\n",
            "Epoch [1/5], Step [4960/15231], Loss: 0.3743\n",
            "Epoch [1/5], Step [4980/15231], Loss: 0.3516\n",
            "Epoch [1/5], Step [5000/15231], Loss: 0.3561\n",
            "Epoch [1/5], Step [5020/15231], Loss: 0.3389\n",
            "Epoch [1/5], Step [5040/15231], Loss: 0.3814\n",
            "Epoch [1/5], Step [5060/15231], Loss: 0.3604\n",
            "Epoch [1/5], Step [5080/15231], Loss: 0.3503\n",
            "Epoch [1/5], Step [5100/15231], Loss: 0.3477\n",
            "Epoch [1/5], Step [5120/15231], Loss: 0.3401\n",
            "Epoch [1/5], Step [5140/15231], Loss: 0.3716\n",
            "Epoch [1/5], Step [5160/15231], Loss: 0.3486\n",
            "Epoch [1/5], Step [5180/15231], Loss: 0.3242\n",
            "Epoch [1/5], Step [5200/15231], Loss: 0.3303\n",
            "Epoch [1/5], Step [5220/15231], Loss: 0.3478\n",
            "Epoch [1/5], Step [5240/15231], Loss: 0.3570\n",
            "Epoch [1/5], Step [5260/15231], Loss: 0.3245\n",
            "Epoch [1/5], Step [5280/15231], Loss: 0.3554\n",
            "Epoch [1/5], Step [5300/15231], Loss: 0.3855\n",
            "Epoch [1/5], Step [5320/15231], Loss: 0.3260\n",
            "Epoch [1/5], Step [5340/15231], Loss: 0.3784\n",
            "Epoch [1/5], Step [5360/15231], Loss: 0.3762\n",
            "Epoch [1/5], Step [5380/15231], Loss: 0.3520\n",
            "Epoch [1/5], Step [5400/15231], Loss: 0.3686\n",
            "Epoch [1/5], Step [5420/15231], Loss: 0.3343\n",
            "Epoch [1/5], Step [5440/15231], Loss: 0.3543\n",
            "Epoch [1/5], Step [5460/15231], Loss: 0.3842\n",
            "Epoch [1/5], Step [5480/15231], Loss: 0.3902\n",
            "Epoch [1/5], Step [5500/15231], Loss: 0.3340\n",
            "Epoch [1/5], Step [5520/15231], Loss: 0.3698\n",
            "Epoch [1/5], Step [5540/15231], Loss: 0.3441\n",
            "Epoch [1/5], Step [5560/15231], Loss: 0.3260\n",
            "Epoch [1/5], Step [5580/15231], Loss: 0.3486\n",
            "Epoch [1/5], Step [5600/15231], Loss: 0.3410\n",
            "Epoch [1/5], Step [5620/15231], Loss: 0.3309\n",
            "Epoch [1/5], Step [5640/15231], Loss: 0.3360\n",
            "Epoch [1/5], Step [5660/15231], Loss: 0.3888\n",
            "Epoch [1/5], Step [5680/15231], Loss: 0.3503\n",
            "Epoch [1/5], Step [5700/15231], Loss: 0.3535\n",
            "Epoch [1/5], Step [5720/15231], Loss: 0.3306\n",
            "Epoch [1/5], Step [5740/15231], Loss: 0.3209\n",
            "Epoch [1/5], Step [5760/15231], Loss: 0.3358\n",
            "Epoch [1/5], Step [5780/15231], Loss: 0.3144\n",
            "Epoch [1/5], Step [5800/15231], Loss: 0.3606\n",
            "Epoch [1/5], Step [5820/15231], Loss: 0.3214\n",
            "Epoch [1/5], Step [5840/15231], Loss: 0.3637\n",
            "Epoch [1/5], Step [5860/15231], Loss: 0.3745\n",
            "Epoch [1/5], Step [5880/15231], Loss: 0.3483\n",
            "Epoch [1/5], Step [5900/15231], Loss: 0.3079\n",
            "Epoch [1/5], Step [5920/15231], Loss: 0.3568\n",
            "Epoch [1/5], Step [5940/15231], Loss: 0.3651\n",
            "Epoch [1/5], Step [5960/15231], Loss: 0.3398\n",
            "Epoch [1/5], Step [5980/15231], Loss: 0.3321\n",
            "Epoch [1/5], Step [6000/15231], Loss: 0.3355\n",
            "Epoch [1/5], Step [6020/15231], Loss: 0.3729\n",
            "Epoch [1/5], Step [6040/15231], Loss: 0.3493\n",
            "Epoch [1/5], Step [6060/15231], Loss: 0.3645\n",
            "Epoch [1/5], Step [6080/15231], Loss: 0.3505\n",
            "Epoch [1/5], Step [6100/15231], Loss: 0.3345\n",
            "Epoch [1/5], Step [6120/15231], Loss: 0.3588\n",
            "Epoch [1/5], Step [6140/15231], Loss: 0.3000\n",
            "Epoch [1/5], Step [6160/15231], Loss: 0.3398\n",
            "Epoch [1/5], Step [6180/15231], Loss: 0.3010\n",
            "Epoch [1/5], Step [6200/15231], Loss: 0.3286\n",
            "Epoch [1/5], Step [6220/15231], Loss: 0.3272\n",
            "Epoch [1/5], Step [6240/15231], Loss: 0.3317\n",
            "Epoch [1/5], Step [6260/15231], Loss: 0.3046\n",
            "Epoch [1/5], Step [6280/15231], Loss: 0.3129\n",
            "Epoch [1/5], Step [6300/15231], Loss: 0.2929\n",
            "Epoch [1/5], Step [6320/15231], Loss: 0.3310\n",
            "Epoch [1/5], Step [6340/15231], Loss: 0.3335\n",
            "Epoch [1/5], Step [6360/15231], Loss: 0.3543\n",
            "Epoch [1/5], Step [6380/15231], Loss: 0.2990\n",
            "Epoch [1/5], Step [6400/15231], Loss: 0.3343\n",
            "Epoch [1/5], Step [6420/15231], Loss: 0.3483\n",
            "Epoch [1/5], Step [6440/15231], Loss: 0.3455\n",
            "Epoch [1/5], Step [6460/15231], Loss: 0.3201\n",
            "Epoch [1/5], Step [6480/15231], Loss: 0.3308\n",
            "Epoch [1/5], Step [6500/15231], Loss: 0.3273\n",
            "Epoch [1/5], Step [6520/15231], Loss: 0.2850\n",
            "Epoch [1/5], Step [6540/15231], Loss: 0.3372\n",
            "Epoch [1/5], Step [6560/15231], Loss: 0.3153\n",
            "Epoch [1/5], Step [6580/15231], Loss: 0.3053\n",
            "Epoch [1/5], Step [6600/15231], Loss: 0.3484\n",
            "Epoch [1/5], Step [6620/15231], Loss: 0.3422\n",
            "Epoch [1/5], Step [6640/15231], Loss: 0.3338\n",
            "Epoch [1/5], Step [6660/15231], Loss: 0.3188\n",
            "Epoch [1/5], Step [6680/15231], Loss: 0.3455\n",
            "Epoch [1/5], Step [6700/15231], Loss: 0.2768\n",
            "Epoch [1/5], Step [6720/15231], Loss: 0.3610\n",
            "Epoch [1/5], Step [6740/15231], Loss: 0.3475\n",
            "Epoch [1/5], Step [6760/15231], Loss: 0.3352\n",
            "Epoch [1/5], Step [6780/15231], Loss: 0.3178\n",
            "Epoch [1/5], Step [6800/15231], Loss: 0.3085\n",
            "Epoch [1/5], Step [6820/15231], Loss: 0.3524\n",
            "Epoch [1/5], Step [6840/15231], Loss: 0.2827\n",
            "Epoch [1/5], Step [6860/15231], Loss: 0.3168\n",
            "Epoch [1/5], Step [6880/15231], Loss: 0.3195\n",
            "Epoch [1/5], Step [6900/15231], Loss: 0.3640\n",
            "Epoch [1/5], Step [6920/15231], Loss: 0.3559\n",
            "Epoch [1/5], Step [6940/15231], Loss: 0.3454\n",
            "Epoch [1/5], Step [6960/15231], Loss: 0.3511\n",
            "Epoch [1/5], Step [6980/15231], Loss: 0.3700\n",
            "Epoch [1/5], Step [7000/15231], Loss: 0.3347\n",
            "Epoch [1/5], Step [7020/15231], Loss: 0.3222\n",
            "Epoch [1/5], Step [7040/15231], Loss: 0.3457\n",
            "Epoch [1/5], Step [7060/15231], Loss: 0.3275\n",
            "Epoch [1/5], Step [7080/15231], Loss: 0.3414\n",
            "Epoch [1/5], Step [7100/15231], Loss: 0.3244\n",
            "Epoch [1/5], Step [7120/15231], Loss: 0.3027\n",
            "Epoch [1/5], Step [7140/15231], Loss: 0.3103\n",
            "Epoch [1/5], Step [7160/15231], Loss: 0.3257\n",
            "Epoch [1/5], Step [7180/15231], Loss: 0.3165\n",
            "Epoch [1/5], Step [7200/15231], Loss: 0.3831\n",
            "Epoch [1/5], Step [7220/15231], Loss: 0.3236\n",
            "Epoch [1/5], Step [7240/15231], Loss: 0.3032\n",
            "Epoch [1/5], Step [7260/15231], Loss: 0.3513\n",
            "Epoch [1/5], Step [7280/15231], Loss: 0.3067\n",
            "Epoch [1/5], Step [7300/15231], Loss: 0.3402\n",
            "Epoch [1/5], Step [7320/15231], Loss: 0.2987\n",
            "Epoch [1/5], Step [7340/15231], Loss: 0.3041\n",
            "Epoch [1/5], Step [7360/15231], Loss: 0.3353\n",
            "Epoch [1/5], Step [7380/15231], Loss: 0.3239\n",
            "Epoch [1/5], Step [7400/15231], Loss: 0.3284\n",
            "Epoch [1/5], Step [7420/15231], Loss: 0.3397\n",
            "Epoch [1/5], Step [7440/15231], Loss: 0.3270\n",
            "Epoch [1/5], Step [7460/15231], Loss: 0.3298\n",
            "Epoch [1/5], Step [7480/15231], Loss: 0.3403\n",
            "Epoch [1/5], Step [7500/15231], Loss: 0.3583\n",
            "Epoch [1/5], Step [7520/15231], Loss: 0.3447\n",
            "Epoch [1/5], Step [7540/15231], Loss: 0.3082\n",
            "Epoch [1/5], Step [7560/15231], Loss: 0.3036\n",
            "Epoch [1/5], Step [7580/15231], Loss: 0.3359\n",
            "Epoch [1/5], Step [7600/15231], Loss: 0.3073\n",
            "Epoch [1/5], Step [7620/15231], Loss: 0.3504\n",
            "Epoch [1/5], Step [7640/15231], Loss: 0.3157\n",
            "Epoch [1/5], Step [7660/15231], Loss: 0.3224\n",
            "Epoch [1/5], Step [7680/15231], Loss: 0.3033\n",
            "Epoch [1/5], Step [7700/15231], Loss: 0.3247\n",
            "Epoch [1/5], Step [7720/15231], Loss: 0.3221\n",
            "Epoch [1/5], Step [7740/15231], Loss: 0.3036\n",
            "Epoch [1/5], Step [7760/15231], Loss: 0.3265\n",
            "Epoch [1/5], Step [7780/15231], Loss: 0.3395\n",
            "Epoch [1/5], Step [7800/15231], Loss: 0.3155\n",
            "Epoch [1/5], Step [7820/15231], Loss: 0.3420\n",
            "Epoch [1/5], Step [7840/15231], Loss: 0.3191\n",
            "Epoch [1/5], Step [7860/15231], Loss: 0.3056\n",
            "Epoch [1/5], Step [7880/15231], Loss: 0.3397\n",
            "Epoch [1/5], Step [7900/15231], Loss: 0.3386\n",
            "Epoch [1/5], Step [7920/15231], Loss: 0.3346\n",
            "Epoch [1/5], Step [7940/15231], Loss: 0.3589\n",
            "Epoch [1/5], Step [7960/15231], Loss: 0.3800\n",
            "Epoch [1/5], Step [7980/15231], Loss: 0.3494\n",
            "Epoch [1/5], Step [8000/15231], Loss: 0.3200\n",
            "Epoch [1/5], Step [8020/15231], Loss: 0.3258\n",
            "Epoch [1/5], Step [8040/15231], Loss: 0.3263\n",
            "Epoch [1/5], Step [8060/15231], Loss: 0.3382\n",
            "Epoch [1/5], Step [8080/15231], Loss: 0.3142\n",
            "Epoch [1/5], Step [8100/15231], Loss: 0.3259\n",
            "Epoch [1/5], Step [8120/15231], Loss: 0.3043\n",
            "Epoch [1/5], Step [8140/15231], Loss: 0.2860\n",
            "Epoch [1/5], Step [8160/15231], Loss: 0.2826\n",
            "Epoch [1/5], Step [8180/15231], Loss: 0.3226\n",
            "Epoch [1/5], Step [8200/15231], Loss: 0.3083\n",
            "Epoch [1/5], Step [8220/15231], Loss: 0.3094\n",
            "Epoch [1/5], Step [8240/15231], Loss: 0.3377\n",
            "Epoch [1/5], Step [8260/15231], Loss: 0.3406\n",
            "Epoch [1/5], Step [8280/15231], Loss: 0.3028\n",
            "Epoch [1/5], Step [8300/15231], Loss: 0.3109\n",
            "Epoch [1/5], Step [8320/15231], Loss: 0.2902\n",
            "Epoch [1/5], Step [8340/15231], Loss: 0.3191\n",
            "Epoch [1/5], Step [8360/15231], Loss: 0.3304\n",
            "Epoch [1/5], Step [8380/15231], Loss: 0.3244\n",
            "Epoch [1/5], Step [8400/15231], Loss: 0.3187\n",
            "Epoch [1/5], Step [8420/15231], Loss: 0.3284\n",
            "Epoch [1/5], Step [8440/15231], Loss: 0.3199\n",
            "Epoch [1/5], Step [8460/15231], Loss: 0.3052\n",
            "Epoch [1/5], Step [8480/15231], Loss: 0.2996\n",
            "Epoch [1/5], Step [8500/15231], Loss: 0.3060\n",
            "Epoch [1/5], Step [8520/15231], Loss: 0.3178\n",
            "Epoch [1/5], Step [8540/15231], Loss: 0.3022\n",
            "Epoch [1/5], Step [8560/15231], Loss: 0.3033\n",
            "Epoch [1/5], Step [8580/15231], Loss: 0.3334\n",
            "Epoch [1/5], Step [8600/15231], Loss: 0.3131\n",
            "Epoch [1/5], Step [8620/15231], Loss: 0.3152\n",
            "Epoch [1/5], Step [8640/15231], Loss: 0.3039\n",
            "Epoch [1/5], Step [8660/15231], Loss: 0.3115\n",
            "Epoch [1/5], Step [8680/15231], Loss: 0.3127\n",
            "Epoch [1/5], Step [8700/15231], Loss: 0.3375\n",
            "Epoch [1/5], Step [8720/15231], Loss: 0.3394\n",
            "Epoch [1/5], Step [8740/15231], Loss: 0.3485\n",
            "Epoch [1/5], Step [8760/15231], Loss: 0.3411\n",
            "Epoch [1/5], Step [8780/15231], Loss: 0.3031\n",
            "Epoch [1/5], Step [8800/15231], Loss: 0.2997\n",
            "Epoch [1/5], Step [8820/15231], Loss: 0.3543\n",
            "Epoch [1/5], Step [8840/15231], Loss: 0.3272\n",
            "Epoch [1/5], Step [8860/15231], Loss: 0.3262\n",
            "Epoch [1/5], Step [8880/15231], Loss: 0.3089\n",
            "Epoch [1/5], Step [8900/15231], Loss: 0.3311\n",
            "Epoch [1/5], Step [8920/15231], Loss: 0.3046\n",
            "Epoch [1/5], Step [8940/15231], Loss: 0.3022\n",
            "Epoch [1/5], Step [8960/15231], Loss: 0.3287\n",
            "Epoch [1/5], Step [8980/15231], Loss: 0.2958\n",
            "Epoch [1/5], Step [9000/15231], Loss: 0.3295\n",
            "Epoch [1/5], Step [9020/15231], Loss: 0.2921\n",
            "Epoch [1/5], Step [9040/15231], Loss: 0.3069\n",
            "Epoch [1/5], Step [9060/15231], Loss: 0.3031\n",
            "Epoch [1/5], Step [9080/15231], Loss: 0.2924\n",
            "Epoch [1/5], Step [9100/15231], Loss: 0.2871\n",
            "Epoch [1/5], Step [9120/15231], Loss: 0.3650\n",
            "Epoch [1/5], Step [9140/15231], Loss: 0.3166\n",
            "Epoch [1/5], Step [9160/15231], Loss: 0.2892\n",
            "Epoch [1/5], Step [9180/15231], Loss: 0.3051\n",
            "Epoch [1/5], Step [9200/15231], Loss: 0.3039\n",
            "Epoch [1/5], Step [9220/15231], Loss: 0.3011\n",
            "Epoch [1/5], Step [9240/15231], Loss: 0.3314\n",
            "Epoch [1/5], Step [9260/15231], Loss: 0.3016\n",
            "Epoch [1/5], Step [9280/15231], Loss: 0.3129\n",
            "Epoch [1/5], Step [9300/15231], Loss: 0.3139\n",
            "Epoch [1/5], Step [9320/15231], Loss: 0.3033\n",
            "Epoch [1/5], Step [9340/15231], Loss: 0.3202\n",
            "Epoch [1/5], Step [9360/15231], Loss: 0.2763\n",
            "Epoch [1/5], Step [9380/15231], Loss: 0.2954\n",
            "Epoch [1/5], Step [9400/15231], Loss: 0.2879\n",
            "Epoch [1/5], Step [9420/15231], Loss: 0.2901\n",
            "Epoch [1/5], Step [9440/15231], Loss: 0.2596\n",
            "Epoch [1/5], Step [9460/15231], Loss: 0.3276\n",
            "Epoch [1/5], Step [9480/15231], Loss: 0.3432\n",
            "Epoch [1/5], Step [9500/15231], Loss: 0.2876\n",
            "Epoch [1/5], Step [9520/15231], Loss: 0.2843\n",
            "Epoch [1/5], Step [9540/15231], Loss: 0.3510\n",
            "Epoch [1/5], Step [9560/15231], Loss: 0.3342\n",
            "Epoch [1/5], Step [9580/15231], Loss: 0.3096\n",
            "Epoch [1/5], Step [9600/15231], Loss: 0.3593\n",
            "Epoch [1/5], Step [9620/15231], Loss: 0.3174\n",
            "Epoch [1/5], Step [9640/15231], Loss: 0.3117\n",
            "Epoch [1/5], Step [9660/15231], Loss: 0.3171\n",
            "Epoch [1/5], Step [9680/15231], Loss: 0.3041\n",
            "Epoch [1/5], Step [9700/15231], Loss: 0.3186\n",
            "Epoch [1/5], Step [9720/15231], Loss: 0.3268\n",
            "Epoch [1/5], Step [9740/15231], Loss: 0.2740\n",
            "Epoch [1/5], Step [9760/15231], Loss: 0.3229\n",
            "Epoch [1/5], Step [9780/15231], Loss: 0.3065\n",
            "Epoch [1/5], Step [9800/15231], Loss: 0.3249\n",
            "Epoch [1/5], Step [9820/15231], Loss: 0.3015\n",
            "Epoch [1/5], Step [9840/15231], Loss: 0.2953\n",
            "Epoch [1/5], Step [9860/15231], Loss: 0.2865\n",
            "Epoch [1/5], Step [9880/15231], Loss: 0.2749\n",
            "Epoch [1/5], Step [9900/15231], Loss: 0.3345\n",
            "Epoch [1/5], Step [9920/15231], Loss: 0.2973\n",
            "Epoch [1/5], Step [9940/15231], Loss: 0.3070\n",
            "Epoch [1/5], Step [9960/15231], Loss: 0.2720\n",
            "Epoch [1/5], Step [9980/15231], Loss: 0.3086\n",
            "Epoch [1/5], Step [10000/15231], Loss: 0.3375\n",
            "Epoch [1/5], Step [10020/15231], Loss: 0.3012\n",
            "Epoch [1/5], Step [10040/15231], Loss: 0.2963\n",
            "Epoch [1/5], Step [10060/15231], Loss: 0.3491\n",
            "Epoch [1/5], Step [10080/15231], Loss: 0.3054\n",
            "Epoch [1/5], Step [10100/15231], Loss: 0.2913\n",
            "Epoch [1/5], Step [10120/15231], Loss: 0.3288\n",
            "Epoch [1/5], Step [10140/15231], Loss: 0.2770\n",
            "Epoch [1/5], Step [10160/15231], Loss: 0.2766\n",
            "Epoch [1/5], Step [10180/15231], Loss: 0.3190\n",
            "Epoch [1/5], Step [10200/15231], Loss: 0.3413\n",
            "Epoch [1/5], Step [10220/15231], Loss: 0.3073\n",
            "Epoch [1/5], Step [10240/15231], Loss: 0.3074\n",
            "Epoch [1/5], Step [10260/15231], Loss: 0.3346\n",
            "Epoch [1/5], Step [10280/15231], Loss: 0.3042\n",
            "Epoch [1/5], Step [10300/15231], Loss: 0.3000\n",
            "Epoch [1/5], Step [10320/15231], Loss: 0.3280\n",
            "Epoch [1/5], Step [10340/15231], Loss: 0.3012\n",
            "Epoch [1/5], Step [10360/15231], Loss: 0.3496\n",
            "Epoch [1/5], Step [10380/15231], Loss: 0.2853\n",
            "Epoch [1/5], Step [10400/15231], Loss: 0.3051\n",
            "Epoch [1/5], Step [10420/15231], Loss: 0.3188\n",
            "Epoch [1/5], Step [10440/15231], Loss: 0.2847\n",
            "Epoch [1/5], Step [10460/15231], Loss: 0.2784\n",
            "Epoch [1/5], Step [10480/15231], Loss: 0.2957\n",
            "Epoch [1/5], Step [10500/15231], Loss: 0.2982\n",
            "Epoch [1/5], Step [10520/15231], Loss: 0.3028\n",
            "Epoch [1/5], Step [10540/15231], Loss: 0.2803\n",
            "Epoch [1/5], Step [10560/15231], Loss: 0.3219\n",
            "Epoch [1/5], Step [10580/15231], Loss: 0.3352\n",
            "Epoch [1/5], Step [10600/15231], Loss: 0.3321\n",
            "Epoch [1/5], Step [10620/15231], Loss: 0.3398\n",
            "Epoch [1/5], Step [10640/15231], Loss: 0.3221\n",
            "Epoch [1/5], Step [10660/15231], Loss: 0.3200\n",
            "Epoch [1/5], Step [10680/15231], Loss: 0.2991\n",
            "Epoch [1/5], Step [10700/15231], Loss: 0.3218\n",
            "Epoch [1/5], Step [10720/15231], Loss: 0.2943\n",
            "Epoch [1/5], Step [10740/15231], Loss: 0.2979\n",
            "Epoch [1/5], Step [10760/15231], Loss: 0.3519\n",
            "Epoch [1/5], Step [10780/15231], Loss: 0.3146\n",
            "Epoch [1/5], Step [10800/15231], Loss: 0.3272\n",
            "Epoch [1/5], Step [10820/15231], Loss: 0.3045\n",
            "Epoch [1/5], Step [10840/15231], Loss: 0.2851\n",
            "Epoch [1/5], Step [10860/15231], Loss: 0.2698\n",
            "Epoch [1/5], Step [10880/15231], Loss: 0.2603\n",
            "Epoch [1/5], Step [10900/15231], Loss: 0.3065\n",
            "Epoch [1/5], Step [10920/15231], Loss: 0.2821\n",
            "Epoch [1/5], Step [10940/15231], Loss: 0.3298\n",
            "Epoch [1/5], Step [10960/15231], Loss: 0.3046\n",
            "Epoch [1/5], Step [10980/15231], Loss: 0.3130\n",
            "Epoch [1/5], Step [11000/15231], Loss: 0.3131\n",
            "Epoch [1/5], Step [11020/15231], Loss: 0.2738\n",
            "Epoch [1/5], Step [11040/15231], Loss: 0.3087\n",
            "Epoch [1/5], Step [11060/15231], Loss: 0.2961\n",
            "Epoch [1/5], Step [11080/15231], Loss: 0.3090\n",
            "Epoch [1/5], Step [11100/15231], Loss: 0.3283\n",
            "Epoch [1/5], Step [11120/15231], Loss: 0.3052\n",
            "Epoch [1/5], Step [11140/15231], Loss: 0.2835\n",
            "Epoch [1/5], Step [11160/15231], Loss: 0.2913\n",
            "Epoch [1/5], Step [11180/15231], Loss: 0.3286\n",
            "Epoch [1/5], Step [11200/15231], Loss: 0.3059\n",
            "Epoch [1/5], Step [11220/15231], Loss: 0.2896\n",
            "Epoch [1/5], Step [11240/15231], Loss: 0.3058\n",
            "Epoch [1/5], Step [11260/15231], Loss: 0.2886\n",
            "Epoch [1/5], Step [11280/15231], Loss: 0.3074\n",
            "Epoch [1/5], Step [11300/15231], Loss: 0.3382\n",
            "Epoch [1/5], Step [11320/15231], Loss: 0.2787\n",
            "Epoch [1/5], Step [11340/15231], Loss: 0.3182\n",
            "Epoch [1/5], Step [11360/15231], Loss: 0.3593\n",
            "Epoch [1/5], Step [11380/15231], Loss: 0.2732\n",
            "Epoch [1/5], Step [11400/15231], Loss: 0.3103\n",
            "Epoch [1/5], Step [11420/15231], Loss: 0.2819\n",
            "Epoch [1/5], Step [11440/15231], Loss: 0.3013\n",
            "Epoch [1/5], Step [11460/15231], Loss: 0.2694\n",
            "Epoch [1/5], Step [11480/15231], Loss: 0.3071\n",
            "Epoch [1/5], Step [11500/15231], Loss: 0.3035\n",
            "Epoch [1/5], Step [11520/15231], Loss: 0.3017\n",
            "Epoch [1/5], Step [11540/15231], Loss: 0.3080\n",
            "Epoch [1/5], Step [11560/15231], Loss: 0.2878\n",
            "Epoch [1/5], Step [11580/15231], Loss: 0.2881\n",
            "Epoch [1/5], Step [11600/15231], Loss: 0.3001\n",
            "Epoch [1/5], Step [11620/15231], Loss: 0.3386\n",
            "Epoch [1/5], Step [11640/15231], Loss: 0.3153\n",
            "Epoch [1/5], Step [11660/15231], Loss: 0.2731\n",
            "Epoch [1/5], Step [11680/15231], Loss: 0.3189\n",
            "Epoch [1/5], Step [11700/15231], Loss: 0.3107\n",
            "Epoch [1/5], Step [11720/15231], Loss: 0.3298\n",
            "Epoch [1/5], Step [11740/15231], Loss: 0.3187\n",
            "Epoch [1/5], Step [11760/15231], Loss: 0.2877\n",
            "Epoch [1/5], Step [11780/15231], Loss: 0.2889\n",
            "Epoch [1/5], Step [11800/15231], Loss: 0.2954\n",
            "Epoch [1/5], Step [11820/15231], Loss: 0.2895\n",
            "Epoch [1/5], Step [11840/15231], Loss: 0.3170\n",
            "Epoch [1/5], Step [11860/15231], Loss: 0.3080\n",
            "Epoch [1/5], Step [11880/15231], Loss: 0.2895\n",
            "Epoch [1/5], Step [11900/15231], Loss: 0.2914\n",
            "Epoch [1/5], Step [11920/15231], Loss: 0.2723\n",
            "Epoch [1/5], Step [11940/15231], Loss: 0.3095\n",
            "Epoch [1/5], Step [11960/15231], Loss: 0.3106\n",
            "Epoch [1/5], Step [11980/15231], Loss: 0.2878\n",
            "Epoch [1/5], Step [12000/15231], Loss: 0.3487\n",
            "Epoch [1/5], Step [12020/15231], Loss: 0.2827\n",
            "Epoch [1/5], Step [12040/15231], Loss: 0.3343\n",
            "Epoch [1/5], Step [12060/15231], Loss: 0.2990\n",
            "Epoch [1/5], Step [12080/15231], Loss: 0.3232\n",
            "Epoch [1/5], Step [12100/15231], Loss: 0.3192\n",
            "Epoch [1/5], Step [12120/15231], Loss: 0.3233\n",
            "Epoch [1/5], Step [12140/15231], Loss: 0.2909\n",
            "Epoch [1/5], Step [12160/15231], Loss: 0.2882\n",
            "Epoch [1/5], Step [12180/15231], Loss: 0.2928\n",
            "Epoch [1/5], Step [12200/15231], Loss: 0.3150\n",
            "Epoch [1/5], Step [12220/15231], Loss: 0.2651\n",
            "Epoch [1/5], Step [12240/15231], Loss: 0.3406\n",
            "Epoch [1/5], Step [12260/15231], Loss: 0.3056\n",
            "Epoch [1/5], Step [12280/15231], Loss: 0.3065\n",
            "Epoch [1/5], Step [12300/15231], Loss: 0.2734\n",
            "Epoch [1/5], Step [12320/15231], Loss: 0.2926\n",
            "Epoch [1/5], Step [12340/15231], Loss: 0.3257\n",
            "Epoch [1/5], Step [12360/15231], Loss: 0.3104\n",
            "Epoch [1/5], Step [12380/15231], Loss: 0.2869\n",
            "Epoch [1/5], Step [12400/15231], Loss: 0.2943\n",
            "Epoch [1/5], Step [12420/15231], Loss: 0.3376\n",
            "Epoch [1/5], Step [12440/15231], Loss: 0.3002\n",
            "Epoch [1/5], Step [12460/15231], Loss: 0.3067\n",
            "Epoch [1/5], Step [12480/15231], Loss: 0.2697\n",
            "Epoch [1/5], Step [12500/15231], Loss: 0.3108\n",
            "Epoch [1/5], Step [12520/15231], Loss: 0.2742\n",
            "Epoch [1/5], Step [12540/15231], Loss: 0.3137\n",
            "Epoch [1/5], Step [12560/15231], Loss: 0.3123\n",
            "Epoch [1/5], Step [12580/15231], Loss: 0.2869\n",
            "Epoch [1/5], Step [12600/15231], Loss: 0.2676\n",
            "Epoch [1/5], Step [12620/15231], Loss: 0.2847\n",
            "Epoch [1/5], Step [12640/15231], Loss: 0.2645\n",
            "Epoch [1/5], Step [12660/15231], Loss: 0.2819\n",
            "Epoch [1/5], Step [12680/15231], Loss: 0.2841\n",
            "Epoch [1/5], Step [12700/15231], Loss: 0.2881\n",
            "Epoch [1/5], Step [12720/15231], Loss: 0.2677\n",
            "Epoch [1/5], Step [12740/15231], Loss: 0.2911\n",
            "Epoch [1/5], Step [12760/15231], Loss: 0.3087\n",
            "Epoch [1/5], Step [12780/15231], Loss: 0.2957\n",
            "Epoch [1/5], Step [12800/15231], Loss: 0.2924\n",
            "Epoch [1/5], Step [12820/15231], Loss: 0.2861\n",
            "Epoch [1/5], Step [12840/15231], Loss: 0.2678\n",
            "Epoch [1/5], Step [12860/15231], Loss: 0.2835\n",
            "Epoch [1/5], Step [12880/15231], Loss: 0.2971\n",
            "Epoch [1/5], Step [12900/15231], Loss: 0.3068\n",
            "Epoch [1/5], Step [12920/15231], Loss: 0.2588\n",
            "Epoch [1/5], Step [12940/15231], Loss: 0.2759\n",
            "Epoch [1/5], Step [12960/15231], Loss: 0.3109\n",
            "Epoch [1/5], Step [12980/15231], Loss: 0.2929\n",
            "Epoch [1/5], Step [13000/15231], Loss: 0.2771\n",
            "Epoch [1/5], Step [13020/15231], Loss: 0.2860\n",
            "Epoch [1/5], Step [13040/15231], Loss: 0.2619\n",
            "Epoch [1/5], Step [13060/15231], Loss: 0.2994\n",
            "Epoch [1/5], Step [13080/15231], Loss: 0.2768\n",
            "Epoch [1/5], Step [13100/15231], Loss: 0.3195\n",
            "Epoch [1/5], Step [13120/15231], Loss: 0.2719\n",
            "Epoch [1/5], Step [13140/15231], Loss: 0.2912\n",
            "Epoch [1/5], Step [13160/15231], Loss: 0.3022\n",
            "Epoch [1/5], Step [13180/15231], Loss: 0.2713\n",
            "Epoch [1/5], Step [13200/15231], Loss: 0.2900\n",
            "Epoch [1/5], Step [13220/15231], Loss: 0.2988\n",
            "Epoch [1/5], Step [13240/15231], Loss: 0.3005\n",
            "Epoch [1/5], Step [13260/15231], Loss: 0.2715\n",
            "Epoch [1/5], Step [13280/15231], Loss: 0.2831\n",
            "Epoch [1/5], Step [13300/15231], Loss: 0.3316\n",
            "Epoch [1/5], Step [13320/15231], Loss: 0.2605\n",
            "Epoch [1/5], Step [13340/15231], Loss: 0.2889\n",
            "Epoch [1/5], Step [13360/15231], Loss: 0.2919\n",
            "Epoch [1/5], Step [13380/15231], Loss: 0.3420\n",
            "Epoch [1/5], Step [13400/15231], Loss: 0.2967\n",
            "Epoch [1/5], Step [13420/15231], Loss: 0.3020\n",
            "Epoch [1/5], Step [13440/15231], Loss: 0.3099\n",
            "Epoch [1/5], Step [13460/15231], Loss: 0.2955\n",
            "Epoch [1/5], Step [13480/15231], Loss: 0.3053\n",
            "Epoch [1/5], Step [13500/15231], Loss: 0.3064\n",
            "Epoch [1/5], Step [13520/15231], Loss: 0.3095\n",
            "Epoch [1/5], Step [13540/15231], Loss: 0.3183\n",
            "Epoch [1/5], Step [13560/15231], Loss: 0.2738\n",
            "Epoch [1/5], Step [13580/15231], Loss: 0.2551\n",
            "Epoch [1/5], Step [13600/15231], Loss: 0.2683\n",
            "Epoch [1/5], Step [13620/15231], Loss: 0.2935\n",
            "Epoch [1/5], Step [13640/15231], Loss: 0.2997\n",
            "Epoch [1/5], Step [13660/15231], Loss: 0.2710\n",
            "Epoch [1/5], Step [13680/15231], Loss: 0.2717\n",
            "Epoch [1/5], Step [13700/15231], Loss: 0.2787\n",
            "Epoch [1/5], Step [13720/15231], Loss: 0.2817\n",
            "Epoch [1/5], Step [13740/15231], Loss: 0.2865\n",
            "Epoch [1/5], Step [13760/15231], Loss: 0.2950\n",
            "Epoch [1/5], Step [13780/15231], Loss: 0.2794\n",
            "Epoch [1/5], Step [13800/15231], Loss: 0.3380\n",
            "Epoch [1/5], Step [13820/15231], Loss: 0.2526\n",
            "Epoch [1/5], Step [13840/15231], Loss: 0.3035\n",
            "Epoch [1/5], Step [13860/15231], Loss: 0.2741\n",
            "Epoch [1/5], Step [13880/15231], Loss: 0.2842\n",
            "Epoch [1/5], Step [13900/15231], Loss: 0.2909\n",
            "Epoch [1/5], Step [13920/15231], Loss: 0.3328\n",
            "Epoch [1/5], Step [13940/15231], Loss: 0.2878\n",
            "Epoch [1/5], Step [13960/15231], Loss: 0.2798\n",
            "Epoch [1/5], Step [13980/15231], Loss: 0.3416\n",
            "Epoch [1/5], Step [14000/15231], Loss: 0.2742\n",
            "Epoch [1/5], Step [14020/15231], Loss: 0.2825\n",
            "Epoch [1/5], Step [14040/15231], Loss: 0.2980\n",
            "Epoch [1/5], Step [14060/15231], Loss: 0.2794\n",
            "Epoch [1/5], Step [14080/15231], Loss: 0.2676\n",
            "Epoch [1/5], Step [14100/15231], Loss: 0.2859\n",
            "Epoch [1/5], Step [14120/15231], Loss: 0.3046\n",
            "Epoch [1/5], Step [14140/15231], Loss: 0.2850\n",
            "Epoch [1/5], Step [14160/15231], Loss: 0.3119\n",
            "Epoch [1/5], Step [14180/15231], Loss: 0.3034\n",
            "Epoch [1/5], Step [14200/15231], Loss: 0.2710\n",
            "Epoch [1/5], Step [14220/15231], Loss: 0.2750\n",
            "Epoch [1/5], Step [14240/15231], Loss: 0.2573\n",
            "Epoch [1/5], Step [14260/15231], Loss: 0.3002\n",
            "Epoch [1/5], Step [14280/15231], Loss: 0.2993\n",
            "Epoch [1/5], Step [14300/15231], Loss: 0.2905\n",
            "Epoch [1/5], Step [14320/15231], Loss: 0.2901\n",
            "Epoch [1/5], Step [14340/15231], Loss: 0.2949\n",
            "Epoch [1/5], Step [14360/15231], Loss: 0.3277\n",
            "Epoch [1/5], Step [14380/15231], Loss: 0.2839\n",
            "Epoch [1/5], Step [14400/15231], Loss: 0.2711\n",
            "Epoch [1/5], Step [14420/15231], Loss: 0.3032\n",
            "Epoch [1/5], Step [14440/15231], Loss: 0.2938\n",
            "Epoch [1/5], Step [14460/15231], Loss: 0.2970\n",
            "Epoch [1/5], Step [14480/15231], Loss: 0.3365\n",
            "Epoch [1/5], Step [14500/15231], Loss: 0.2596\n",
            "Epoch [1/5], Step [14520/15231], Loss: 0.3047\n",
            "Epoch [1/5], Step [14540/15231], Loss: 0.3105\n",
            "Epoch [1/5], Step [14560/15231], Loss: 0.2744\n",
            "Epoch [1/5], Step [14580/15231], Loss: 0.2890\n",
            "Epoch [1/5], Step [14600/15231], Loss: 0.2844\n",
            "Epoch [1/5], Step [14620/15231], Loss: 0.2843\n",
            "Epoch [1/5], Step [14640/15231], Loss: 0.2955\n",
            "Epoch [1/5], Step [14660/15231], Loss: 0.3386\n",
            "Epoch [1/5], Step [14680/15231], Loss: 0.2956\n",
            "Epoch [1/5], Step [14700/15231], Loss: 0.3031\n",
            "Epoch [1/5], Step [14720/15231], Loss: 0.2713\n",
            "Epoch [1/5], Step [14740/15231], Loss: 0.2817\n",
            "Epoch [1/5], Step [14760/15231], Loss: 0.2629\n",
            "Epoch [1/5], Step [14780/15231], Loss: 0.2789\n",
            "Epoch [1/5], Step [14800/15231], Loss: 0.2682\n",
            "Epoch [1/5], Step [14820/15231], Loss: 0.2968\n",
            "Epoch [1/5], Step [14840/15231], Loss: 0.2851\n",
            "Epoch [1/5], Step [14860/15231], Loss: 0.3274\n",
            "Epoch [1/5], Step [14880/15231], Loss: 0.3002\n",
            "Epoch [1/5], Step [14900/15231], Loss: 0.2971\n",
            "Epoch [1/5], Step [14920/15231], Loss: 0.2900\n",
            "Epoch [1/5], Step [14940/15231], Loss: 0.3146\n",
            "Epoch [1/5], Step [14960/15231], Loss: 0.2775\n",
            "Epoch [1/5], Step [14980/15231], Loss: 0.3308\n",
            "Epoch [1/5], Step [15000/15231], Loss: 0.2802\n",
            "Epoch [1/5], Step [15020/15231], Loss: 0.2837\n",
            "Epoch [1/5], Step [15040/15231], Loss: 0.2503\n",
            "Epoch [1/5], Step [15060/15231], Loss: 0.2799\n",
            "Epoch [1/5], Step [15080/15231], Loss: 0.2975\n",
            "Epoch [1/5], Step [15100/15231], Loss: 0.2836\n",
            "Epoch [1/5], Step [15120/15231], Loss: 0.2865\n",
            "Epoch [1/5], Step [15140/15231], Loss: 0.2834\n",
            "Epoch [1/5], Step [15160/15231], Loss: 0.2950\n",
            "Epoch [1/5], Step [15180/15231], Loss: 0.3005\n",
            "Epoch [1/5], Step [15200/15231], Loss: 0.2921\n",
            "Epoch [1/5], Step [15220/15231], Loss: 0.3021\n",
            "Epoch [1/5] | Train Loss: 0.3513 | Train Acc: 87.70% | Test Loss: 0.6435 | Test Acc: 75.80%\n",
            "Epoch [2/5], Step [20/15231], Loss: 0.2656\n",
            "Epoch [2/5], Step [40/15231], Loss: 0.2929\n",
            "Epoch [2/5], Step [60/15231], Loss: 0.2759\n",
            "Epoch [2/5], Step [80/15231], Loss: 0.2645\n",
            "Epoch [2/5], Step [100/15231], Loss: 0.2663\n",
            "Epoch [2/5], Step [120/15231], Loss: 0.3327\n",
            "Epoch [2/5], Step [140/15231], Loss: 0.2922\n",
            "Epoch [2/5], Step [160/15231], Loss: 0.2959\n",
            "Epoch [2/5], Step [180/15231], Loss: 0.2858\n",
            "Epoch [2/5], Step [200/15231], Loss: 0.2966\n",
            "Epoch [2/5], Step [220/15231], Loss: 0.2709\n",
            "Epoch [2/5], Step [240/15231], Loss: 0.2574\n",
            "Epoch [2/5], Step [260/15231], Loss: 0.2945\n",
            "Epoch [2/5], Step [280/15231], Loss: 0.3226\n",
            "Epoch [2/5], Step [300/15231], Loss: 0.2811\n",
            "Epoch [2/5], Step [320/15231], Loss: 0.2593\n",
            "Epoch [2/5], Step [340/15231], Loss: 0.2534\n",
            "Epoch [2/5], Step [360/15231], Loss: 0.2692\n",
            "Epoch [2/5], Step [380/15231], Loss: 0.2746\n",
            "Epoch [2/5], Step [400/15231], Loss: 0.2784\n",
            "Epoch [2/5], Step [420/15231], Loss: 0.2711\n",
            "Epoch [2/5], Step [440/15231], Loss: 0.2701\n",
            "Epoch [2/5], Step [460/15231], Loss: 0.2479\n",
            "Epoch [2/5], Step [480/15231], Loss: 0.2517\n",
            "Epoch [2/5], Step [500/15231], Loss: 0.2930\n",
            "Epoch [2/5], Step [520/15231], Loss: 0.2733\n",
            "Epoch [2/5], Step [540/15231], Loss: 0.2479\n",
            "Epoch [2/5], Step [560/15231], Loss: 0.2488\n",
            "Epoch [2/5], Step [580/15231], Loss: 0.2822\n",
            "Epoch [2/5], Step [600/15231], Loss: 0.2950\n",
            "Epoch [2/5], Step [620/15231], Loss: 0.3016\n",
            "Epoch [2/5], Step [640/15231], Loss: 0.2435\n",
            "Epoch [2/5], Step [660/15231], Loss: 0.3065\n",
            "Epoch [2/5], Step [680/15231], Loss: 0.2971\n",
            "Epoch [2/5], Step [700/15231], Loss: 0.2937\n",
            "Epoch [2/5], Step [720/15231], Loss: 0.2831\n",
            "Epoch [2/5], Step [740/15231], Loss: 0.2556\n",
            "Epoch [2/5], Step [760/15231], Loss: 0.3075\n",
            "Epoch [2/5], Step [780/15231], Loss: 0.2778\n",
            "Epoch [2/5], Step [800/15231], Loss: 0.3208\n",
            "Epoch [2/5], Step [820/15231], Loss: 0.2589\n",
            "Epoch [2/5], Step [840/15231], Loss: 0.3116\n",
            "Epoch [2/5], Step [860/15231], Loss: 0.2657\n",
            "Epoch [2/5], Step [880/15231], Loss: 0.2644\n",
            "Epoch [2/5], Step [900/15231], Loss: 0.2857\n",
            "Epoch [2/5], Step [920/15231], Loss: 0.2603\n",
            "Epoch [2/5], Step [940/15231], Loss: 0.2634\n",
            "Epoch [2/5], Step [960/15231], Loss: 0.2637\n",
            "Epoch [2/5], Step [980/15231], Loss: 0.2731\n",
            "Epoch [2/5], Step [1000/15231], Loss: 0.2887\n",
            "Epoch [2/5], Step [1020/15231], Loss: 0.2713\n",
            "Epoch [2/5], Step [1040/15231], Loss: 0.2639\n",
            "Epoch [2/5], Step [1060/15231], Loss: 0.2915\n",
            "Epoch [2/5], Step [1080/15231], Loss: 0.2912\n",
            "Epoch [2/5], Step [1100/15231], Loss: 0.3287\n",
            "Epoch [2/5], Step [1120/15231], Loss: 0.3134\n",
            "Epoch [2/5], Step [1140/15231], Loss: 0.2680\n",
            "Epoch [2/5], Step [1160/15231], Loss: 0.2755\n",
            "Epoch [2/5], Step [1180/15231], Loss: 0.2882\n",
            "Epoch [2/5], Step [1200/15231], Loss: 0.2740\n",
            "Epoch [2/5], Step [1220/15231], Loss: 0.2680\n",
            "Epoch [2/5], Step [1240/15231], Loss: 0.2927\n",
            "Epoch [2/5], Step [1260/15231], Loss: 0.3157\n",
            "Epoch [2/5], Step [1280/15231], Loss: 0.3032\n",
            "Epoch [2/5], Step [1300/15231], Loss: 0.2941\n",
            "Epoch [2/5], Step [1320/15231], Loss: 0.2641\n",
            "Epoch [2/5], Step [1340/15231], Loss: 0.2934\n",
            "Epoch [2/5], Step [1360/15231], Loss: 0.2858\n",
            "Epoch [2/5], Step [1380/15231], Loss: 0.2644\n",
            "Epoch [2/5], Step [1400/15231], Loss: 0.2795\n",
            "Epoch [2/5], Step [1420/15231], Loss: 0.2968\n",
            "Epoch [2/5], Step [1440/15231], Loss: 0.2716\n",
            "Epoch [2/5], Step [1460/15231], Loss: 0.2535\n",
            "Epoch [2/5], Step [1480/15231], Loss: 0.2712\n",
            "Epoch [2/5], Step [1500/15231], Loss: 0.2796\n",
            "Epoch [2/5], Step [1520/15231], Loss: 0.2733\n",
            "Epoch [2/5], Step [1540/15231], Loss: 0.3023\n",
            "Epoch [2/5], Step [1560/15231], Loss: 0.2708\n",
            "Epoch [2/5], Step [1580/15231], Loss: 0.2504\n",
            "Epoch [2/5], Step [1600/15231], Loss: 0.2744\n",
            "Epoch [2/5], Step [1620/15231], Loss: 0.3150\n",
            "Epoch [2/5], Step [1640/15231], Loss: 0.2630\n",
            "Epoch [2/5], Step [1660/15231], Loss: 0.3219\n",
            "Epoch [2/5], Step [1680/15231], Loss: 0.3181\n",
            "Epoch [2/5], Step [1700/15231], Loss: 0.2836\n",
            "Epoch [2/5], Step [1720/15231], Loss: 0.2930\n",
            "Epoch [2/5], Step [1740/15231], Loss: 0.2961\n",
            "Epoch [2/5], Step [1760/15231], Loss: 0.2976\n",
            "Epoch [2/5], Step [1780/15231], Loss: 0.2565\n",
            "Epoch [2/5], Step [1800/15231], Loss: 0.2839\n",
            "Epoch [2/5], Step [1820/15231], Loss: 0.2904\n",
            "Epoch [2/5], Step [1840/15231], Loss: 0.2677\n",
            "Epoch [2/5], Step [1860/15231], Loss: 0.2414\n",
            "Epoch [2/5], Step [1880/15231], Loss: 0.3222\n",
            "Epoch [2/5], Step [1900/15231], Loss: 0.2996\n",
            "Epoch [2/5], Step [1920/15231], Loss: 0.2891\n",
            "Epoch [2/5], Step [1940/15231], Loss: 0.2945\n",
            "Epoch [2/5], Step [1960/15231], Loss: 0.2565\n",
            "Epoch [2/5], Step [1980/15231], Loss: 0.2751\n",
            "Epoch [2/5], Step [2000/15231], Loss: 0.2928\n",
            "Epoch [2/5], Step [2020/15231], Loss: 0.2668\n",
            "Epoch [2/5], Step [2040/15231], Loss: 0.3031\n",
            "Epoch [2/5], Step [2060/15231], Loss: 0.2560\n",
            "Epoch [2/5], Step [2080/15231], Loss: 0.2858\n",
            "Epoch [2/5], Step [2100/15231], Loss: 0.2859\n",
            "Epoch [2/5], Step [2120/15231], Loss: 0.3037\n",
            "Epoch [2/5], Step [2140/15231], Loss: 0.2805\n",
            "Epoch [2/5], Step [2160/15231], Loss: 0.2711\n",
            "Epoch [2/5], Step [2180/15231], Loss: 0.2792\n",
            "Epoch [2/5], Step [2200/15231], Loss: 0.3407\n",
            "Epoch [2/5], Step [2220/15231], Loss: 0.3065\n",
            "Epoch [2/5], Step [2240/15231], Loss: 0.2524\n",
            "Epoch [2/5], Step [2260/15231], Loss: 0.3196\n",
            "Epoch [2/5], Step [2280/15231], Loss: 0.2881\n",
            "Epoch [2/5], Step [2300/15231], Loss: 0.2764\n",
            "Epoch [2/5], Step [2320/15231], Loss: 0.2771\n",
            "Epoch [2/5], Step [2340/15231], Loss: 0.2831\n",
            "Epoch [2/5], Step [2360/15231], Loss: 0.2596\n",
            "Epoch [2/5], Step [2380/15231], Loss: 0.2991\n",
            "Epoch [2/5], Step [2400/15231], Loss: 0.2709\n",
            "Epoch [2/5], Step [2420/15231], Loss: 0.2737\n",
            "Epoch [2/5], Step [2440/15231], Loss: 0.2900\n",
            "Epoch [2/5], Step [2460/15231], Loss: 0.2639\n",
            "Epoch [2/5], Step [2480/15231], Loss: 0.2708\n",
            "Epoch [2/5], Step [2500/15231], Loss: 0.2472\n",
            "Epoch [2/5], Step [2520/15231], Loss: 0.2860\n",
            "Epoch [2/5], Step [2540/15231], Loss: 0.2971\n",
            "Epoch [2/5], Step [2560/15231], Loss: 0.2515\n",
            "Epoch [2/5], Step [2580/15231], Loss: 0.2993\n",
            "Epoch [2/5], Step [2600/15231], Loss: 0.2652\n",
            "Epoch [2/5], Step [2620/15231], Loss: 0.2590\n",
            "Epoch [2/5], Step [2640/15231], Loss: 0.3103\n",
            "Epoch [2/5], Step [2660/15231], Loss: 0.3212\n",
            "Epoch [2/5], Step [2680/15231], Loss: 0.2645\n",
            "Epoch [2/5], Step [2700/15231], Loss: 0.2959\n",
            "Epoch [2/5], Step [2720/15231], Loss: 0.2811\n",
            "Epoch [2/5], Step [2740/15231], Loss: 0.2464\n",
            "Epoch [2/5], Step [2760/15231], Loss: 0.3001\n",
            "Epoch [2/5], Step [2780/15231], Loss: 0.3098\n",
            "Epoch [2/5], Step [2800/15231], Loss: 0.2770\n",
            "Epoch [2/5], Step [2820/15231], Loss: 0.2884\n",
            "Epoch [2/5], Step [2840/15231], Loss: 0.3370\n",
            "Epoch [2/5], Step [2860/15231], Loss: 0.2607\n",
            "Epoch [2/5], Step [2880/15231], Loss: 0.2893\n",
            "Epoch [2/5], Step [2900/15231], Loss: 0.2525\n",
            "Epoch [2/5], Step [2920/15231], Loss: 0.2876\n",
            "Epoch [2/5], Step [2940/15231], Loss: 0.2494\n",
            "Epoch [2/5], Step [2960/15231], Loss: 0.2710\n",
            "Epoch [2/5], Step [2980/15231], Loss: 0.2636\n",
            "Epoch [2/5], Step [3000/15231], Loss: 0.3077\n",
            "Epoch [2/5], Step [3020/15231], Loss: 0.2593\n",
            "Epoch [2/5], Step [3040/15231], Loss: 0.2690\n",
            "Epoch [2/5], Step [3060/15231], Loss: 0.2468\n",
            "Epoch [2/5], Step [3080/15231], Loss: 0.2773\n",
            "Epoch [2/5], Step [3100/15231], Loss: 0.2744\n",
            "Epoch [2/5], Step [3120/15231], Loss: 0.2537\n",
            "Epoch [2/5], Step [3140/15231], Loss: 0.2515\n",
            "Epoch [2/5], Step [3160/15231], Loss: 0.2841\n",
            "Epoch [2/5], Step [3180/15231], Loss: 0.3026\n",
            "Epoch [2/5], Step [3200/15231], Loss: 0.2827\n",
            "Epoch [2/5], Step [3220/15231], Loss: 0.2978\n",
            "Epoch [2/5], Step [3240/15231], Loss: 0.2772\n",
            "Epoch [2/5], Step [3260/15231], Loss: 0.2431\n",
            "Epoch [2/5], Step [3280/15231], Loss: 0.2992\n",
            "Epoch [2/5], Step [3300/15231], Loss: 0.2752\n",
            "Epoch [2/5], Step [3320/15231], Loss: 0.3091\n",
            "Epoch [2/5], Step [3340/15231], Loss: 0.2899\n",
            "Epoch [2/5], Step [3360/15231], Loss: 0.2909\n",
            "Epoch [2/5], Step [3380/15231], Loss: 0.2613\n",
            "Epoch [2/5], Step [3400/15231], Loss: 0.2704\n",
            "Epoch [2/5], Step [3420/15231], Loss: 0.2561\n",
            "Epoch [2/5], Step [3440/15231], Loss: 0.2568\n",
            "Epoch [2/5], Step [3460/15231], Loss: 0.2847\n",
            "Epoch [2/5], Step [3480/15231], Loss: 0.2821\n",
            "Epoch [2/5], Step [3500/15231], Loss: 0.2719\n",
            "Epoch [2/5], Step [3520/15231], Loss: 0.2776\n",
            "Epoch [2/5], Step [3540/15231], Loss: 0.2666\n",
            "Epoch [2/5], Step [3560/15231], Loss: 0.2788\n",
            "Epoch [2/5], Step [3580/15231], Loss: 0.2589\n",
            "Epoch [2/5], Step [3600/15231], Loss: 0.2441\n",
            "Epoch [2/5], Step [3620/15231], Loss: 0.3086\n",
            "Epoch [2/5], Step [3640/15231], Loss: 0.2889\n",
            "Epoch [2/5], Step [3660/15231], Loss: 0.2713\n",
            "Epoch [2/5], Step [3680/15231], Loss: 0.3000\n",
            "Epoch [2/5], Step [3700/15231], Loss: 0.2668\n",
            "Epoch [2/5], Step [3720/15231], Loss: 0.2594\n",
            "Epoch [2/5], Step [3740/15231], Loss: 0.2467\n",
            "Epoch [2/5], Step [3760/15231], Loss: 0.2762\n",
            "Epoch [2/5], Step [3780/15231], Loss: 0.2499\n",
            "Epoch [2/5], Step [3800/15231], Loss: 0.2548\n",
            "Epoch [2/5], Step [3820/15231], Loss: 0.2449\n",
            "Epoch [2/5], Step [3840/15231], Loss: 0.2649\n",
            "Epoch [2/5], Step [3860/15231], Loss: 0.2521\n",
            "Epoch [2/5], Step [3880/15231], Loss: 0.2627\n",
            "Epoch [2/5], Step [3900/15231], Loss: 0.2822\n",
            "Epoch [2/5], Step [3920/15231], Loss: 0.2594\n",
            "Epoch [2/5], Step [3940/15231], Loss: 0.2716\n",
            "Epoch [2/5], Step [3960/15231], Loss: 0.2532\n",
            "Epoch [2/5], Step [3980/15231], Loss: 0.2555\n",
            "Epoch [2/5], Step [4000/15231], Loss: 0.2531\n",
            "Epoch [2/5], Step [4020/15231], Loss: 0.2938\n",
            "Epoch [2/5], Step [4040/15231], Loss: 0.3048\n",
            "Epoch [2/5], Step [4060/15231], Loss: 0.2457\n",
            "Epoch [2/5], Step [4080/15231], Loss: 0.2915\n",
            "Epoch [2/5], Step [4100/15231], Loss: 0.2901\n",
            "Epoch [2/5], Step [4120/15231], Loss: 0.2894\n",
            "Epoch [2/5], Step [4140/15231], Loss: 0.2640\n",
            "Epoch [2/5], Step [4160/15231], Loss: 0.2544\n",
            "Epoch [2/5], Step [4180/15231], Loss: 0.2629\n",
            "Epoch [2/5], Step [4200/15231], Loss: 0.2737\n",
            "Epoch [2/5], Step [4220/15231], Loss: 0.2959\n",
            "Epoch [2/5], Step [4240/15231], Loss: 0.2800\n",
            "Epoch [2/5], Step [4260/15231], Loss: 0.2509\n",
            "Epoch [2/5], Step [4280/15231], Loss: 0.2946\n",
            "Epoch [2/5], Step [4300/15231], Loss: 0.2680\n",
            "Epoch [2/5], Step [4320/15231], Loss: 0.2635\n",
            "Epoch [2/5], Step [4340/15231], Loss: 0.2740\n",
            "Epoch [2/5], Step [4360/15231], Loss: 0.2755\n",
            "Epoch [2/5], Step [4380/15231], Loss: 0.2721\n",
            "Epoch [2/5], Step [4400/15231], Loss: 0.3195\n",
            "Epoch [2/5], Step [4420/15231], Loss: 0.2942\n",
            "Epoch [2/5], Step [4440/15231], Loss: 0.2780\n",
            "Epoch [2/5], Step [4460/15231], Loss: 0.2601\n",
            "Epoch [2/5], Step [4480/15231], Loss: 0.3085\n",
            "Epoch [2/5], Step [4500/15231], Loss: 0.2628\n",
            "Epoch [2/5], Step [4520/15231], Loss: 0.2789\n",
            "Epoch [2/5], Step [4540/15231], Loss: 0.2696\n",
            "Epoch [2/5], Step [4560/15231], Loss: 0.2707\n",
            "Epoch [2/5], Step [4580/15231], Loss: 0.2795\n",
            "Epoch [2/5], Step [4600/15231], Loss: 0.2466\n",
            "Epoch [2/5], Step [4620/15231], Loss: 0.2846\n",
            "Epoch [2/5], Step [4640/15231], Loss: 0.2932\n",
            "Epoch [2/5], Step [4660/15231], Loss: 0.2728\n",
            "Epoch [2/5], Step [4680/15231], Loss: 0.2737\n",
            "Epoch [2/5], Step [4700/15231], Loss: 0.3010\n",
            "Epoch [2/5], Step [4720/15231], Loss: 0.2574\n",
            "Epoch [2/5], Step [4740/15231], Loss: 0.2794\n",
            "Epoch [2/5], Step [4760/15231], Loss: 0.2499\n",
            "Epoch [2/5], Step [4780/15231], Loss: 0.2532\n",
            "Epoch [2/5], Step [4800/15231], Loss: 0.2760\n",
            "Epoch [2/5], Step [4820/15231], Loss: 0.2566\n",
            "Epoch [2/5], Step [4840/15231], Loss: 0.2612\n",
            "Epoch [2/5], Step [4860/15231], Loss: 0.2684\n",
            "Epoch [2/5], Step [4880/15231], Loss: 0.2657\n",
            "Epoch [2/5], Step [4900/15231], Loss: 0.2753\n",
            "Epoch [2/5], Step [4920/15231], Loss: 0.2701\n",
            "Epoch [2/5], Step [4940/15231], Loss: 0.2879\n",
            "Epoch [2/5], Step [4960/15231], Loss: 0.2978\n",
            "Epoch [2/5], Step [4980/15231], Loss: 0.2638\n",
            "Epoch [2/5], Step [5000/15231], Loss: 0.2973\n",
            "Epoch [2/5], Step [5020/15231], Loss: 0.3086\n",
            "Epoch [2/5], Step [5040/15231], Loss: 0.2676\n",
            "Epoch [2/5], Step [5060/15231], Loss: 0.2609\n",
            "Epoch [2/5], Step [5080/15231], Loss: 0.2816\n",
            "Epoch [2/5], Step [5100/15231], Loss: 0.2531\n",
            "Epoch [2/5], Step [5120/15231], Loss: 0.2786\n",
            "Epoch [2/5], Step [5140/15231], Loss: 0.2522\n",
            "Epoch [2/5], Step [5160/15231], Loss: 0.2616\n",
            "Epoch [2/5], Step [5180/15231], Loss: 0.2560\n",
            "Epoch [2/5], Step [5200/15231], Loss: 0.2811\n",
            "Epoch [2/5], Step [5220/15231], Loss: 0.2502\n",
            "Epoch [2/5], Step [5240/15231], Loss: 0.2884\n",
            "Epoch [2/5], Step [5260/15231], Loss: 0.2726\n",
            "Epoch [2/5], Step [5280/15231], Loss: 0.2711\n",
            "Epoch [2/5], Step [5300/15231], Loss: 0.2602\n",
            "Epoch [2/5], Step [5320/15231], Loss: 0.2483\n",
            "Epoch [2/5], Step [5340/15231], Loss: 0.2861\n",
            "Epoch [2/5], Step [5360/15231], Loss: 0.3006\n",
            "Epoch [2/5], Step [5380/15231], Loss: 0.2984\n",
            "Epoch [2/5], Step [5400/15231], Loss: 0.2521\n",
            "Epoch [2/5], Step [5420/15231], Loss: 0.2870\n",
            "Epoch [2/5], Step [5440/15231], Loss: 0.2664\n",
            "Epoch [2/5], Step [5460/15231], Loss: 0.2653\n",
            "Epoch [2/5], Step [5480/15231], Loss: 0.2868\n",
            "Epoch [2/5], Step [5500/15231], Loss: 0.2811\n",
            "Epoch [2/5], Step [5520/15231], Loss: 0.2832\n",
            "Epoch [2/5], Step [5540/15231], Loss: 0.2960\n",
            "Epoch [2/5], Step [5560/15231], Loss: 0.2722\n",
            "Epoch [2/5], Step [5580/15231], Loss: 0.2348\n",
            "Epoch [2/5], Step [5600/15231], Loss: 0.2831\n",
            "Epoch [2/5], Step [5620/15231], Loss: 0.2843\n",
            "Epoch [2/5], Step [5640/15231], Loss: 0.2697\n",
            "Epoch [2/5], Step [5660/15231], Loss: 0.2704\n",
            "Epoch [2/5], Step [5680/15231], Loss: 0.2618\n",
            "Epoch [2/5], Step [5700/15231], Loss: 0.2883\n",
            "Epoch [2/5], Step [5720/15231], Loss: 0.2351\n",
            "Epoch [2/5], Step [5740/15231], Loss: 0.2458\n",
            "Epoch [2/5], Step [5760/15231], Loss: 0.2696\n",
            "Epoch [2/5], Step [5780/15231], Loss: 0.2572\n",
            "Epoch [2/5], Step [5800/15231], Loss: 0.2794\n",
            "Epoch [2/5], Step [5820/15231], Loss: 0.2743\n",
            "Epoch [2/5], Step [5840/15231], Loss: 0.2342\n",
            "Epoch [2/5], Step [5860/15231], Loss: 0.2860\n",
            "Epoch [2/5], Step [5880/15231], Loss: 0.2450\n",
            "Epoch [2/5], Step [5900/15231], Loss: 0.2889\n",
            "Epoch [2/5], Step [5920/15231], Loss: 0.2758\n",
            "Epoch [2/5], Step [5940/15231], Loss: 0.2714\n",
            "Epoch [2/5], Step [5960/15231], Loss: 0.2977\n",
            "Epoch [2/5], Step [5980/15231], Loss: 0.2856\n",
            "Epoch [2/5], Step [6000/15231], Loss: 0.2307\n",
            "Epoch [2/5], Step [6020/15231], Loss: 0.2627\n",
            "Epoch [2/5], Step [6040/15231], Loss: 0.2333\n",
            "Epoch [2/5], Step [6060/15231], Loss: 0.3018\n",
            "Epoch [2/5], Step [6080/15231], Loss: 0.2661\n",
            "Epoch [2/5], Step [6100/15231], Loss: 0.3047\n",
            "Epoch [2/5], Step [6120/15231], Loss: 0.2539\n",
            "Epoch [2/5], Step [6140/15231], Loss: 0.2835\n",
            "Epoch [2/5], Step [6160/15231], Loss: 0.2662\n",
            "Epoch [2/5], Step [6180/15231], Loss: 0.2827\n",
            "Epoch [2/5], Step [6200/15231], Loss: 0.2399\n",
            "Epoch [2/5], Step [6220/15231], Loss: 0.2981\n",
            "Epoch [2/5], Step [6240/15231], Loss: 0.2473\n",
            "Epoch [2/5], Step [6260/15231], Loss: 0.2678\n",
            "Epoch [2/5], Step [6280/15231], Loss: 0.2510\n",
            "Epoch [2/5], Step [6300/15231], Loss: 0.2592\n",
            "Epoch [2/5], Step [6320/15231], Loss: 0.2708\n",
            "Epoch [2/5], Step [6340/15231], Loss: 0.2665\n",
            "Epoch [2/5], Step [6360/15231], Loss: 0.2936\n",
            "Epoch [2/5], Step [6380/15231], Loss: 0.2708\n",
            "Epoch [2/5], Step [6400/15231], Loss: 0.2397\n",
            "Epoch [2/5], Step [6420/15231], Loss: 0.3116\n",
            "Epoch [2/5], Step [6440/15231], Loss: 0.2850\n",
            "Epoch [2/5], Step [6460/15231], Loss: 0.2908\n",
            "Epoch [2/5], Step [6480/15231], Loss: 0.2978\n",
            "Epoch [2/5], Step [6500/15231], Loss: 0.2836\n",
            "Epoch [2/5], Step [6520/15231], Loss: 0.2768\n",
            "Epoch [2/5], Step [6540/15231], Loss: 0.2612\n",
            "Epoch [2/5], Step [6560/15231], Loss: 0.2782\n",
            "Epoch [2/5], Step [6580/15231], Loss: 0.2777\n",
            "Epoch [2/5], Step [6600/15231], Loss: 0.3061\n",
            "Epoch [2/5], Step [6620/15231], Loss: 0.2715\n",
            "Epoch [2/5], Step [6640/15231], Loss: 0.2348\n",
            "Epoch [2/5], Step [6660/15231], Loss: 0.2431\n",
            "Epoch [2/5], Step [6680/15231], Loss: 0.2845\n",
            "Epoch [2/5], Step [6700/15231], Loss: 0.2362\n",
            "Epoch [2/5], Step [6720/15231], Loss: 0.2482\n",
            "Epoch [2/5], Step [6740/15231], Loss: 0.2421\n",
            "Epoch [2/5], Step [6760/15231], Loss: 0.2338\n",
            "Epoch [2/5], Step [6780/15231], Loss: 0.2477\n",
            "Epoch [2/5], Step [6800/15231], Loss: 0.2414\n",
            "Epoch [2/5], Step [6820/15231], Loss: 0.2458\n",
            "Epoch [2/5], Step [6840/15231], Loss: 0.2790\n",
            "Epoch [2/5], Step [6860/15231], Loss: 0.2647\n",
            "Epoch [2/5], Step [6880/15231], Loss: 0.2608\n",
            "Epoch [2/5], Step [6900/15231], Loss: 0.2691\n",
            "Epoch [2/5], Step [6920/15231], Loss: 0.2533\n",
            "Epoch [2/5], Step [6940/15231], Loss: 0.2413\n",
            "Epoch [2/5], Step [6960/15231], Loss: 0.2787\n",
            "Epoch [2/5], Step [6980/15231], Loss: 0.2764\n",
            "Epoch [2/5], Step [7000/15231], Loss: 0.2563\n",
            "Epoch [2/5], Step [7020/15231], Loss: 0.2548\n",
            "Epoch [2/5], Step [7040/15231], Loss: 0.2937\n",
            "Epoch [2/5], Step [7060/15231], Loss: 0.2776\n",
            "Epoch [2/5], Step [7080/15231], Loss: 0.2580\n",
            "Epoch [2/5], Step [7100/15231], Loss: 0.2548\n",
            "Epoch [2/5], Step [7120/15231], Loss: 0.2852\n",
            "Epoch [2/5], Step [7140/15231], Loss: 0.2833\n",
            "Epoch [2/5], Step [7160/15231], Loss: 0.2794\n",
            "Epoch [2/5], Step [7180/15231], Loss: 0.2603\n",
            "Epoch [2/5], Step [7200/15231], Loss: 0.2643\n",
            "Epoch [2/5], Step [7220/15231], Loss: 0.2761\n",
            "Epoch [2/5], Step [7240/15231], Loss: 0.2465\n",
            "Epoch [2/5], Step [7260/15231], Loss: 0.2441\n",
            "Epoch [2/5], Step [7280/15231], Loss: 0.2593\n",
            "Epoch [2/5], Step [7300/15231], Loss: 0.2743\n",
            "Epoch [2/5], Step [7320/15231], Loss: 0.2668\n",
            "Epoch [2/5], Step [7340/15231], Loss: 0.3006\n",
            "Epoch [2/5], Step [7360/15231], Loss: 0.2775\n",
            "Epoch [2/5], Step [7380/15231], Loss: 0.2252\n",
            "Epoch [2/5], Step [7400/15231], Loss: 0.2904\n",
            "Epoch [2/5], Step [7420/15231], Loss: 0.2687\n",
            "Epoch [2/5], Step [7440/15231], Loss: 0.2626\n",
            "Epoch [2/5], Step [7460/15231], Loss: 0.2561\n",
            "Epoch [2/5], Step [7480/15231], Loss: 0.2847\n",
            "Epoch [2/5], Step [7500/15231], Loss: 0.2433\n",
            "Epoch [2/5], Step [7520/15231], Loss: 0.2567\n",
            "Epoch [2/5], Step [7540/15231], Loss: 0.2669\n",
            "Epoch [2/5], Step [7560/15231], Loss: 0.3075\n",
            "Epoch [2/5], Step [7580/15231], Loss: 0.2678\n",
            "Epoch [2/5], Step [7600/15231], Loss: 0.2726\n",
            "Epoch [2/5], Step [7620/15231], Loss: 0.2551\n",
            "Epoch [2/5], Step [7640/15231], Loss: 0.2567\n",
            "Epoch [2/5], Step [7660/15231], Loss: 0.2481\n",
            "Epoch [2/5], Step [7680/15231], Loss: 0.2276\n",
            "Epoch [2/5], Step [7700/15231], Loss: 0.2664\n",
            "Epoch [2/5], Step [7720/15231], Loss: 0.2492\n",
            "Epoch [2/5], Step [7740/15231], Loss: 0.3054\n",
            "Epoch [2/5], Step [7760/15231], Loss: 0.3064\n",
            "Epoch [2/5], Step [7780/15231], Loss: 0.2381\n",
            "Epoch [2/5], Step [7800/15231], Loss: 0.2631\n",
            "Epoch [2/5], Step [7820/15231], Loss: 0.2237\n",
            "Epoch [2/5], Step [7840/15231], Loss: 0.2536\n",
            "Epoch [2/5], Step [7860/15231], Loss: 0.2573\n",
            "Epoch [2/5], Step [7880/15231], Loss: 0.2401\n",
            "Epoch [2/5], Step [7900/15231], Loss: 0.2953\n",
            "Epoch [2/5], Step [7920/15231], Loss: 0.2645\n",
            "Epoch [2/5], Step [7940/15231], Loss: 0.2627\n",
            "Epoch [2/5], Step [7960/15231], Loss: 0.3101\n",
            "Epoch [2/5], Step [7980/15231], Loss: 0.2369\n",
            "Epoch [2/5], Step [8000/15231], Loss: 0.2638\n",
            "Epoch [2/5], Step [8020/15231], Loss: 0.2658\n",
            "Epoch [2/5], Step [8040/15231], Loss: 0.2431\n",
            "Epoch [2/5], Step [8060/15231], Loss: 0.2310\n",
            "Epoch [2/5], Step [8080/15231], Loss: 0.2681\n",
            "Epoch [2/5], Step [8100/15231], Loss: 0.2592\n",
            "Epoch [2/5], Step [8120/15231], Loss: 0.2751\n",
            "Epoch [2/5], Step [8140/15231], Loss: 0.2850\n",
            "Epoch [2/5], Step [8160/15231], Loss: 0.2199\n",
            "Epoch [2/5], Step [8180/15231], Loss: 0.2549\n",
            "Epoch [2/5], Step [8200/15231], Loss: 0.2411\n",
            "Epoch [2/5], Step [8220/15231], Loss: 0.2596\n",
            "Epoch [2/5], Step [8240/15231], Loss: 0.2706\n",
            "Epoch [2/5], Step [8260/15231], Loss: 0.2966\n",
            "Epoch [2/5], Step [8280/15231], Loss: 0.3313\n",
            "Epoch [2/5], Step [8300/15231], Loss: 0.2581\n",
            "Epoch [2/5], Step [8320/15231], Loss: 0.2553\n",
            "Epoch [2/5], Step [8340/15231], Loss: 0.2546\n",
            "Epoch [2/5], Step [8360/15231], Loss: 0.2425\n",
            "Epoch [2/5], Step [8380/15231], Loss: 0.2563\n",
            "Epoch [2/5], Step [8400/15231], Loss: 0.2623\n",
            "Epoch [2/5], Step [8420/15231], Loss: 0.2806\n",
            "Epoch [2/5], Step [8440/15231], Loss: 0.2865\n",
            "Epoch [2/5], Step [8460/15231], Loss: 0.2655\n",
            "Epoch [2/5], Step [8480/15231], Loss: 0.2632\n",
            "Epoch [2/5], Step [8500/15231], Loss: 0.2768\n",
            "Epoch [2/5], Step [8520/15231], Loss: 0.2754\n",
            "Epoch [2/5], Step [8540/15231], Loss: 0.2829\n",
            "Epoch [2/5], Step [8560/15231], Loss: 0.2785\n",
            "Epoch [2/5], Step [8580/15231], Loss: 0.2921\n",
            "Epoch [2/5], Step [8600/15231], Loss: 0.2404\n",
            "Epoch [2/5], Step [8620/15231], Loss: 0.2463\n",
            "Epoch [2/5], Step [8640/15231], Loss: 0.2781\n",
            "Epoch [2/5], Step [8660/15231], Loss: 0.2611\n",
            "Epoch [2/5], Step [8680/15231], Loss: 0.2781\n",
            "Epoch [2/5], Step [8700/15231], Loss: 0.2318\n",
            "Epoch [2/5], Step [8720/15231], Loss: 0.2912\n",
            "Epoch [2/5], Step [8740/15231], Loss: 0.2299\n",
            "Epoch [2/5], Step [8760/15231], Loss: 0.2601\n",
            "Epoch [2/5], Step [8780/15231], Loss: 0.2856\n",
            "Epoch [2/5], Step [8800/15231], Loss: 0.2995\n",
            "Epoch [2/5], Step [8820/15231], Loss: 0.2972\n",
            "Epoch [2/5], Step [8840/15231], Loss: 0.2477\n",
            "Epoch [2/5], Step [8860/15231], Loss: 0.2773\n",
            "Epoch [2/5], Step [8880/15231], Loss: 0.2506\n",
            "Epoch [2/5], Step [8900/15231], Loss: 0.2020\n",
            "Epoch [2/5], Step [8920/15231], Loss: 0.2538\n",
            "Epoch [2/5], Step [8940/15231], Loss: 0.2753\n",
            "Epoch [2/5], Step [8960/15231], Loss: 0.2524\n",
            "Epoch [2/5], Step [8980/15231], Loss: 0.2266\n",
            "Epoch [2/5], Step [9000/15231], Loss: 0.2278\n",
            "Epoch [2/5], Step [9020/15231], Loss: 0.2222\n",
            "Epoch [2/5], Step [9040/15231], Loss: 0.2056\n",
            "Epoch [2/5], Step [9060/15231], Loss: 0.2743\n",
            "Epoch [2/5], Step [9080/15231], Loss: 0.2710\n",
            "Epoch [2/5], Step [9100/15231], Loss: 0.2697\n",
            "Epoch [2/5], Step [9120/15231], Loss: 0.2610\n",
            "Epoch [2/5], Step [9140/15231], Loss: 0.2758\n",
            "Epoch [2/5], Step [9160/15231], Loss: 0.2932\n",
            "Epoch [2/5], Step [9180/15231], Loss: 0.2468\n",
            "Epoch [2/5], Step [9200/15231], Loss: 0.2481\n",
            "Epoch [2/5], Step [9220/15231], Loss: 0.2869\n",
            "Epoch [2/5], Step [9240/15231], Loss: 0.2622\n",
            "Epoch [2/5], Step [9260/15231], Loss: 0.2512\n",
            "Epoch [2/5], Step [9280/15231], Loss: 0.2445\n",
            "Epoch [2/5], Step [9300/15231], Loss: 0.2479\n",
            "Epoch [2/5], Step [9320/15231], Loss: 0.2877\n",
            "Epoch [2/5], Step [9340/15231], Loss: 0.2726\n",
            "Epoch [2/5], Step [9360/15231], Loss: 0.2592\n",
            "Epoch [2/5], Step [9380/15231], Loss: 0.2871\n",
            "Epoch [2/5], Step [9400/15231], Loss: 0.2514\n",
            "Epoch [2/5], Step [9420/15231], Loss: 0.2145\n",
            "Epoch [2/5], Step [9440/15231], Loss: 0.2738\n",
            "Epoch [2/5], Step [9460/15231], Loss: 0.2812\n",
            "Epoch [2/5], Step [9480/15231], Loss: 0.2496\n",
            "Epoch [2/5], Step [9500/15231], Loss: 0.2877\n",
            "Epoch [2/5], Step [9520/15231], Loss: 0.2793\n",
            "Epoch [2/5], Step [9540/15231], Loss: 0.2469\n",
            "Epoch [2/5], Step [9560/15231], Loss: 0.2548\n",
            "Epoch [2/5], Step [9580/15231], Loss: 0.2486\n",
            "Epoch [2/5], Step [9600/15231], Loss: 0.2394\n",
            "Epoch [2/5], Step [9620/15231], Loss: 0.2994\n",
            "Epoch [2/5], Step [9640/15231], Loss: 0.2567\n",
            "Epoch [2/5], Step [9660/15231], Loss: 0.2701\n",
            "Epoch [2/5], Step [9680/15231], Loss: 0.2922\n",
            "Epoch [2/5], Step [9700/15231], Loss: 0.2858\n",
            "Epoch [2/5], Step [9720/15231], Loss: 0.2861\n",
            "Epoch [2/5], Step [9740/15231], Loss: 0.2737\n",
            "Epoch [2/5], Step [9760/15231], Loss: 0.2514\n",
            "Epoch [2/5], Step [9780/15231], Loss: 0.2348\n",
            "Epoch [2/5], Step [9800/15231], Loss: 0.2457\n",
            "Epoch [2/5], Step [9820/15231], Loss: 0.2561\n",
            "Epoch [2/5], Step [9840/15231], Loss: 0.2878\n",
            "Epoch [2/5], Step [9860/15231], Loss: 0.2709\n",
            "Epoch [2/5], Step [9880/15231], Loss: 0.2556\n",
            "Epoch [2/5], Step [9900/15231], Loss: 0.2229\n",
            "Epoch [2/5], Step [9920/15231], Loss: 0.2623\n",
            "Epoch [2/5], Step [9940/15231], Loss: 0.2181\n",
            "Epoch [2/5], Step [9960/15231], Loss: 0.2481\n",
            "Epoch [2/5], Step [9980/15231], Loss: 0.2289\n",
            "Epoch [2/5], Step [10000/15231], Loss: 0.2556\n",
            "Epoch [2/5], Step [10020/15231], Loss: 0.2442\n",
            "Epoch [2/5], Step [10040/15231], Loss: 0.2657\n",
            "Epoch [2/5], Step [10060/15231], Loss: 0.2802\n",
            "Epoch [2/5], Step [10080/15231], Loss: 0.2963\n",
            "Epoch [2/5], Step [10100/15231], Loss: 0.2683\n",
            "Epoch [2/5], Step [10120/15231], Loss: 0.2930\n",
            "Epoch [2/5], Step [10140/15231], Loss: 0.2932\n",
            "Epoch [2/5], Step [10160/15231], Loss: 0.2768\n",
            "Epoch [2/5], Step [10180/15231], Loss: 0.2454\n",
            "Epoch [2/5], Step [10200/15231], Loss: 0.2777\n",
            "Epoch [2/5], Step [10220/15231], Loss: 0.2763\n",
            "Epoch [2/5], Step [10240/15231], Loss: 0.2455\n",
            "Epoch [2/5], Step [10260/15231], Loss: 0.2887\n",
            "Epoch [2/5], Step [10280/15231], Loss: 0.2536\n",
            "Epoch [2/5], Step [10300/15231], Loss: 0.2665\n",
            "Epoch [2/5], Step [10320/15231], Loss: 0.2509\n",
            "Epoch [2/5], Step [10340/15231], Loss: 0.2660\n",
            "Epoch [2/5], Step [10360/15231], Loss: 0.2395\n",
            "Epoch [2/5], Step [10380/15231], Loss: 0.2118\n",
            "Epoch [2/5], Step [10400/15231], Loss: 0.2568\n",
            "Epoch [2/5], Step [10420/15231], Loss: 0.2875\n",
            "Epoch [2/5], Step [10440/15231], Loss: 0.2075\n",
            "Epoch [2/5], Step [10460/15231], Loss: 0.2539\n",
            "Epoch [2/5], Step [10480/15231], Loss: 0.2295\n",
            "Epoch [2/5], Step [10500/15231], Loss: 0.2701\n",
            "Epoch [2/5], Step [10520/15231], Loss: 0.2163\n",
            "Epoch [2/5], Step [10540/15231], Loss: 0.2434\n",
            "Epoch [2/5], Step [10560/15231], Loss: 0.2572\n",
            "Epoch [2/5], Step [10580/15231], Loss: 0.2499\n",
            "Epoch [2/5], Step [10600/15231], Loss: 0.2355\n",
            "Epoch [2/5], Step [10620/15231], Loss: 0.2896\n",
            "Epoch [2/5], Step [10640/15231], Loss: 0.2686\n",
            "Epoch [2/5], Step [10660/15231], Loss: 0.2176\n",
            "Epoch [2/5], Step [10680/15231], Loss: 0.2742\n",
            "Epoch [2/5], Step [10700/15231], Loss: 0.2551\n",
            "Epoch [2/5], Step [10720/15231], Loss: 0.2531\n",
            "Epoch [2/5], Step [10740/15231], Loss: 0.2438\n",
            "Epoch [2/5], Step [10760/15231], Loss: 0.2781\n",
            "Epoch [2/5], Step [10780/15231], Loss: 0.2347\n",
            "Epoch [2/5], Step [10800/15231], Loss: 0.2438\n",
            "Epoch [2/5], Step [10820/15231], Loss: 0.2803\n",
            "Epoch [2/5], Step [10840/15231], Loss: 0.2628\n",
            "Epoch [2/5], Step [10860/15231], Loss: 0.2823\n",
            "Epoch [2/5], Step [10880/15231], Loss: 0.2533\n",
            "Epoch [2/5], Step [10900/15231], Loss: 0.2153\n",
            "Epoch [2/5], Step [10920/15231], Loss: 0.2686\n",
            "Epoch [2/5], Step [10940/15231], Loss: 0.2834\n",
            "Epoch [2/5], Step [10960/15231], Loss: 0.2681\n",
            "Epoch [2/5], Step [10980/15231], Loss: 0.2461\n",
            "Epoch [2/5], Step [11000/15231], Loss: 0.2697\n",
            "Epoch [2/5], Step [11020/15231], Loss: 0.2440\n",
            "Epoch [2/5], Step [11040/15231], Loss: 0.2465\n",
            "Epoch [2/5], Step [11060/15231], Loss: 0.2553\n",
            "Epoch [2/5], Step [11080/15231], Loss: 0.2741\n",
            "Epoch [2/5], Step [11100/15231], Loss: 0.2830\n",
            "Epoch [2/5], Step [11120/15231], Loss: 0.2629\n",
            "Epoch [2/5], Step [11140/15231], Loss: 0.2261\n",
            "Epoch [2/5], Step [11160/15231], Loss: 0.2746\n",
            "Epoch [2/5], Step [11180/15231], Loss: 0.2613\n",
            "Epoch [2/5], Step [11200/15231], Loss: 0.3110\n",
            "Epoch [2/5], Step [11220/15231], Loss: 0.3007\n",
            "Epoch [2/5], Step [11240/15231], Loss: 0.2573\n",
            "Epoch [2/5], Step [11260/15231], Loss: 0.2373\n",
            "Epoch [2/5], Step [11280/15231], Loss: 0.2666\n",
            "Epoch [2/5], Step [11300/15231], Loss: 0.2578\n",
            "Epoch [2/5], Step [11320/15231], Loss: 0.2696\n",
            "Epoch [2/5], Step [11340/15231], Loss: 0.2507\n",
            "Epoch [2/5], Step [11360/15231], Loss: 0.2353\n",
            "Epoch [2/5], Step [11380/15231], Loss: 0.2612\n",
            "Epoch [2/5], Step [11400/15231], Loss: 0.2935\n",
            "Epoch [2/5], Step [11420/15231], Loss: 0.2473\n",
            "Epoch [2/5], Step [11440/15231], Loss: 0.2599\n",
            "Epoch [2/5], Step [11460/15231], Loss: 0.2761\n",
            "Epoch [2/5], Step [11480/15231], Loss: 0.2292\n",
            "Epoch [2/5], Step [11500/15231], Loss: 0.2800\n",
            "Epoch [2/5], Step [11520/15231], Loss: 0.2586\n",
            "Epoch [2/5], Step [11540/15231], Loss: 0.3017\n",
            "Epoch [2/5], Step [11560/15231], Loss: 0.2420\n",
            "Epoch [2/5], Step [11580/15231], Loss: 0.2094\n",
            "Epoch [2/5], Step [11600/15231], Loss: 0.2808\n",
            "Epoch [2/5], Step [11620/15231], Loss: 0.2545\n",
            "Epoch [2/5], Step [11640/15231], Loss: 0.2495\n",
            "Epoch [2/5], Step [11660/15231], Loss: 0.2809\n",
            "Epoch [2/5], Step [11680/15231], Loss: 0.2661\n",
            "Epoch [2/5], Step [11700/15231], Loss: 0.2564\n",
            "Epoch [2/5], Step [11720/15231], Loss: 0.2721\n",
            "Epoch [2/5], Step [11740/15231], Loss: 0.2782\n",
            "Epoch [2/5], Step [11760/15231], Loss: 0.2447\n",
            "Epoch [2/5], Step [11780/15231], Loss: 0.2206\n",
            "Epoch [2/5], Step [11800/15231], Loss: 0.2579\n",
            "Epoch [2/5], Step [11820/15231], Loss: 0.2938\n",
            "Epoch [2/5], Step [11840/15231], Loss: 0.2889\n",
            "Epoch [2/5], Step [11860/15231], Loss: 0.2837\n",
            "Epoch [2/5], Step [11880/15231], Loss: 0.2665\n",
            "Epoch [2/5], Step [11900/15231], Loss: 0.2637\n",
            "Epoch [2/5], Step [11920/15231], Loss: 0.2671\n",
            "Epoch [2/5], Step [11940/15231], Loss: 0.2770\n",
            "Epoch [2/5], Step [11960/15231], Loss: 0.2657\n",
            "Epoch [2/5], Step [11980/15231], Loss: 0.2921\n",
            "Epoch [2/5], Step [12000/15231], Loss: 0.2423\n",
            "Epoch [2/5], Step [12020/15231], Loss: 0.2521\n",
            "Epoch [2/5], Step [12040/15231], Loss: 0.2814\n",
            "Epoch [2/5], Step [12060/15231], Loss: 0.2992\n",
            "Epoch [2/5], Step [12080/15231], Loss: 0.2816\n",
            "Epoch [2/5], Step [12100/15231], Loss: 0.2585\n",
            "Epoch [2/5], Step [12120/15231], Loss: 0.2983\n",
            "Epoch [2/5], Step [12140/15231], Loss: 0.2648\n",
            "Epoch [2/5], Step [12160/15231], Loss: 0.2386\n",
            "Epoch [2/5], Step [12180/15231], Loss: 0.2540\n",
            "Epoch [2/5], Step [12200/15231], Loss: 0.2562\n",
            "Epoch [2/5], Step [12220/15231], Loss: 0.2616\n",
            "Epoch [2/5], Step [12240/15231], Loss: 0.2387\n",
            "Epoch [2/5], Step [12260/15231], Loss: 0.2558\n",
            "Epoch [2/5], Step [12280/15231], Loss: 0.2758\n",
            "Epoch [2/5], Step [12300/15231], Loss: 0.2758\n",
            "Epoch [2/5], Step [12320/15231], Loss: 0.2683\n",
            "Epoch [2/5], Step [12340/15231], Loss: 0.2871\n",
            "Epoch [2/5], Step [12360/15231], Loss: 0.2710\n",
            "Epoch [2/5], Step [12380/15231], Loss: 0.2933\n",
            "Epoch [2/5], Step [12400/15231], Loss: 0.2472\n",
            "Epoch [2/5], Step [12420/15231], Loss: 0.2560\n",
            "Epoch [2/5], Step [12440/15231], Loss: 0.2461\n",
            "Epoch [2/5], Step [12460/15231], Loss: 0.2882\n",
            "Epoch [2/5], Step [12480/15231], Loss: 0.2529\n",
            "Epoch [2/5], Step [12500/15231], Loss: 0.2363\n",
            "Epoch [2/5], Step [12520/15231], Loss: 0.2597\n",
            "Epoch [2/5], Step [12540/15231], Loss: 0.2771\n",
            "Epoch [2/5], Step [12560/15231], Loss: 0.2343\n",
            "Epoch [2/5], Step [12580/15231], Loss: 0.2491\n",
            "Epoch [2/5], Step [12600/15231], Loss: 0.2903\n",
            "Epoch [2/5], Step [12620/15231], Loss: 0.2720\n",
            "Epoch [2/5], Step [12640/15231], Loss: 0.2579\n",
            "Epoch [2/5], Step [12660/15231], Loss: 0.2406\n",
            "Epoch [2/5], Step [12680/15231], Loss: 0.2583\n",
            "Epoch [2/5], Step [12700/15231], Loss: 0.2643\n",
            "Epoch [2/5], Step [12720/15231], Loss: 0.2443\n",
            "Epoch [2/5], Step [12740/15231], Loss: 0.2669\n",
            "Epoch [2/5], Step [12760/15231], Loss: 0.2620\n",
            "Epoch [2/5], Step [12780/15231], Loss: 0.2519\n",
            "Epoch [2/5], Step [12800/15231], Loss: 0.2525\n",
            "Epoch [2/5], Step [12820/15231], Loss: 0.2703\n",
            "Epoch [2/5], Step [12840/15231], Loss: 0.2897\n",
            "Epoch [2/5], Step [12860/15231], Loss: 0.2624\n",
            "Epoch [2/5], Step [12880/15231], Loss: 0.2826\n",
            "Epoch [2/5], Step [12900/15231], Loss: 0.2478\n",
            "Epoch [2/5], Step [12920/15231], Loss: 0.2730\n",
            "Epoch [2/5], Step [12940/15231], Loss: 0.2494\n",
            "Epoch [2/5], Step [12960/15231], Loss: 0.2108\n",
            "Epoch [2/5], Step [12980/15231], Loss: 0.2545\n",
            "Epoch [2/5], Step [13000/15231], Loss: 0.2672\n",
            "Epoch [2/5], Step [13020/15231], Loss: 0.2913\n",
            "Epoch [2/5], Step [13040/15231], Loss: 0.2469\n",
            "Epoch [2/5], Step [13060/15231], Loss: 0.2698\n",
            "Epoch [2/5], Step [13080/15231], Loss: 0.2493\n",
            "Epoch [2/5], Step [13100/15231], Loss: 0.2598\n",
            "Epoch [2/5], Step [13120/15231], Loss: 0.2763\n",
            "Epoch [2/5], Step [13140/15231], Loss: 0.2658\n",
            "Epoch [2/5], Step [13160/15231], Loss: 0.2456\n",
            "Epoch [2/5], Step [13180/15231], Loss: 0.2695\n",
            "Epoch [2/5], Step [13200/15231], Loss: 0.2573\n",
            "Epoch [2/5], Step [13220/15231], Loss: 0.2198\n",
            "Epoch [2/5], Step [13240/15231], Loss: 0.2651\n",
            "Epoch [2/5], Step [13260/15231], Loss: 0.2670\n",
            "Epoch [2/5], Step [13280/15231], Loss: 0.2741\n",
            "Epoch [2/5], Step [13300/15231], Loss: 0.2364\n",
            "Epoch [2/5], Step [13320/15231], Loss: 0.2395\n",
            "Epoch [2/5], Step [13340/15231], Loss: 0.2262\n",
            "Epoch [2/5], Step [13360/15231], Loss: 0.2223\n",
            "Epoch [2/5], Step [13380/15231], Loss: 0.2984\n",
            "Epoch [2/5], Step [13400/15231], Loss: 0.2765\n",
            "Epoch [2/5], Step [13420/15231], Loss: 0.2829\n",
            "Epoch [2/5], Step [13440/15231], Loss: 0.2702\n",
            "Epoch [2/5], Step [13460/15231], Loss: 0.2449\n",
            "Epoch [2/5], Step [13480/15231], Loss: 0.2663\n",
            "Epoch [2/5], Step [13500/15231], Loss: 0.2814\n",
            "Epoch [2/5], Step [13520/15231], Loss: 0.2503\n",
            "Epoch [2/5], Step [13540/15231], Loss: 0.2480\n",
            "Epoch [2/5], Step [13560/15231], Loss: 0.2457\n",
            "Epoch [2/5], Step [13580/15231], Loss: 0.2660\n",
            "Epoch [2/5], Step [13600/15231], Loss: 0.2251\n",
            "Epoch [2/5], Step [13620/15231], Loss: 0.2478\n",
            "Epoch [2/5], Step [13640/15231], Loss: 0.2693\n",
            "Epoch [2/5], Step [13660/15231], Loss: 0.2471\n",
            "Epoch [2/5], Step [13680/15231], Loss: 0.2513\n",
            "Epoch [2/5], Step [13700/15231], Loss: 0.2577\n",
            "Epoch [2/5], Step [13720/15231], Loss: 0.2752\n",
            "Epoch [2/5], Step [13740/15231], Loss: 0.2607\n",
            "Epoch [2/5], Step [13760/15231], Loss: 0.2819\n",
            "Epoch [2/5], Step [13780/15231], Loss: 0.3106\n",
            "Epoch [2/5], Step [13800/15231], Loss: 0.2560\n",
            "Epoch [2/5], Step [13820/15231], Loss: 0.2829\n",
            "Epoch [2/5], Step [13840/15231], Loss: 0.2503\n",
            "Epoch [2/5], Step [13860/15231], Loss: 0.2413\n",
            "Epoch [2/5], Step [13880/15231], Loss: 0.2646\n",
            "Epoch [2/5], Step [13900/15231], Loss: 0.2524\n",
            "Epoch [2/5], Step [13920/15231], Loss: 0.2187\n",
            "Epoch [2/5], Step [13940/15231], Loss: 0.2452\n",
            "Epoch [2/5], Step [13960/15231], Loss: 0.2325\n",
            "Epoch [2/5], Step [13980/15231], Loss: 0.2728\n",
            "Epoch [2/5], Step [14000/15231], Loss: 0.2367\n",
            "Epoch [2/5], Step [14020/15231], Loss: 0.2912\n",
            "Epoch [2/5], Step [14040/15231], Loss: 0.2640\n",
            "Epoch [2/5], Step [14060/15231], Loss: 0.2302\n",
            "Epoch [2/5], Step [14080/15231], Loss: 0.2653\n",
            "Epoch [2/5], Step [14100/15231], Loss: 0.2402\n",
            "Epoch [2/5], Step [14120/15231], Loss: 0.2490\n",
            "Epoch [2/5], Step [14140/15231], Loss: 0.2438\n",
            "Epoch [2/5], Step [14160/15231], Loss: 0.2498\n",
            "Epoch [2/5], Step [14180/15231], Loss: 0.2775\n",
            "Epoch [2/5], Step [14200/15231], Loss: 0.2671\n",
            "Epoch [2/5], Step [14220/15231], Loss: 0.2286\n",
            "Epoch [2/5], Step [14240/15231], Loss: 0.2502\n",
            "Epoch [2/5], Step [14260/15231], Loss: 0.2304\n",
            "Epoch [2/5], Step [14280/15231], Loss: 0.2476\n",
            "Epoch [2/5], Step [14300/15231], Loss: 0.2333\n",
            "Epoch [2/5], Step [14320/15231], Loss: 0.3026\n",
            "Epoch [2/5], Step [14340/15231], Loss: 0.2415\n",
            "Epoch [2/5], Step [14360/15231], Loss: 0.2451\n",
            "Epoch [2/5], Step [14380/15231], Loss: 0.2361\n",
            "Epoch [2/5], Step [14400/15231], Loss: 0.2363\n",
            "Epoch [2/5], Step [14420/15231], Loss: 0.2461\n",
            "Epoch [2/5], Step [14440/15231], Loss: 0.2141\n",
            "Epoch [2/5], Step [14460/15231], Loss: 0.2371\n",
            "Epoch [2/5], Step [14480/15231], Loss: 0.2510\n",
            "Epoch [2/5], Step [14500/15231], Loss: 0.2769\n",
            "Epoch [2/5], Step [14520/15231], Loss: 0.2472\n",
            "Epoch [2/5], Step [14540/15231], Loss: 0.2702\n",
            "Epoch [2/5], Step [14560/15231], Loss: 0.2495\n",
            "Epoch [2/5], Step [14580/15231], Loss: 0.2435\n",
            "Epoch [2/5], Step [14600/15231], Loss: 0.2392\n",
            "Epoch [2/5], Step [14620/15231], Loss: 0.2274\n",
            "Epoch [2/5], Step [14640/15231], Loss: 0.2529\n",
            "Epoch [2/5], Step [14660/15231], Loss: 0.2942\n",
            "Epoch [2/5], Step [14680/15231], Loss: 0.2511\n",
            "Epoch [2/5], Step [14700/15231], Loss: 0.2729\n",
            "Epoch [2/5], Step [14720/15231], Loss: 0.2591\n",
            "Epoch [2/5], Step [14740/15231], Loss: 0.2434\n",
            "Epoch [2/5], Step [14760/15231], Loss: 0.2481\n",
            "Epoch [2/5], Step [14780/15231], Loss: 0.2729\n",
            "Epoch [2/5], Step [14800/15231], Loss: 0.2395\n",
            "Epoch [2/5], Step [14820/15231], Loss: 0.2541\n",
            "Epoch [2/5], Step [14840/15231], Loss: 0.2316\n",
            "Epoch [2/5], Step [14860/15231], Loss: 0.2258\n",
            "Epoch [2/5], Step [14880/15231], Loss: 0.2514\n",
            "Epoch [2/5], Step [14900/15231], Loss: 0.2590\n",
            "Epoch [2/5], Step [14920/15231], Loss: 0.2460\n",
            "Epoch [2/5], Step [14940/15231], Loss: 0.2751\n",
            "Epoch [2/5], Step [14960/15231], Loss: 0.2806\n",
            "Epoch [2/5], Step [14980/15231], Loss: 0.2492\n",
            "Epoch [2/5], Step [15000/15231], Loss: 0.2390\n",
            "Epoch [2/5], Step [15020/15231], Loss: 0.2163\n",
            "Epoch [2/5], Step [15040/15231], Loss: 0.2410\n",
            "Epoch [2/5], Step [15060/15231], Loss: 0.2746\n",
            "Epoch [2/5], Step [15080/15231], Loss: 0.2613\n",
            "Epoch [2/5], Step [15100/15231], Loss: 0.2314\n",
            "Epoch [2/5], Step [15120/15231], Loss: 0.2718\n",
            "Epoch [2/5], Step [15140/15231], Loss: 0.2370\n",
            "Epoch [2/5], Step [15160/15231], Loss: 0.2810\n",
            "Epoch [2/5], Step [15180/15231], Loss: 0.2554\n",
            "Epoch [2/5], Step [15200/15231], Loss: 0.2762\n",
            "Epoch [2/5], Step [15220/15231], Loss: 0.2589\n",
            "Epoch [2/5] | Train Loss: 0.2673 | Train Acc: 90.63% | Test Loss: 0.6569 | Test Acc: 78.30%\n",
            "Epoch [3/5], Step [20/15231], Loss: 0.2507\n",
            "Epoch [3/5], Step [40/15231], Loss: 0.2434\n",
            "Epoch [3/5], Step [60/15231], Loss: 0.2738\n",
            "Epoch [3/5], Step [80/15231], Loss: 0.2291\n",
            "Epoch [3/5], Step [100/15231], Loss: 0.2708\n",
            "Epoch [3/5], Step [120/15231], Loss: 0.2354\n",
            "Epoch [3/5], Step [140/15231], Loss: 0.2312\n",
            "Epoch [3/5], Step [160/15231], Loss: 0.2309\n",
            "Epoch [3/5], Step [180/15231], Loss: 0.2255\n",
            "Epoch [3/5], Step [200/15231], Loss: 0.2578\n",
            "Epoch [3/5], Step [220/15231], Loss: 0.2595\n",
            "Epoch [3/5], Step [240/15231], Loss: 0.2576\n",
            "Epoch [3/5], Step [260/15231], Loss: 0.2447\n",
            "Epoch [3/5], Step [280/15231], Loss: 0.2405\n",
            "Epoch [3/5], Step [300/15231], Loss: 0.2294\n",
            "Epoch [3/5], Step [320/15231], Loss: 0.2370\n",
            "Epoch [3/5], Step [340/15231], Loss: 0.2368\n",
            "Epoch [3/5], Step [360/15231], Loss: 0.2400\n",
            "Epoch [3/5], Step [380/15231], Loss: 0.2477\n",
            "Epoch [3/5], Step [400/15231], Loss: 0.2554\n",
            "Epoch [3/5], Step [420/15231], Loss: 0.2381\n",
            "Epoch [3/5], Step [440/15231], Loss: 0.2551\n",
            "Epoch [3/5], Step [460/15231], Loss: 0.2595\n",
            "Epoch [3/5], Step [480/15231], Loss: 0.2189\n",
            "Epoch [3/5], Step [500/15231], Loss: 0.2625\n",
            "Epoch [3/5], Step [520/15231], Loss: 0.2631\n",
            "Epoch [3/5], Step [540/15231], Loss: 0.2552\n",
            "Epoch [3/5], Step [560/15231], Loss: 0.2376\n",
            "Epoch [3/5], Step [580/15231], Loss: 0.2647\n",
            "Epoch [3/5], Step [600/15231], Loss: 0.2505\n",
            "Epoch [3/5], Step [620/15231], Loss: 0.2778\n",
            "Epoch [3/5], Step [640/15231], Loss: 0.2432\n",
            "Epoch [3/5], Step [660/15231], Loss: 0.2696\n",
            "Epoch [3/5], Step [680/15231], Loss: 0.2626\n",
            "Epoch [3/5], Step [700/15231], Loss: 0.2559\n",
            "Epoch [3/5], Step [720/15231], Loss: 0.2510\n",
            "Epoch [3/5], Step [740/15231], Loss: 0.2493\n",
            "Epoch [3/5], Step [760/15231], Loss: 0.2444\n",
            "Epoch [3/5], Step [780/15231], Loss: 0.2464\n",
            "Epoch [3/5], Step [800/15231], Loss: 0.2202\n",
            "Epoch [3/5], Step [820/15231], Loss: 0.2240\n",
            "Epoch [3/5], Step [840/15231], Loss: 0.2637\n",
            "Epoch [3/5], Step [860/15231], Loss: 0.2669\n",
            "Epoch [3/5], Step [880/15231], Loss: 0.2613\n",
            "Epoch [3/5], Step [900/15231], Loss: 0.2334\n",
            "Epoch [3/5], Step [920/15231], Loss: 0.2524\n",
            "Epoch [3/5], Step [940/15231], Loss: 0.2348\n",
            "Epoch [3/5], Step [960/15231], Loss: 0.2391\n",
            "Epoch [3/5], Step [980/15231], Loss: 0.2155\n",
            "Epoch [3/5], Step [1000/15231], Loss: 0.2580\n",
            "Epoch [3/5], Step [1020/15231], Loss: 0.2300\n",
            "Epoch [3/5], Step [1040/15231], Loss: 0.2592\n",
            "Epoch [3/5], Step [1060/15231], Loss: 0.2883\n",
            "Epoch [3/5], Step [1080/15231], Loss: 0.2395\n",
            "Epoch [3/5], Step [1100/15231], Loss: 0.2293\n",
            "Epoch [3/5], Step [1120/15231], Loss: 0.2382\n",
            "Epoch [3/5], Step [1140/15231], Loss: 0.2537\n",
            "Epoch [3/5], Step [1160/15231], Loss: 0.2766\n",
            "Epoch [3/5], Step [1180/15231], Loss: 0.2531\n",
            "Epoch [3/5], Step [1200/15231], Loss: 0.2524\n",
            "Epoch [3/5], Step [1220/15231], Loss: 0.2249\n",
            "Epoch [3/5], Step [1240/15231], Loss: 0.2620\n",
            "Epoch [3/5], Step [1260/15231], Loss: 0.2430\n",
            "Epoch [3/5], Step [1280/15231], Loss: 0.2160\n",
            "Epoch [3/5], Step [1300/15231], Loss: 0.2169\n",
            "Epoch [3/5], Step [1320/15231], Loss: 0.2556\n",
            "Epoch [3/5], Step [1340/15231], Loss: 0.2376\n",
            "Epoch [3/5], Step [1360/15231], Loss: 0.2610\n",
            "Epoch [3/5], Step [1380/15231], Loss: 0.2296\n",
            "Epoch [3/5], Step [1400/15231], Loss: 0.2623\n",
            "Epoch [3/5], Step [1420/15231], Loss: 0.2586\n",
            "Epoch [3/5], Step [1440/15231], Loss: 0.2421\n",
            "Epoch [3/5], Step [1460/15231], Loss: 0.2321\n",
            "Epoch [3/5], Step [1480/15231], Loss: 0.2717\n",
            "Epoch [3/5], Step [1500/15231], Loss: 0.2430\n",
            "Epoch [3/5], Step [1520/15231], Loss: 0.2495\n",
            "Epoch [3/5], Step [1540/15231], Loss: 0.2331\n",
            "Epoch [3/5], Step [1560/15231], Loss: 0.2772\n",
            "Epoch [3/5], Step [1580/15231], Loss: 0.2650\n",
            "Epoch [3/5], Step [1600/15231], Loss: 0.2508\n",
            "Epoch [3/5], Step [1620/15231], Loss: 0.2686\n",
            "Epoch [3/5], Step [1640/15231], Loss: 0.2431\n",
            "Epoch [3/5], Step [1660/15231], Loss: 0.2339\n",
            "Epoch [3/5], Step [1680/15231], Loss: 0.2440\n",
            "Epoch [3/5], Step [1700/15231], Loss: 0.2536\n",
            "Epoch [3/5], Step [1720/15231], Loss: 0.2638\n",
            "Epoch [3/5], Step [1740/15231], Loss: 0.2441\n",
            "Epoch [3/5], Step [1760/15231], Loss: 0.2374\n",
            "Epoch [3/5], Step [1780/15231], Loss: 0.2560\n",
            "Epoch [3/5], Step [1800/15231], Loss: 0.2171\n",
            "Epoch [3/5], Step [1820/15231], Loss: 0.2216\n",
            "Epoch [3/5], Step [1840/15231], Loss: 0.2471\n",
            "Epoch [3/5], Step [1860/15231], Loss: 0.2469\n",
            "Epoch [3/5], Step [1880/15231], Loss: 0.2750\n",
            "Epoch [3/5], Step [1900/15231], Loss: 0.2376\n",
            "Epoch [3/5], Step [1920/15231], Loss: 0.2571\n",
            "Epoch [3/5], Step [1940/15231], Loss: 0.2219\n",
            "Epoch [3/5], Step [1960/15231], Loss: 0.2578\n",
            "Epoch [3/5], Step [1980/15231], Loss: 0.2612\n",
            "Epoch [3/5], Step [2000/15231], Loss: 0.2341\n",
            "Epoch [3/5], Step [2020/15231], Loss: 0.2530\n",
            "Epoch [3/5], Step [2040/15231], Loss: 0.2601\n",
            "Epoch [3/5], Step [2060/15231], Loss: 0.2526\n",
            "Epoch [3/5], Step [2080/15231], Loss: 0.2402\n",
            "Epoch [3/5], Step [2100/15231], Loss: 0.2346\n",
            "Epoch [3/5], Step [2120/15231], Loss: 0.2451\n",
            "Epoch [3/5], Step [2140/15231], Loss: 0.2196\n",
            "Epoch [3/5], Step [2160/15231], Loss: 0.2606\n",
            "Epoch [3/5], Step [2180/15231], Loss: 0.2654\n",
            "Epoch [3/5], Step [2200/15231], Loss: 0.2265\n",
            "Epoch [3/5], Step [2220/15231], Loss: 0.2327\n",
            "Epoch [3/5], Step [2240/15231], Loss: 0.2394\n",
            "Epoch [3/5], Step [2260/15231], Loss: 0.2779\n",
            "Epoch [3/5], Step [2280/15231], Loss: 0.2408\n",
            "Epoch [3/5], Step [2300/15231], Loss: 0.2309\n",
            "Epoch [3/5], Step [2320/15231], Loss: 0.2448\n",
            "Epoch [3/5], Step [2340/15231], Loss: 0.2527\n",
            "Epoch [3/5], Step [2360/15231], Loss: 0.2183\n",
            "Epoch [3/5], Step [2380/15231], Loss: 0.2614\n",
            "Epoch [3/5], Step [2400/15231], Loss: 0.2397\n",
            "Epoch [3/5], Step [2420/15231], Loss: 0.2508\n",
            "Epoch [3/5], Step [2440/15231], Loss: 0.2579\n",
            "Epoch [3/5], Step [2460/15231], Loss: 0.2514\n",
            "Epoch [3/5], Step [2480/15231], Loss: 0.3113\n",
            "Epoch [3/5], Step [2500/15231], Loss: 0.2427\n",
            "Epoch [3/5], Step [2520/15231], Loss: 0.2340\n",
            "Epoch [3/5], Step [2540/15231], Loss: 0.2640\n",
            "Epoch [3/5], Step [2560/15231], Loss: 0.2460\n",
            "Epoch [3/5], Step [2580/15231], Loss: 0.2425\n",
            "Epoch [3/5], Step [2600/15231], Loss: 0.2691\n",
            "Epoch [3/5], Step [2620/15231], Loss: 0.2440\n",
            "Epoch [3/5], Step [2640/15231], Loss: 0.2516\n",
            "Epoch [3/5], Step [2660/15231], Loss: 0.2532\n",
            "Epoch [3/5], Step [2680/15231], Loss: 0.2462\n",
            "Epoch [3/5], Step [2700/15231], Loss: 0.2673\n",
            "Epoch [3/5], Step [2720/15231], Loss: 0.2534\n",
            "Epoch [3/5], Step [2740/15231], Loss: 0.2512\n",
            "Epoch [3/5], Step [2760/15231], Loss: 0.2259\n",
            "Epoch [3/5], Step [2780/15231], Loss: 0.2578\n",
            "Epoch [3/5], Step [2800/15231], Loss: 0.2291\n",
            "Epoch [3/5], Step [2820/15231], Loss: 0.2575\n",
            "Epoch [3/5], Step [2840/15231], Loss: 0.2442\n",
            "Epoch [3/5], Step [2860/15231], Loss: 0.2146\n",
            "Epoch [3/5], Step [2880/15231], Loss: 0.2336\n",
            "Epoch [3/5], Step [2900/15231], Loss: 0.2367\n",
            "Epoch [3/5], Step [2920/15231], Loss: 0.2222\n",
            "Epoch [3/5], Step [2940/15231], Loss: 0.2492\n",
            "Epoch [3/5], Step [2960/15231], Loss: 0.2212\n",
            "Epoch [3/5], Step [2980/15231], Loss: 0.2020\n",
            "Epoch [3/5], Step [3000/15231], Loss: 0.2788\n",
            "Epoch [3/5], Step [3020/15231], Loss: 0.2170\n",
            "Epoch [3/5], Step [3040/15231], Loss: 0.2369\n",
            "Epoch [3/5], Step [3060/15231], Loss: 0.2275\n",
            "Epoch [3/5], Step [3080/15231], Loss: 0.2776\n",
            "Epoch [3/5], Step [3100/15231], Loss: 0.2681\n",
            "Epoch [3/5], Step [3120/15231], Loss: 0.2394\n",
            "Epoch [3/5], Step [3140/15231], Loss: 0.2017\n",
            "Epoch [3/5], Step [3160/15231], Loss: 0.2241\n",
            "Epoch [3/5], Step [3180/15231], Loss: 0.2194\n",
            "Epoch [3/5], Step [3200/15231], Loss: 0.2396\n",
            "Epoch [3/5], Step [3220/15231], Loss: 0.2106\n",
            "Epoch [3/5], Step [3240/15231], Loss: 0.2726\n",
            "Epoch [3/5], Step [3260/15231], Loss: 0.2500\n",
            "Epoch [3/5], Step [3280/15231], Loss: 0.2405\n",
            "Epoch [3/5], Step [3300/15231], Loss: 0.2314\n",
            "Epoch [3/5], Step [3320/15231], Loss: 0.2553\n",
            "Epoch [3/5], Step [3340/15231], Loss: 0.2444\n",
            "Epoch [3/5], Step [3360/15231], Loss: 0.2437\n",
            "Epoch [3/5], Step [3380/15231], Loss: 0.2483\n",
            "Epoch [3/5], Step [3400/15231], Loss: 0.2876\n",
            "Epoch [3/5], Step [3420/15231], Loss: 0.2197\n",
            "Epoch [3/5], Step [3440/15231], Loss: 0.2560\n",
            "Epoch [3/5], Step [3460/15231], Loss: 0.2331\n",
            "Epoch [3/5], Step [3480/15231], Loss: 0.2302\n",
            "Epoch [3/5], Step [3500/15231], Loss: 0.2472\n",
            "Epoch [3/5], Step [3520/15231], Loss: 0.2377\n",
            "Epoch [3/5], Step [3540/15231], Loss: 0.2397\n",
            "Epoch [3/5], Step [3560/15231], Loss: 0.2406\n",
            "Epoch [3/5], Step [3580/15231], Loss: 0.2651\n",
            "Epoch [3/5], Step [3600/15231], Loss: 0.2463\n",
            "Epoch [3/5], Step [3620/15231], Loss: 0.2781\n",
            "Epoch [3/5], Step [3640/15231], Loss: 0.2250\n",
            "Epoch [3/5], Step [3660/15231], Loss: 0.2598\n",
            "Epoch [3/5], Step [3680/15231], Loss: 0.2470\n",
            "Epoch [3/5], Step [3700/15231], Loss: 0.2876\n",
            "Epoch [3/5], Step [3720/15231], Loss: 0.2410\n",
            "Epoch [3/5], Step [3740/15231], Loss: 0.2610\n",
            "Epoch [3/5], Step [3760/15231], Loss: 0.2653\n",
            "Epoch [3/5], Step [3780/15231], Loss: 0.2127\n",
            "Epoch [3/5], Step [3800/15231], Loss: 0.2511\n",
            "Epoch [3/5], Step [3820/15231], Loss: 0.2428\n",
            "Epoch [3/5], Step [3840/15231], Loss: 0.2408\n",
            "Epoch [3/5], Step [3860/15231], Loss: 0.2057\n",
            "Epoch [3/5], Step [3880/15231], Loss: 0.2414\n",
            "Epoch [3/5], Step [3900/15231], Loss: 0.2274\n",
            "Epoch [3/5], Step [3920/15231], Loss: 0.2910\n",
            "Epoch [3/5], Step [3940/15231], Loss: 0.2359\n",
            "Epoch [3/5], Step [3960/15231], Loss: 0.2133\n",
            "Epoch [3/5], Step [3980/15231], Loss: 0.2542\n",
            "Epoch [3/5], Step [4000/15231], Loss: 0.2349\n",
            "Epoch [3/5], Step [4020/15231], Loss: 0.2260\n",
            "Epoch [3/5], Step [4040/15231], Loss: 0.2558\n",
            "Epoch [3/5], Step [4060/15231], Loss: 0.2778\n",
            "Epoch [3/5], Step [4080/15231], Loss: 0.2492\n",
            "Epoch [3/5], Step [4100/15231], Loss: 0.2515\n",
            "Epoch [3/5], Step [4120/15231], Loss: 0.2376\n",
            "Epoch [3/5], Step [4140/15231], Loss: 0.2388\n",
            "Epoch [3/5], Step [4160/15231], Loss: 0.2366\n",
            "Epoch [3/5], Step [4180/15231], Loss: 0.2818\n",
            "Epoch [3/5], Step [4200/15231], Loss: 0.2544\n",
            "Epoch [3/5], Step [4220/15231], Loss: 0.2387\n",
            "Epoch [3/5], Step [4240/15231], Loss: 0.2323\n",
            "Epoch [3/5], Step [4260/15231], Loss: 0.2400\n",
            "Epoch [3/5], Step [4280/15231], Loss: 0.2400\n",
            "Epoch [3/5], Step [4300/15231], Loss: 0.2547\n",
            "Epoch [3/5], Step [4320/15231], Loss: 0.2433\n",
            "Epoch [3/5], Step [4340/15231], Loss: 0.2743\n",
            "Epoch [3/5], Step [4360/15231], Loss: 0.2286\n",
            "Epoch [3/5], Step [4380/15231], Loss: 0.2674\n",
            "Epoch [3/5], Step [4400/15231], Loss: 0.2254\n",
            "Epoch [3/5], Step [4420/15231], Loss: 0.2582\n",
            "Epoch [3/5], Step [4440/15231], Loss: 0.2558\n",
            "Epoch [3/5], Step [4460/15231], Loss: 0.2631\n",
            "Epoch [3/5], Step [4480/15231], Loss: 0.2652\n",
            "Epoch [3/5], Step [4500/15231], Loss: 0.2105\n",
            "Epoch [3/5], Step [4520/15231], Loss: 0.2115\n",
            "Epoch [3/5], Step [4540/15231], Loss: 0.2485\n",
            "Epoch [3/5], Step [4560/15231], Loss: 0.2299\n",
            "Epoch [3/5], Step [4580/15231], Loss: 0.2371\n",
            "Epoch [3/5], Step [4600/15231], Loss: 0.2635\n",
            "Epoch [3/5], Step [4620/15231], Loss: 0.2458\n",
            "Epoch [3/5], Step [4640/15231], Loss: 0.2280\n",
            "Epoch [3/5], Step [4660/15231], Loss: 0.2627\n",
            "Epoch [3/5], Step [4680/15231], Loss: 0.2500\n",
            "Epoch [3/5], Step [4700/15231], Loss: 0.2343\n",
            "Epoch [3/5], Step [4720/15231], Loss: 0.2195\n",
            "Epoch [3/5], Step [4740/15231], Loss: 0.2356\n",
            "Epoch [3/5], Step [4760/15231], Loss: 0.2459\n",
            "Epoch [3/5], Step [4780/15231], Loss: 0.2721\n",
            "Epoch [3/5], Step [4800/15231], Loss: 0.2085\n",
            "Epoch [3/5], Step [4820/15231], Loss: 0.2253\n",
            "Epoch [3/5], Step [4840/15231], Loss: 0.2290\n",
            "Epoch [3/5], Step [4860/15231], Loss: 0.2351\n",
            "Epoch [3/5], Step [4880/15231], Loss: 0.2507\n",
            "Epoch [3/5], Step [4900/15231], Loss: 0.2230\n",
            "Epoch [3/5], Step [4920/15231], Loss: 0.2266\n",
            "Epoch [3/5], Step [4940/15231], Loss: 0.2375\n",
            "Epoch [3/5], Step [4960/15231], Loss: 0.2493\n",
            "Epoch [3/5], Step [4980/15231], Loss: 0.2342\n",
            "Epoch [3/5], Step [5000/15231], Loss: 0.2096\n",
            "Epoch [3/5], Step [5020/15231], Loss: 0.2451\n",
            "Epoch [3/5], Step [5040/15231], Loss: 0.2254\n",
            "Epoch [3/5], Step [5060/15231], Loss: 0.2873\n",
            "Epoch [3/5], Step [5080/15231], Loss: 0.2555\n",
            "Epoch [3/5], Step [5100/15231], Loss: 0.2173\n",
            "Epoch [3/5], Step [5120/15231], Loss: 0.2512\n",
            "Epoch [3/5], Step [5140/15231], Loss: 0.2356\n",
            "Epoch [3/5], Step [5160/15231], Loss: 0.2126\n",
            "Epoch [3/5], Step [5180/15231], Loss: 0.2154\n",
            "Epoch [3/5], Step [5200/15231], Loss: 0.2070\n",
            "Epoch [3/5], Step [5220/15231], Loss: 0.2302\n",
            "Epoch [3/5], Step [5240/15231], Loss: 0.2301\n",
            "Epoch [3/5], Step [5260/15231], Loss: 0.2516\n",
            "Epoch [3/5], Step [5280/15231], Loss: 0.2306\n",
            "Epoch [3/5], Step [5300/15231], Loss: 0.2084\n",
            "Epoch [3/5], Step [5320/15231], Loss: 0.2678\n",
            "Epoch [3/5], Step [5340/15231], Loss: 0.2660\n",
            "Epoch [3/5], Step [5360/15231], Loss: 0.2256\n",
            "Epoch [3/5], Step [5380/15231], Loss: 0.2386\n",
            "Epoch [3/5], Step [5400/15231], Loss: 0.2381\n",
            "Epoch [3/5], Step [5420/15231], Loss: 0.2407\n",
            "Epoch [3/5], Step [5440/15231], Loss: 0.2191\n",
            "Epoch [3/5], Step [5460/15231], Loss: 0.2759\n",
            "Epoch [3/5], Step [5480/15231], Loss: 0.2536\n",
            "Epoch [3/5], Step [5500/15231], Loss: 0.2400\n",
            "Epoch [3/5], Step [5520/15231], Loss: 0.2515\n",
            "Epoch [3/5], Step [5540/15231], Loss: 0.2482\n",
            "Epoch [3/5], Step [5560/15231], Loss: 0.2463\n",
            "Epoch [3/5], Step [5580/15231], Loss: 0.2478\n",
            "Epoch [3/5], Step [5600/15231], Loss: 0.2232\n",
            "Epoch [3/5], Step [5620/15231], Loss: 0.2662\n",
            "Epoch [3/5], Step [5640/15231], Loss: 0.2481\n",
            "Epoch [3/5], Step [5660/15231], Loss: 0.2493\n",
            "Epoch [3/5], Step [5680/15231], Loss: 0.2459\n",
            "Epoch [3/5], Step [5700/15231], Loss: 0.2430\n",
            "Epoch [3/5], Step [5720/15231], Loss: 0.2382\n",
            "Epoch [3/5], Step [5740/15231], Loss: 0.2210\n",
            "Epoch [3/5], Step [5760/15231], Loss: 0.2539\n",
            "Epoch [3/5], Step [5780/15231], Loss: 0.2554\n",
            "Epoch [3/5], Step [5800/15231], Loss: 0.2465\n",
            "Epoch [3/5], Step [5820/15231], Loss: 0.2426\n",
            "Epoch [3/5], Step [5840/15231], Loss: 0.2291\n",
            "Epoch [3/5], Step [5860/15231], Loss: 0.2436\n",
            "Epoch [3/5], Step [5880/15231], Loss: 0.2368\n",
            "Epoch [3/5], Step [5900/15231], Loss: 0.2633\n",
            "Epoch [3/5], Step [5920/15231], Loss: 0.2572\n",
            "Epoch [3/5], Step [5940/15231], Loss: 0.2140\n",
            "Epoch [3/5], Step [5960/15231], Loss: 0.2485\n",
            "Epoch [3/5], Step [5980/15231], Loss: 0.2450\n",
            "Epoch [3/5], Step [6000/15231], Loss: 0.2509\n",
            "Epoch [3/5], Step [6020/15231], Loss: 0.2271\n",
            "Epoch [3/5], Step [6040/15231], Loss: 0.2437\n",
            "Epoch [3/5], Step [6060/15231], Loss: 0.2255\n",
            "Epoch [3/5], Step [6080/15231], Loss: 0.2185\n",
            "Epoch [3/5], Step [6100/15231], Loss: 0.2252\n",
            "Epoch [3/5], Step [6120/15231], Loss: 0.2478\n",
            "Epoch [3/5], Step [6140/15231], Loss: 0.2414\n",
            "Epoch [3/5], Step [6160/15231], Loss: 0.2732\n",
            "Epoch [3/5], Step [6180/15231], Loss: 0.2415\n",
            "Epoch [3/5], Step [6200/15231], Loss: 0.2606\n",
            "Epoch [3/5], Step [6220/15231], Loss: 0.2555\n",
            "Epoch [3/5], Step [6240/15231], Loss: 0.2349\n",
            "Epoch [3/5], Step [6260/15231], Loss: 0.2512\n",
            "Epoch [3/5], Step [6280/15231], Loss: 0.2222\n",
            "Epoch [3/5], Step [6300/15231], Loss: 0.2352\n",
            "Epoch [3/5], Step [6320/15231], Loss: 0.2155\n",
            "Epoch [3/5], Step [6340/15231], Loss: 0.2381\n",
            "Epoch [3/5], Step [6360/15231], Loss: 0.2395\n",
            "Epoch [3/5], Step [6380/15231], Loss: 0.2333\n",
            "Epoch [3/5], Step [6400/15231], Loss: 0.2319\n",
            "Epoch [3/5], Step [6420/15231], Loss: 0.2616\n",
            "Epoch [3/5], Step [6440/15231], Loss: 0.2365\n",
            "Epoch [3/5], Step [6460/15231], Loss: 0.2328\n",
            "Epoch [3/5], Step [6480/15231], Loss: 0.2553\n",
            "Epoch [3/5], Step [6500/15231], Loss: 0.2613\n",
            "Epoch [3/5], Step [6520/15231], Loss: 0.2166\n",
            "Epoch [3/5], Step [6540/15231], Loss: 0.2229\n",
            "Epoch [3/5], Step [6560/15231], Loss: 0.2461\n",
            "Epoch [3/5], Step [6580/15231], Loss: 0.2282\n",
            "Epoch [3/5], Step [6600/15231], Loss: 0.2192\n",
            "Epoch [3/5], Step [6620/15231], Loss: 0.2440\n",
            "Epoch [3/5], Step [6640/15231], Loss: 0.2182\n",
            "Epoch [3/5], Step [6660/15231], Loss: 0.2073\n",
            "Epoch [3/5], Step [6680/15231], Loss: 0.2423\n",
            "Epoch [3/5], Step [6700/15231], Loss: 0.2458\n",
            "Epoch [3/5], Step [6720/15231], Loss: 0.2399\n",
            "Epoch [3/5], Step [6740/15231], Loss: 0.2281\n",
            "Epoch [3/5], Step [6760/15231], Loss: 0.2340\n",
            "Epoch [3/5], Step [6780/15231], Loss: 0.2501\n",
            "Epoch [3/5], Step [6800/15231], Loss: 0.2376\n",
            "Epoch [3/5], Step [6820/15231], Loss: 0.2479\n",
            "Epoch [3/5], Step [6840/15231], Loss: 0.2525\n",
            "Epoch [3/5], Step [6860/15231], Loss: 0.2195\n",
            "Epoch [3/5], Step [6880/15231], Loss: 0.2468\n",
            "Epoch [3/5], Step [6900/15231], Loss: 0.2453\n",
            "Epoch [3/5], Step [6920/15231], Loss: 0.2564\n",
            "Epoch [3/5], Step [6940/15231], Loss: 0.2439\n",
            "Epoch [3/5], Step [6960/15231], Loss: 0.2410\n",
            "Epoch [3/5], Step [6980/15231], Loss: 0.2255\n",
            "Epoch [3/5], Step [7000/15231], Loss: 0.2382\n",
            "Epoch [3/5], Step [7020/15231], Loss: 0.2498\n",
            "Epoch [3/5], Step [7040/15231], Loss: 0.2409\n",
            "Epoch [3/5], Step [7060/15231], Loss: 0.2517\n",
            "Epoch [3/5], Step [7080/15231], Loss: 0.2453\n",
            "Epoch [3/5], Step [7100/15231], Loss: 0.2809\n",
            "Epoch [3/5], Step [7120/15231], Loss: 0.2576\n",
            "Epoch [3/5], Step [7140/15231], Loss: 0.2500\n",
            "Epoch [3/5], Step [7160/15231], Loss: 0.2400\n",
            "Epoch [3/5], Step [7180/15231], Loss: 0.2460\n",
            "Epoch [3/5], Step [7200/15231], Loss: 0.2393\n",
            "Epoch [3/5], Step [7220/15231], Loss: 0.2369\n",
            "Epoch [3/5], Step [7240/15231], Loss: 0.2128\n",
            "Epoch [3/5], Step [7260/15231], Loss: 0.2298\n",
            "Epoch [3/5], Step [7280/15231], Loss: 0.2524\n",
            "Epoch [3/5], Step [7300/15231], Loss: 0.2395\n",
            "Epoch [3/5], Step [7320/15231], Loss: 0.2666\n",
            "Epoch [3/5], Step [7340/15231], Loss: 0.2257\n",
            "Epoch [3/5], Step [7360/15231], Loss: 0.2126\n",
            "Epoch [3/5], Step [7380/15231], Loss: 0.2263\n",
            "Epoch [3/5], Step [7400/15231], Loss: 0.2824\n",
            "Epoch [3/5], Step [7420/15231], Loss: 0.2346\n",
            "Epoch [3/5], Step [7440/15231], Loss: 0.2202\n",
            "Epoch [3/5], Step [7460/15231], Loss: 0.2363\n",
            "Epoch [3/5], Step [7480/15231], Loss: 0.2264\n",
            "Epoch [3/5], Step [7500/15231], Loss: 0.2871\n",
            "Epoch [3/5], Step [7520/15231], Loss: 0.2368\n",
            "Epoch [3/5], Step [7540/15231], Loss: 0.2360\n",
            "Epoch [3/5], Step [7560/15231], Loss: 0.2472\n",
            "Epoch [3/5], Step [7580/15231], Loss: 0.2433\n",
            "Epoch [3/5], Step [7600/15231], Loss: 0.2688\n",
            "Epoch [3/5], Step [7620/15231], Loss: 0.2317\n",
            "Epoch [3/5], Step [7640/15231], Loss: 0.2721\n",
            "Epoch [3/5], Step [7660/15231], Loss: 0.2320\n",
            "Epoch [3/5], Step [7680/15231], Loss: 0.2144\n",
            "Epoch [3/5], Step [7700/15231], Loss: 0.2483\n",
            "Epoch [3/5], Step [7720/15231], Loss: 0.2441\n",
            "Epoch [3/5], Step [7740/15231], Loss: 0.2375\n",
            "Epoch [3/5], Step [7760/15231], Loss: 0.2180\n",
            "Epoch [3/5], Step [7780/15231], Loss: 0.2185\n",
            "Epoch [3/5], Step [7800/15231], Loss: 0.2490\n",
            "Epoch [3/5], Step [7820/15231], Loss: 0.2223\n",
            "Epoch [3/5], Step [7840/15231], Loss: 0.1897\n",
            "Epoch [3/5], Step [7860/15231], Loss: 0.2138\n",
            "Epoch [3/5], Step [7880/15231], Loss: 0.2238\n",
            "Epoch [3/5], Step [7900/15231], Loss: 0.2635\n",
            "Epoch [3/5], Step [7920/15231], Loss: 0.2453\n",
            "Epoch [3/5], Step [7940/15231], Loss: 0.2736\n",
            "Epoch [3/5], Step [7960/15231], Loss: 0.2387\n",
            "Epoch [3/5], Step [7980/15231], Loss: 0.2382\n",
            "Epoch [3/5], Step [8000/15231], Loss: 0.2607\n",
            "Epoch [3/5], Step [8020/15231], Loss: 0.2318\n",
            "Epoch [3/5], Step [8040/15231], Loss: 0.2455\n",
            "Epoch [3/5], Step [8060/15231], Loss: 0.2306\n",
            "Epoch [3/5], Step [8080/15231], Loss: 0.2189\n",
            "Epoch [3/5], Step [8100/15231], Loss: 0.2568\n",
            "Epoch [3/5], Step [8120/15231], Loss: 0.2516\n",
            "Epoch [3/5], Step [8140/15231], Loss: 0.2705\n",
            "Epoch [3/5], Step [8160/15231], Loss: 0.2426\n",
            "Epoch [3/5], Step [8180/15231], Loss: 0.2572\n",
            "Epoch [3/5], Step [8200/15231], Loss: 0.2009\n",
            "Epoch [3/5], Step [8220/15231], Loss: 0.2144\n",
            "Epoch [3/5], Step [8240/15231], Loss: 0.2690\n",
            "Epoch [3/5], Step [8260/15231], Loss: 0.2330\n",
            "Epoch [3/5], Step [8280/15231], Loss: 0.2179\n",
            "Epoch [3/5], Step [8300/15231], Loss: 0.2745\n",
            "Epoch [3/5], Step [8320/15231], Loss: 0.2237\n",
            "Epoch [3/5], Step [8340/15231], Loss: 0.2390\n",
            "Epoch [3/5], Step [8360/15231], Loss: 0.2124\n",
            "Epoch [3/5], Step [8380/15231], Loss: 0.2173\n",
            "Epoch [3/5], Step [8400/15231], Loss: 0.2586\n",
            "Epoch [3/5], Step [8420/15231], Loss: 0.2551\n",
            "Epoch [3/5], Step [8440/15231], Loss: 0.2455\n",
            "Epoch [3/5], Step [8460/15231], Loss: 0.2281\n",
            "Epoch [3/5], Step [8480/15231], Loss: 0.2132\n",
            "Epoch [3/5], Step [8500/15231], Loss: 0.2502\n",
            "Epoch [3/5], Step [8520/15231], Loss: 0.2181\n",
            "Epoch [3/5], Step [8540/15231], Loss: 0.2622\n",
            "Epoch [3/5], Step [8560/15231], Loss: 0.2257\n",
            "Epoch [3/5], Step [8580/15231], Loss: 0.2386\n",
            "Epoch [3/5], Step [8600/15231], Loss: 0.2482\n",
            "Epoch [3/5], Step [8620/15231], Loss: 0.2056\n",
            "Epoch [3/5], Step [8640/15231], Loss: 0.2509\n",
            "Epoch [3/5], Step [8660/15231], Loss: 0.2244\n",
            "Epoch [3/5], Step [8680/15231], Loss: 0.2255\n",
            "Epoch [3/5], Step [8700/15231], Loss: 0.2094\n",
            "Epoch [3/5], Step [8720/15231], Loss: 0.2511\n",
            "Epoch [3/5], Step [8740/15231], Loss: 0.2424\n",
            "Epoch [3/5], Step [8760/15231], Loss: 0.2741\n",
            "Epoch [3/5], Step [8780/15231], Loss: 0.2509\n",
            "Epoch [3/5], Step [8800/15231], Loss: 0.2158\n",
            "Epoch [3/5], Step [8820/15231], Loss: 0.2264\n",
            "Epoch [3/5], Step [8840/15231], Loss: 0.2444\n",
            "Epoch [3/5], Step [8860/15231], Loss: 0.2294\n",
            "Epoch [3/5], Step [8880/15231], Loss: 0.2343\n",
            "Epoch [3/5], Step [8900/15231], Loss: 0.2392\n",
            "Epoch [3/5], Step [8920/15231], Loss: 0.2546\n",
            "Epoch [3/5], Step [8940/15231], Loss: 0.2504\n",
            "Epoch [3/5], Step [8960/15231], Loss: 0.2421\n",
            "Epoch [3/5], Step [8980/15231], Loss: 0.2508\n",
            "Epoch [3/5], Step [9000/15231], Loss: 0.2051\n",
            "Epoch [3/5], Step [9020/15231], Loss: 0.2464\n",
            "Epoch [3/5], Step [9040/15231], Loss: 0.2513\n",
            "Epoch [3/5], Step [9060/15231], Loss: 0.2592\n",
            "Epoch [3/5], Step [9080/15231], Loss: 0.2538\n",
            "Epoch [3/5], Step [9100/15231], Loss: 0.2493\n",
            "Epoch [3/5], Step [9120/15231], Loss: 0.2287\n",
            "Epoch [3/5], Step [9140/15231], Loss: 0.2301\n",
            "Epoch [3/5], Step [9160/15231], Loss: 0.2230\n",
            "Epoch [3/5], Step [9180/15231], Loss: 0.2294\n",
            "Epoch [3/5], Step [9200/15231], Loss: 0.2596\n",
            "Epoch [3/5], Step [9220/15231], Loss: 0.2311\n",
            "Epoch [3/5], Step [9240/15231], Loss: 0.2471\n",
            "Epoch [3/5], Step [9260/15231], Loss: 0.2280\n",
            "Epoch [3/5], Step [9280/15231], Loss: 0.2553\n",
            "Epoch [3/5], Step [9300/15231], Loss: 0.2645\n",
            "Epoch [3/5], Step [9320/15231], Loss: 0.2680\n",
            "Epoch [3/5], Step [9340/15231], Loss: 0.2310\n",
            "Epoch [3/5], Step [9360/15231], Loss: 0.2609\n",
            "Epoch [3/5], Step [9380/15231], Loss: 0.2591\n",
            "Epoch [3/5], Step [9400/15231], Loss: 0.2581\n",
            "Epoch [3/5], Step [9420/15231], Loss: 0.2512\n",
            "Epoch [3/5], Step [9440/15231], Loss: 0.2276\n",
            "Epoch [3/5], Step [9460/15231], Loss: 0.2201\n",
            "Epoch [3/5], Step [9480/15231], Loss: 0.2716\n",
            "Epoch [3/5], Step [9500/15231], Loss: 0.2536\n",
            "Epoch [3/5], Step [9520/15231], Loss: 0.2582\n",
            "Epoch [3/5], Step [9540/15231], Loss: 0.2297\n",
            "Epoch [3/5], Step [9560/15231], Loss: 0.2731\n",
            "Epoch [3/5], Step [9580/15231], Loss: 0.2255\n",
            "Epoch [3/5], Step [9600/15231], Loss: 0.2063\n",
            "Epoch [3/5], Step [9620/15231], Loss: 0.2454\n",
            "Epoch [3/5], Step [9640/15231], Loss: 0.2396\n",
            "Epoch [3/5], Step [9660/15231], Loss: 0.2323\n",
            "Epoch [3/5], Step [9680/15231], Loss: 0.2514\n",
            "Epoch [3/5], Step [9700/15231], Loss: 0.2285\n",
            "Epoch [3/5], Step [9720/15231], Loss: 0.2399\n",
            "Epoch [3/5], Step [9740/15231], Loss: 0.2490\n",
            "Epoch [3/5], Step [9760/15231], Loss: 0.2062\n",
            "Epoch [3/5], Step [9780/15231], Loss: 0.2695\n",
            "Epoch [3/5], Step [9800/15231], Loss: 0.2680\n",
            "Epoch [3/5], Step [9820/15231], Loss: 0.2303\n",
            "Epoch [3/5], Step [9840/15231], Loss: 0.1889\n",
            "Epoch [3/5], Step [9860/15231], Loss: 0.2099\n",
            "Epoch [3/5], Step [9880/15231], Loss: 0.2025\n",
            "Epoch [3/5], Step [9900/15231], Loss: 0.1990\n",
            "Epoch [3/5], Step [9920/15231], Loss: 0.2087\n",
            "Epoch [3/5], Step [9940/15231], Loss: 0.2159\n",
            "Epoch [3/5], Step [9960/15231], Loss: 0.2016\n",
            "Epoch [3/5], Step [9980/15231], Loss: 0.2422\n",
            "Epoch [3/5], Step [10000/15231], Loss: 0.2304\n",
            "Epoch [3/5], Step [10020/15231], Loss: 0.2277\n",
            "Epoch [3/5], Step [10040/15231], Loss: 0.2542\n",
            "Epoch [3/5], Step [10060/15231], Loss: 0.2180\n",
            "Epoch [3/5], Step [10080/15231], Loss: 0.2311\n",
            "Epoch [3/5], Step [10100/15231], Loss: 0.2029\n",
            "Epoch [3/5], Step [10120/15231], Loss: 0.2665\n",
            "Epoch [3/5], Step [10140/15231], Loss: 0.2423\n",
            "Epoch [3/5], Step [10160/15231], Loss: 0.2398\n",
            "Epoch [3/5], Step [10180/15231], Loss: 0.2552\n",
            "Epoch [3/5], Step [10200/15231], Loss: 0.2509\n",
            "Epoch [3/5], Step [10220/15231], Loss: 0.2287\n",
            "Epoch [3/5], Step [10240/15231], Loss: 0.2125\n",
            "Epoch [3/5], Step [10260/15231], Loss: 0.2473\n",
            "Epoch [3/5], Step [10280/15231], Loss: 0.2410\n",
            "Epoch [3/5], Step [10300/15231], Loss: 0.2465\n",
            "Epoch [3/5], Step [10320/15231], Loss: 0.2687\n",
            "Epoch [3/5], Step [10340/15231], Loss: 0.2320\n",
            "Epoch [3/5], Step [10360/15231], Loss: 0.2264\n",
            "Epoch [3/5], Step [10380/15231], Loss: 0.2129\n",
            "Epoch [3/5], Step [10400/15231], Loss: 0.2100\n",
            "Epoch [3/5], Step [10420/15231], Loss: 0.2706\n",
            "Epoch [3/5], Step [10440/15231], Loss: 0.2454\n",
            "Epoch [3/5], Step [10460/15231], Loss: 0.2451\n",
            "Epoch [3/5], Step [10480/15231], Loss: 0.1938\n",
            "Epoch [3/5], Step [10500/15231], Loss: 0.2180\n",
            "Epoch [3/5], Step [10520/15231], Loss: 0.2355\n",
            "Epoch [3/5], Step [10540/15231], Loss: 0.2191\n",
            "Epoch [3/5], Step [10560/15231], Loss: 0.2239\n",
            "Epoch [3/5], Step [10580/15231], Loss: 0.2315\n",
            "Epoch [3/5], Step [10600/15231], Loss: 0.2479\n",
            "Epoch [3/5], Step [10620/15231], Loss: 0.2683\n",
            "Epoch [3/5], Step [10640/15231], Loss: 0.2342\n",
            "Epoch [3/5], Step [10660/15231], Loss: 0.2362\n",
            "Epoch [3/5], Step [10680/15231], Loss: 0.2540\n",
            "Epoch [3/5], Step [10700/15231], Loss: 0.2386\n",
            "Epoch [3/5], Step [10720/15231], Loss: 0.2610\n",
            "Epoch [3/5], Step [10740/15231], Loss: 0.2535\n",
            "Epoch [3/5], Step [10760/15231], Loss: 0.2015\n",
            "Epoch [3/5], Step [10780/15231], Loss: 0.2446\n",
            "Epoch [3/5], Step [10800/15231], Loss: 0.2421\n",
            "Epoch [3/5], Step [10820/15231], Loss: 0.2253\n",
            "Epoch [3/5], Step [10840/15231], Loss: 0.2739\n",
            "Epoch [3/5], Step [10860/15231], Loss: 0.2145\n",
            "Epoch [3/5], Step [10880/15231], Loss: 0.2478\n",
            "Epoch [3/5], Step [10900/15231], Loss: 0.2248\n",
            "Epoch [3/5], Step [10920/15231], Loss: 0.2134\n",
            "Epoch [3/5], Step [10940/15231], Loss: 0.2445\n",
            "Epoch [3/5], Step [10960/15231], Loss: 0.2338\n",
            "Epoch [3/5], Step [10980/15231], Loss: 0.2553\n",
            "Epoch [3/5], Step [11000/15231], Loss: 0.2509\n",
            "Epoch [3/5], Step [11020/15231], Loss: 0.2392\n",
            "Epoch [3/5], Step [11040/15231], Loss: 0.2038\n",
            "Epoch [3/5], Step [11060/15231], Loss: 0.2556\n",
            "Epoch [3/5], Step [11080/15231], Loss: 0.2453\n",
            "Epoch [3/5], Step [11100/15231], Loss: 0.2220\n",
            "Epoch [3/5], Step [11120/15231], Loss: 0.2376\n",
            "Epoch [3/5], Step [11140/15231], Loss: 0.2287\n",
            "Epoch [3/5], Step [11160/15231], Loss: 0.2515\n",
            "Epoch [3/5], Step [11180/15231], Loss: 0.2387\n",
            "Epoch [3/5], Step [11200/15231], Loss: 0.2179\n",
            "Epoch [3/5], Step [11220/15231], Loss: 0.2265\n",
            "Epoch [3/5], Step [11240/15231], Loss: 0.2289\n",
            "Epoch [3/5], Step [11260/15231], Loss: 0.2479\n",
            "Epoch [3/5], Step [11280/15231], Loss: 0.2117\n",
            "Epoch [3/5], Step [11300/15231], Loss: 0.2421\n",
            "Epoch [3/5], Step [11320/15231], Loss: 0.2220\n",
            "Epoch [3/5], Step [11340/15231], Loss: 0.2510\n",
            "Epoch [3/5], Step [11360/15231], Loss: 0.2305\n",
            "Epoch [3/5], Step [11380/15231], Loss: 0.2468\n",
            "Epoch [3/5], Step [11400/15231], Loss: 0.2347\n",
            "Epoch [3/5], Step [11420/15231], Loss: 0.2452\n",
            "Epoch [3/5], Step [11440/15231], Loss: 0.2325\n",
            "Epoch [3/5], Step [11460/15231], Loss: 0.2419\n",
            "Epoch [3/5], Step [11480/15231], Loss: 0.2345\n",
            "Epoch [3/5], Step [11500/15231], Loss: 0.2107\n",
            "Epoch [3/5], Step [11520/15231], Loss: 0.2228\n",
            "Epoch [3/5], Step [11540/15231], Loss: 0.2125\n",
            "Epoch [3/5], Step [11560/15231], Loss: 0.2299\n",
            "Epoch [3/5], Step [11580/15231], Loss: 0.2109\n",
            "Epoch [3/5], Step [11600/15231], Loss: 0.2372\n",
            "Epoch [3/5], Step [11620/15231], Loss: 0.2371\n",
            "Epoch [3/5], Step [11640/15231], Loss: 0.2396\n",
            "Epoch [3/5], Step [11660/15231], Loss: 0.2328\n",
            "Epoch [3/5], Step [11680/15231], Loss: 0.2453\n",
            "Epoch [3/5], Step [11700/15231], Loss: 0.2681\n",
            "Epoch [3/5], Step [11720/15231], Loss: 0.2405\n",
            "Epoch [3/5], Step [11740/15231], Loss: 0.2609\n",
            "Epoch [3/5], Step [11760/15231], Loss: 0.2163\n",
            "Epoch [3/5], Step [11780/15231], Loss: 0.2371\n",
            "Epoch [3/5], Step [11800/15231], Loss: 0.2243\n",
            "Epoch [3/5], Step [11820/15231], Loss: 0.2294\n",
            "Epoch [3/5], Step [11840/15231], Loss: 0.2494\n",
            "Epoch [3/5], Step [11860/15231], Loss: 0.2879\n",
            "Epoch [3/5], Step [11880/15231], Loss: 0.2452\n",
            "Epoch [3/5], Step [11900/15231], Loss: 0.2240\n",
            "Epoch [3/5], Step [11920/15231], Loss: 0.2533\n",
            "Epoch [3/5], Step [11940/15231], Loss: 0.2204\n",
            "Epoch [3/5], Step [11960/15231], Loss: 0.2165\n",
            "Epoch [3/5], Step [11980/15231], Loss: 0.2418\n",
            "Epoch [3/5], Step [12000/15231], Loss: 0.2385\n",
            "Epoch [3/5], Step [12020/15231], Loss: 0.2314\n",
            "Epoch [3/5], Step [12040/15231], Loss: 0.2407\n",
            "Epoch [3/5], Step [12060/15231], Loss: 0.2479\n",
            "Epoch [3/5], Step [12080/15231], Loss: 0.2243\n",
            "Epoch [3/5], Step [12100/15231], Loss: 0.2608\n",
            "Epoch [3/5], Step [12120/15231], Loss: 0.2305\n",
            "Epoch [3/5], Step [12140/15231], Loss: 0.2001\n",
            "Epoch [3/5], Step [12160/15231], Loss: 0.2436\n",
            "Epoch [3/5], Step [12180/15231], Loss: 0.2191\n",
            "Epoch [3/5], Step [12200/15231], Loss: 0.2386\n",
            "Epoch [3/5], Step [12220/15231], Loss: 0.2380\n",
            "Epoch [3/5], Step [12240/15231], Loss: 0.2130\n",
            "Epoch [3/5], Step [12260/15231], Loss: 0.2562\n",
            "Epoch [3/5], Step [12280/15231], Loss: 0.2619\n",
            "Epoch [3/5], Step [12300/15231], Loss: 0.2535\n",
            "Epoch [3/5], Step [12320/15231], Loss: 0.1979\n",
            "Epoch [3/5], Step [12340/15231], Loss: 0.2619\n",
            "Epoch [3/5], Step [12360/15231], Loss: 0.2484\n",
            "Epoch [3/5], Step [12380/15231], Loss: 0.2261\n",
            "Epoch [3/5], Step [12400/15231], Loss: 0.2206\n",
            "Epoch [3/5], Step [12420/15231], Loss: 0.2342\n",
            "Epoch [3/5], Step [12440/15231], Loss: 0.2303\n",
            "Epoch [3/5], Step [12460/15231], Loss: 0.2263\n",
            "Epoch [3/5], Step [12480/15231], Loss: 0.2398\n",
            "Epoch [3/5], Step [12500/15231], Loss: 0.2067\n",
            "Epoch [3/5], Step [12520/15231], Loss: 0.2591\n",
            "Epoch [3/5], Step [12540/15231], Loss: 0.2542\n",
            "Epoch [3/5], Step [12560/15231], Loss: 0.2314\n",
            "Epoch [3/5], Step [12580/15231], Loss: 0.2180\n",
            "Epoch [3/5], Step [12600/15231], Loss: 0.2362\n",
            "Epoch [3/5], Step [12620/15231], Loss: 0.2380\n",
            "Epoch [3/5], Step [12640/15231], Loss: 0.2155\n",
            "Epoch [3/5], Step [12660/15231], Loss: 0.2458\n",
            "Epoch [3/5], Step [12680/15231], Loss: 0.1823\n",
            "Epoch [3/5], Step [12700/15231], Loss: 0.2366\n",
            "Epoch [3/5], Step [12720/15231], Loss: 0.2014\n",
            "Epoch [3/5], Step [12740/15231], Loss: 0.2466\n",
            "Epoch [3/5], Step [12760/15231], Loss: 0.2763\n",
            "Epoch [3/5], Step [12780/15231], Loss: 0.1992\n",
            "Epoch [3/5], Step [12800/15231], Loss: 0.2399\n",
            "Epoch [3/5], Step [12820/15231], Loss: 0.2351\n",
            "Epoch [3/5], Step [12840/15231], Loss: 0.2038\n",
            "Epoch [3/5], Step [12860/15231], Loss: 0.2541\n",
            "Epoch [3/5], Step [12880/15231], Loss: 0.2410\n",
            "Epoch [3/5], Step [12900/15231], Loss: 0.2182\n",
            "Epoch [3/5], Step [12920/15231], Loss: 0.2769\n",
            "Epoch [3/5], Step [12940/15231], Loss: 0.2320\n",
            "Epoch [3/5], Step [12960/15231], Loss: 0.2299\n",
            "Epoch [3/5], Step [12980/15231], Loss: 0.2171\n",
            "Epoch [3/5], Step [13000/15231], Loss: 0.2615\n",
            "Epoch [3/5], Step [13020/15231], Loss: 0.2085\n",
            "Epoch [3/5], Step [13040/15231], Loss: 0.2082\n",
            "Epoch [3/5], Step [13060/15231], Loss: 0.2823\n",
            "Epoch [3/5], Step [13080/15231], Loss: 0.2347\n",
            "Epoch [3/5], Step [13100/15231], Loss: 0.2209\n",
            "Epoch [3/5], Step [13120/15231], Loss: 0.1998\n",
            "Epoch [3/5], Step [13140/15231], Loss: 0.2161\n",
            "Epoch [3/5], Step [13160/15231], Loss: 0.2065\n",
            "Epoch [3/5], Step [13180/15231], Loss: 0.2436\n",
            "Epoch [3/5], Step [13200/15231], Loss: 0.2251\n",
            "Epoch [3/5], Step [13220/15231], Loss: 0.2469\n",
            "Epoch [3/5], Step [13240/15231], Loss: 0.2220\n",
            "Epoch [3/5], Step [13260/15231], Loss: 0.2183\n",
            "Epoch [3/5], Step [13280/15231], Loss: 0.2622\n",
            "Epoch [3/5], Step [13300/15231], Loss: 0.2233\n",
            "Epoch [3/5], Step [13320/15231], Loss: 0.2439\n",
            "Epoch [3/5], Step [13340/15231], Loss: 0.1971\n",
            "Epoch [3/5], Step [13360/15231], Loss: 0.2148\n",
            "Epoch [3/5], Step [13380/15231], Loss: 0.2304\n",
            "Epoch [3/5], Step [13400/15231], Loss: 0.2458\n",
            "Epoch [3/5], Step [13420/15231], Loss: 0.2504\n",
            "Epoch [3/5], Step [13440/15231], Loss: 0.2362\n",
            "Epoch [3/5], Step [13460/15231], Loss: 0.2064\n",
            "Epoch [3/5], Step [13480/15231], Loss: 0.1997\n",
            "Epoch [3/5], Step [13500/15231], Loss: 0.2374\n",
            "Epoch [3/5], Step [13520/15231], Loss: 0.2530\n",
            "Epoch [3/5], Step [13540/15231], Loss: 0.2262\n",
            "Epoch [3/5], Step [13560/15231], Loss: 0.2485\n",
            "Epoch [3/5], Step [13580/15231], Loss: 0.2571\n",
            "Epoch [3/5], Step [13600/15231], Loss: 0.2498\n",
            "Epoch [3/5], Step [13620/15231], Loss: 0.2496\n",
            "Epoch [3/5], Step [13640/15231], Loss: 0.2258\n",
            "Epoch [3/5], Step [13660/15231], Loss: 0.1969\n",
            "Epoch [3/5], Step [13680/15231], Loss: 0.2330\n",
            "Epoch [3/5], Step [13700/15231], Loss: 0.2065\n",
            "Epoch [3/5], Step [13720/15231], Loss: 0.2700\n",
            "Epoch [3/5], Step [13740/15231], Loss: 0.2414\n",
            "Epoch [3/5], Step [13760/15231], Loss: 0.2084\n",
            "Epoch [3/5], Step [13780/15231], Loss: 0.2378\n",
            "Epoch [3/5], Step [13800/15231], Loss: 0.2817\n",
            "Epoch [3/5], Step [13820/15231], Loss: 0.2420\n",
            "Epoch [3/5], Step [13840/15231], Loss: 0.2102\n",
            "Epoch [3/5], Step [13860/15231], Loss: 0.2383\n",
            "Epoch [3/5], Step [13880/15231], Loss: 0.2251\n",
            "Epoch [3/5], Step [13900/15231], Loss: 0.2468\n",
            "Epoch [3/5], Step [13920/15231], Loss: 0.2238\n",
            "Epoch [3/5], Step [13940/15231], Loss: 0.2779\n",
            "Epoch [3/5], Step [13960/15231], Loss: 0.2813\n",
            "Epoch [3/5], Step [13980/15231], Loss: 0.2320\n",
            "Epoch [3/5], Step [14000/15231], Loss: 0.2489\n",
            "Epoch [3/5], Step [14020/15231], Loss: 0.2728\n",
            "Epoch [3/5], Step [14040/15231], Loss: 0.2057\n",
            "Epoch [3/5], Step [14060/15231], Loss: 0.2172\n",
            "Epoch [3/5], Step [14080/15231], Loss: 0.2633\n",
            "Epoch [3/5], Step [14100/15231], Loss: 0.2126\n",
            "Epoch [3/5], Step [14120/15231], Loss: 0.2108\n",
            "Epoch [3/5], Step [14140/15231], Loss: 0.2089\n",
            "Epoch [3/5], Step [14160/15231], Loss: 0.2221\n",
            "Epoch [3/5], Step [14180/15231], Loss: 0.2413\n",
            "Epoch [3/5], Step [14200/15231], Loss: 0.2390\n",
            "Epoch [3/5], Step [14220/15231], Loss: 0.2340\n",
            "Epoch [3/5], Step [14240/15231], Loss: 0.2308\n",
            "Epoch [3/5], Step [14260/15231], Loss: 0.1987\n",
            "Epoch [3/5], Step [14280/15231], Loss: 0.2521\n",
            "Epoch [3/5], Step [14300/15231], Loss: 0.2499\n",
            "Epoch [3/5], Step [14320/15231], Loss: 0.2359\n",
            "Epoch [3/5], Step [14340/15231], Loss: 0.2329\n",
            "Epoch [3/5], Step [14360/15231], Loss: 0.2441\n",
            "Epoch [3/5], Step [14380/15231], Loss: 0.2263\n",
            "Epoch [3/5], Step [14400/15231], Loss: 0.2133\n",
            "Epoch [3/5], Step [14420/15231], Loss: 0.2228\n",
            "Epoch [3/5], Step [14440/15231], Loss: 0.2018\n",
            "Epoch [3/5], Step [14460/15231], Loss: 0.2328\n",
            "Epoch [3/5], Step [14480/15231], Loss: 0.2425\n",
            "Epoch [3/5], Step [14500/15231], Loss: 0.2418\n",
            "Epoch [3/5], Step [14520/15231], Loss: 0.2131\n",
            "Epoch [3/5], Step [14540/15231], Loss: 0.2283\n",
            "Epoch [3/5], Step [14560/15231], Loss: 0.2094\n",
            "Epoch [3/5], Step [14580/15231], Loss: 0.2602\n",
            "Epoch [3/5], Step [14600/15231], Loss: 0.2227\n",
            "Epoch [3/5], Step [14620/15231], Loss: 0.2273\n",
            "Epoch [3/5], Step [14640/15231], Loss: 0.2137\n",
            "Epoch [3/5], Step [14660/15231], Loss: 0.2273\n",
            "Epoch [3/5], Step [14680/15231], Loss: 0.2563\n",
            "Epoch [3/5], Step [14700/15231], Loss: 0.2386\n",
            "Epoch [3/5], Step [14720/15231], Loss: 0.2318\n",
            "Epoch [3/5], Step [14740/15231], Loss: 0.2380\n",
            "Epoch [3/5], Step [14760/15231], Loss: 0.2571\n",
            "Epoch [3/5], Step [14780/15231], Loss: 0.2334\n",
            "Epoch [3/5], Step [14800/15231], Loss: 0.2292\n",
            "Epoch [3/5], Step [14820/15231], Loss: 0.2299\n",
            "Epoch [3/5], Step [14840/15231], Loss: 0.2198\n",
            "Epoch [3/5], Step [14860/15231], Loss: 0.2372\n",
            "Epoch [3/5], Step [14880/15231], Loss: 0.2437\n",
            "Epoch [3/5], Step [14900/15231], Loss: 0.1982\n",
            "Epoch [3/5], Step [14920/15231], Loss: 0.2129\n",
            "Epoch [3/5], Step [14940/15231], Loss: 0.2771\n",
            "Epoch [3/5], Step [14960/15231], Loss: 0.2296\n",
            "Epoch [3/5], Step [14980/15231], Loss: 0.2483\n",
            "Epoch [3/5], Step [15000/15231], Loss: 0.2191\n",
            "Epoch [3/5], Step [15020/15231], Loss: 0.2299\n",
            "Epoch [3/5], Step [15040/15231], Loss: 0.2397\n",
            "Epoch [3/5], Step [15060/15231], Loss: 0.2358\n",
            "Epoch [3/5], Step [15080/15231], Loss: 0.2287\n",
            "Epoch [3/5], Step [15100/15231], Loss: 0.2150\n",
            "Epoch [3/5], Step [15120/15231], Loss: 0.2155\n",
            "Epoch [3/5], Step [15140/15231], Loss: 0.2306\n",
            "Epoch [3/5], Step [15160/15231], Loss: 0.2585\n",
            "Epoch [3/5], Step [15180/15231], Loss: 0.2489\n",
            "Epoch [3/5], Step [15200/15231], Loss: 0.2186\n",
            "Epoch [3/5], Step [15220/15231], Loss: 0.2363\n",
            "Epoch [3/5] | Train Loss: 0.2396 | Train Acc: 91.56% | Test Loss: 0.7164 | Test Acc: 76.40%\n",
            "Epoch [4/5], Step [20/15231], Loss: 0.2353\n",
            "Epoch [4/5], Step [40/15231], Loss: 0.2593\n",
            "Epoch [4/5], Step [60/15231], Loss: 0.2300\n",
            "Epoch [4/5], Step [80/15231], Loss: 0.2406\n",
            "Epoch [4/5], Step [100/15231], Loss: 0.2021\n",
            "Epoch [4/5], Step [120/15231], Loss: 0.2255\n",
            "Epoch [4/5], Step [140/15231], Loss: 0.2140\n",
            "Epoch [4/5], Step [160/15231], Loss: 0.2040\n",
            "Epoch [4/5], Step [180/15231], Loss: 0.2203\n",
            "Epoch [4/5], Step [200/15231], Loss: 0.2167\n",
            "Epoch [4/5], Step [220/15231], Loss: 0.2343\n",
            "Epoch [4/5], Step [240/15231], Loss: 0.2193\n",
            "Epoch [4/5], Step [260/15231], Loss: 0.2166\n",
            "Epoch [4/5], Step [280/15231], Loss: 0.2404\n",
            "Epoch [4/5], Step [300/15231], Loss: 0.2409\n",
            "Epoch [4/5], Step [320/15231], Loss: 0.2487\n",
            "Epoch [4/5], Step [340/15231], Loss: 0.2433\n",
            "Epoch [4/5], Step [360/15231], Loss: 0.2130\n",
            "Epoch [4/5], Step [380/15231], Loss: 0.2075\n",
            "Epoch [4/5], Step [400/15231], Loss: 0.2355\n",
            "Epoch [4/5], Step [420/15231], Loss: 0.2155\n",
            "Epoch [4/5], Step [440/15231], Loss: 0.2256\n",
            "Epoch [4/5], Step [460/15231], Loss: 0.2361\n",
            "Epoch [4/5], Step [480/15231], Loss: 0.2382\n",
            "Epoch [4/5], Step [500/15231], Loss: 0.2510\n",
            "Epoch [4/5], Step [520/15231], Loss: 0.2376\n",
            "Epoch [4/5], Step [540/15231], Loss: 0.2049\n",
            "Epoch [4/5], Step [560/15231], Loss: 0.2076\n",
            "Epoch [4/5], Step [580/15231], Loss: 0.1986\n",
            "Epoch [4/5], Step [600/15231], Loss: 0.2009\n",
            "Epoch [4/5], Step [620/15231], Loss: 0.2147\n",
            "Epoch [4/5], Step [640/15231], Loss: 0.2090\n",
            "Epoch [4/5], Step [660/15231], Loss: 0.2394\n",
            "Epoch [4/5], Step [680/15231], Loss: 0.2297\n",
            "Epoch [4/5], Step [700/15231], Loss: 0.2119\n",
            "Epoch [4/5], Step [720/15231], Loss: 0.2235\n",
            "Epoch [4/5], Step [740/15231], Loss: 0.2026\n",
            "Epoch [4/5], Step [760/15231], Loss: 0.2069\n",
            "Epoch [4/5], Step [780/15231], Loss: 0.2128\n",
            "Epoch [4/5], Step [800/15231], Loss: 0.2406\n",
            "Epoch [4/5], Step [820/15231], Loss: 0.2525\n",
            "Epoch [4/5], Step [840/15231], Loss: 0.2150\n",
            "Epoch [4/5], Step [860/15231], Loss: 0.2246\n",
            "Epoch [4/5], Step [880/15231], Loss: 0.2303\n",
            "Epoch [4/5], Step [900/15231], Loss: 0.1986\n",
            "Epoch [4/5], Step [920/15231], Loss: 0.2383\n",
            "Epoch [4/5], Step [940/15231], Loss: 0.2117\n",
            "Epoch [4/5], Step [960/15231], Loss: 0.2251\n",
            "Epoch [4/5], Step [980/15231], Loss: 0.2444\n",
            "Epoch [4/5], Step [1000/15231], Loss: 0.2422\n",
            "Epoch [4/5], Step [1020/15231], Loss: 0.2322\n",
            "Epoch [4/5], Step [1040/15231], Loss: 0.2651\n",
            "Epoch [4/5], Step [1060/15231], Loss: 0.2534\n",
            "Epoch [4/5], Step [1080/15231], Loss: 0.2446\n",
            "Epoch [4/5], Step [1100/15231], Loss: 0.2429\n",
            "Epoch [4/5], Step [1120/15231], Loss: 0.2040\n",
            "Epoch [4/5], Step [1140/15231], Loss: 0.2217\n",
            "Epoch [4/5], Step [1160/15231], Loss: 0.2861\n",
            "Epoch [4/5], Step [1180/15231], Loss: 0.2281\n",
            "Epoch [4/5], Step [1200/15231], Loss: 0.2264\n",
            "Epoch [4/5], Step [1220/15231], Loss: 0.2018\n",
            "Epoch [4/5], Step [1240/15231], Loss: 0.2083\n",
            "Epoch [4/5], Step [1260/15231], Loss: 0.2080\n",
            "Epoch [4/5], Step [1280/15231], Loss: 0.2202\n",
            "Epoch [4/5], Step [1300/15231], Loss: 0.2128\n",
            "Epoch [4/5], Step [1320/15231], Loss: 0.1830\n",
            "Epoch [4/5], Step [1340/15231], Loss: 0.1980\n",
            "Epoch [4/5], Step [1360/15231], Loss: 0.2125\n",
            "Epoch [4/5], Step [1380/15231], Loss: 0.2213\n",
            "Epoch [4/5], Step [1400/15231], Loss: 0.2453\n",
            "Epoch [4/5], Step [1420/15231], Loss: 0.2240\n",
            "Epoch [4/5], Step [1440/15231], Loss: 0.2226\n",
            "Epoch [4/5], Step [1460/15231], Loss: 0.2269\n",
            "Epoch [4/5], Step [1480/15231], Loss: 0.2404\n",
            "Epoch [4/5], Step [1500/15231], Loss: 0.2141\n",
            "Epoch [4/5], Step [1520/15231], Loss: 0.2096\n",
            "Epoch [4/5], Step [1540/15231], Loss: 0.1831\n",
            "Epoch [4/5], Step [1560/15231], Loss: 0.2049\n",
            "Epoch [4/5], Step [1580/15231], Loss: 0.1873\n",
            "Epoch [4/5], Step [1600/15231], Loss: 0.2488\n",
            "Epoch [4/5], Step [1620/15231], Loss: 0.2348\n",
            "Epoch [4/5], Step [1640/15231], Loss: 0.2399\n",
            "Epoch [4/5], Step [1660/15231], Loss: 0.2430\n",
            "Epoch [4/5], Step [1680/15231], Loss: 0.2365\n",
            "Epoch [4/5], Step [1700/15231], Loss: 0.2235\n",
            "Epoch [4/5], Step [1720/15231], Loss: 0.2190\n",
            "Epoch [4/5], Step [1740/15231], Loss: 0.2236\n",
            "Epoch [4/5], Step [1760/15231], Loss: 0.2083\n",
            "Epoch [4/5], Step [1780/15231], Loss: 0.2320\n",
            "Epoch [4/5], Step [1800/15231], Loss: 0.2066\n",
            "Epoch [4/5], Step [1820/15231], Loss: 0.2254\n",
            "Epoch [4/5], Step [1840/15231], Loss: 0.2211\n",
            "Epoch [4/5], Step [1860/15231], Loss: 0.2336\n",
            "Epoch [4/5], Step [1880/15231], Loss: 0.2346\n",
            "Epoch [4/5], Step [1900/15231], Loss: 0.2257\n",
            "Epoch [4/5], Step [1920/15231], Loss: 0.2256\n",
            "Epoch [4/5], Step [1940/15231], Loss: 0.2010\n",
            "Epoch [4/5], Step [1960/15231], Loss: 0.1987\n",
            "Epoch [4/5], Step [1980/15231], Loss: 0.2218\n",
            "Epoch [4/5], Step [2000/15231], Loss: 0.2183\n",
            "Epoch [4/5], Step [2020/15231], Loss: 0.1749\n",
            "Epoch [4/5], Step [2040/15231], Loss: 0.2200\n",
            "Epoch [4/5], Step [2060/15231], Loss: 0.2321\n",
            "Epoch [4/5], Step [2080/15231], Loss: 0.2340\n",
            "Epoch [4/5], Step [2100/15231], Loss: 0.2127\n",
            "Epoch [4/5], Step [2120/15231], Loss: 0.2014\n",
            "Epoch [4/5], Step [2140/15231], Loss: 0.2520\n",
            "Epoch [4/5], Step [2160/15231], Loss: 0.2145\n",
            "Epoch [4/5], Step [2180/15231], Loss: 0.2498\n",
            "Epoch [4/5], Step [2200/15231], Loss: 0.2204\n",
            "Epoch [4/5], Step [2220/15231], Loss: 0.2155\n",
            "Epoch [4/5], Step [2240/15231], Loss: 0.2338\n",
            "Epoch [4/5], Step [2260/15231], Loss: 0.2081\n",
            "Epoch [4/5], Step [2280/15231], Loss: 0.2143\n",
            "Epoch [4/5], Step [2300/15231], Loss: 0.2466\n",
            "Epoch [4/5], Step [2320/15231], Loss: 0.2280\n",
            "Epoch [4/5], Step [2340/15231], Loss: 0.2086\n",
            "Epoch [4/5], Step [2360/15231], Loss: 0.2189\n",
            "Epoch [4/5], Step [2380/15231], Loss: 0.2191\n",
            "Epoch [4/5], Step [2400/15231], Loss: 0.2198\n",
            "Epoch [4/5], Step [2420/15231], Loss: 0.2421\n",
            "Epoch [4/5], Step [2440/15231], Loss: 0.2209\n",
            "Epoch [4/5], Step [2460/15231], Loss: 0.2312\n",
            "Epoch [4/5], Step [2480/15231], Loss: 0.2360\n",
            "Epoch [4/5], Step [2500/15231], Loss: 0.2633\n",
            "Epoch [4/5], Step [2520/15231], Loss: 0.2336\n",
            "Epoch [4/5], Step [2540/15231], Loss: 0.2098\n",
            "Epoch [4/5], Step [2560/15231], Loss: 0.2237\n",
            "Epoch [4/5], Step [2580/15231], Loss: 0.2181\n",
            "Epoch [4/5], Step [2600/15231], Loss: 0.2303\n",
            "Epoch [4/5], Step [2620/15231], Loss: 0.2272\n",
            "Epoch [4/5], Step [2640/15231], Loss: 0.2336\n",
            "Epoch [4/5], Step [2660/15231], Loss: 0.2376\n",
            "Epoch [4/5], Step [2680/15231], Loss: 0.2347\n",
            "Epoch [4/5], Step [2700/15231], Loss: 0.2454\n",
            "Epoch [4/5], Step [2720/15231], Loss: 0.2472\n",
            "Epoch [4/5], Step [2740/15231], Loss: 0.2048\n",
            "Epoch [4/5], Step [2760/15231], Loss: 0.2348\n",
            "Epoch [4/5], Step [2780/15231], Loss: 0.2592\n",
            "Epoch [4/5], Step [2800/15231], Loss: 0.2097\n",
            "Epoch [4/5], Step [2820/15231], Loss: 0.2408\n",
            "Epoch [4/5], Step [2840/15231], Loss: 0.2257\n",
            "Epoch [4/5], Step [2860/15231], Loss: 0.2160\n",
            "Epoch [4/5], Step [2880/15231], Loss: 0.2670\n",
            "Epoch [4/5], Step [2900/15231], Loss: 0.2276\n",
            "Epoch [4/5], Step [2920/15231], Loss: 0.2295\n",
            "Epoch [4/5], Step [2940/15231], Loss: 0.2266\n",
            "Epoch [4/5], Step [2960/15231], Loss: 0.2627\n",
            "Epoch [4/5], Step [2980/15231], Loss: 0.2565\n",
            "Epoch [4/5], Step [3000/15231], Loss: 0.1960\n",
            "Epoch [4/5], Step [3020/15231], Loss: 0.2011\n",
            "Epoch [4/5], Step [3040/15231], Loss: 0.2105\n",
            "Epoch [4/5], Step [3060/15231], Loss: 0.2257\n",
            "Epoch [4/5], Step [3080/15231], Loss: 0.2368\n",
            "Epoch [4/5], Step [3100/15231], Loss: 0.1890\n",
            "Epoch [4/5], Step [3120/15231], Loss: 0.2032\n",
            "Epoch [4/5], Step [3140/15231], Loss: 0.2279\n",
            "Epoch [4/5], Step [3160/15231], Loss: 0.2344\n",
            "Epoch [4/5], Step [3180/15231], Loss: 0.2244\n",
            "Epoch [4/5], Step [3200/15231], Loss: 0.1823\n",
            "Epoch [4/5], Step [3220/15231], Loss: 0.2487\n",
            "Epoch [4/5], Step [3240/15231], Loss: 0.2220\n",
            "Epoch [4/5], Step [3260/15231], Loss: 0.2278\n",
            "Epoch [4/5], Step [3280/15231], Loss: 0.2031\n",
            "Epoch [4/5], Step [3300/15231], Loss: 0.2414\n",
            "Epoch [4/5], Step [3320/15231], Loss: 0.2569\n",
            "Epoch [4/5], Step [3340/15231], Loss: 0.2361\n",
            "Epoch [4/5], Step [3360/15231], Loss: 0.2327\n",
            "Epoch [4/5], Step [3380/15231], Loss: 0.2365\n",
            "Epoch [4/5], Step [3400/15231], Loss: 0.2056\n",
            "Epoch [4/5], Step [3420/15231], Loss: 0.2108\n",
            "Epoch [4/5], Step [3440/15231], Loss: 0.2110\n",
            "Epoch [4/5], Step [3460/15231], Loss: 0.2343\n",
            "Epoch [4/5], Step [3480/15231], Loss: 0.2127\n",
            "Epoch [4/5], Step [3500/15231], Loss: 0.1972\n",
            "Epoch [4/5], Step [3520/15231], Loss: 0.2107\n",
            "Epoch [4/5], Step [3540/15231], Loss: 0.2717\n",
            "Epoch [4/5], Step [3560/15231], Loss: 0.2171\n",
            "Epoch [4/5], Step [3580/15231], Loss: 0.2303\n",
            "Epoch [4/5], Step [3600/15231], Loss: 0.2014\n",
            "Epoch [4/5], Step [3620/15231], Loss: 0.2269\n",
            "Epoch [4/5], Step [3640/15231], Loss: 0.2444\n",
            "Epoch [4/5], Step [3660/15231], Loss: 0.2371\n",
            "Epoch [4/5], Step [3680/15231], Loss: 0.2276\n",
            "Epoch [4/5], Step [3700/15231], Loss: 0.1954\n",
            "Epoch [4/5], Step [3720/15231], Loss: 0.2762\n",
            "Epoch [4/5], Step [3740/15231], Loss: 0.2249\n",
            "Epoch [4/5], Step [3760/15231], Loss: 0.2187\n",
            "Epoch [4/5], Step [3780/15231], Loss: 0.2073\n",
            "Epoch [4/5], Step [3800/15231], Loss: 0.2147\n",
            "Epoch [4/5], Step [3820/15231], Loss: 0.2304\n",
            "Epoch [4/5], Step [3840/15231], Loss: 0.2021\n",
            "Epoch [4/5], Step [3860/15231], Loss: 0.2478\n",
            "Epoch [4/5], Step [3880/15231], Loss: 0.2290\n",
            "Epoch [4/5], Step [3900/15231], Loss: 0.2449\n",
            "Epoch [4/5], Step [3920/15231], Loss: 0.2238\n",
            "Epoch [4/5], Step [3940/15231], Loss: 0.2294\n",
            "Epoch [4/5], Step [3960/15231], Loss: 0.2160\n",
            "Epoch [4/5], Step [3980/15231], Loss: 0.2156\n",
            "Epoch [4/5], Step [4000/15231], Loss: 0.2181\n",
            "Epoch [4/5], Step [4020/15231], Loss: 0.2541\n",
            "Epoch [4/5], Step [4040/15231], Loss: 0.1968\n",
            "Epoch [4/5], Step [4060/15231], Loss: 0.2169\n",
            "Epoch [4/5], Step [4080/15231], Loss: 0.2395\n",
            "Epoch [4/5], Step [4100/15231], Loss: 0.2188\n",
            "Epoch [4/5], Step [4120/15231], Loss: 0.2234\n",
            "Epoch [4/5], Step [4140/15231], Loss: 0.2541\n",
            "Epoch [4/5], Step [4160/15231], Loss: 0.2138\n",
            "Epoch [4/5], Step [4180/15231], Loss: 0.2202\n",
            "Epoch [4/5], Step [4200/15231], Loss: 0.2033\n",
            "Epoch [4/5], Step [4220/15231], Loss: 0.2176\n",
            "Epoch [4/5], Step [4240/15231], Loss: 0.2359\n",
            "Epoch [4/5], Step [4260/15231], Loss: 0.2037\n",
            "Epoch [4/5], Step [4280/15231], Loss: 0.2328\n",
            "Epoch [4/5], Step [4300/15231], Loss: 0.2184\n",
            "Epoch [4/5], Step [4320/15231], Loss: 0.2119\n",
            "Epoch [4/5], Step [4340/15231], Loss: 0.2073\n",
            "Epoch [4/5], Step [4360/15231], Loss: 0.2510\n",
            "Epoch [4/5], Step [4380/15231], Loss: 0.2048\n",
            "Epoch [4/5], Step [4400/15231], Loss: 0.2256\n",
            "Epoch [4/5], Step [4420/15231], Loss: 0.2088\n",
            "Epoch [4/5], Step [4440/15231], Loss: 0.2242\n",
            "Epoch [4/5], Step [4460/15231], Loss: 0.2158\n",
            "Epoch [4/5], Step [4480/15231], Loss: 0.1949\n",
            "Epoch [4/5], Step [4500/15231], Loss: 0.2128\n",
            "Epoch [4/5], Step [4520/15231], Loss: 0.2182\n",
            "Epoch [4/5], Step [4540/15231], Loss: 0.2169\n",
            "Epoch [4/5], Step [4560/15231], Loss: 0.2372\n",
            "Epoch [4/5], Step [4580/15231], Loss: 0.2178\n",
            "Epoch [4/5], Step [4600/15231], Loss: 0.2339\n",
            "Epoch [4/5], Step [4620/15231], Loss: 0.2258\n",
            "Epoch [4/5], Step [4640/15231], Loss: 0.2273\n",
            "Epoch [4/5], Step [4660/15231], Loss: 0.2182\n",
            "Epoch [4/5], Step [4680/15231], Loss: 0.2121\n",
            "Epoch [4/5], Step [4700/15231], Loss: 0.2248\n",
            "Epoch [4/5], Step [4720/15231], Loss: 0.2423\n",
            "Epoch [4/5], Step [4740/15231], Loss: 0.2360\n",
            "Epoch [4/5], Step [4760/15231], Loss: 0.1975\n",
            "Epoch [4/5], Step [4780/15231], Loss: 0.2533\n",
            "Epoch [4/5], Step [4800/15231], Loss: 0.2557\n",
            "Epoch [4/5], Step [4820/15231], Loss: 0.2396\n",
            "Epoch [4/5], Step [4840/15231], Loss: 0.2447\n",
            "Epoch [4/5], Step [4860/15231], Loss: 0.2436\n",
            "Epoch [4/5], Step [4880/15231], Loss: 0.2206\n",
            "Epoch [4/5], Step [4900/15231], Loss: 0.2264\n",
            "Epoch [4/5], Step [4920/15231], Loss: 0.2206\n",
            "Epoch [4/5], Step [4940/15231], Loss: 0.2147\n",
            "Epoch [4/5], Step [4960/15231], Loss: 0.2359\n",
            "Epoch [4/5], Step [4980/15231], Loss: 0.2110\n",
            "Epoch [4/5], Step [5000/15231], Loss: 0.2442\n",
            "Epoch [4/5], Step [5020/15231], Loss: 0.2363\n",
            "Epoch [4/5], Step [5040/15231], Loss: 0.2302\n",
            "Epoch [4/5], Step [5060/15231], Loss: 0.2041\n",
            "Epoch [4/5], Step [5080/15231], Loss: 0.2526\n",
            "Epoch [4/5], Step [5100/15231], Loss: 0.2328\n",
            "Epoch [4/5], Step [5120/15231], Loss: 0.2174\n",
            "Epoch [4/5], Step [5140/15231], Loss: 0.2128\n",
            "Epoch [4/5], Step [5160/15231], Loss: 0.2259\n",
            "Epoch [4/5], Step [5180/15231], Loss: 0.2203\n",
            "Epoch [4/5], Step [5200/15231], Loss: 0.2192\n",
            "Epoch [4/5], Step [5220/15231], Loss: 0.2289\n",
            "Epoch [4/5], Step [5240/15231], Loss: 0.2081\n",
            "Epoch [4/5], Step [5260/15231], Loss: 0.2374\n",
            "Epoch [4/5], Step [5280/15231], Loss: 0.2455\n",
            "Epoch [4/5], Step [5300/15231], Loss: 0.2319\n",
            "Epoch [4/5], Step [5320/15231], Loss: 0.2021\n",
            "Epoch [4/5], Step [5340/15231], Loss: 0.2134\n",
            "Epoch [4/5], Step [5360/15231], Loss: 0.2243\n",
            "Epoch [4/5], Step [5380/15231], Loss: 0.2045\n",
            "Epoch [4/5], Step [5400/15231], Loss: 0.2488\n",
            "Epoch [4/5], Step [5420/15231], Loss: 0.2291\n",
            "Epoch [4/5], Step [5440/15231], Loss: 0.2359\n",
            "Epoch [4/5], Step [5460/15231], Loss: 0.1945\n",
            "Epoch [4/5], Step [5480/15231], Loss: 0.2359\n",
            "Epoch [4/5], Step [5500/15231], Loss: 0.2110\n",
            "Epoch [4/5], Step [5520/15231], Loss: 0.2267\n",
            "Epoch [4/5], Step [5540/15231], Loss: 0.1911\n",
            "Epoch [4/5], Step [5560/15231], Loss: 0.2361\n",
            "Epoch [4/5], Step [5580/15231], Loss: 0.2341\n",
            "Epoch [4/5], Step [5600/15231], Loss: 0.2286\n",
            "Epoch [4/5], Step [5620/15231], Loss: 0.2387\n",
            "Epoch [4/5], Step [5640/15231], Loss: 0.2459\n",
            "Epoch [4/5], Step [5660/15231], Loss: 0.1800\n",
            "Epoch [4/5], Step [5680/15231], Loss: 0.1909\n",
            "Epoch [4/5], Step [5700/15231], Loss: 0.2063\n",
            "Epoch [4/5], Step [5720/15231], Loss: 0.2400\n",
            "Epoch [4/5], Step [5740/15231], Loss: 0.2399\n",
            "Epoch [4/5], Step [5760/15231], Loss: 0.2496\n",
            "Epoch [4/5], Step [5780/15231], Loss: 0.1922\n",
            "Epoch [4/5], Step [5800/15231], Loss: 0.2067\n",
            "Epoch [4/5], Step [5820/15231], Loss: 0.2515\n",
            "Epoch [4/5], Step [5840/15231], Loss: 0.2135\n",
            "Epoch [4/5], Step [5860/15231], Loss: 0.2142\n",
            "Epoch [4/5], Step [5880/15231], Loss: 0.2691\n",
            "Epoch [4/5], Step [5900/15231], Loss: 0.2564\n",
            "Epoch [4/5], Step [5920/15231], Loss: 0.2138\n",
            "Epoch [4/5], Step [5940/15231], Loss: 0.1970\n",
            "Epoch [4/5], Step [5960/15231], Loss: 0.2188\n",
            "Epoch [4/5], Step [5980/15231], Loss: 0.2002\n",
            "Epoch [4/5], Step [6000/15231], Loss: 0.2355\n",
            "Epoch [4/5], Step [6020/15231], Loss: 0.2330\n",
            "Epoch [4/5], Step [6040/15231], Loss: 0.2587\n",
            "Epoch [4/5], Step [6060/15231], Loss: 0.2239\n",
            "Epoch [4/5], Step [6080/15231], Loss: 0.2231\n",
            "Epoch [4/5], Step [6100/15231], Loss: 0.2541\n",
            "Epoch [4/5], Step [6120/15231], Loss: 0.2342\n",
            "Epoch [4/5], Step [6140/15231], Loss: 0.2219\n",
            "Epoch [4/5], Step [6160/15231], Loss: 0.2406\n",
            "Epoch [4/5], Step [6180/15231], Loss: 0.2258\n",
            "Epoch [4/5], Step [6200/15231], Loss: 0.2208\n",
            "Epoch [4/5], Step [6220/15231], Loss: 0.2104\n",
            "Epoch [4/5], Step [6240/15231], Loss: 0.2415\n",
            "Epoch [4/5], Step [6260/15231], Loss: 0.2549\n",
            "Epoch [4/5], Step [6280/15231], Loss: 0.2157\n",
            "Epoch [4/5], Step [6300/15231], Loss: 0.2351\n",
            "Epoch [4/5], Step [6320/15231], Loss: 0.2204\n",
            "Epoch [4/5], Step [6340/15231], Loss: 0.2204\n",
            "Epoch [4/5], Step [6360/15231], Loss: 0.2369\n",
            "Epoch [4/5], Step [6380/15231], Loss: 0.2084\n",
            "Epoch [4/5], Step [6400/15231], Loss: 0.1974\n",
            "Epoch [4/5], Step [6420/15231], Loss: 0.2630\n",
            "Epoch [4/5], Step [6440/15231], Loss: 0.2305\n",
            "Epoch [4/5], Step [6460/15231], Loss: 0.2115\n",
            "Epoch [4/5], Step [6480/15231], Loss: 0.2164\n",
            "Epoch [4/5], Step [6500/15231], Loss: 0.2298\n",
            "Epoch [4/5], Step [6520/15231], Loss: 0.2024\n",
            "Epoch [4/5], Step [6540/15231], Loss: 0.2061\n",
            "Epoch [4/5], Step [6560/15231], Loss: 0.2242\n",
            "Epoch [4/5], Step [6580/15231], Loss: 0.1979\n",
            "Epoch [4/5], Step [6600/15231], Loss: 0.2069\n",
            "Epoch [4/5], Step [6620/15231], Loss: 0.2176\n",
            "Epoch [4/5], Step [6640/15231], Loss: 0.1843\n",
            "Epoch [4/5], Step [6660/15231], Loss: 0.2341\n",
            "Epoch [4/5], Step [6680/15231], Loss: 0.2180\n",
            "Epoch [4/5], Step [6700/15231], Loss: 0.2448\n",
            "Epoch [4/5], Step [6720/15231], Loss: 0.2509\n",
            "Epoch [4/5], Step [6740/15231], Loss: 0.2423\n",
            "Epoch [4/5], Step [6760/15231], Loss: 0.2270\n",
            "Epoch [4/5], Step [6780/15231], Loss: 0.2199\n",
            "Epoch [4/5], Step [6800/15231], Loss: 0.2063\n",
            "Epoch [4/5], Step [6820/15231], Loss: 0.2255\n",
            "Epoch [4/5], Step [6840/15231], Loss: 0.2126\n",
            "Epoch [4/5], Step [6860/15231], Loss: 0.2239\n",
            "Epoch [4/5], Step [6880/15231], Loss: 0.2365\n",
            "Epoch [4/5], Step [6900/15231], Loss: 0.2205\n",
            "Epoch [4/5], Step [6920/15231], Loss: 0.2113\n",
            "Epoch [4/5], Step [6940/15231], Loss: 0.2079\n",
            "Epoch [4/5], Step [6960/15231], Loss: 0.2136\n",
            "Epoch [4/5], Step [6980/15231], Loss: 0.2224\n",
            "Epoch [4/5], Step [7000/15231], Loss: 0.2205\n",
            "Epoch [4/5], Step [7020/15231], Loss: 0.2492\n",
            "Epoch [4/5], Step [7040/15231], Loss: 0.2417\n",
            "Epoch [4/5], Step [7060/15231], Loss: 0.2207\n",
            "Epoch [4/5], Step [7080/15231], Loss: 0.2156\n",
            "Epoch [4/5], Step [7100/15231], Loss: 0.2089\n",
            "Epoch [4/5], Step [7120/15231], Loss: 0.1951\n",
            "Epoch [4/5], Step [7140/15231], Loss: 0.2384\n",
            "Epoch [4/5], Step [7160/15231], Loss: 0.1881\n",
            "Epoch [4/5], Step [7180/15231], Loss: 0.2188\n",
            "Epoch [4/5], Step [7200/15231], Loss: 0.2528\n",
            "Epoch [4/5], Step [7220/15231], Loss: 0.2098\n",
            "Epoch [4/5], Step [7240/15231], Loss: 0.2412\n",
            "Epoch [4/5], Step [7260/15231], Loss: 0.2151\n",
            "Epoch [4/5], Step [7280/15231], Loss: 0.2486\n",
            "Epoch [4/5], Step [7300/15231], Loss: 0.2021\n",
            "Epoch [4/5], Step [7320/15231], Loss: 0.2436\n",
            "Epoch [4/5], Step [7340/15231], Loss: 0.2350\n",
            "Epoch [4/5], Step [7360/15231], Loss: 0.2260\n",
            "Epoch [4/5], Step [7380/15231], Loss: 0.2132\n",
            "Epoch [4/5], Step [7400/15231], Loss: 0.2934\n",
            "Epoch [4/5], Step [7420/15231], Loss: 0.2266\n",
            "Epoch [4/5], Step [7440/15231], Loss: 0.1971\n",
            "Epoch [4/5], Step [7460/15231], Loss: 0.2127\n",
            "Epoch [4/5], Step [7480/15231], Loss: 0.2352\n",
            "Epoch [4/5], Step [7500/15231], Loss: 0.2277\n",
            "Epoch [4/5], Step [7520/15231], Loss: 0.2156\n",
            "Epoch [4/5], Step [7540/15231], Loss: 0.2015\n",
            "Epoch [4/5], Step [7560/15231], Loss: 0.2077\n",
            "Epoch [4/5], Step [7580/15231], Loss: 0.1902\n",
            "Epoch [4/5], Step [7600/15231], Loss: 0.2242\n",
            "Epoch [4/5], Step [7620/15231], Loss: 0.2067\n",
            "Epoch [4/5], Step [7640/15231], Loss: 0.2145\n",
            "Epoch [4/5], Step [7660/15231], Loss: 0.1946\n",
            "Epoch [4/5], Step [7680/15231], Loss: 0.1945\n",
            "Epoch [4/5], Step [7700/15231], Loss: 0.2324\n",
            "Epoch [4/5], Step [7720/15231], Loss: 0.2172\n",
            "Epoch [4/5], Step [7740/15231], Loss: 0.1956\n",
            "Epoch [4/5], Step [7760/15231], Loss: 0.2411\n",
            "Epoch [4/5], Step [7780/15231], Loss: 0.2157\n",
            "Epoch [4/5], Step [7800/15231], Loss: 0.2336\n",
            "Epoch [4/5], Step [7820/15231], Loss: 0.2071\n",
            "Epoch [4/5], Step [7840/15231], Loss: 0.2311\n",
            "Epoch [4/5], Step [7860/15231], Loss: 0.1991\n",
            "Epoch [4/5], Step [7880/15231], Loss: 0.2133\n",
            "Epoch [4/5], Step [7900/15231], Loss: 0.2013\n",
            "Epoch [4/5], Step [7920/15231], Loss: 0.2081\n",
            "Epoch [4/5], Step [7940/15231], Loss: 0.2275\n",
            "Epoch [4/5], Step [7960/15231], Loss: 0.1888\n",
            "Epoch [4/5], Step [7980/15231], Loss: 0.2582\n",
            "Epoch [4/5], Step [8000/15231], Loss: 0.2023\n",
            "Epoch [4/5], Step [8020/15231], Loss: 0.2154\n",
            "Epoch [4/5], Step [8040/15231], Loss: 0.2215\n",
            "Epoch [4/5], Step [8060/15231], Loss: 0.2441\n",
            "Epoch [4/5], Step [8080/15231], Loss: 0.2191\n",
            "Epoch [4/5], Step [8100/15231], Loss: 0.2047\n",
            "Epoch [4/5], Step [8120/15231], Loss: 0.1890\n",
            "Epoch [4/5], Step [8140/15231], Loss: 0.2161\n",
            "Epoch [4/5], Step [8160/15231], Loss: 0.2380\n",
            "Epoch [4/5], Step [8180/15231], Loss: 0.2126\n",
            "Epoch [4/5], Step [8200/15231], Loss: 0.2037\n",
            "Epoch [4/5], Step [8220/15231], Loss: 0.2157\n",
            "Epoch [4/5], Step [8240/15231], Loss: 0.2055\n",
            "Epoch [4/5], Step [8260/15231], Loss: 0.2286\n",
            "Epoch [4/5], Step [8280/15231], Loss: 0.2145\n",
            "Epoch [4/5], Step [8300/15231], Loss: 0.2164\n",
            "Epoch [4/5], Step [8320/15231], Loss: 0.1877\n",
            "Epoch [4/5], Step [8340/15231], Loss: 0.1974\n",
            "Epoch [4/5], Step [8360/15231], Loss: 0.2176\n",
            "Epoch [4/5], Step [8380/15231], Loss: 0.1917\n",
            "Epoch [4/5], Step [8400/15231], Loss: 0.2362\n",
            "Epoch [4/5], Step [8420/15231], Loss: 0.2203\n",
            "Epoch [4/5], Step [8440/15231], Loss: 0.2130\n",
            "Epoch [4/5], Step [8460/15231], Loss: 0.2321\n",
            "Epoch [4/5], Step [8480/15231], Loss: 0.1972\n",
            "Epoch [4/5], Step [8500/15231], Loss: 0.2098\n",
            "Epoch [4/5], Step [8520/15231], Loss: 0.2215\n",
            "Epoch [4/5], Step [8540/15231], Loss: 0.2043\n",
            "Epoch [4/5], Step [8560/15231], Loss: 0.2307\n",
            "Epoch [4/5], Step [8580/15231], Loss: 0.2357\n",
            "Epoch [4/5], Step [8600/15231], Loss: 0.2056\n",
            "Epoch [4/5], Step [8620/15231], Loss: 0.1660\n",
            "Epoch [4/5], Step [8640/15231], Loss: 0.2064\n",
            "Epoch [4/5], Step [8660/15231], Loss: 0.2201\n",
            "Epoch [4/5], Step [8680/15231], Loss: 0.2270\n",
            "Epoch [4/5], Step [8700/15231], Loss: 0.2220\n",
            "Epoch [4/5], Step [8720/15231], Loss: 0.2190\n",
            "Epoch [4/5], Step [8740/15231], Loss: 0.2185\n",
            "Epoch [4/5], Step [8760/15231], Loss: 0.2217\n",
            "Epoch [4/5], Step [8780/15231], Loss: 0.2380\n",
            "Epoch [4/5], Step [8800/15231], Loss: 0.2146\n",
            "Epoch [4/5], Step [8820/15231], Loss: 0.2128\n",
            "Epoch [4/5], Step [8840/15231], Loss: 0.2200\n",
            "Epoch [4/5], Step [8860/15231], Loss: 0.2024\n",
            "Epoch [4/5], Step [8880/15231], Loss: 0.2112\n",
            "Epoch [4/5], Step [8900/15231], Loss: 0.2286\n",
            "Epoch [4/5], Step [8920/15231], Loss: 0.2329\n",
            "Epoch [4/5], Step [8940/15231], Loss: 0.2408\n",
            "Epoch [4/5], Step [8960/15231], Loss: 0.2045\n",
            "Epoch [4/5], Step [8980/15231], Loss: 0.2174\n",
            "Epoch [4/5], Step [9000/15231], Loss: 0.2104\n",
            "Epoch [4/5], Step [9020/15231], Loss: 0.2197\n",
            "Epoch [4/5], Step [9040/15231], Loss: 0.2143\n",
            "Epoch [4/5], Step [9060/15231], Loss: 0.2213\n",
            "Epoch [4/5], Step [9080/15231], Loss: 0.2215\n",
            "Epoch [4/5], Step [9100/15231], Loss: 0.2118\n",
            "Epoch [4/5], Step [9120/15231], Loss: 0.2252\n",
            "Epoch [4/5], Step [9140/15231], Loss: 0.2277\n",
            "Epoch [4/5], Step [9160/15231], Loss: 0.2390\n",
            "Epoch [4/5], Step [9180/15231], Loss: 0.1865\n",
            "Epoch [4/5], Step [9200/15231], Loss: 0.2103\n",
            "Epoch [4/5], Step [9220/15231], Loss: 0.2410\n",
            "Epoch [4/5], Step [9240/15231], Loss: 0.2327\n",
            "Epoch [4/5], Step [9260/15231], Loss: 0.2070\n",
            "Epoch [4/5], Step [9280/15231], Loss: 0.2259\n",
            "Epoch [4/5], Step [9300/15231], Loss: 0.1991\n",
            "Epoch [4/5], Step [9320/15231], Loss: 0.2207\n",
            "Epoch [4/5], Step [9340/15231], Loss: 0.2336\n",
            "Epoch [4/5], Step [9360/15231], Loss: 0.2038\n",
            "Epoch [4/5], Step [9380/15231], Loss: 0.2194\n",
            "Epoch [4/5], Step [9400/15231], Loss: 0.2252\n",
            "Epoch [4/5], Step [9420/15231], Loss: 0.2627\n",
            "Epoch [4/5], Step [9440/15231], Loss: 0.2122\n",
            "Epoch [4/5], Step [9460/15231], Loss: 0.2217\n",
            "Epoch [4/5], Step [9480/15231], Loss: 0.2037\n",
            "Epoch [4/5], Step [9500/15231], Loss: 0.2005\n",
            "Epoch [4/5], Step [9520/15231], Loss: 0.1984\n",
            "Epoch [4/5], Step [9540/15231], Loss: 0.2367\n",
            "Epoch [4/5], Step [9560/15231], Loss: 0.2045\n",
            "Epoch [4/5], Step [9580/15231], Loss: 0.2028\n",
            "Epoch [4/5], Step [9600/15231], Loss: 0.2333\n",
            "Epoch [4/5], Step [9620/15231], Loss: 0.2211\n",
            "Epoch [4/5], Step [9640/15231], Loss: 0.2208\n",
            "Epoch [4/5], Step [9660/15231], Loss: 0.2404\n",
            "Epoch [4/5], Step [9680/15231], Loss: 0.2048\n",
            "Epoch [4/5], Step [9700/15231], Loss: 0.2305\n",
            "Epoch [4/5], Step [9720/15231], Loss: 0.2091\n",
            "Epoch [4/5], Step [9740/15231], Loss: 0.2184\n",
            "Epoch [4/5], Step [9760/15231], Loss: 0.2316\n",
            "Epoch [4/5], Step [9780/15231], Loss: 0.2349\n",
            "Epoch [4/5], Step [9800/15231], Loss: 0.2132\n",
            "Epoch [4/5], Step [9820/15231], Loss: 0.1975\n",
            "Epoch [4/5], Step [9840/15231], Loss: 0.2311\n",
            "Epoch [4/5], Step [9860/15231], Loss: 0.2127\n",
            "Epoch [4/5], Step [9880/15231], Loss: 0.2250\n",
            "Epoch [4/5], Step [9900/15231], Loss: 0.2088\n",
            "Epoch [4/5], Step [9920/15231], Loss: 0.2217\n",
            "Epoch [4/5], Step [9940/15231], Loss: 0.2188\n",
            "Epoch [4/5], Step [9960/15231], Loss: 0.2502\n",
            "Epoch [4/5], Step [9980/15231], Loss: 0.2705\n",
            "Epoch [4/5], Step [10000/15231], Loss: 0.2476\n",
            "Epoch [4/5], Step [10020/15231], Loss: 0.1891\n",
            "Epoch [4/5], Step [10040/15231], Loss: 0.2045\n",
            "Epoch [4/5], Step [10060/15231], Loss: 0.2335\n",
            "Epoch [4/5], Step [10080/15231], Loss: 0.2232\n",
            "Epoch [4/5], Step [10100/15231], Loss: 0.2194\n",
            "Epoch [4/5], Step [10120/15231], Loss: 0.2316\n",
            "Epoch [4/5], Step [10140/15231], Loss: 0.2479\n",
            "Epoch [4/5], Step [10160/15231], Loss: 0.2022\n",
            "Epoch [4/5], Step [10180/15231], Loss: 0.2133\n",
            "Epoch [4/5], Step [10200/15231], Loss: 0.2580\n",
            "Epoch [4/5], Step [10220/15231], Loss: 0.2169\n",
            "Epoch [4/5], Step [10240/15231], Loss: 0.2325\n",
            "Epoch [4/5], Step [10260/15231], Loss: 0.1825\n",
            "Epoch [4/5], Step [10280/15231], Loss: 0.2390\n",
            "Epoch [4/5], Step [10300/15231], Loss: 0.2172\n",
            "Epoch [4/5], Step [10320/15231], Loss: 0.2051\n",
            "Epoch [4/5], Step [10340/15231], Loss: 0.2157\n",
            "Epoch [4/5], Step [10360/15231], Loss: 0.2288\n",
            "Epoch [4/5], Step [10380/15231], Loss: 0.2213\n",
            "Epoch [4/5], Step [10400/15231], Loss: 0.2273\n",
            "Epoch [4/5], Step [10420/15231], Loss: 0.2448\n",
            "Epoch [4/5], Step [10440/15231], Loss: 0.1762\n",
            "Epoch [4/5], Step [10460/15231], Loss: 0.2410\n",
            "Epoch [4/5], Step [10480/15231], Loss: 0.1696\n",
            "Epoch [4/5], Step [10500/15231], Loss: 0.2337\n",
            "Epoch [4/5], Step [10520/15231], Loss: 0.1942\n",
            "Epoch [4/5], Step [10540/15231], Loss: 0.2483\n",
            "Epoch [4/5], Step [10560/15231], Loss: 0.2468\n",
            "Epoch [4/5], Step [10580/15231], Loss: 0.2238\n",
            "Epoch [4/5], Step [10600/15231], Loss: 0.2405\n",
            "Epoch [4/5], Step [10620/15231], Loss: 0.2230\n",
            "Epoch [4/5], Step [10640/15231], Loss: 0.2617\n",
            "Epoch [4/5], Step [10660/15231], Loss: 0.2174\n",
            "Epoch [4/5], Step [10680/15231], Loss: 0.2481\n",
            "Epoch [4/5], Step [10700/15231], Loss: 0.1840\n",
            "Epoch [4/5], Step [10720/15231], Loss: 0.2029\n",
            "Epoch [4/5], Step [10740/15231], Loss: 0.2233\n",
            "Epoch [4/5], Step [10760/15231], Loss: 0.1911\n",
            "Epoch [4/5], Step [10780/15231], Loss: 0.2241\n",
            "Epoch [4/5], Step [10800/15231], Loss: 0.1912\n",
            "Epoch [4/5], Step [10820/15231], Loss: 0.2090\n",
            "Epoch [4/5], Step [10840/15231], Loss: 0.2299\n",
            "Epoch [4/5], Step [10860/15231], Loss: 0.2473\n",
            "Epoch [4/5], Step [10880/15231], Loss: 0.2640\n",
            "Epoch [4/5], Step [10900/15231], Loss: 0.2339\n",
            "Epoch [4/5], Step [10920/15231], Loss: 0.2161\n",
            "Epoch [4/5], Step [10940/15231], Loss: 0.2035\n",
            "Epoch [4/5], Step [10960/15231], Loss: 0.2380\n",
            "Epoch [4/5], Step [10980/15231], Loss: 0.2305\n",
            "Epoch [4/5], Step [11000/15231], Loss: 0.2377\n",
            "Epoch [4/5], Step [11020/15231], Loss: 0.2110\n",
            "Epoch [4/5], Step [11040/15231], Loss: 0.2288\n",
            "Epoch [4/5], Step [11060/15231], Loss: 0.2428\n",
            "Epoch [4/5], Step [11080/15231], Loss: 0.2215\n",
            "Epoch [4/5], Step [11100/15231], Loss: 0.2144\n",
            "Epoch [4/5], Step [11120/15231], Loss: 0.2008\n",
            "Epoch [4/5], Step [11140/15231], Loss: 0.2125\n",
            "Epoch [4/5], Step [11160/15231], Loss: 0.2277\n",
            "Epoch [4/5], Step [11180/15231], Loss: 0.2238\n",
            "Epoch [4/5], Step [11200/15231], Loss: 0.2189\n",
            "Epoch [4/5], Step [11220/15231], Loss: 0.2059\n",
            "Epoch [4/5], Step [11240/15231], Loss: 0.2148\n",
            "Epoch [4/5], Step [11260/15231], Loss: 0.1971\n",
            "Epoch [4/5], Step [11280/15231], Loss: 0.1887\n",
            "Epoch [4/5], Step [11300/15231], Loss: 0.2085\n",
            "Epoch [4/5], Step [11320/15231], Loss: 0.2133\n",
            "Epoch [4/5], Step [11340/15231], Loss: 0.2235\n",
            "Epoch [4/5], Step [11360/15231], Loss: 0.2276\n",
            "Epoch [4/5], Step [11380/15231], Loss: 0.2290\n",
            "Epoch [4/5], Step [11400/15231], Loss: 0.2290\n",
            "Epoch [4/5], Step [11420/15231], Loss: 0.2220\n",
            "Epoch [4/5], Step [11440/15231], Loss: 0.1950\n",
            "Epoch [4/5], Step [11460/15231], Loss: 0.2219\n",
            "Epoch [4/5], Step [11480/15231], Loss: 0.2414\n",
            "Epoch [4/5], Step [11500/15231], Loss: 0.2022\n",
            "Epoch [4/5], Step [11520/15231], Loss: 0.2301\n",
            "Epoch [4/5], Step [11540/15231], Loss: 0.2143\n",
            "Epoch [4/5], Step [11560/15231], Loss: 0.2169\n",
            "Epoch [4/5], Step [11580/15231], Loss: 0.2234\n",
            "Epoch [4/5], Step [11600/15231], Loss: 0.2213\n",
            "Epoch [4/5], Step [11620/15231], Loss: 0.2167\n",
            "Epoch [4/5], Step [11640/15231], Loss: 0.2115\n",
            "Epoch [4/5], Step [11660/15231], Loss: 0.2574\n",
            "Epoch [4/5], Step [11680/15231], Loss: 0.1968\n",
            "Epoch [4/5], Step [11700/15231], Loss: 0.2306\n",
            "Epoch [4/5], Step [11720/15231], Loss: 0.2102\n",
            "Epoch [4/5], Step [11740/15231], Loss: 0.2235\n",
            "Epoch [4/5], Step [11760/15231], Loss: 0.2144\n",
            "Epoch [4/5], Step [11780/15231], Loss: 0.2198\n",
            "Epoch [4/5], Step [11800/15231], Loss: 0.2189\n",
            "Epoch [4/5], Step [11820/15231], Loss: 0.2097\n",
            "Epoch [4/5], Step [11840/15231], Loss: 0.2071\n",
            "Epoch [4/5], Step [11860/15231], Loss: 0.1993\n",
            "Epoch [4/5], Step [11880/15231], Loss: 0.2026\n",
            "Epoch [4/5], Step [11900/15231], Loss: 0.2237\n",
            "Epoch [4/5], Step [11920/15231], Loss: 0.2274\n",
            "Epoch [4/5], Step [11940/15231], Loss: 0.2430\n",
            "Epoch [4/5], Step [11960/15231], Loss: 0.2196\n",
            "Epoch [4/5], Step [11980/15231], Loss: 0.2182\n",
            "Epoch [4/5], Step [12000/15231], Loss: 0.2323\n",
            "Epoch [4/5], Step [12020/15231], Loss: 0.2098\n",
            "Epoch [4/5], Step [12040/15231], Loss: 0.1886\n",
            "Epoch [4/5], Step [12060/15231], Loss: 0.2045\n",
            "Epoch [4/5], Step [12080/15231], Loss: 0.2272\n",
            "Epoch [4/5], Step [12100/15231], Loss: 0.2147\n",
            "Epoch [4/5], Step [12120/15231], Loss: 0.1771\n",
            "Epoch [4/5], Step [12140/15231], Loss: 0.2157\n",
            "Epoch [4/5], Step [12160/15231], Loss: 0.2332\n",
            "Epoch [4/5], Step [12180/15231], Loss: 0.2226\n",
            "Epoch [4/5], Step [12200/15231], Loss: 0.2319\n",
            "Epoch [4/5], Step [12220/15231], Loss: 0.2213\n",
            "Epoch [4/5], Step [12240/15231], Loss: 0.1866\n",
            "Epoch [4/5], Step [12260/15231], Loss: 0.2261\n",
            "Epoch [4/5], Step [12280/15231], Loss: 0.1998\n",
            "Epoch [4/5], Step [12300/15231], Loss: 0.2185\n",
            "Epoch [4/5], Step [12320/15231], Loss: 0.2337\n",
            "Epoch [4/5], Step [12340/15231], Loss: 0.2297\n",
            "Epoch [4/5], Step [12360/15231], Loss: 0.2066\n",
            "Epoch [4/5], Step [12380/15231], Loss: 0.2242\n",
            "Epoch [4/5], Step [12400/15231], Loss: 0.2233\n",
            "Epoch [4/5], Step [12420/15231], Loss: 0.2049\n",
            "Epoch [4/5], Step [12440/15231], Loss: 0.1785\n",
            "Epoch [4/5], Step [12460/15231], Loss: 0.1865\n",
            "Epoch [4/5], Step [12480/15231], Loss: 0.1930\n",
            "Epoch [4/5], Step [12500/15231], Loss: 0.2195\n",
            "Epoch [4/5], Step [12520/15231], Loss: 0.2247\n",
            "Epoch [4/5], Step [12540/15231], Loss: 0.2076\n",
            "Epoch [4/5], Step [12560/15231], Loss: 0.1957\n",
            "Epoch [4/5], Step [12580/15231], Loss: 0.2027\n",
            "Epoch [4/5], Step [12600/15231], Loss: 0.1946\n",
            "Epoch [4/5], Step [12620/15231], Loss: 0.2325\n",
            "Epoch [4/5], Step [12640/15231], Loss: 0.2173\n",
            "Epoch [4/5], Step [12660/15231], Loss: 0.2347\n",
            "Epoch [4/5], Step [12680/15231], Loss: 0.2085\n",
            "Epoch [4/5], Step [12700/15231], Loss: 0.2284\n",
            "Epoch [4/5], Step [12720/15231], Loss: 0.1731\n",
            "Epoch [4/5], Step [12740/15231], Loss: 0.2239\n",
            "Epoch [4/5], Step [12760/15231], Loss: 0.2379\n",
            "Epoch [4/5], Step [12780/15231], Loss: 0.2045\n",
            "Epoch [4/5], Step [12800/15231], Loss: 0.2276\n",
            "Epoch [4/5], Step [12820/15231], Loss: 0.2054\n",
            "Epoch [4/5], Step [12840/15231], Loss: 0.2182\n",
            "Epoch [4/5], Step [12860/15231], Loss: 0.1956\n",
            "Epoch [4/5], Step [12880/15231], Loss: 0.2180\n",
            "Epoch [4/5], Step [12900/15231], Loss: 0.1834\n",
            "Epoch [4/5], Step [12920/15231], Loss: 0.2125\n",
            "Epoch [4/5], Step [12940/15231], Loss: 0.2366\n",
            "Epoch [4/5], Step [12960/15231], Loss: 0.2257\n",
            "Epoch [4/5], Step [12980/15231], Loss: 0.2009\n",
            "Epoch [4/5], Step [13000/15231], Loss: 0.1836\n",
            "Epoch [4/5], Step [13020/15231], Loss: 0.2030\n",
            "Epoch [4/5], Step [13040/15231], Loss: 0.2268\n",
            "Epoch [4/5], Step [13060/15231], Loss: 0.2008\n",
            "Epoch [4/5], Step [13080/15231], Loss: 0.2088\n",
            "Epoch [4/5], Step [13100/15231], Loss: 0.2152\n",
            "Epoch [4/5], Step [13120/15231], Loss: 0.2221\n",
            "Epoch [4/5], Step [13140/15231], Loss: 0.1877\n",
            "Epoch [4/5], Step [13160/15231], Loss: 0.2407\n",
            "Epoch [4/5], Step [13180/15231], Loss: 0.2220\n",
            "Epoch [4/5], Step [13200/15231], Loss: 0.2027\n",
            "Epoch [4/5], Step [13220/15231], Loss: 0.2079\n",
            "Epoch [4/5], Step [13240/15231], Loss: 0.1685\n",
            "Epoch [4/5], Step [13260/15231], Loss: 0.2389\n",
            "Epoch [4/5], Step [13280/15231], Loss: 0.2169\n",
            "Epoch [4/5], Step [13300/15231], Loss: 0.1739\n",
            "Epoch [4/5], Step [13320/15231], Loss: 0.2691\n",
            "Epoch [4/5], Step [13340/15231], Loss: 0.2133\n",
            "Epoch [4/5], Step [13360/15231], Loss: 0.2224\n",
            "Epoch [4/5], Step [13380/15231], Loss: 0.1793\n",
            "Epoch [4/5], Step [13400/15231], Loss: 0.2553\n",
            "Epoch [4/5], Step [13420/15231], Loss: 0.2087\n",
            "Epoch [4/5], Step [13440/15231], Loss: 0.1806\n",
            "Epoch [4/5], Step [13460/15231], Loss: 0.2169\n",
            "Epoch [4/5], Step [13480/15231], Loss: 0.1973\n",
            "Epoch [4/5], Step [13500/15231], Loss: 0.2037\n",
            "Epoch [4/5], Step [13520/15231], Loss: 0.2249\n",
            "Epoch [4/5], Step [13540/15231], Loss: 0.2247\n",
            "Epoch [4/5], Step [13560/15231], Loss: 0.1964\n",
            "Epoch [4/5], Step [13580/15231], Loss: 0.2171\n",
            "Epoch [4/5], Step [13600/15231], Loss: 0.1821\n",
            "Epoch [4/5], Step [13620/15231], Loss: 0.2001\n",
            "Epoch [4/5], Step [13640/15231], Loss: 0.1966\n",
            "Epoch [4/5], Step [13660/15231], Loss: 0.2018\n",
            "Epoch [4/5], Step [13680/15231], Loss: 0.2039\n",
            "Epoch [4/5], Step [13700/15231], Loss: 0.2048\n",
            "Epoch [4/5], Step [13720/15231], Loss: 0.2057\n",
            "Epoch [4/5], Step [13740/15231], Loss: 0.2083\n",
            "Epoch [4/5], Step [13760/15231], Loss: 0.1992\n",
            "Epoch [4/5], Step [13780/15231], Loss: 0.2161\n",
            "Epoch [4/5], Step [13800/15231], Loss: 0.2581\n",
            "Epoch [4/5], Step [13820/15231], Loss: 0.2242\n",
            "Epoch [4/5], Step [13840/15231], Loss: 0.2087\n",
            "Epoch [4/5], Step [13860/15231], Loss: 0.2142\n",
            "Epoch [4/5], Step [13880/15231], Loss: 0.2049\n",
            "Epoch [4/5], Step [13900/15231], Loss: 0.1751\n",
            "Epoch [4/5], Step [13920/15231], Loss: 0.2192\n",
            "Epoch [4/5], Step [13940/15231], Loss: 0.2147\n",
            "Epoch [4/5], Step [13960/15231], Loss: 0.2074\n",
            "Epoch [4/5], Step [13980/15231], Loss: 0.2180\n",
            "Epoch [4/5], Step [14000/15231], Loss: 0.1683\n",
            "Epoch [4/5], Step [14020/15231], Loss: 0.1928\n",
            "Epoch [4/5], Step [14040/15231], Loss: 0.2048\n",
            "Epoch [4/5], Step [14060/15231], Loss: 0.2172\n",
            "Epoch [4/5], Step [14080/15231], Loss: 0.1986\n",
            "Epoch [4/5], Step [14100/15231], Loss: 0.2200\n",
            "Epoch [4/5], Step [14120/15231], Loss: 0.2134\n",
            "Epoch [4/5], Step [14140/15231], Loss: 0.2395\n",
            "Epoch [4/5], Step [14160/15231], Loss: 0.2018\n",
            "Epoch [4/5], Step [14180/15231], Loss: 0.2625\n",
            "Epoch [4/5], Step [14200/15231], Loss: 0.2566\n",
            "Epoch [4/5], Step [14220/15231], Loss: 0.1961\n",
            "Epoch [4/5], Step [14240/15231], Loss: 0.2077\n",
            "Epoch [4/5], Step [14260/15231], Loss: 0.2202\n",
            "Epoch [4/5], Step [14280/15231], Loss: 0.2059\n",
            "Epoch [4/5], Step [14300/15231], Loss: 0.2380\n",
            "Epoch [4/5], Step [14320/15231], Loss: 0.2121\n",
            "Epoch [4/5], Step [14340/15231], Loss: 0.1864\n",
            "Epoch [4/5], Step [14360/15231], Loss: 0.2125\n",
            "Epoch [4/5], Step [14380/15231], Loss: 0.2153\n",
            "Epoch [4/5], Step [14400/15231], Loss: 0.2365\n",
            "Epoch [4/5], Step [14420/15231], Loss: 0.2336\n",
            "Epoch [4/5], Step [14440/15231], Loss: 0.2209\n",
            "Epoch [4/5], Step [14460/15231], Loss: 0.1881\n",
            "Epoch [4/5], Step [14480/15231], Loss: 0.2291\n",
            "Epoch [4/5], Step [14500/15231], Loss: 0.2260\n",
            "Epoch [4/5], Step [14520/15231], Loss: 0.2392\n",
            "Epoch [4/5], Step [14540/15231], Loss: 0.1970\n",
            "Epoch [4/5], Step [14560/15231], Loss: 0.2118\n",
            "Epoch [4/5], Step [14580/15231], Loss: 0.2206\n",
            "Epoch [4/5], Step [14600/15231], Loss: 0.2240\n",
            "Epoch [4/5], Step [14620/15231], Loss: 0.2081\n",
            "Epoch [4/5], Step [14640/15231], Loss: 0.2379\n",
            "Epoch [4/5], Step [14660/15231], Loss: 0.2117\n",
            "Epoch [4/5], Step [14680/15231], Loss: 0.2233\n",
            "Epoch [4/5], Step [14700/15231], Loss: 0.1917\n",
            "Epoch [4/5], Step [14720/15231], Loss: 0.2323\n",
            "Epoch [4/5], Step [14740/15231], Loss: 0.2205\n",
            "Epoch [4/5], Step [14760/15231], Loss: 0.2687\n",
            "Epoch [4/5], Step [14780/15231], Loss: 0.1976\n",
            "Epoch [4/5], Step [14800/15231], Loss: 0.1994\n",
            "Epoch [4/5], Step [14820/15231], Loss: 0.1851\n",
            "Epoch [4/5], Step [14840/15231], Loss: 0.2112\n",
            "Epoch [4/5], Step [14860/15231], Loss: 0.2083\n",
            "Epoch [4/5], Step [14880/15231], Loss: 0.2229\n",
            "Epoch [4/5], Step [14900/15231], Loss: 0.2156\n",
            "Epoch [4/5], Step [14920/15231], Loss: 0.2123\n",
            "Epoch [4/5], Step [14940/15231], Loss: 0.2003\n",
            "Epoch [4/5], Step [14960/15231], Loss: 0.2105\n",
            "Epoch [4/5], Step [14980/15231], Loss: 0.2175\n",
            "Epoch [4/5], Step [15000/15231], Loss: 0.2229\n",
            "Epoch [4/5], Step [15020/15231], Loss: 0.2148\n",
            "Epoch [4/5], Step [15040/15231], Loss: 0.2186\n",
            "Epoch [4/5], Step [15060/15231], Loss: 0.1918\n",
            "Epoch [4/5], Step [15080/15231], Loss: 0.2186\n",
            "Epoch [4/5], Step [15100/15231], Loss: 0.2162\n",
            "Epoch [4/5], Step [15120/15231], Loss: 0.2039\n",
            "Epoch [4/5], Step [15140/15231], Loss: 0.2063\n",
            "Epoch [4/5], Step [15160/15231], Loss: 0.1974\n",
            "Epoch [4/5], Step [15180/15231], Loss: 0.2047\n",
            "Epoch [4/5], Step [15200/15231], Loss: 0.1981\n",
            "Epoch [4/5], Step [15220/15231], Loss: 0.2073\n",
            "Epoch [4/5] | Train Loss: 0.2202 | Train Acc: 92.22% | Test Loss: 0.6624 | Test Acc: 78.10%\n",
            "Epoch [5/5], Step [20/15231], Loss: 0.2194\n",
            "Epoch [5/5], Step [40/15231], Loss: 0.2083\n",
            "Epoch [5/5], Step [60/15231], Loss: 0.1933\n",
            "Epoch [5/5], Step [80/15231], Loss: 0.2248\n",
            "Epoch [5/5], Step [100/15231], Loss: 0.1968\n",
            "Epoch [5/5], Step [120/15231], Loss: 0.1816\n",
            "Epoch [5/5], Step [140/15231], Loss: 0.2120\n",
            "Epoch [5/5], Step [160/15231], Loss: 0.2252\n",
            "Epoch [5/5], Step [180/15231], Loss: 0.2196\n",
            "Epoch [5/5], Step [200/15231], Loss: 0.2019\n",
            "Epoch [5/5], Step [220/15231], Loss: 0.1911\n",
            "Epoch [5/5], Step [240/15231], Loss: 0.2092\n",
            "Epoch [5/5], Step [260/15231], Loss: 0.2355\n",
            "Epoch [5/5], Step [280/15231], Loss: 0.2124\n",
            "Epoch [5/5], Step [300/15231], Loss: 0.2020\n",
            "Epoch [5/5], Step [320/15231], Loss: 0.2032\n",
            "Epoch [5/5], Step [340/15231], Loss: 0.1939\n",
            "Epoch [5/5], Step [360/15231], Loss: 0.1980\n",
            "Epoch [5/5], Step [380/15231], Loss: 0.1691\n",
            "Epoch [5/5], Step [400/15231], Loss: 0.1655\n",
            "Epoch [5/5], Step [420/15231], Loss: 0.2055\n",
            "Epoch [5/5], Step [440/15231], Loss: 0.1929\n",
            "Epoch [5/5], Step [460/15231], Loss: 0.2086\n",
            "Epoch [5/5], Step [480/15231], Loss: 0.2366\n",
            "Epoch [5/5], Step [500/15231], Loss: 0.2180\n",
            "Epoch [5/5], Step [520/15231], Loss: 0.2128\n",
            "Epoch [5/5], Step [540/15231], Loss: 0.1988\n",
            "Epoch [5/5], Step [560/15231], Loss: 0.1974\n",
            "Epoch [5/5], Step [580/15231], Loss: 0.2364\n",
            "Epoch [5/5], Step [600/15231], Loss: 0.1891\n",
            "Epoch [5/5], Step [620/15231], Loss: 0.2252\n",
            "Epoch [5/5], Step [640/15231], Loss: 0.2076\n",
            "Epoch [5/5], Step [660/15231], Loss: 0.1955\n",
            "Epoch [5/5], Step [680/15231], Loss: 0.1966\n",
            "Epoch [5/5], Step [700/15231], Loss: 0.1676\n",
            "Epoch [5/5], Step [720/15231], Loss: 0.2153\n",
            "Epoch [5/5], Step [740/15231], Loss: 0.2298\n",
            "Epoch [5/5], Step [760/15231], Loss: 0.2039\n",
            "Epoch [5/5], Step [780/15231], Loss: 0.2144\n",
            "Epoch [5/5], Step [800/15231], Loss: 0.1834\n",
            "Epoch [5/5], Step [820/15231], Loss: 0.2217\n",
            "Epoch [5/5], Step [840/15231], Loss: 0.2112\n",
            "Epoch [5/5], Step [860/15231], Loss: 0.2101\n",
            "Epoch [5/5], Step [880/15231], Loss: 0.2335\n",
            "Epoch [5/5], Step [900/15231], Loss: 0.2104\n",
            "Epoch [5/5], Step [920/15231], Loss: 0.2296\n",
            "Epoch [5/5], Step [940/15231], Loss: 0.1692\n",
            "Epoch [5/5], Step [960/15231], Loss: 0.2572\n",
            "Epoch [5/5], Step [980/15231], Loss: 0.1986\n",
            "Epoch [5/5], Step [1000/15231], Loss: 0.2134\n",
            "Epoch [5/5], Step [1020/15231], Loss: 0.2093\n",
            "Epoch [5/5], Step [1040/15231], Loss: 0.2055\n",
            "Epoch [5/5], Step [1060/15231], Loss: 0.2026\n",
            "Epoch [5/5], Step [1080/15231], Loss: 0.1858\n",
            "Epoch [5/5], Step [1100/15231], Loss: 0.2227\n",
            "Epoch [5/5], Step [1120/15231], Loss: 0.1926\n",
            "Epoch [5/5], Step [1140/15231], Loss: 0.1937\n",
            "Epoch [5/5], Step [1160/15231], Loss: 0.1881\n",
            "Epoch [5/5], Step [1180/15231], Loss: 0.2331\n",
            "Epoch [5/5], Step [1200/15231], Loss: 0.2112\n",
            "Epoch [5/5], Step [1220/15231], Loss: 0.2149\n",
            "Epoch [5/5], Step [1240/15231], Loss: 0.1777\n",
            "Epoch [5/5], Step [1260/15231], Loss: 0.2079\n",
            "Epoch [5/5], Step [1280/15231], Loss: 0.1909\n",
            "Epoch [5/5], Step [1300/15231], Loss: 0.2414\n",
            "Epoch [5/5], Step [1320/15231], Loss: 0.2040\n",
            "Epoch [5/5], Step [1340/15231], Loss: 0.2191\n",
            "Epoch [5/5], Step [1360/15231], Loss: 0.1909\n",
            "Epoch [5/5], Step [1380/15231], Loss: 0.1984\n",
            "Epoch [5/5], Step [1400/15231], Loss: 0.2215\n",
            "Epoch [5/5], Step [1420/15231], Loss: 0.2327\n",
            "Epoch [5/5], Step [1440/15231], Loss: 0.1958\n",
            "Epoch [5/5], Step [1460/15231], Loss: 0.2308\n",
            "Epoch [5/5], Step [1480/15231], Loss: 0.2174\n",
            "Epoch [5/5], Step [1500/15231], Loss: 0.2479\n",
            "Epoch [5/5], Step [1520/15231], Loss: 0.1677\n",
            "Epoch [5/5], Step [1540/15231], Loss: 0.2157\n",
            "Epoch [5/5], Step [1560/15231], Loss: 0.2095\n",
            "Epoch [5/5], Step [1580/15231], Loss: 0.2115\n",
            "Epoch [5/5], Step [1600/15231], Loss: 0.2101\n",
            "Epoch [5/5], Step [1620/15231], Loss: 0.1965\n",
            "Epoch [5/5], Step [1640/15231], Loss: 0.1915\n",
            "Epoch [5/5], Step [1660/15231], Loss: 0.1966\n",
            "Epoch [5/5], Step [1680/15231], Loss: 0.2171\n",
            "Epoch [5/5], Step [1700/15231], Loss: 0.2471\n",
            "Epoch [5/5], Step [1720/15231], Loss: 0.2032\n",
            "Epoch [5/5], Step [1740/15231], Loss: 0.2090\n",
            "Epoch [5/5], Step [1760/15231], Loss: 0.2044\n",
            "Epoch [5/5], Step [1780/15231], Loss: 0.2071\n",
            "Epoch [5/5], Step [1800/15231], Loss: 0.2291\n",
            "Epoch [5/5], Step [1820/15231], Loss: 0.2284\n",
            "Epoch [5/5], Step [1840/15231], Loss: 0.2017\n",
            "Epoch [5/5], Step [1860/15231], Loss: 0.2187\n",
            "Epoch [5/5], Step [1880/15231], Loss: 0.2390\n",
            "Epoch [5/5], Step [1900/15231], Loss: 0.2306\n",
            "Epoch [5/5], Step [1920/15231], Loss: 0.2112\n",
            "Epoch [5/5], Step [1940/15231], Loss: 0.2332\n",
            "Epoch [5/5], Step [1960/15231], Loss: 0.2182\n",
            "Epoch [5/5], Step [1980/15231], Loss: 0.1670\n",
            "Epoch [5/5], Step [2000/15231], Loss: 0.2049\n",
            "Epoch [5/5], Step [2020/15231], Loss: 0.2163\n",
            "Epoch [5/5], Step [2040/15231], Loss: 0.2019\n",
            "Epoch [5/5], Step [2060/15231], Loss: 0.2098\n",
            "Epoch [5/5], Step [2080/15231], Loss: 0.2006\n",
            "Epoch [5/5], Step [2100/15231], Loss: 0.1791\n",
            "Epoch [5/5], Step [2120/15231], Loss: 0.1925\n",
            "Epoch [5/5], Step [2140/15231], Loss: 0.2184\n",
            "Epoch [5/5], Step [2160/15231], Loss: 0.2453\n",
            "Epoch [5/5], Step [2180/15231], Loss: 0.2002\n",
            "Epoch [5/5], Step [2200/15231], Loss: 0.2232\n",
            "Epoch [5/5], Step [2220/15231], Loss: 0.2425\n",
            "Epoch [5/5], Step [2240/15231], Loss: 0.2005\n",
            "Epoch [5/5], Step [2260/15231], Loss: 0.1686\n",
            "Epoch [5/5], Step [2280/15231], Loss: 0.2211\n",
            "Epoch [5/5], Step [2300/15231], Loss: 0.1972\n",
            "Epoch [5/5], Step [2320/15231], Loss: 0.2031\n",
            "Epoch [5/5], Step [2340/15231], Loss: 0.2314\n",
            "Epoch [5/5], Step [2360/15231], Loss: 0.2115\n",
            "Epoch [5/5], Step [2380/15231], Loss: 0.1947\n",
            "Epoch [5/5], Step [2400/15231], Loss: 0.2057\n",
            "Epoch [5/5], Step [2420/15231], Loss: 0.2247\n",
            "Epoch [5/5], Step [2440/15231], Loss: 0.2011\n",
            "Epoch [5/5], Step [2460/15231], Loss: 0.1984\n",
            "Epoch [5/5], Step [2480/15231], Loss: 0.2071\n",
            "Epoch [5/5], Step [2500/15231], Loss: 0.2206\n",
            "Epoch [5/5], Step [2520/15231], Loss: 0.2305\n",
            "Epoch [5/5], Step [2540/15231], Loss: 0.2127\n",
            "Epoch [5/5], Step [2560/15231], Loss: 0.1900\n",
            "Epoch [5/5], Step [2580/15231], Loss: 0.2263\n",
            "Epoch [5/5], Step [2600/15231], Loss: 0.2070\n",
            "Epoch [5/5], Step [2620/15231], Loss: 0.2085\n",
            "Epoch [5/5], Step [2640/15231], Loss: 0.1911\n",
            "Epoch [5/5], Step [2660/15231], Loss: 0.1950\n",
            "Epoch [5/5], Step [2680/15231], Loss: 0.1900\n",
            "Epoch [5/5], Step [2700/15231], Loss: 0.1960\n",
            "Epoch [5/5], Step [2720/15231], Loss: 0.2309\n",
            "Epoch [5/5], Step [2740/15231], Loss: 0.1991\n",
            "Epoch [5/5], Step [2760/15231], Loss: 0.2076\n",
            "Epoch [5/5], Step [2780/15231], Loss: 0.1871\n",
            "Epoch [5/5], Step [2800/15231], Loss: 0.2052\n",
            "Epoch [5/5], Step [2820/15231], Loss: 0.2289\n",
            "Epoch [5/5], Step [2840/15231], Loss: 0.2135\n",
            "Epoch [5/5], Step [2860/15231], Loss: 0.2203\n",
            "Epoch [5/5], Step [2880/15231], Loss: 0.2328\n",
            "Epoch [5/5], Step [2900/15231], Loss: 0.1919\n",
            "Epoch [5/5], Step [2920/15231], Loss: 0.1884\n",
            "Epoch [5/5], Step [2940/15231], Loss: 0.2189\n",
            "Epoch [5/5], Step [2960/15231], Loss: 0.2265\n",
            "Epoch [5/5], Step [2980/15231], Loss: 0.1990\n",
            "Epoch [5/5], Step [3000/15231], Loss: 0.1856\n",
            "Epoch [5/5], Step [3020/15231], Loss: 0.1871\n",
            "Epoch [5/5], Step [3040/15231], Loss: 0.2120\n",
            "Epoch [5/5], Step [3060/15231], Loss: 0.2039\n",
            "Epoch [5/5], Step [3080/15231], Loss: 0.2157\n",
            "Epoch [5/5], Step [3100/15231], Loss: 0.2216\n",
            "Epoch [5/5], Step [3120/15231], Loss: 0.2202\n",
            "Epoch [5/5], Step [3140/15231], Loss: 0.1873\n",
            "Epoch [5/5], Step [3160/15231], Loss: 0.1986\n",
            "Epoch [5/5], Step [3180/15231], Loss: 0.2205\n",
            "Epoch [5/5], Step [3200/15231], Loss: 0.1817\n",
            "Epoch [5/5], Step [3220/15231], Loss: 0.1774\n",
            "Epoch [5/5], Step [3240/15231], Loss: 0.2039\n",
            "Epoch [5/5], Step [3260/15231], Loss: 0.2660\n",
            "Epoch [5/5], Step [3280/15231], Loss: 0.2091\n",
            "Epoch [5/5], Step [3300/15231], Loss: 0.1957\n",
            "Epoch [5/5], Step [3320/15231], Loss: 0.2087\n",
            "Epoch [5/5], Step [3340/15231], Loss: 0.1916\n",
            "Epoch [5/5], Step [3360/15231], Loss: 0.2195\n",
            "Epoch [5/5], Step [3380/15231], Loss: 0.2097\n",
            "Epoch [5/5], Step [3400/15231], Loss: 0.1977\n",
            "Epoch [5/5], Step [3420/15231], Loss: 0.2016\n",
            "Epoch [5/5], Step [3440/15231], Loss: 0.2303\n",
            "Epoch [5/5], Step [3460/15231], Loss: 0.2433\n",
            "Epoch [5/5], Step [3480/15231], Loss: 0.2209\n",
            "Epoch [5/5], Step [3500/15231], Loss: 0.2250\n",
            "Epoch [5/5], Step [3520/15231], Loss: 0.1956\n",
            "Epoch [5/5], Step [3540/15231], Loss: 0.1904\n",
            "Epoch [5/5], Step [3560/15231], Loss: 0.1984\n",
            "Epoch [5/5], Step [3580/15231], Loss: 0.1821\n",
            "Epoch [5/5], Step [3600/15231], Loss: 0.2126\n",
            "Epoch [5/5], Step [3620/15231], Loss: 0.2186\n",
            "Epoch [5/5], Step [3640/15231], Loss: 0.2118\n",
            "Epoch [5/5], Step [3660/15231], Loss: 0.1898\n",
            "Epoch [5/5], Step [3680/15231], Loss: 0.2296\n",
            "Epoch [5/5], Step [3700/15231], Loss: 0.2108\n",
            "Epoch [5/5], Step [3720/15231], Loss: 0.2259\n",
            "Epoch [5/5], Step [3740/15231], Loss: 0.1947\n",
            "Epoch [5/5], Step [3760/15231], Loss: 0.2065\n",
            "Epoch [5/5], Step [3780/15231], Loss: 0.2268\n",
            "Epoch [5/5], Step [3800/15231], Loss: 0.1766\n",
            "Epoch [5/5], Step [3820/15231], Loss: 0.2206\n",
            "Epoch [5/5], Step [3840/15231], Loss: 0.1882\n",
            "Epoch [5/5], Step [3860/15231], Loss: 0.2045\n",
            "Epoch [5/5], Step [3880/15231], Loss: 0.1693\n",
            "Epoch [5/5], Step [3900/15231], Loss: 0.2141\n",
            "Epoch [5/5], Step [3920/15231], Loss: 0.1973\n",
            "Epoch [5/5], Step [3940/15231], Loss: 0.2074\n",
            "Epoch [5/5], Step [3960/15231], Loss: 0.2203\n",
            "Epoch [5/5], Step [3980/15231], Loss: 0.2227\n",
            "Epoch [5/5], Step [4000/15231], Loss: 0.1897\n",
            "Epoch [5/5], Step [4020/15231], Loss: 0.2007\n",
            "Epoch [5/5], Step [4040/15231], Loss: 0.1919\n",
            "Epoch [5/5], Step [4060/15231], Loss: 0.1937\n",
            "Epoch [5/5], Step [4080/15231], Loss: 0.2004\n",
            "Epoch [5/5], Step [4100/15231], Loss: 0.1918\n",
            "Epoch [5/5], Step [4120/15231], Loss: 0.2335\n",
            "Epoch [5/5], Step [4140/15231], Loss: 0.2179\n",
            "Epoch [5/5], Step [4160/15231], Loss: 0.2109\n",
            "Epoch [5/5], Step [4180/15231], Loss: 0.2083\n",
            "Epoch [5/5], Step [4200/15231], Loss: 0.2291\n",
            "Epoch [5/5], Step [4220/15231], Loss: 0.1915\n",
            "Epoch [5/5], Step [4240/15231], Loss: 0.1956\n",
            "Epoch [5/5], Step [4260/15231], Loss: 0.2001\n",
            "Epoch [5/5], Step [4280/15231], Loss: 0.2401\n",
            "Epoch [5/5], Step [4300/15231], Loss: 0.1994\n",
            "Epoch [5/5], Step [4320/15231], Loss: 0.2171\n",
            "Epoch [5/5], Step [4340/15231], Loss: 0.2027\n",
            "Epoch [5/5], Step [4360/15231], Loss: 0.2248\n",
            "Epoch [5/5], Step [4380/15231], Loss: 0.1775\n",
            "Epoch [5/5], Step [4400/15231], Loss: 0.1810\n",
            "Epoch [5/5], Step [4420/15231], Loss: 0.2006\n",
            "Epoch [5/5], Step [4440/15231], Loss: 0.2008\n",
            "Epoch [5/5], Step [4460/15231], Loss: 0.1987\n",
            "Epoch [5/5], Step [4480/15231], Loss: 0.2079\n",
            "Epoch [5/5], Step [4500/15231], Loss: 0.2056\n",
            "Epoch [5/5], Step [4520/15231], Loss: 0.2104\n",
            "Epoch [5/5], Step [4540/15231], Loss: 0.1901\n",
            "Epoch [5/5], Step [4560/15231], Loss: 0.2362\n",
            "Epoch [5/5], Step [4580/15231], Loss: 0.1996\n",
            "Epoch [5/5], Step [4600/15231], Loss: 0.1946\n",
            "Epoch [5/5], Step [4620/15231], Loss: 0.2437\n",
            "Epoch [5/5], Step [4640/15231], Loss: 0.1857\n",
            "Epoch [5/5], Step [4660/15231], Loss: 0.2017\n",
            "Epoch [5/5], Step [4680/15231], Loss: 0.2273\n",
            "Epoch [5/5], Step [4700/15231], Loss: 0.1871\n",
            "Epoch [5/5], Step [4720/15231], Loss: 0.1885\n",
            "Epoch [5/5], Step [4740/15231], Loss: 0.1950\n",
            "Epoch [5/5], Step [4760/15231], Loss: 0.1986\n",
            "Epoch [5/5], Step [4780/15231], Loss: 0.2086\n",
            "Epoch [5/5], Step [4800/15231], Loss: 0.2041\n",
            "Epoch [5/5], Step [4820/15231], Loss: 0.2378\n",
            "Epoch [5/5], Step [4840/15231], Loss: 0.1790\n",
            "Epoch [5/5], Step [4860/15231], Loss: 0.2020\n",
            "Epoch [5/5], Step [4880/15231], Loss: 0.1753\n",
            "Epoch [5/5], Step [4900/15231], Loss: 0.1841\n",
            "Epoch [5/5], Step [4920/15231], Loss: 0.2142\n",
            "Epoch [5/5], Step [4940/15231], Loss: 0.2175\n",
            "Epoch [5/5], Step [4960/15231], Loss: 0.2308\n",
            "Epoch [5/5], Step [4980/15231], Loss: 0.2035\n",
            "Epoch [5/5], Step [5000/15231], Loss: 0.2337\n",
            "Epoch [5/5], Step [5020/15231], Loss: 0.2325\n",
            "Epoch [5/5], Step [5040/15231], Loss: 0.2277\n",
            "Epoch [5/5], Step [5060/15231], Loss: 0.1945\n",
            "Epoch [5/5], Step [5080/15231], Loss: 0.2084\n",
            "Epoch [5/5], Step [5100/15231], Loss: 0.1777\n",
            "Epoch [5/5], Step [5120/15231], Loss: 0.2011\n",
            "Epoch [5/5], Step [5140/15231], Loss: 0.1943\n",
            "Epoch [5/5], Step [5160/15231], Loss: 0.2193\n",
            "Epoch [5/5], Step [5180/15231], Loss: 0.1983\n",
            "Epoch [5/5], Step [5200/15231], Loss: 0.1883\n",
            "Epoch [5/5], Step [5220/15231], Loss: 0.2170\n",
            "Epoch [5/5], Step [5240/15231], Loss: 0.2199\n",
            "Epoch [5/5], Step [5260/15231], Loss: 0.2145\n",
            "Epoch [5/5], Step [5280/15231], Loss: 0.2248\n",
            "Epoch [5/5], Step [5300/15231], Loss: 0.1969\n",
            "Epoch [5/5], Step [5320/15231], Loss: 0.1914\n",
            "Epoch [5/5], Step [5340/15231], Loss: 0.1818\n",
            "Epoch [5/5], Step [5360/15231], Loss: 0.2080\n",
            "Epoch [5/5], Step [5380/15231], Loss: 0.1912\n",
            "Epoch [5/5], Step [5400/15231], Loss: 0.2164\n",
            "Epoch [5/5], Step [5420/15231], Loss: 0.1796\n",
            "Epoch [5/5], Step [5440/15231], Loss: 0.1902\n",
            "Epoch [5/5], Step [5460/15231], Loss: 0.1878\n",
            "Epoch [5/5], Step [5480/15231], Loss: 0.2110\n",
            "Epoch [5/5], Step [5500/15231], Loss: 0.2084\n",
            "Epoch [5/5], Step [5520/15231], Loss: 0.2067\n",
            "Epoch [5/5], Step [5540/15231], Loss: 0.1969\n",
            "Epoch [5/5], Step [5560/15231], Loss: 0.1851\n",
            "Epoch [5/5], Step [5580/15231], Loss: 0.2055\n",
            "Epoch [5/5], Step [5600/15231], Loss: 0.2269\n",
            "Epoch [5/5], Step [5620/15231], Loss: 0.2170\n",
            "Epoch [5/5], Step [5640/15231], Loss: 0.2450\n",
            "Epoch [5/5], Step [5660/15231], Loss: 0.2296\n",
            "Epoch [5/5], Step [5680/15231], Loss: 0.1880\n",
            "Epoch [5/5], Step [5700/15231], Loss: 0.2278\n",
            "Epoch [5/5], Step [5720/15231], Loss: 0.2103\n",
            "Epoch [5/5], Step [5740/15231], Loss: 0.1934\n",
            "Epoch [5/5], Step [5760/15231], Loss: 0.1847\n",
            "Epoch [5/5], Step [5780/15231], Loss: 0.1935\n",
            "Epoch [5/5], Step [5800/15231], Loss: 0.2104\n",
            "Epoch [5/5], Step [5820/15231], Loss: 0.1987\n",
            "Epoch [5/5], Step [5840/15231], Loss: 0.2050\n",
            "Epoch [5/5], Step [5860/15231], Loss: 0.2022\n",
            "Epoch [5/5], Step [5880/15231], Loss: 0.1999\n",
            "Epoch [5/5], Step [5900/15231], Loss: 0.1816\n",
            "Epoch [5/5], Step [5920/15231], Loss: 0.1999\n",
            "Epoch [5/5], Step [5940/15231], Loss: 0.2191\n",
            "Epoch [5/5], Step [5960/15231], Loss: 0.1957\n",
            "Epoch [5/5], Step [5980/15231], Loss: 0.1902\n",
            "Epoch [5/5], Step [6000/15231], Loss: 0.1764\n",
            "Epoch [5/5], Step [6020/15231], Loss: 0.2053\n",
            "Epoch [5/5], Step [6040/15231], Loss: 0.1874\n",
            "Epoch [5/5], Step [6060/15231], Loss: 0.1955\n",
            "Epoch [5/5], Step [6080/15231], Loss: 0.1933\n",
            "Epoch [5/5], Step [6100/15231], Loss: 0.2004\n",
            "Epoch [5/5], Step [6120/15231], Loss: 0.2151\n",
            "Epoch [5/5], Step [6140/15231], Loss: 0.2080\n",
            "Epoch [5/5], Step [6160/15231], Loss: 0.1918\n",
            "Epoch [5/5], Step [6180/15231], Loss: 0.2242\n",
            "Epoch [5/5], Step [6200/15231], Loss: 0.1911\n",
            "Epoch [5/5], Step [6220/15231], Loss: 0.2087\n",
            "Epoch [5/5], Step [6240/15231], Loss: 0.2129\n",
            "Epoch [5/5], Step [6260/15231], Loss: 0.2326\n",
            "Epoch [5/5], Step [6280/15231], Loss: 0.2212\n",
            "Epoch [5/5], Step [6300/15231], Loss: 0.2109\n",
            "Epoch [5/5], Step [6320/15231], Loss: 0.1936\n",
            "Epoch [5/5], Step [6340/15231], Loss: 0.1970\n",
            "Epoch [5/5], Step [6360/15231], Loss: 0.2191\n",
            "Epoch [5/5], Step [6380/15231], Loss: 0.2048\n",
            "Epoch [5/5], Step [6400/15231], Loss: 0.2165\n",
            "Epoch [5/5], Step [6420/15231], Loss: 0.2366\n",
            "Epoch [5/5], Step [6440/15231], Loss: 0.2121\n",
            "Epoch [5/5], Step [6460/15231], Loss: 0.1845\n",
            "Epoch [5/5], Step [6480/15231], Loss: 0.2023\n",
            "Epoch [5/5], Step [6500/15231], Loss: 0.2209\n",
            "Epoch [5/5], Step [6520/15231], Loss: 0.2049\n",
            "Epoch [5/5], Step [6540/15231], Loss: 0.2484\n",
            "Epoch [5/5], Step [6560/15231], Loss: 0.2014\n",
            "Epoch [5/5], Step [6580/15231], Loss: 0.2215\n",
            "Epoch [5/5], Step [6600/15231], Loss: 0.2229\n",
            "Epoch [5/5], Step [6620/15231], Loss: 0.1721\n",
            "Epoch [5/5], Step [6640/15231], Loss: 0.2157\n",
            "Epoch [5/5], Step [6660/15231], Loss: 0.1874\n",
            "Epoch [5/5], Step [6680/15231], Loss: 0.2072\n",
            "Epoch [5/5], Step [6700/15231], Loss: 0.2107\n",
            "Epoch [5/5], Step [6720/15231], Loss: 0.2100\n",
            "Epoch [5/5], Step [6740/15231], Loss: 0.2214\n",
            "Epoch [5/5], Step [6760/15231], Loss: 0.1835\n",
            "Epoch [5/5], Step [6780/15231], Loss: 0.2084\n",
            "Epoch [5/5], Step [6800/15231], Loss: 0.2058\n",
            "Epoch [5/5], Step [6820/15231], Loss: 0.2163\n",
            "Epoch [5/5], Step [6840/15231], Loss: 0.1981\n",
            "Epoch [5/5], Step [6860/15231], Loss: 0.2023\n",
            "Epoch [5/5], Step [6880/15231], Loss: 0.1729\n",
            "Epoch [5/5], Step [6900/15231], Loss: 0.2254\n",
            "Epoch [5/5], Step [6920/15231], Loss: 0.1847\n",
            "Epoch [5/5], Step [6940/15231], Loss: 0.2177\n",
            "Epoch [5/5], Step [6960/15231], Loss: 0.1979\n",
            "Epoch [5/5], Step [6980/15231], Loss: 0.2358\n",
            "Epoch [5/5], Step [7000/15231], Loss: 0.2312\n",
            "Epoch [5/5], Step [7020/15231], Loss: 0.1992\n",
            "Epoch [5/5], Step [7040/15231], Loss: 0.2092\n",
            "Epoch [5/5], Step [7060/15231], Loss: 0.2055\n",
            "Epoch [5/5], Step [7080/15231], Loss: 0.2329\n",
            "Epoch [5/5], Step [7100/15231], Loss: 0.2570\n",
            "Epoch [5/5], Step [7120/15231], Loss: 0.2121\n",
            "Epoch [5/5], Step [7140/15231], Loss: 0.2056\n",
            "Epoch [5/5], Step [7160/15231], Loss: 0.2239\n",
            "Epoch [5/5], Step [7180/15231], Loss: 0.1916\n",
            "Epoch [5/5], Step [7200/15231], Loss: 0.2336\n",
            "Epoch [5/5], Step [7220/15231], Loss: 0.1941\n",
            "Epoch [5/5], Step [7240/15231], Loss: 0.1919\n",
            "Epoch [5/5], Step [7260/15231], Loss: 0.2102\n",
            "Epoch [5/5], Step [7280/15231], Loss: 0.1920\n",
            "Epoch [5/5], Step [7300/15231], Loss: 0.1859\n",
            "Epoch [5/5], Step [7320/15231], Loss: 0.1999\n",
            "Epoch [5/5], Step [7340/15231], Loss: 0.2079\n",
            "Epoch [5/5], Step [7360/15231], Loss: 0.2258\n",
            "Epoch [5/5], Step [7380/15231], Loss: 0.1984\n",
            "Epoch [5/5], Step [7400/15231], Loss: 0.2108\n",
            "Epoch [5/5], Step [7420/15231], Loss: 0.2039\n",
            "Epoch [5/5], Step [7440/15231], Loss: 0.1763\n",
            "Epoch [5/5], Step [7460/15231], Loss: 0.1924\n",
            "Epoch [5/5], Step [7480/15231], Loss: 0.1940\n",
            "Epoch [5/5], Step [7500/15231], Loss: 0.1727\n",
            "Epoch [5/5], Step [7520/15231], Loss: 0.1642\n",
            "Epoch [5/5], Step [7540/15231], Loss: 0.2152\n",
            "Epoch [5/5], Step [7560/15231], Loss: 0.1918\n",
            "Epoch [5/5], Step [7580/15231], Loss: 0.2207\n",
            "Epoch [5/5], Step [7600/15231], Loss: 0.2030\n",
            "Epoch [5/5], Step [7620/15231], Loss: 0.2147\n",
            "Epoch [5/5], Step [7640/15231], Loss: 0.2402\n",
            "Epoch [5/5], Step [7660/15231], Loss: 0.2036\n",
            "Epoch [5/5], Step [7680/15231], Loss: 0.2034\n",
            "Epoch [5/5], Step [7700/15231], Loss: 0.1913\n",
            "Epoch [5/5], Step [7720/15231], Loss: 0.2118\n",
            "Epoch [5/5], Step [7740/15231], Loss: 0.2054\n",
            "Epoch [5/5], Step [7760/15231], Loss: 0.2182\n",
            "Epoch [5/5], Step [7780/15231], Loss: 0.2263\n",
            "Epoch [5/5], Step [7800/15231], Loss: 0.2283\n",
            "Epoch [5/5], Step [7820/15231], Loss: 0.1972\n",
            "Epoch [5/5], Step [7840/15231], Loss: 0.2023\n",
            "Epoch [5/5], Step [7860/15231], Loss: 0.1959\n",
            "Epoch [5/5], Step [7880/15231], Loss: 0.1769\n",
            "Epoch [5/5], Step [7900/15231], Loss: 0.2269\n",
            "Epoch [5/5], Step [7920/15231], Loss: 0.2100\n",
            "Epoch [5/5], Step [7940/15231], Loss: 0.2068\n",
            "Epoch [5/5], Step [7960/15231], Loss: 0.1987\n",
            "Epoch [5/5], Step [7980/15231], Loss: 0.2134\n",
            "Epoch [5/5], Step [8000/15231], Loss: 0.1739\n",
            "Epoch [5/5], Step [8020/15231], Loss: 0.2080\n",
            "Epoch [5/5], Step [8040/15231], Loss: 0.2219\n",
            "Epoch [5/5], Step [8060/15231], Loss: 0.1914\n",
            "Epoch [5/5], Step [8080/15231], Loss: 0.1871\n",
            "Epoch [5/5], Step [8100/15231], Loss: 0.1894\n",
            "Epoch [5/5], Step [8120/15231], Loss: 0.2075\n",
            "Epoch [5/5], Step [8140/15231], Loss: 0.2134\n",
            "Epoch [5/5], Step [8160/15231], Loss: 0.2238\n",
            "Epoch [5/5], Step [8180/15231], Loss: 0.2068\n",
            "Epoch [5/5], Step [8200/15231], Loss: 0.1966\n",
            "Epoch [5/5], Step [8220/15231], Loss: 0.1818\n",
            "Epoch [5/5], Step [8240/15231], Loss: 0.1957\n",
            "Epoch [5/5], Step [8260/15231], Loss: 0.2026\n",
            "Epoch [5/5], Step [8280/15231], Loss: 0.2122\n",
            "Epoch [5/5], Step [8300/15231], Loss: 0.2028\n",
            "Epoch [5/5], Step [8320/15231], Loss: 0.2188\n",
            "Epoch [5/5], Step [8340/15231], Loss: 0.2098\n",
            "Epoch [5/5], Step [8360/15231], Loss: 0.1961\n",
            "Epoch [5/5], Step [8380/15231], Loss: 0.1927\n",
            "Epoch [5/5], Step [8400/15231], Loss: 0.1806\n",
            "Epoch [5/5], Step [8420/15231], Loss: 0.2027\n",
            "Epoch [5/5], Step [8440/15231], Loss: 0.2020\n",
            "Epoch [5/5], Step [8460/15231], Loss: 0.2325\n",
            "Epoch [5/5], Step [8480/15231], Loss: 0.2186\n",
            "Epoch [5/5], Step [8500/15231], Loss: 0.1963\n",
            "Epoch [5/5], Step [8520/15231], Loss: 0.2317\n",
            "Epoch [5/5], Step [8540/15231], Loss: 0.1838\n",
            "Epoch [5/5], Step [8560/15231], Loss: 0.1827\n",
            "Epoch [5/5], Step [8580/15231], Loss: 0.1983\n",
            "Epoch [5/5], Step [8600/15231], Loss: 0.2368\n",
            "Epoch [5/5], Step [8620/15231], Loss: 0.2091\n",
            "Epoch [5/5], Step [8640/15231], Loss: 0.2032\n",
            "Epoch [5/5], Step [8660/15231], Loss: 0.1820\n",
            "Epoch [5/5], Step [8680/15231], Loss: 0.2254\n",
            "Epoch [5/5], Step [8700/15231], Loss: 0.2407\n",
            "Epoch [5/5], Step [8720/15231], Loss: 0.1951\n",
            "Epoch [5/5], Step [8740/15231], Loss: 0.2068\n",
            "Epoch [5/5], Step [8760/15231], Loss: 0.2114\n",
            "Epoch [5/5], Step [8780/15231], Loss: 0.1834\n",
            "Epoch [5/5], Step [8800/15231], Loss: 0.2309\n",
            "Epoch [5/5], Step [8820/15231], Loss: 0.1953\n",
            "Epoch [5/5], Step [8840/15231], Loss: 0.1924\n",
            "Epoch [5/5], Step [8860/15231], Loss: 0.1996\n",
            "Epoch [5/5], Step [8880/15231], Loss: 0.1885\n",
            "Epoch [5/5], Step [8900/15231], Loss: 0.1900\n",
            "Epoch [5/5], Step [8920/15231], Loss: 0.2141\n",
            "Epoch [5/5], Step [8940/15231], Loss: 0.2146\n",
            "Epoch [5/5], Step [8960/15231], Loss: 0.1998\n",
            "Epoch [5/5], Step [8980/15231], Loss: 0.1908\n",
            "Epoch [5/5], Step [9000/15231], Loss: 0.2090\n",
            "Epoch [5/5], Step [9020/15231], Loss: 0.1874\n",
            "Epoch [5/5], Step [9040/15231], Loss: 0.1987\n",
            "Epoch [5/5], Step [9060/15231], Loss: 0.1740\n",
            "Epoch [5/5], Step [9080/15231], Loss: 0.1925\n",
            "Epoch [5/5], Step [9100/15231], Loss: 0.2121\n",
            "Epoch [5/5], Step [9120/15231], Loss: 0.2065\n",
            "Epoch [5/5], Step [9140/15231], Loss: 0.2207\n",
            "Epoch [5/5], Step [9160/15231], Loss: 0.1876\n",
            "Epoch [5/5], Step [9180/15231], Loss: 0.2189\n",
            "Epoch [5/5], Step [9200/15231], Loss: 0.1934\n",
            "Epoch [5/5], Step [9220/15231], Loss: 0.2166\n",
            "Epoch [5/5], Step [9240/15231], Loss: 0.1904\n",
            "Epoch [5/5], Step [9260/15231], Loss: 0.2159\n",
            "Epoch [5/5], Step [9280/15231], Loss: 0.1968\n",
            "Epoch [5/5], Step [9300/15231], Loss: 0.1911\n",
            "Epoch [5/5], Step [9320/15231], Loss: 0.2589\n",
            "Epoch [5/5], Step [9340/15231], Loss: 0.2349\n",
            "Epoch [5/5], Step [9360/15231], Loss: 0.2092\n",
            "Epoch [5/5], Step [9380/15231], Loss: 0.1899\n",
            "Epoch [5/5], Step [9400/15231], Loss: 0.2219\n",
            "Epoch [5/5], Step [9420/15231], Loss: 0.2104\n",
            "Epoch [5/5], Step [9440/15231], Loss: 0.1932\n",
            "Epoch [5/5], Step [9460/15231], Loss: 0.1964\n",
            "Epoch [5/5], Step [9480/15231], Loss: 0.1770\n",
            "Epoch [5/5], Step [9500/15231], Loss: 0.1717\n",
            "Epoch [5/5], Step [9520/15231], Loss: 0.2078\n",
            "Epoch [5/5], Step [9540/15231], Loss: 0.2093\n",
            "Epoch [5/5], Step [9560/15231], Loss: 0.2152\n",
            "Epoch [5/5], Step [9580/15231], Loss: 0.2269\n",
            "Epoch [5/5], Step [9600/15231], Loss: 0.1713\n",
            "Epoch [5/5], Step [9620/15231], Loss: 0.2264\n",
            "Epoch [5/5], Step [9640/15231], Loss: 0.1935\n",
            "Epoch [5/5], Step [9660/15231], Loss: 0.2141\n",
            "Epoch [5/5], Step [9680/15231], Loss: 0.1929\n",
            "Epoch [5/5], Step [9700/15231], Loss: 0.2054\n",
            "Epoch [5/5], Step [9720/15231], Loss: 0.2088\n",
            "Epoch [5/5], Step [9740/15231], Loss: 0.2056\n",
            "Epoch [5/5], Step [9760/15231], Loss: 0.1866\n",
            "Epoch [5/5], Step [9780/15231], Loss: 0.2166\n",
            "Epoch [5/5], Step [9800/15231], Loss: 0.2028\n",
            "Epoch [5/5], Step [9820/15231], Loss: 0.1718\n",
            "Epoch [5/5], Step [9840/15231], Loss: 0.2153\n",
            "Epoch [5/5], Step [9860/15231], Loss: 0.2191\n",
            "Epoch [5/5], Step [9880/15231], Loss: 0.1988\n",
            "Epoch [5/5], Step [9900/15231], Loss: 0.2421\n",
            "Epoch [5/5], Step [9920/15231], Loss: 0.2094\n",
            "Epoch [5/5], Step [9940/15231], Loss: 0.1946\n",
            "Epoch [5/5], Step [9960/15231], Loss: 0.1703\n",
            "Epoch [5/5], Step [9980/15231], Loss: 0.2094\n",
            "Epoch [5/5], Step [10000/15231], Loss: 0.2057\n",
            "Epoch [5/5], Step [10020/15231], Loss: 0.2166\n",
            "Epoch [5/5], Step [10040/15231], Loss: 0.2054\n",
            "Epoch [5/5], Step [10060/15231], Loss: 0.1812\n",
            "Epoch [5/5], Step [10080/15231], Loss: 0.1847\n",
            "Epoch [5/5], Step [10100/15231], Loss: 0.1940\n",
            "Epoch [5/5], Step [10120/15231], Loss: 0.1655\n",
            "Epoch [5/5], Step [10140/15231], Loss: 0.2251\n",
            "Epoch [5/5], Step [10160/15231], Loss: 0.2321\n",
            "Epoch [5/5], Step [10180/15231], Loss: 0.2342\n",
            "Epoch [5/5], Step [10200/15231], Loss: 0.1703\n",
            "Epoch [5/5], Step [10220/15231], Loss: 0.2046\n",
            "Epoch [5/5], Step [10240/15231], Loss: 0.2279\n",
            "Epoch [5/5], Step [10260/15231], Loss: 0.2087\n",
            "Epoch [5/5], Step [10280/15231], Loss: 0.1900\n",
            "Epoch [5/5], Step [10300/15231], Loss: 0.2090\n",
            "Epoch [5/5], Step [10320/15231], Loss: 0.1963\n",
            "Epoch [5/5], Step [10340/15231], Loss: 0.1981\n",
            "Epoch [5/5], Step [10360/15231], Loss: 0.2109\n",
            "Epoch [5/5], Step [10380/15231], Loss: 0.1898\n",
            "Epoch [5/5], Step [10400/15231], Loss: 0.2035\n",
            "Epoch [5/5], Step [10420/15231], Loss: 0.1930\n",
            "Epoch [5/5], Step [10440/15231], Loss: 0.2045\n",
            "Epoch [5/5], Step [10460/15231], Loss: 0.2027\n",
            "Epoch [5/5], Step [10480/15231], Loss: 0.1939\n",
            "Epoch [5/5], Step [10500/15231], Loss: 0.2266\n",
            "Epoch [5/5], Step [10520/15231], Loss: 0.2129\n",
            "Epoch [5/5], Step [10540/15231], Loss: 0.2105\n",
            "Epoch [5/5], Step [10560/15231], Loss: 0.1666\n",
            "Epoch [5/5], Step [10580/15231], Loss: 0.1585\n",
            "Epoch [5/5], Step [10600/15231], Loss: 0.2147\n",
            "Epoch [5/5], Step [10620/15231], Loss: 0.1958\n",
            "Epoch [5/5], Step [10640/15231], Loss: 0.2032\n",
            "Epoch [5/5], Step [10660/15231], Loss: 0.2206\n",
            "Epoch [5/5], Step [10680/15231], Loss: 0.1834\n",
            "Epoch [5/5], Step [10700/15231], Loss: 0.2054\n",
            "Epoch [5/5], Step [10720/15231], Loss: 0.2044\n",
            "Epoch [5/5], Step [10740/15231], Loss: 0.2074\n",
            "Epoch [5/5], Step [10760/15231], Loss: 0.2108\n",
            "Epoch [5/5], Step [10780/15231], Loss: 0.1972\n",
            "Epoch [5/5], Step [10800/15231], Loss: 0.2081\n",
            "Epoch [5/5], Step [10820/15231], Loss: 0.2242\n",
            "Epoch [5/5], Step [10840/15231], Loss: 0.2279\n",
            "Epoch [5/5], Step [10860/15231], Loss: 0.2083\n",
            "Epoch [5/5], Step [10880/15231], Loss: 0.1789\n",
            "Epoch [5/5], Step [10900/15231], Loss: 0.1948\n",
            "Epoch [5/5], Step [10920/15231], Loss: 0.2137\n",
            "Epoch [5/5], Step [10940/15231], Loss: 0.2171\n",
            "Epoch [5/5], Step [10960/15231], Loss: 0.1682\n",
            "Epoch [5/5], Step [10980/15231], Loss: 0.2143\n",
            "Epoch [5/5], Step [11000/15231], Loss: 0.2053\n",
            "Epoch [5/5], Step [11020/15231], Loss: 0.2253\n",
            "Epoch [5/5], Step [11040/15231], Loss: 0.2014\n",
            "Epoch [5/5], Step [11060/15231], Loss: 0.2106\n",
            "Epoch [5/5], Step [11080/15231], Loss: 0.1853\n",
            "Epoch [5/5], Step [11100/15231], Loss: 0.2161\n",
            "Epoch [5/5], Step [11120/15231], Loss: 0.2128\n",
            "Epoch [5/5], Step [11140/15231], Loss: 0.1945\n",
            "Epoch [5/5], Step [11160/15231], Loss: 0.1878\n",
            "Epoch [5/5], Step [11180/15231], Loss: 0.1658\n",
            "Epoch [5/5], Step [11200/15231], Loss: 0.2142\n",
            "Epoch [5/5], Step [11220/15231], Loss: 0.1861\n",
            "Epoch [5/5], Step [11240/15231], Loss: 0.2002\n",
            "Epoch [5/5], Step [11260/15231], Loss: 0.2034\n",
            "Epoch [5/5], Step [11280/15231], Loss: 0.1931\n",
            "Epoch [5/5], Step [11300/15231], Loss: 0.1921\n",
            "Epoch [5/5], Step [11320/15231], Loss: 0.2141\n",
            "Epoch [5/5], Step [11340/15231], Loss: 0.1871\n",
            "Epoch [5/5], Step [11360/15231], Loss: 0.1938\n",
            "Epoch [5/5], Step [11380/15231], Loss: 0.2170\n",
            "Epoch [5/5], Step [11400/15231], Loss: 0.2037\n",
            "Epoch [5/5], Step [11420/15231], Loss: 0.2199\n",
            "Epoch [5/5], Step [11440/15231], Loss: 0.2182\n",
            "Epoch [5/5], Step [11460/15231], Loss: 0.1879\n",
            "Epoch [5/5], Step [11480/15231], Loss: 0.1786\n",
            "Epoch [5/5], Step [11500/15231], Loss: 0.2068\n",
            "Epoch [5/5], Step [11520/15231], Loss: 0.1833\n",
            "Epoch [5/5], Step [11540/15231], Loss: 0.2196\n",
            "Epoch [5/5], Step [11560/15231], Loss: 0.1945\n",
            "Epoch [5/5], Step [11580/15231], Loss: 0.2122\n",
            "Epoch [5/5], Step [11600/15231], Loss: 0.1988\n",
            "Epoch [5/5], Step [11620/15231], Loss: 0.1726\n",
            "Epoch [5/5], Step [11640/15231], Loss: 0.1936\n",
            "Epoch [5/5], Step [11660/15231], Loss: 0.1945\n",
            "Epoch [5/5], Step [11680/15231], Loss: 0.2193\n",
            "Epoch [5/5], Step [11700/15231], Loss: 0.2006\n",
            "Epoch [5/5], Step [11720/15231], Loss: 0.1737\n",
            "Epoch [5/5], Step [11740/15231], Loss: 0.1962\n",
            "Epoch [5/5], Step [11760/15231], Loss: 0.2209\n",
            "Epoch [5/5], Step [11780/15231], Loss: 0.2193\n",
            "Epoch [5/5], Step [11800/15231], Loss: 0.1928\n",
            "Epoch [5/5], Step [11820/15231], Loss: 0.1941\n",
            "Epoch [5/5], Step [11840/15231], Loss: 0.1910\n",
            "Epoch [5/5], Step [11860/15231], Loss: 0.2211\n",
            "Epoch [5/5], Step [11880/15231], Loss: 0.1958\n",
            "Epoch [5/5], Step [11900/15231], Loss: 0.1870\n",
            "Epoch [5/5], Step [11920/15231], Loss: 0.1706\n",
            "Epoch [5/5], Step [11940/15231], Loss: 0.1824\n",
            "Epoch [5/5], Step [11960/15231], Loss: 0.2041\n",
            "Epoch [5/5], Step [11980/15231], Loss: 0.1907\n",
            "Epoch [5/5], Step [12000/15231], Loss: 0.1816\n",
            "Epoch [5/5], Step [12020/15231], Loss: 0.2017\n",
            "Epoch [5/5], Step [12040/15231], Loss: 0.1993\n",
            "Epoch [5/5], Step [12060/15231], Loss: 0.1950\n",
            "Epoch [5/5], Step [12080/15231], Loss: 0.1738\n",
            "Epoch [5/5], Step [12100/15231], Loss: 0.1963\n",
            "Epoch [5/5], Step [12120/15231], Loss: 0.1989\n",
            "Epoch [5/5], Step [12140/15231], Loss: 0.2280\n",
            "Epoch [5/5], Step [12160/15231], Loss: 0.1863\n",
            "Epoch [5/5], Step [12180/15231], Loss: 0.1864\n",
            "Epoch [5/5], Step [12200/15231], Loss: 0.2074\n",
            "Epoch [5/5], Step [12220/15231], Loss: 0.1595\n",
            "Epoch [5/5], Step [12240/15231], Loss: 0.1948\n",
            "Epoch [5/5], Step [12260/15231], Loss: 0.1851\n",
            "Epoch [5/5], Step [12280/15231], Loss: 0.2077\n",
            "Epoch [5/5], Step [12300/15231], Loss: 0.1954\n",
            "Epoch [5/5], Step [12320/15231], Loss: 0.1965\n",
            "Epoch [5/5], Step [12340/15231], Loss: 0.2067\n",
            "Epoch [5/5], Step [12360/15231], Loss: 0.1968\n",
            "Epoch [5/5], Step [12380/15231], Loss: 0.1948\n",
            "Epoch [5/5], Step [12400/15231], Loss: 0.1998\n",
            "Epoch [5/5], Step [12420/15231], Loss: 0.2024\n",
            "Epoch [5/5], Step [12440/15231], Loss: 0.1789\n",
            "Epoch [5/5], Step [12460/15231], Loss: 0.1865\n",
            "Epoch [5/5], Step [12480/15231], Loss: 0.1952\n",
            "Epoch [5/5], Step [12500/15231], Loss: 0.1960\n",
            "Epoch [5/5], Step [12520/15231], Loss: 0.1940\n",
            "Epoch [5/5], Step [12540/15231], Loss: 0.2001\n",
            "Epoch [5/5], Step [12560/15231], Loss: 0.2186\n",
            "Epoch [5/5], Step [12580/15231], Loss: 0.2551\n",
            "Epoch [5/5], Step [12600/15231], Loss: 0.1977\n",
            "Epoch [5/5], Step [12620/15231], Loss: 0.2171\n",
            "Epoch [5/5], Step [12640/15231], Loss: 0.1987\n",
            "Epoch [5/5], Step [12660/15231], Loss: 0.1985\n",
            "Epoch [5/5], Step [12680/15231], Loss: 0.2230\n",
            "Epoch [5/5], Step [12700/15231], Loss: 0.1983\n",
            "Epoch [5/5], Step [12720/15231], Loss: 0.1859\n",
            "Epoch [5/5], Step [12740/15231], Loss: 0.1920\n",
            "Epoch [5/5], Step [12760/15231], Loss: 0.1785\n",
            "Epoch [5/5], Step [12780/15231], Loss: 0.2139\n",
            "Epoch [5/5], Step [12800/15231], Loss: 0.1802\n",
            "Epoch [5/5], Step [12820/15231], Loss: 0.1980\n",
            "Epoch [5/5], Step [12840/15231], Loss: 0.2229\n",
            "Epoch [5/5], Step [12860/15231], Loss: 0.1917\n",
            "Epoch [5/5], Step [12880/15231], Loss: 0.1966\n",
            "Epoch [5/5], Step [12900/15231], Loss: 0.2062\n",
            "Epoch [5/5], Step [12920/15231], Loss: 0.2171\n",
            "Epoch [5/5], Step [12940/15231], Loss: 0.1994\n",
            "Epoch [5/5], Step [12960/15231], Loss: 0.2105\n",
            "Epoch [5/5], Step [12980/15231], Loss: 0.2031\n",
            "Epoch [5/5], Step [13000/15231], Loss: 0.2051\n",
            "Epoch [5/5], Step [13020/15231], Loss: 0.1905\n",
            "Epoch [5/5], Step [13040/15231], Loss: 0.1811\n",
            "Epoch [5/5], Step [13060/15231], Loss: 0.1766\n",
            "Epoch [5/5], Step [13080/15231], Loss: 0.2106\n",
            "Epoch [5/5], Step [13100/15231], Loss: 0.2140\n",
            "Epoch [5/5], Step [13120/15231], Loss: 0.1968\n",
            "Epoch [5/5], Step [13140/15231], Loss: 0.2076\n",
            "Epoch [5/5], Step [13160/15231], Loss: 0.2058\n",
            "Epoch [5/5], Step [13180/15231], Loss: 0.1727\n",
            "Epoch [5/5], Step [13200/15231], Loss: 0.2047\n",
            "Epoch [5/5], Step [13220/15231], Loss: 0.1994\n",
            "Epoch [5/5], Step [13240/15231], Loss: 0.1889\n",
            "Epoch [5/5], Step [13260/15231], Loss: 0.2195\n",
            "Epoch [5/5], Step [13280/15231], Loss: 0.1919\n",
            "Epoch [5/5], Step [13300/15231], Loss: 0.2129\n",
            "Epoch [5/5], Step [13320/15231], Loss: 0.1903\n",
            "Epoch [5/5], Step [13340/15231], Loss: 0.2132\n",
            "Epoch [5/5], Step [13360/15231], Loss: 0.1911\n",
            "Epoch [5/5], Step [13380/15231], Loss: 0.1952\n",
            "Epoch [5/5], Step [13400/15231], Loss: 0.2135\n",
            "Epoch [5/5], Step [13420/15231], Loss: 0.1947\n",
            "Epoch [5/5], Step [13440/15231], Loss: 0.2100\n",
            "Epoch [5/5], Step [13460/15231], Loss: 0.2322\n",
            "Epoch [5/5], Step [13480/15231], Loss: 0.2422\n",
            "Epoch [5/5], Step [13500/15231], Loss: 0.2239\n",
            "Epoch [5/5], Step [13520/15231], Loss: 0.2076\n",
            "Epoch [5/5], Step [13540/15231], Loss: 0.1931\n",
            "Epoch [5/5], Step [13560/15231], Loss: 0.1780\n",
            "Epoch [5/5], Step [13580/15231], Loss: 0.1960\n",
            "Epoch [5/5], Step [13600/15231], Loss: 0.2181\n",
            "Epoch [5/5], Step [13620/15231], Loss: 0.1927\n",
            "Epoch [5/5], Step [13640/15231], Loss: 0.1939\n",
            "Epoch [5/5], Step [13660/15231], Loss: 0.1813\n",
            "Epoch [5/5], Step [13680/15231], Loss: 0.1871\n",
            "Epoch [5/5], Step [13700/15231], Loss: 0.2342\n",
            "Epoch [5/5], Step [13720/15231], Loss: 0.2182\n",
            "Epoch [5/5], Step [13740/15231], Loss: 0.2274\n",
            "Epoch [5/5], Step [13760/15231], Loss: 0.1771\n",
            "Epoch [5/5], Step [13780/15231], Loss: 0.1959\n",
            "Epoch [5/5], Step [13800/15231], Loss: 0.2088\n",
            "Epoch [5/5], Step [13820/15231], Loss: 0.1666\n",
            "Epoch [5/5], Step [13840/15231], Loss: 0.2057\n",
            "Epoch [5/5], Step [13860/15231], Loss: 0.2146\n",
            "Epoch [5/5], Step [13880/15231], Loss: 0.1989\n",
            "Epoch [5/5], Step [13900/15231], Loss: 0.1828\n",
            "Epoch [5/5], Step [13920/15231], Loss: 0.1821\n",
            "Epoch [5/5], Step [13940/15231], Loss: 0.2018\n",
            "Epoch [5/5], Step [13960/15231], Loss: 0.1873\n",
            "Epoch [5/5], Step [13980/15231], Loss: 0.1764\n",
            "Epoch [5/5], Step [14000/15231], Loss: 0.2057\n",
            "Epoch [5/5], Step [14020/15231], Loss: 0.1894\n",
            "Epoch [5/5], Step [14040/15231], Loss: 0.1838\n",
            "Epoch [5/5], Step [14060/15231], Loss: 0.2014\n",
            "Epoch [5/5], Step [14080/15231], Loss: 0.1943\n",
            "Epoch [5/5], Step [14100/15231], Loss: 0.1744\n",
            "Epoch [5/5], Step [14120/15231], Loss: 0.1735\n",
            "Epoch [5/5], Step [14140/15231], Loss: 0.2293\n",
            "Epoch [5/5], Step [14160/15231], Loss: 0.2240\n",
            "Epoch [5/5], Step [14180/15231], Loss: 0.2086\n",
            "Epoch [5/5], Step [14200/15231], Loss: 0.1866\n",
            "Epoch [5/5], Step [14220/15231], Loss: 0.1996\n",
            "Epoch [5/5], Step [14240/15231], Loss: 0.2093\n",
            "Epoch [5/5], Step [14260/15231], Loss: 0.1877\n",
            "Epoch [5/5], Step [14280/15231], Loss: 0.1871\n",
            "Epoch [5/5], Step [14300/15231], Loss: 0.2066\n",
            "Epoch [5/5], Step [14320/15231], Loss: 0.2099\n",
            "Epoch [5/5], Step [14340/15231], Loss: 0.2743\n",
            "Epoch [5/5], Step [14360/15231], Loss: 0.2164\n",
            "Epoch [5/5], Step [14380/15231], Loss: 0.2173\n",
            "Epoch [5/5], Step [14400/15231], Loss: 0.2255\n",
            "Epoch [5/5], Step [14420/15231], Loss: 0.1945\n",
            "Epoch [5/5], Step [14440/15231], Loss: 0.1998\n",
            "Epoch [5/5], Step [14460/15231], Loss: 0.1576\n",
            "Epoch [5/5], Step [14480/15231], Loss: 0.2288\n",
            "Epoch [5/5], Step [14500/15231], Loss: 0.2136\n",
            "Epoch [5/5], Step [14520/15231], Loss: 0.2005\n",
            "Epoch [5/5], Step [14540/15231], Loss: 0.2013\n",
            "Epoch [5/5], Step [14560/15231], Loss: 0.1886\n",
            "Epoch [5/5], Step [14580/15231], Loss: 0.2030\n",
            "Epoch [5/5], Step [14600/15231], Loss: 0.2161\n",
            "Epoch [5/5], Step [14620/15231], Loss: 0.2023\n",
            "Epoch [5/5], Step [14640/15231], Loss: 0.1850\n",
            "Epoch [5/5], Step [14660/15231], Loss: 0.1914\n",
            "Epoch [5/5], Step [14680/15231], Loss: 0.1729\n",
            "Epoch [5/5], Step [14700/15231], Loss: 0.2204\n",
            "Epoch [5/5], Step [14720/15231], Loss: 0.2334\n",
            "Epoch [5/5], Step [14740/15231], Loss: 0.2337\n",
            "Epoch [5/5], Step [14760/15231], Loss: 0.1663\n",
            "Epoch [5/5], Step [14780/15231], Loss: 0.2067\n",
            "Epoch [5/5], Step [14800/15231], Loss: 0.1985\n",
            "Epoch [5/5], Step [14820/15231], Loss: 0.1912\n",
            "Epoch [5/5], Step [14840/15231], Loss: 0.1744\n",
            "Epoch [5/5], Step [14860/15231], Loss: 0.2336\n",
            "Epoch [5/5], Step [14880/15231], Loss: 0.2143\n",
            "Epoch [5/5], Step [14900/15231], Loss: 0.1930\n",
            "Epoch [5/5], Step [14920/15231], Loss: 0.1590\n",
            "Epoch [5/5], Step [14940/15231], Loss: 0.2028\n",
            "Epoch [5/5], Step [14960/15231], Loss: 0.2196\n",
            "Epoch [5/5], Step [14980/15231], Loss: 0.2017\n",
            "Epoch [5/5], Step [15000/15231], Loss: 0.2119\n",
            "Epoch [5/5], Step [15020/15231], Loss: 0.1877\n",
            "Epoch [5/5], Step [15040/15231], Loss: 0.1749\n",
            "Epoch [5/5], Step [15060/15231], Loss: 0.2148\n",
            "Epoch [5/5], Step [15080/15231], Loss: 0.1902\n",
            "Epoch [5/5], Step [15100/15231], Loss: 0.1919\n",
            "Epoch [5/5], Step [15120/15231], Loss: 0.1803\n",
            "Epoch [5/5], Step [15140/15231], Loss: 0.1931\n",
            "Epoch [5/5], Step [15160/15231], Loss: 0.2053\n",
            "Epoch [5/5], Step [15180/15231], Loss: 0.1937\n",
            "Epoch [5/5], Step [15200/15231], Loss: 0.2057\n",
            "Epoch [5/5], Step [15220/15231], Loss: 0.1907\n",
            "Epoch [5/5] | Train Loss: 0.2041 | Train Acc: 92.80% | Test Loss: 0.9084 | Test Acc: 72.50%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▆▆▆▆▆▆████████</td></tr><tr><td>test_accuracy per epoch</td><td>▅█▆█▁</td></tr><tr><td>train_accuracy per epoch</td><td>▁▅▆▇█</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▃▂▂▂▁▂▂▁▂▂▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>test_accuracy per epoch</td><td>72.5</td></tr><tr><td>train_accuracy per epoch</td><td>92.79902</td></tr><tr><td>train_loss</td><td>0.19068</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet-3 with data aug</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ckvbfm2w' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ckvbfm2w</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT</a><br>Synced 5 W&B file(s), 2 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250404_134801-ckvbfm2w/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "\n",
        "name = \"resnet-3 with data aug (2 epochs)\"\n",
        "\n",
        "# Configuration dictionary for wandb\n",
        "config = {\n",
        "    \"in_channels\": 1,\n",
        "    \"num_epochs\": 2,            # Adjust number of epochs as necessary\n",
        "    \"lr\": 0.001,                # Learning rate\n",
        "    \"batch_size\": 64,           # Batch size (should match your DataLoader)\n",
        "    \"num_blocks\": 3,            # Number of residual blocks in the network\n",
        "    \"base_channels\": 64,        # Number of filters in the first block\n",
        "    \"kernel_size\": 3,           # Kernel size for convolutions\n",
        "    \"activation\": \"LeakyReLU\",  # Name of the activation function\n",
        "    \"use_batchnorm\": True,      # Whether to use batch normalization\n",
        "    \"num_classes\": 4            # OCTMNIST has 4 classes\n",
        "}\n",
        "\n",
        "# Create the model with custom configuration\n",
        "model = CustomResNet(\n",
        "        in_channels=config['in_channels'],  # OCTMNIST images are single-channel if you have preprocessed accordingly\n",
        "        num_blocks=config[\"num_blocks\"],\n",
        "        base_channels=config[\"base_channels\"],\n",
        "        kernel_size=config[\"kernel_size\"],\n",
        "        activation=nn.LeakyReLU(0.1, inplace=True),\n",
        "        use_batchnorm=config[\"use_batchnorm\"],\n",
        "        num_classes=config[\"num_classes\"]\n",
        "    )\n",
        "print(model)\n",
        "\n",
        "print(\"number of parameters : \")\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "# Initialize wandb run and train the model\n",
        "with wandb.init(\n",
        "    config=config,\n",
        "    project='CNN VS ViT',   # Title of your project\n",
        "    group='Test',           # Group name for your run\n",
        "    save_code=True,\n",
        "    name=name,\n",
        "):\n",
        "\n",
        "    # Train the model (train_loader and test_loader should be defined before)\n",
        "    train_model(model, config, name)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ubadggG0Ki4d",
        "outputId": "bc87cce3-12eb-4a9d-ee28-eff8ba5de871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n",
            "CustomResNet(\n",
            "  (initial_conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (initial_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  (residual_layers): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "  )\n",
            "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "number of parameters : \n",
            "222916\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "creating run (1.0s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250404_141931-uxe2kc6f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/uxe2kc6f' target=\"_blank\">resnet-3 with data aug (2 epochs)</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/uxe2kc6f' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/uxe2kc6f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Step [20/15231], Loss: 1.0435\n",
            "Epoch [1/2], Step [40/15231], Loss: 0.8554\n",
            "Epoch [1/2], Step [60/15231], Loss: 0.7751\n",
            "Epoch [1/2], Step [80/15231], Loss: 0.7318\n",
            "Epoch [1/2], Step [100/15231], Loss: 0.7532\n",
            "Epoch [1/2], Step [120/15231], Loss: 0.7272\n",
            "Epoch [1/2], Step [140/15231], Loss: 0.7388\n",
            "Epoch [1/2], Step [160/15231], Loss: 0.6885\n",
            "Epoch [1/2], Step [180/15231], Loss: 0.6293\n",
            "Epoch [1/2], Step [200/15231], Loss: 0.6348\n",
            "Epoch [1/2], Step [220/15231], Loss: 0.6612\n",
            "Epoch [1/2], Step [240/15231], Loss: 0.6682\n",
            "Epoch [1/2], Step [260/15231], Loss: 0.6389\n",
            "Epoch [1/2], Step [280/15231], Loss: 0.5653\n",
            "Epoch [1/2], Step [300/15231], Loss: 0.6137\n",
            "Epoch [1/2], Step [320/15231], Loss: 0.5874\n",
            "Epoch [1/2], Step [340/15231], Loss: 0.6039\n",
            "Epoch [1/2], Step [360/15231], Loss: 0.5826\n",
            "Epoch [1/2], Step [380/15231], Loss: 0.5823\n",
            "Epoch [1/2], Step [400/15231], Loss: 0.5901\n",
            "Epoch [1/2], Step [420/15231], Loss: 0.5533\n",
            "Epoch [1/2], Step [440/15231], Loss: 0.6158\n",
            "Epoch [1/2], Step [460/15231], Loss: 0.5662\n",
            "Epoch [1/2], Step [480/15231], Loss: 0.5711\n",
            "Epoch [1/2], Step [500/15231], Loss: 0.5673\n",
            "Epoch [1/2], Step [520/15231], Loss: 0.5620\n",
            "Epoch [1/2], Step [540/15231], Loss: 0.5835\n",
            "Epoch [1/2], Step [560/15231], Loss: 0.5262\n",
            "Epoch [1/2], Step [580/15231], Loss: 0.5435\n",
            "Epoch [1/2], Step [600/15231], Loss: 0.5304\n",
            "Epoch [1/2], Step [620/15231], Loss: 0.5274\n",
            "Epoch [1/2], Step [640/15231], Loss: 0.5318\n",
            "Epoch [1/2], Step [660/15231], Loss: 0.5420\n",
            "Epoch [1/2], Step [680/15231], Loss: 0.5355\n",
            "Epoch [1/2], Step [700/15231], Loss: 0.5191\n",
            "Epoch [1/2], Step [720/15231], Loss: 0.5741\n",
            "Epoch [1/2], Step [740/15231], Loss: 0.5226\n",
            "Epoch [1/2], Step [760/15231], Loss: 0.5543\n",
            "Epoch [1/2], Step [780/15231], Loss: 0.5124\n",
            "Epoch [1/2], Step [800/15231], Loss: 0.5145\n",
            "Epoch [1/2], Step [820/15231], Loss: 0.5055\n",
            "Epoch [1/2], Step [840/15231], Loss: 0.5225\n",
            "Epoch [1/2], Step [860/15231], Loss: 0.4973\n",
            "Epoch [1/2], Step [880/15231], Loss: 0.4459\n",
            "Epoch [1/2], Step [900/15231], Loss: 0.4689\n",
            "Epoch [1/2], Step [920/15231], Loss: 0.5067\n",
            "Epoch [1/2], Step [940/15231], Loss: 0.5411\n",
            "Epoch [1/2], Step [960/15231], Loss: 0.4969\n",
            "Epoch [1/2], Step [980/15231], Loss: 0.5036\n",
            "Epoch [1/2], Step [1000/15231], Loss: 0.5271\n",
            "Epoch [1/2], Step [1020/15231], Loss: 0.4961\n",
            "Epoch [1/2], Step [1040/15231], Loss: 0.5081\n",
            "Epoch [1/2], Step [1060/15231], Loss: 0.4511\n",
            "Epoch [1/2], Step [1080/15231], Loss: 0.4881\n",
            "Epoch [1/2], Step [1100/15231], Loss: 0.4815\n",
            "Epoch [1/2], Step [1120/15231], Loss: 0.4859\n",
            "Epoch [1/2], Step [1140/15231], Loss: 0.4714\n",
            "Epoch [1/2], Step [1160/15231], Loss: 0.4509\n",
            "Epoch [1/2], Step [1180/15231], Loss: 0.4304\n",
            "Epoch [1/2], Step [1200/15231], Loss: 0.4893\n",
            "Epoch [1/2], Step [1220/15231], Loss: 0.4598\n",
            "Epoch [1/2], Step [1240/15231], Loss: 0.5127\n",
            "Epoch [1/2], Step [1260/15231], Loss: 0.4933\n",
            "Epoch [1/2], Step [1280/15231], Loss: 0.4536\n",
            "Epoch [1/2], Step [1300/15231], Loss: 0.5025\n",
            "Epoch [1/2], Step [1320/15231], Loss: 0.4750\n",
            "Epoch [1/2], Step [1340/15231], Loss: 0.4843\n",
            "Epoch [1/2], Step [1360/15231], Loss: 0.4777\n",
            "Epoch [1/2], Step [1380/15231], Loss: 0.5229\n",
            "Epoch [1/2], Step [1400/15231], Loss: 0.5012\n",
            "Epoch [1/2], Step [1420/15231], Loss: 0.4796\n",
            "Epoch [1/2], Step [1440/15231], Loss: 0.4950\n",
            "Epoch [1/2], Step [1460/15231], Loss: 0.4272\n",
            "Epoch [1/2], Step [1480/15231], Loss: 0.4882\n",
            "Epoch [1/2], Step [1500/15231], Loss: 0.4199\n",
            "Epoch [1/2], Step [1520/15231], Loss: 0.4237\n",
            "Epoch [1/2], Step [1540/15231], Loss: 0.4552\n",
            "Epoch [1/2], Step [1560/15231], Loss: 0.4535\n",
            "Epoch [1/2], Step [1580/15231], Loss: 0.4450\n",
            "Epoch [1/2], Step [1600/15231], Loss: 0.4653\n",
            "Epoch [1/2], Step [1620/15231], Loss: 0.4204\n",
            "Epoch [1/2], Step [1640/15231], Loss: 0.4326\n",
            "Epoch [1/2], Step [1660/15231], Loss: 0.4315\n",
            "Epoch [1/2], Step [1680/15231], Loss: 0.4386\n",
            "Epoch [1/2], Step [1700/15231], Loss: 0.4534\n",
            "Epoch [1/2], Step [1720/15231], Loss: 0.4377\n",
            "Epoch [1/2], Step [1740/15231], Loss: 0.4604\n",
            "Epoch [1/2], Step [1760/15231], Loss: 0.4542\n",
            "Epoch [1/2], Step [1780/15231], Loss: 0.4413\n",
            "Epoch [1/2], Step [1800/15231], Loss: 0.4638\n",
            "Epoch [1/2], Step [1820/15231], Loss: 0.4255\n",
            "Epoch [1/2], Step [1840/15231], Loss: 0.4383\n",
            "Epoch [1/2], Step [1860/15231], Loss: 0.4329\n",
            "Epoch [1/2], Step [1880/15231], Loss: 0.3942\n",
            "Epoch [1/2], Step [1900/15231], Loss: 0.4133\n",
            "Epoch [1/2], Step [1920/15231], Loss: 0.4170\n",
            "Epoch [1/2], Step [1940/15231], Loss: 0.4070\n",
            "Epoch [1/2], Step [1960/15231], Loss: 0.4676\n",
            "Epoch [1/2], Step [1980/15231], Loss: 0.4390\n",
            "Epoch [1/2], Step [2000/15231], Loss: 0.4937\n",
            "Epoch [1/2], Step [2020/15231], Loss: 0.4422\n",
            "Epoch [1/2], Step [2040/15231], Loss: 0.4168\n",
            "Epoch [1/2], Step [2060/15231], Loss: 0.4686\n",
            "Epoch [1/2], Step [2080/15231], Loss: 0.4725\n",
            "Epoch [1/2], Step [2100/15231], Loss: 0.4007\n",
            "Epoch [1/2], Step [2120/15231], Loss: 0.4828\n",
            "Epoch [1/2], Step [2140/15231], Loss: 0.4829\n",
            "Epoch [1/2], Step [2160/15231], Loss: 0.4200\n",
            "Epoch [1/2], Step [2180/15231], Loss: 0.4440\n",
            "Epoch [1/2], Step [2200/15231], Loss: 0.4265\n",
            "Epoch [1/2], Step [2220/15231], Loss: 0.3984\n",
            "Epoch [1/2], Step [2240/15231], Loss: 0.4551\n",
            "Epoch [1/2], Step [2260/15231], Loss: 0.4477\n",
            "Epoch [1/2], Step [2280/15231], Loss: 0.4220\n",
            "Epoch [1/2], Step [2300/15231], Loss: 0.4343\n",
            "Epoch [1/2], Step [2320/15231], Loss: 0.3831\n",
            "Epoch [1/2], Step [2340/15231], Loss: 0.3934\n",
            "Epoch [1/2], Step [2360/15231], Loss: 0.4125\n",
            "Epoch [1/2], Step [2380/15231], Loss: 0.4110\n",
            "Epoch [1/2], Step [2400/15231], Loss: 0.3970\n",
            "Epoch [1/2], Step [2420/15231], Loss: 0.4352\n",
            "Epoch [1/2], Step [2440/15231], Loss: 0.4558\n",
            "Epoch [1/2], Step [2460/15231], Loss: 0.3743\n",
            "Epoch [1/2], Step [2480/15231], Loss: 0.4263\n",
            "Epoch [1/2], Step [2500/15231], Loss: 0.3753\n",
            "Epoch [1/2], Step [2520/15231], Loss: 0.4313\n",
            "Epoch [1/2], Step [2540/15231], Loss: 0.4793\n",
            "Epoch [1/2], Step [2560/15231], Loss: 0.3963\n",
            "Epoch [1/2], Step [2580/15231], Loss: 0.4269\n",
            "Epoch [1/2], Step [2600/15231], Loss: 0.4351\n",
            "Epoch [1/2], Step [2620/15231], Loss: 0.4133\n",
            "Epoch [1/2], Step [2640/15231], Loss: 0.4060\n",
            "Epoch [1/2], Step [2660/15231], Loss: 0.4344\n",
            "Epoch [1/2], Step [2680/15231], Loss: 0.3929\n",
            "Epoch [1/2], Step [2700/15231], Loss: 0.3977\n",
            "Epoch [1/2], Step [2720/15231], Loss: 0.4495\n",
            "Epoch [1/2], Step [2740/15231], Loss: 0.4120\n",
            "Epoch [1/2], Step [2760/15231], Loss: 0.3981\n",
            "Epoch [1/2], Step [2780/15231], Loss: 0.3943\n",
            "Epoch [1/2], Step [2800/15231], Loss: 0.4172\n",
            "Epoch [1/2], Step [2820/15231], Loss: 0.4385\n",
            "Epoch [1/2], Step [2840/15231], Loss: 0.4237\n",
            "Epoch [1/2], Step [2860/15231], Loss: 0.3724\n",
            "Epoch [1/2], Step [2880/15231], Loss: 0.3769\n",
            "Epoch [1/2], Step [2900/15231], Loss: 0.4031\n",
            "Epoch [1/2], Step [2920/15231], Loss: 0.4225\n",
            "Epoch [1/2], Step [2940/15231], Loss: 0.3790\n",
            "Epoch [1/2], Step [2960/15231], Loss: 0.4025\n",
            "Epoch [1/2], Step [2980/15231], Loss: 0.4074\n",
            "Epoch [1/2], Step [3000/15231], Loss: 0.3796\n",
            "Epoch [1/2], Step [3020/15231], Loss: 0.4197\n",
            "Epoch [1/2], Step [3040/15231], Loss: 0.3888\n",
            "Epoch [1/2], Step [3060/15231], Loss: 0.3759\n",
            "Epoch [1/2], Step [3080/15231], Loss: 0.3968\n",
            "Epoch [1/2], Step [3100/15231], Loss: 0.3741\n",
            "Epoch [1/2], Step [3120/15231], Loss: 0.4063\n",
            "Epoch [1/2], Step [3140/15231], Loss: 0.3721\n",
            "Epoch [1/2], Step [3160/15231], Loss: 0.4085\n",
            "Epoch [1/2], Step [3180/15231], Loss: 0.3800\n",
            "Epoch [1/2], Step [3200/15231], Loss: 0.4079\n",
            "Epoch [1/2], Step [3220/15231], Loss: 0.3608\n",
            "Epoch [1/2], Step [3240/15231], Loss: 0.3677\n",
            "Epoch [1/2], Step [3260/15231], Loss: 0.3682\n",
            "Epoch [1/2], Step [3280/15231], Loss: 0.3607\n",
            "Epoch [1/2], Step [3300/15231], Loss: 0.3799\n",
            "Epoch [1/2], Step [3320/15231], Loss: 0.3782\n",
            "Epoch [1/2], Step [3340/15231], Loss: 0.3856\n",
            "Epoch [1/2], Step [3360/15231], Loss: 0.3929\n",
            "Epoch [1/2], Step [3380/15231], Loss: 0.3984\n",
            "Epoch [1/2], Step [3400/15231], Loss: 0.3558\n",
            "Epoch [1/2], Step [3420/15231], Loss: 0.4225\n",
            "Epoch [1/2], Step [3440/15231], Loss: 0.3735\n",
            "Epoch [1/2], Step [3460/15231], Loss: 0.3642\n",
            "Epoch [1/2], Step [3480/15231], Loss: 0.3440\n",
            "Epoch [1/2], Step [3500/15231], Loss: 0.4031\n",
            "Epoch [1/2], Step [3520/15231], Loss: 0.3743\n",
            "Epoch [1/2], Step [3540/15231], Loss: 0.3817\n",
            "Epoch [1/2], Step [3560/15231], Loss: 0.3934\n",
            "Epoch [1/2], Step [3580/15231], Loss: 0.4310\n",
            "Epoch [1/2], Step [3600/15231], Loss: 0.3541\n",
            "Epoch [1/2], Step [3620/15231], Loss: 0.3654\n",
            "Epoch [1/2], Step [3640/15231], Loss: 0.3777\n",
            "Epoch [1/2], Step [3660/15231], Loss: 0.3799\n",
            "Epoch [1/2], Step [3680/15231], Loss: 0.4229\n",
            "Epoch [1/2], Step [3700/15231], Loss: 0.3872\n",
            "Epoch [1/2], Step [3720/15231], Loss: 0.4199\n",
            "Epoch [1/2], Step [3740/15231], Loss: 0.3368\n",
            "Epoch [1/2], Step [3760/15231], Loss: 0.3853\n",
            "Epoch [1/2], Step [3780/15231], Loss: 0.3518\n",
            "Epoch [1/2], Step [3800/15231], Loss: 0.3872\n",
            "Epoch [1/2], Step [3820/15231], Loss: 0.3742\n",
            "Epoch [1/2], Step [3840/15231], Loss: 0.4146\n",
            "Epoch [1/2], Step [3860/15231], Loss: 0.4042\n",
            "Epoch [1/2], Step [3880/15231], Loss: 0.3833\n",
            "Epoch [1/2], Step [3900/15231], Loss: 0.4113\n",
            "Epoch [1/2], Step [3920/15231], Loss: 0.3874\n",
            "Epoch [1/2], Step [3940/15231], Loss: 0.3883\n",
            "Epoch [1/2], Step [3960/15231], Loss: 0.3834\n",
            "Epoch [1/2], Step [3980/15231], Loss: 0.3660\n",
            "Epoch [1/2], Step [4000/15231], Loss: 0.3591\n",
            "Epoch [1/2], Step [4020/15231], Loss: 0.4058\n",
            "Epoch [1/2], Step [4040/15231], Loss: 0.3477\n",
            "Epoch [1/2], Step [4060/15231], Loss: 0.3743\n",
            "Epoch [1/2], Step [4080/15231], Loss: 0.4060\n",
            "Epoch [1/2], Step [4100/15231], Loss: 0.3981\n",
            "Epoch [1/2], Step [4120/15231], Loss: 0.3680\n",
            "Epoch [1/2], Step [4140/15231], Loss: 0.3727\n",
            "Epoch [1/2], Step [4160/15231], Loss: 0.3528\n",
            "Epoch [1/2], Step [4180/15231], Loss: 0.3513\n",
            "Epoch [1/2], Step [4200/15231], Loss: 0.3904\n",
            "Epoch [1/2], Step [4220/15231], Loss: 0.3726\n",
            "Epoch [1/2], Step [4240/15231], Loss: 0.3389\n",
            "Epoch [1/2], Step [4260/15231], Loss: 0.3927\n",
            "Epoch [1/2], Step [4280/15231], Loss: 0.4455\n",
            "Epoch [1/2], Step [4300/15231], Loss: 0.3430\n",
            "Epoch [1/2], Step [4320/15231], Loss: 0.3253\n",
            "Epoch [1/2], Step [4340/15231], Loss: 0.3761\n",
            "Epoch [1/2], Step [4360/15231], Loss: 0.3348\n",
            "Epoch [1/2], Step [4380/15231], Loss: 0.3423\n",
            "Epoch [1/2], Step [4400/15231], Loss: 0.3977\n",
            "Epoch [1/2], Step [4420/15231], Loss: 0.3578\n",
            "Epoch [1/2], Step [4440/15231], Loss: 0.3580\n",
            "Epoch [1/2], Step [4460/15231], Loss: 0.3383\n",
            "Epoch [1/2], Step [4480/15231], Loss: 0.3666\n",
            "Epoch [1/2], Step [4500/15231], Loss: 0.3650\n",
            "Epoch [1/2], Step [4520/15231], Loss: 0.3343\n",
            "Epoch [1/2], Step [4540/15231], Loss: 0.3744\n",
            "Epoch [1/2], Step [4560/15231], Loss: 0.4001\n",
            "Epoch [1/2], Step [4580/15231], Loss: 0.3905\n",
            "Epoch [1/2], Step [4600/15231], Loss: 0.3863\n",
            "Epoch [1/2], Step [4620/15231], Loss: 0.3440\n",
            "Epoch [1/2], Step [4640/15231], Loss: 0.3604\n",
            "Epoch [1/2], Step [4660/15231], Loss: 0.3569\n",
            "Epoch [1/2], Step [4680/15231], Loss: 0.3675\n",
            "Epoch [1/2], Step [4700/15231], Loss: 0.3519\n",
            "Epoch [1/2], Step [4720/15231], Loss: 0.3597\n",
            "Epoch [1/2], Step [4740/15231], Loss: 0.3506\n",
            "Epoch [1/2], Step [4760/15231], Loss: 0.3571\n",
            "Epoch [1/2], Step [4780/15231], Loss: 0.3641\n",
            "Epoch [1/2], Step [4800/15231], Loss: 0.3903\n",
            "Epoch [1/2], Step [4820/15231], Loss: 0.3759\n",
            "Epoch [1/2], Step [4840/15231], Loss: 0.3549\n",
            "Epoch [1/2], Step [4860/15231], Loss: 0.3582\n",
            "Epoch [1/2], Step [4880/15231], Loss: 0.3700\n",
            "Epoch [1/2], Step [4900/15231], Loss: 0.3660\n",
            "Epoch [1/2], Step [4920/15231], Loss: 0.3722\n",
            "Epoch [1/2], Step [4940/15231], Loss: 0.3352\n",
            "Epoch [1/2], Step [4960/15231], Loss: 0.3547\n",
            "Epoch [1/2], Step [4980/15231], Loss: 0.3751\n",
            "Epoch [1/2], Step [5000/15231], Loss: 0.3174\n",
            "Epoch [1/2], Step [5020/15231], Loss: 0.3648\n",
            "Epoch [1/2], Step [5040/15231], Loss: 0.3808\n",
            "Epoch [1/2], Step [5060/15231], Loss: 0.3543\n",
            "Epoch [1/2], Step [5080/15231], Loss: 0.3963\n",
            "Epoch [1/2], Step [5100/15231], Loss: 0.3490\n",
            "Epoch [1/2], Step [5120/15231], Loss: 0.3598\n",
            "Epoch [1/2], Step [5140/15231], Loss: 0.3840\n",
            "Epoch [1/2], Step [5160/15231], Loss: 0.3073\n",
            "Epoch [1/2], Step [5180/15231], Loss: 0.3487\n",
            "Epoch [1/2], Step [5200/15231], Loss: 0.3528\n",
            "Epoch [1/2], Step [5220/15231], Loss: 0.3735\n",
            "Epoch [1/2], Step [5240/15231], Loss: 0.3986\n",
            "Epoch [1/2], Step [5260/15231], Loss: 0.3780\n",
            "Epoch [1/2], Step [5280/15231], Loss: 0.3776\n",
            "Epoch [1/2], Step [5300/15231], Loss: 0.3824\n",
            "Epoch [1/2], Step [5320/15231], Loss: 0.3546\n",
            "Epoch [1/2], Step [5340/15231], Loss: 0.3568\n",
            "Epoch [1/2], Step [5360/15231], Loss: 0.3308\n",
            "Epoch [1/2], Step [5380/15231], Loss: 0.3433\n",
            "Epoch [1/2], Step [5400/15231], Loss: 0.3946\n",
            "Epoch [1/2], Step [5420/15231], Loss: 0.3363\n",
            "Epoch [1/2], Step [5440/15231], Loss: 0.3419\n",
            "Epoch [1/2], Step [5460/15231], Loss: 0.3551\n",
            "Epoch [1/2], Step [5480/15231], Loss: 0.3112\n",
            "Epoch [1/2], Step [5500/15231], Loss: 0.3589\n",
            "Epoch [1/2], Step [5520/15231], Loss: 0.3450\n",
            "Epoch [1/2], Step [5540/15231], Loss: 0.3343\n",
            "Epoch [1/2], Step [5560/15231], Loss: 0.3937\n",
            "Epoch [1/2], Step [5580/15231], Loss: 0.3590\n",
            "Epoch [1/2], Step [5600/15231], Loss: 0.3669\n",
            "Epoch [1/2], Step [5620/15231], Loss: 0.3344\n",
            "Epoch [1/2], Step [5640/15231], Loss: 0.3562\n",
            "Epoch [1/2], Step [5660/15231], Loss: 0.3445\n",
            "Epoch [1/2], Step [5680/15231], Loss: 0.3513\n",
            "Epoch [1/2], Step [5700/15231], Loss: 0.3066\n",
            "Epoch [1/2], Step [5720/15231], Loss: 0.3501\n",
            "Epoch [1/2], Step [5740/15231], Loss: 0.3521\n",
            "Epoch [1/2], Step [5760/15231], Loss: 0.3741\n",
            "Epoch [1/2], Step [5780/15231], Loss: 0.3617\n",
            "Epoch [1/2], Step [5800/15231], Loss: 0.3470\n",
            "Epoch [1/2], Step [5820/15231], Loss: 0.3723\n",
            "Epoch [1/2], Step [5840/15231], Loss: 0.3353\n",
            "Epoch [1/2], Step [5860/15231], Loss: 0.3436\n",
            "Epoch [1/2], Step [5880/15231], Loss: 0.3675\n",
            "Epoch [1/2], Step [5900/15231], Loss: 0.3700\n",
            "Epoch [1/2], Step [5920/15231], Loss: 0.3156\n",
            "Epoch [1/2], Step [5940/15231], Loss: 0.3732\n",
            "Epoch [1/2], Step [5960/15231], Loss: 0.3257\n",
            "Epoch [1/2], Step [5980/15231], Loss: 0.3679\n",
            "Epoch [1/2], Step [6000/15231], Loss: 0.3117\n",
            "Epoch [1/2], Step [6020/15231], Loss: 0.3528\n",
            "Epoch [1/2], Step [6040/15231], Loss: 0.3342\n",
            "Epoch [1/2], Step [6060/15231], Loss: 0.3398\n",
            "Epoch [1/2], Step [6080/15231], Loss: 0.2901\n",
            "Epoch [1/2], Step [6100/15231], Loss: 0.3866\n",
            "Epoch [1/2], Step [6120/15231], Loss: 0.3888\n",
            "Epoch [1/2], Step [6140/15231], Loss: 0.3052\n",
            "Epoch [1/2], Step [6160/15231], Loss: 0.3354\n",
            "Epoch [1/2], Step [6180/15231], Loss: 0.3828\n",
            "Epoch [1/2], Step [6200/15231], Loss: 0.3298\n",
            "Epoch [1/2], Step [6220/15231], Loss: 0.3634\n",
            "Epoch [1/2], Step [6240/15231], Loss: 0.3258\n",
            "Epoch [1/2], Step [6260/15231], Loss: 0.3352\n",
            "Epoch [1/2], Step [6280/15231], Loss: 0.3264\n",
            "Epoch [1/2], Step [6300/15231], Loss: 0.3400\n",
            "Epoch [1/2], Step [6320/15231], Loss: 0.3146\n",
            "Epoch [1/2], Step [6340/15231], Loss: 0.3635\n",
            "Epoch [1/2], Step [6360/15231], Loss: 0.3394\n",
            "Epoch [1/2], Step [6380/15231], Loss: 0.3309\n",
            "Epoch [1/2], Step [6400/15231], Loss: 0.3155\n",
            "Epoch [1/2], Step [6420/15231], Loss: 0.3433\n",
            "Epoch [1/2], Step [6440/15231], Loss: 0.3184\n",
            "Epoch [1/2], Step [6460/15231], Loss: 0.3653\n",
            "Epoch [1/2], Step [6480/15231], Loss: 0.3634\n",
            "Epoch [1/2], Step [6500/15231], Loss: 0.3552\n",
            "Epoch [1/2], Step [6520/15231], Loss: 0.3598\n",
            "Epoch [1/2], Step [6540/15231], Loss: 0.3509\n",
            "Epoch [1/2], Step [6560/15231], Loss: 0.3412\n",
            "Epoch [1/2], Step [6580/15231], Loss: 0.3388\n",
            "Epoch [1/2], Step [6600/15231], Loss: 0.3168\n",
            "Epoch [1/2], Step [6620/15231], Loss: 0.3688\n",
            "Epoch [1/2], Step [6640/15231], Loss: 0.3306\n",
            "Epoch [1/2], Step [6660/15231], Loss: 0.3027\n",
            "Epoch [1/2], Step [6680/15231], Loss: 0.3652\n",
            "Epoch [1/2], Step [6700/15231], Loss: 0.3363\n",
            "Epoch [1/2], Step [6720/15231], Loss: 0.3690\n",
            "Epoch [1/2], Step [6740/15231], Loss: 0.3773\n",
            "Epoch [1/2], Step [6760/15231], Loss: 0.3497\n",
            "Epoch [1/2], Step [6780/15231], Loss: 0.3109\n",
            "Epoch [1/2], Step [6800/15231], Loss: 0.3248\n",
            "Epoch [1/2], Step [6820/15231], Loss: 0.2970\n",
            "Epoch [1/2], Step [6840/15231], Loss: 0.3390\n",
            "Epoch [1/2], Step [6860/15231], Loss: 0.3412\n",
            "Epoch [1/2], Step [6880/15231], Loss: 0.3114\n",
            "Epoch [1/2], Step [6900/15231], Loss: 0.3227\n",
            "Epoch [1/2], Step [6920/15231], Loss: 0.3409\n",
            "Epoch [1/2], Step [6940/15231], Loss: 0.3522\n",
            "Epoch [1/2], Step [6960/15231], Loss: 0.3746\n",
            "Epoch [1/2], Step [6980/15231], Loss: 0.3147\n",
            "Epoch [1/2], Step [7000/15231], Loss: 0.3369\n",
            "Epoch [1/2], Step [7020/15231], Loss: 0.3217\n",
            "Epoch [1/2], Step [7040/15231], Loss: 0.3157\n",
            "Epoch [1/2], Step [7060/15231], Loss: 0.3222\n",
            "Epoch [1/2], Step [7080/15231], Loss: 0.3198\n",
            "Epoch [1/2], Step [7100/15231], Loss: 0.3497\n",
            "Epoch [1/2], Step [7120/15231], Loss: 0.3910\n",
            "Epoch [1/2], Step [7140/15231], Loss: 0.3239\n",
            "Epoch [1/2], Step [7160/15231], Loss: 0.3551\n",
            "Epoch [1/2], Step [7180/15231], Loss: 0.3159\n",
            "Epoch [1/2], Step [7200/15231], Loss: 0.3280\n",
            "Epoch [1/2], Step [7220/15231], Loss: 0.3358\n",
            "Epoch [1/2], Step [7240/15231], Loss: 0.3304\n",
            "Epoch [1/2], Step [7260/15231], Loss: 0.3227\n",
            "Epoch [1/2], Step [7280/15231], Loss: 0.3452\n",
            "Epoch [1/2], Step [7300/15231], Loss: 0.3626\n",
            "Epoch [1/2], Step [7320/15231], Loss: 0.3458\n",
            "Epoch [1/2], Step [7340/15231], Loss: 0.3630\n",
            "Epoch [1/2], Step [7360/15231], Loss: 0.3202\n",
            "Epoch [1/2], Step [7380/15231], Loss: 0.3241\n",
            "Epoch [1/2], Step [7400/15231], Loss: 0.3504\n",
            "Epoch [1/2], Step [7420/15231], Loss: 0.3496\n",
            "Epoch [1/2], Step [7440/15231], Loss: 0.3085\n",
            "Epoch [1/2], Step [7460/15231], Loss: 0.3294\n",
            "Epoch [1/2], Step [7480/15231], Loss: 0.2884\n",
            "Epoch [1/2], Step [7500/15231], Loss: 0.3593\n",
            "Epoch [1/2], Step [7520/15231], Loss: 0.3383\n",
            "Epoch [1/2], Step [7540/15231], Loss: 0.3091\n",
            "Epoch [1/2], Step [7560/15231], Loss: 0.3089\n",
            "Epoch [1/2], Step [7580/15231], Loss: 0.3222\n",
            "Epoch [1/2], Step [7600/15231], Loss: 0.3046\n",
            "Epoch [1/2], Step [7620/15231], Loss: 0.3091\n",
            "Epoch [1/2], Step [7640/15231], Loss: 0.3261\n",
            "Epoch [1/2], Step [7660/15231], Loss: 0.3300\n",
            "Epoch [1/2], Step [7680/15231], Loss: 0.3141\n",
            "Epoch [1/2], Step [7700/15231], Loss: 0.3489\n",
            "Epoch [1/2], Step [7720/15231], Loss: 0.3325\n",
            "Epoch [1/2], Step [7740/15231], Loss: 0.3111\n",
            "Epoch [1/2], Step [7760/15231], Loss: 0.3263\n",
            "Epoch [1/2], Step [7780/15231], Loss: 0.3645\n",
            "Epoch [1/2], Step [7800/15231], Loss: 0.3394\n",
            "Epoch [1/2], Step [7820/15231], Loss: 0.3101\n",
            "Epoch [1/2], Step [7840/15231], Loss: 0.3496\n",
            "Epoch [1/2], Step [7860/15231], Loss: 0.3048\n",
            "Epoch [1/2], Step [7880/15231], Loss: 0.3123\n",
            "Epoch [1/2], Step [7900/15231], Loss: 0.3234\n",
            "Epoch [1/2], Step [7920/15231], Loss: 0.3269\n",
            "Epoch [1/2], Step [7940/15231], Loss: 0.3552\n",
            "Epoch [1/2], Step [7960/15231], Loss: 0.3449\n",
            "Epoch [1/2], Step [7980/15231], Loss: 0.3348\n",
            "Epoch [1/2], Step [8000/15231], Loss: 0.3440\n",
            "Epoch [1/2], Step [8020/15231], Loss: 0.3041\n",
            "Epoch [1/2], Step [8040/15231], Loss: 0.3025\n",
            "Epoch [1/2], Step [8060/15231], Loss: 0.3558\n",
            "Epoch [1/2], Step [8080/15231], Loss: 0.3200\n",
            "Epoch [1/2], Step [8100/15231], Loss: 0.3420\n",
            "Epoch [1/2], Step [8120/15231], Loss: 0.3408\n",
            "Epoch [1/2], Step [8140/15231], Loss: 0.2954\n",
            "Epoch [1/2], Step [8160/15231], Loss: 0.3504\n",
            "Epoch [1/2], Step [8180/15231], Loss: 0.3148\n",
            "Epoch [1/2], Step [8200/15231], Loss: 0.3681\n",
            "Epoch [1/2], Step [8220/15231], Loss: 0.3180\n",
            "Epoch [1/2], Step [8240/15231], Loss: 0.3271\n",
            "Epoch [1/2], Step [8260/15231], Loss: 0.3424\n",
            "Epoch [1/2], Step [8280/15231], Loss: 0.3089\n",
            "Epoch [1/2], Step [8300/15231], Loss: 0.3615\n",
            "Epoch [1/2], Step [8320/15231], Loss: 0.3406\n",
            "Epoch [1/2], Step [8340/15231], Loss: 0.3133\n",
            "Epoch [1/2], Step [8360/15231], Loss: 0.3616\n",
            "Epoch [1/2], Step [8380/15231], Loss: 0.3254\n",
            "Epoch [1/2], Step [8400/15231], Loss: 0.2962\n",
            "Epoch [1/2], Step [8420/15231], Loss: 0.3545\n",
            "Epoch [1/2], Step [8440/15231], Loss: 0.3292\n",
            "Epoch [1/2], Step [8460/15231], Loss: 0.2936\n",
            "Epoch [1/2], Step [8480/15231], Loss: 0.3398\n",
            "Epoch [1/2], Step [8500/15231], Loss: 0.2982\n",
            "Epoch [1/2], Step [8520/15231], Loss: 0.2918\n",
            "Epoch [1/2], Step [8540/15231], Loss: 0.3248\n",
            "Epoch [1/2], Step [8560/15231], Loss: 0.3177\n",
            "Epoch [1/2], Step [8580/15231], Loss: 0.3097\n",
            "Epoch [1/2], Step [8600/15231], Loss: 0.3263\n",
            "Epoch [1/2], Step [8620/15231], Loss: 0.3339\n",
            "Epoch [1/2], Step [8640/15231], Loss: 0.3258\n",
            "Epoch [1/2], Step [8660/15231], Loss: 0.3075\n",
            "Epoch [1/2], Step [8680/15231], Loss: 0.2717\n",
            "Epoch [1/2], Step [8700/15231], Loss: 0.2737\n",
            "Epoch [1/2], Step [8720/15231], Loss: 0.3501\n",
            "Epoch [1/2], Step [8740/15231], Loss: 0.3242\n",
            "Epoch [1/2], Step [8760/15231], Loss: 0.3456\n",
            "Epoch [1/2], Step [8780/15231], Loss: 0.3134\n",
            "Epoch [1/2], Step [8800/15231], Loss: 0.3304\n",
            "Epoch [1/2], Step [8820/15231], Loss: 0.3529\n",
            "Epoch [1/2], Step [8840/15231], Loss: 0.3084\n",
            "Epoch [1/2], Step [8860/15231], Loss: 0.3206\n",
            "Epoch [1/2], Step [8880/15231], Loss: 0.3453\n",
            "Epoch [1/2], Step [8900/15231], Loss: 0.3163\n",
            "Epoch [1/2], Step [8920/15231], Loss: 0.3152\n",
            "Epoch [1/2], Step [8940/15231], Loss: 0.2809\n",
            "Epoch [1/2], Step [8960/15231], Loss: 0.3596\n",
            "Epoch [1/2], Step [8980/15231], Loss: 0.3459\n",
            "Epoch [1/2], Step [9000/15231], Loss: 0.3182\n",
            "Epoch [1/2], Step [9020/15231], Loss: 0.2945\n",
            "Epoch [1/2], Step [9040/15231], Loss: 0.2928\n",
            "Epoch [1/2], Step [9060/15231], Loss: 0.2948\n",
            "Epoch [1/2], Step [9080/15231], Loss: 0.3252\n",
            "Epoch [1/2], Step [9100/15231], Loss: 0.3304\n",
            "Epoch [1/2], Step [9120/15231], Loss: 0.3313\n",
            "Epoch [1/2], Step [9140/15231], Loss: 0.3489\n",
            "Epoch [1/2], Step [9160/15231], Loss: 0.3302\n",
            "Epoch [1/2], Step [9180/15231], Loss: 0.3396\n",
            "Epoch [1/2], Step [9200/15231], Loss: 0.3119\n",
            "Epoch [1/2], Step [9220/15231], Loss: 0.3098\n",
            "Epoch [1/2], Step [9240/15231], Loss: 0.2864\n",
            "Epoch [1/2], Step [9260/15231], Loss: 0.3471\n",
            "Epoch [1/2], Step [9280/15231], Loss: 0.2895\n",
            "Epoch [1/2], Step [9300/15231], Loss: 0.3140\n",
            "Epoch [1/2], Step [9320/15231], Loss: 0.3256\n",
            "Epoch [1/2], Step [9340/15231], Loss: 0.3381\n",
            "Epoch [1/2], Step [9360/15231], Loss: 0.3234\n",
            "Epoch [1/2], Step [9380/15231], Loss: 0.3310\n",
            "Epoch [1/2], Step [9400/15231], Loss: 0.2857\n",
            "Epoch [1/2], Step [9420/15231], Loss: 0.3251\n",
            "Epoch [1/2], Step [9440/15231], Loss: 0.2770\n",
            "Epoch [1/2], Step [9460/15231], Loss: 0.3159\n",
            "Epoch [1/2], Step [9480/15231], Loss: 0.3231\n",
            "Epoch [1/2], Step [9500/15231], Loss: 0.3260\n",
            "Epoch [1/2], Step [9520/15231], Loss: 0.2995\n",
            "Epoch [1/2], Step [9540/15231], Loss: 0.3116\n",
            "Epoch [1/2], Step [9560/15231], Loss: 0.2848\n",
            "Epoch [1/2], Step [9580/15231], Loss: 0.3274\n",
            "Epoch [1/2], Step [9600/15231], Loss: 0.3087\n",
            "Epoch [1/2], Step [9620/15231], Loss: 0.3093\n",
            "Epoch [1/2], Step [9640/15231], Loss: 0.2943\n",
            "Epoch [1/2], Step [9660/15231], Loss: 0.3423\n",
            "Epoch [1/2], Step [9680/15231], Loss: 0.3242\n",
            "Epoch [1/2], Step [9700/15231], Loss: 0.3048\n",
            "Epoch [1/2], Step [9720/15231], Loss: 0.3097\n",
            "Epoch [1/2], Step [9740/15231], Loss: 0.3254\n",
            "Epoch [1/2], Step [9760/15231], Loss: 0.3330\n",
            "Epoch [1/2], Step [9780/15231], Loss: 0.3112\n",
            "Epoch [1/2], Step [9800/15231], Loss: 0.3079\n",
            "Epoch [1/2], Step [9820/15231], Loss: 0.3596\n",
            "Epoch [1/2], Step [9840/15231], Loss: 0.2965\n",
            "Epoch [1/2], Step [9860/15231], Loss: 0.3118\n",
            "Epoch [1/2], Step [9880/15231], Loss: 0.3504\n",
            "Epoch [1/2], Step [9900/15231], Loss: 0.3429\n",
            "Epoch [1/2], Step [9920/15231], Loss: 0.3171\n",
            "Epoch [1/2], Step [9940/15231], Loss: 0.2800\n",
            "Epoch [1/2], Step [9960/15231], Loss: 0.2884\n",
            "Epoch [1/2], Step [9980/15231], Loss: 0.3065\n",
            "Epoch [1/2], Step [10000/15231], Loss: 0.3673\n",
            "Epoch [1/2], Step [10020/15231], Loss: 0.3373\n",
            "Epoch [1/2], Step [10040/15231], Loss: 0.3340\n",
            "Epoch [1/2], Step [10060/15231], Loss: 0.3360\n",
            "Epoch [1/2], Step [10080/15231], Loss: 0.2984\n",
            "Epoch [1/2], Step [10100/15231], Loss: 0.3142\n",
            "Epoch [1/2], Step [10120/15231], Loss: 0.2976\n",
            "Epoch [1/2], Step [10140/15231], Loss: 0.2920\n",
            "Epoch [1/2], Step [10160/15231], Loss: 0.3017\n",
            "Epoch [1/2], Step [10180/15231], Loss: 0.3402\n",
            "Epoch [1/2], Step [10200/15231], Loss: 0.3155\n",
            "Epoch [1/2], Step [10220/15231], Loss: 0.3198\n",
            "Epoch [1/2], Step [10240/15231], Loss: 0.2912\n",
            "Epoch [1/2], Step [10260/15231], Loss: 0.2817\n",
            "Epoch [1/2], Step [10280/15231], Loss: 0.2913\n",
            "Epoch [1/2], Step [10300/15231], Loss: 0.2744\n",
            "Epoch [1/2], Step [10320/15231], Loss: 0.2741\n",
            "Epoch [1/2], Step [10340/15231], Loss: 0.3255\n",
            "Epoch [1/2], Step [10360/15231], Loss: 0.3253\n",
            "Epoch [1/2], Step [10380/15231], Loss: 0.3145\n",
            "Epoch [1/2], Step [10400/15231], Loss: 0.2633\n",
            "Epoch [1/2], Step [10420/15231], Loss: 0.3071\n",
            "Epoch [1/2], Step [10440/15231], Loss: 0.3369\n",
            "Epoch [1/2], Step [10460/15231], Loss: 0.3012\n",
            "Epoch [1/2], Step [10480/15231], Loss: 0.3361\n",
            "Epoch [1/2], Step [10500/15231], Loss: 0.3339\n",
            "Epoch [1/2], Step [10520/15231], Loss: 0.3094\n",
            "Epoch [1/2], Step [10540/15231], Loss: 0.2819\n",
            "Epoch [1/2], Step [10560/15231], Loss: 0.2858\n",
            "Epoch [1/2], Step [10580/15231], Loss: 0.3379\n",
            "Epoch [1/2], Step [10600/15231], Loss: 0.3601\n",
            "Epoch [1/2], Step [10620/15231], Loss: 0.3307\n",
            "Epoch [1/2], Step [10640/15231], Loss: 0.3166\n",
            "Epoch [1/2], Step [10660/15231], Loss: 0.3214\n",
            "Epoch [1/2], Step [10680/15231], Loss: 0.2718\n",
            "Epoch [1/2], Step [10700/15231], Loss: 0.3149\n",
            "Epoch [1/2], Step [10720/15231], Loss: 0.2836\n",
            "Epoch [1/2], Step [10740/15231], Loss: 0.3078\n",
            "Epoch [1/2], Step [10760/15231], Loss: 0.3422\n",
            "Epoch [1/2], Step [10780/15231], Loss: 0.3164\n",
            "Epoch [1/2], Step [10800/15231], Loss: 0.2894\n",
            "Epoch [1/2], Step [10820/15231], Loss: 0.3090\n",
            "Epoch [1/2], Step [10840/15231], Loss: 0.3237\n",
            "Epoch [1/2], Step [10860/15231], Loss: 0.3031\n",
            "Epoch [1/2], Step [10880/15231], Loss: 0.2961\n",
            "Epoch [1/2], Step [10900/15231], Loss: 0.3240\n",
            "Epoch [1/2], Step [10920/15231], Loss: 0.2867\n",
            "Epoch [1/2], Step [10940/15231], Loss: 0.3183\n",
            "Epoch [1/2], Step [10960/15231], Loss: 0.3291\n",
            "Epoch [1/2], Step [10980/15231], Loss: 0.3224\n",
            "Epoch [1/2], Step [11000/15231], Loss: 0.3094\n",
            "Epoch [1/2], Step [11020/15231], Loss: 0.3266\n",
            "Epoch [1/2], Step [11040/15231], Loss: 0.3484\n",
            "Epoch [1/2], Step [11060/15231], Loss: 0.3074\n",
            "Epoch [1/2], Step [11080/15231], Loss: 0.3718\n",
            "Epoch [1/2], Step [11100/15231], Loss: 0.3344\n",
            "Epoch [1/2], Step [11120/15231], Loss: 0.3462\n",
            "Epoch [1/2], Step [11140/15231], Loss: 0.2940\n",
            "Epoch [1/2], Step [11160/15231], Loss: 0.3126\n",
            "Epoch [1/2], Step [11180/15231], Loss: 0.3098\n",
            "Epoch [1/2], Step [11200/15231], Loss: 0.3091\n",
            "Epoch [1/2], Step [11220/15231], Loss: 0.3081\n",
            "Epoch [1/2], Step [11240/15231], Loss: 0.3125\n",
            "Epoch [1/2], Step [11260/15231], Loss: 0.3005\n",
            "Epoch [1/2], Step [11280/15231], Loss: 0.3287\n",
            "Epoch [1/2], Step [11300/15231], Loss: 0.2920\n",
            "Epoch [1/2], Step [11320/15231], Loss: 0.2820\n",
            "Epoch [1/2], Step [11340/15231], Loss: 0.3032\n",
            "Epoch [1/2], Step [11360/15231], Loss: 0.3382\n",
            "Epoch [1/2], Step [11380/15231], Loss: 0.2790\n",
            "Epoch [1/2], Step [11400/15231], Loss: 0.3521\n",
            "Epoch [1/2], Step [11420/15231], Loss: 0.3010\n",
            "Epoch [1/2], Step [11440/15231], Loss: 0.3352\n",
            "Epoch [1/2], Step [11460/15231], Loss: 0.3153\n",
            "Epoch [1/2], Step [11480/15231], Loss: 0.2846\n",
            "Epoch [1/2], Step [11500/15231], Loss: 0.3178\n",
            "Epoch [1/2], Step [11520/15231], Loss: 0.3112\n",
            "Epoch [1/2], Step [11540/15231], Loss: 0.3221\n",
            "Epoch [1/2], Step [11560/15231], Loss: 0.2927\n",
            "Epoch [1/2], Step [11580/15231], Loss: 0.3043\n",
            "Epoch [1/2], Step [11600/15231], Loss: 0.3040\n",
            "Epoch [1/2], Step [11620/15231], Loss: 0.3036\n",
            "Epoch [1/2], Step [11640/15231], Loss: 0.2756\n",
            "Epoch [1/2], Step [11660/15231], Loss: 0.3361\n",
            "Epoch [1/2], Step [11680/15231], Loss: 0.3121\n",
            "Epoch [1/2], Step [11700/15231], Loss: 0.3158\n",
            "Epoch [1/2], Step [11720/15231], Loss: 0.3212\n",
            "Epoch [1/2], Step [11740/15231], Loss: 0.3110\n",
            "Epoch [1/2], Step [11760/15231], Loss: 0.3012\n",
            "Epoch [1/2], Step [11780/15231], Loss: 0.3049\n",
            "Epoch [1/2], Step [11800/15231], Loss: 0.3146\n",
            "Epoch [1/2], Step [11820/15231], Loss: 0.3223\n",
            "Epoch [1/2], Step [11840/15231], Loss: 0.3557\n",
            "Epoch [1/2], Step [11860/15231], Loss: 0.3302\n",
            "Epoch [1/2], Step [11880/15231], Loss: 0.3152\n",
            "Epoch [1/2], Step [11900/15231], Loss: 0.2721\n",
            "Epoch [1/2], Step [11920/15231], Loss: 0.3011\n",
            "Epoch [1/2], Step [11940/15231], Loss: 0.2819\n",
            "Epoch [1/2], Step [11960/15231], Loss: 0.3221\n",
            "Epoch [1/2], Step [11980/15231], Loss: 0.2874\n",
            "Epoch [1/2], Step [12000/15231], Loss: 0.2950\n",
            "Epoch [1/2], Step [12020/15231], Loss: 0.2942\n",
            "Epoch [1/2], Step [12040/15231], Loss: 0.3211\n",
            "Epoch [1/2], Step [12060/15231], Loss: 0.3207\n",
            "Epoch [1/2], Step [12080/15231], Loss: 0.3266\n",
            "Epoch [1/2], Step [12100/15231], Loss: 0.3067\n",
            "Epoch [1/2], Step [12120/15231], Loss: 0.2888\n",
            "Epoch [1/2], Step [12140/15231], Loss: 0.3313\n",
            "Epoch [1/2], Step [12160/15231], Loss: 0.3351\n",
            "Epoch [1/2], Step [12180/15231], Loss: 0.3045\n",
            "Epoch [1/2], Step [12200/15231], Loss: 0.2720\n",
            "Epoch [1/2], Step [12220/15231], Loss: 0.3178\n",
            "Epoch [1/2], Step [12240/15231], Loss: 0.3261\n",
            "Epoch [1/2], Step [12260/15231], Loss: 0.2830\n",
            "Epoch [1/2], Step [12280/15231], Loss: 0.3462\n",
            "Epoch [1/2], Step [12300/15231], Loss: 0.2831\n",
            "Epoch [1/2], Step [12320/15231], Loss: 0.2812\n",
            "Epoch [1/2], Step [12340/15231], Loss: 0.2555\n",
            "Epoch [1/2], Step [12360/15231], Loss: 0.3033\n",
            "Epoch [1/2], Step [12380/15231], Loss: 0.3031\n",
            "Epoch [1/2], Step [12400/15231], Loss: 0.2646\n",
            "Epoch [1/2], Step [12420/15231], Loss: 0.3010\n",
            "Epoch [1/2], Step [12440/15231], Loss: 0.2818\n",
            "Epoch [1/2], Step [12460/15231], Loss: 0.2914\n",
            "Epoch [1/2], Step [12480/15231], Loss: 0.2931\n",
            "Epoch [1/2], Step [12500/15231], Loss: 0.2804\n",
            "Epoch [1/2], Step [12520/15231], Loss: 0.3116\n",
            "Epoch [1/2], Step [12540/15231], Loss: 0.2855\n",
            "Epoch [1/2], Step [12560/15231], Loss: 0.2770\n",
            "Epoch [1/2], Step [12580/15231], Loss: 0.2983\n",
            "Epoch [1/2], Step [12600/15231], Loss: 0.3212\n",
            "Epoch [1/2], Step [12620/15231], Loss: 0.3085\n",
            "Epoch [1/2], Step [12640/15231], Loss: 0.2852\n",
            "Epoch [1/2], Step [12660/15231], Loss: 0.3357\n",
            "Epoch [1/2], Step [12680/15231], Loss: 0.3103\n",
            "Epoch [1/2], Step [12700/15231], Loss: 0.3233\n",
            "Epoch [1/2], Step [12720/15231], Loss: 0.3137\n",
            "Epoch [1/2], Step [12740/15231], Loss: 0.3419\n",
            "Epoch [1/2], Step [12760/15231], Loss: 0.3166\n",
            "Epoch [1/2], Step [12780/15231], Loss: 0.3188\n",
            "Epoch [1/2], Step [12800/15231], Loss: 0.2733\n",
            "Epoch [1/2], Step [12820/15231], Loss: 0.3042\n",
            "Epoch [1/2], Step [12840/15231], Loss: 0.2766\n",
            "Epoch [1/2], Step [12860/15231], Loss: 0.3076\n",
            "Epoch [1/2], Step [12880/15231], Loss: 0.2763\n",
            "Epoch [1/2], Step [12900/15231], Loss: 0.2972\n",
            "Epoch [1/2], Step [12920/15231], Loss: 0.3095\n",
            "Epoch [1/2], Step [12940/15231], Loss: 0.3111\n",
            "Epoch [1/2], Step [12960/15231], Loss: 0.2947\n",
            "Epoch [1/2], Step [12980/15231], Loss: 0.3045\n",
            "Epoch [1/2], Step [13000/15231], Loss: 0.2761\n",
            "Epoch [1/2], Step [13020/15231], Loss: 0.3004\n",
            "Epoch [1/2], Step [13040/15231], Loss: 0.2998\n",
            "Epoch [1/2], Step [13060/15231], Loss: 0.3126\n",
            "Epoch [1/2], Step [13080/15231], Loss: 0.3036\n",
            "Epoch [1/2], Step [13100/15231], Loss: 0.3530\n",
            "Epoch [1/2], Step [13120/15231], Loss: 0.2799\n",
            "Epoch [1/2], Step [13140/15231], Loss: 0.2588\n",
            "Epoch [1/2], Step [13160/15231], Loss: 0.2997\n",
            "Epoch [1/2], Step [13180/15231], Loss: 0.2758\n",
            "Epoch [1/2], Step [13200/15231], Loss: 0.3038\n",
            "Epoch [1/2], Step [13220/15231], Loss: 0.2658\n",
            "Epoch [1/2], Step [13240/15231], Loss: 0.3172\n",
            "Epoch [1/2], Step [13260/15231], Loss: 0.3141\n",
            "Epoch [1/2], Step [13280/15231], Loss: 0.3689\n",
            "Epoch [1/2], Step [13300/15231], Loss: 0.2888\n",
            "Epoch [1/2], Step [13320/15231], Loss: 0.2778\n",
            "Epoch [1/2], Step [13340/15231], Loss: 0.2859\n",
            "Epoch [1/2], Step [13360/15231], Loss: 0.3427\n",
            "Epoch [1/2], Step [13380/15231], Loss: 0.2808\n",
            "Epoch [1/2], Step [13400/15231], Loss: 0.2882\n",
            "Epoch [1/2], Step [13420/15231], Loss: 0.2482\n",
            "Epoch [1/2], Step [13440/15231], Loss: 0.3341\n",
            "Epoch [1/2], Step [13460/15231], Loss: 0.2994\n",
            "Epoch [1/2], Step [13480/15231], Loss: 0.3056\n",
            "Epoch [1/2], Step [13500/15231], Loss: 0.3338\n",
            "Epoch [1/2], Step [13520/15231], Loss: 0.3066\n",
            "Epoch [1/2], Step [13540/15231], Loss: 0.3368\n",
            "Epoch [1/2], Step [13560/15231], Loss: 0.2612\n",
            "Epoch [1/2], Step [13580/15231], Loss: 0.2938\n",
            "Epoch [1/2], Step [13600/15231], Loss: 0.2865\n",
            "Epoch [1/2], Step [13620/15231], Loss: 0.3242\n",
            "Epoch [1/2], Step [13640/15231], Loss: 0.2753\n",
            "Epoch [1/2], Step [13660/15231], Loss: 0.2919\n",
            "Epoch [1/2], Step [13680/15231], Loss: 0.3001\n",
            "Epoch [1/2], Step [13700/15231], Loss: 0.2597\n",
            "Epoch [1/2], Step [13720/15231], Loss: 0.2719\n",
            "Epoch [1/2], Step [13740/15231], Loss: 0.2862\n",
            "Epoch [1/2], Step [13760/15231], Loss: 0.2986\n",
            "Epoch [1/2], Step [13780/15231], Loss: 0.2757\n",
            "Epoch [1/2], Step [13800/15231], Loss: 0.2742\n",
            "Epoch [1/2], Step [13820/15231], Loss: 0.3306\n",
            "Epoch [1/2], Step [13840/15231], Loss: 0.2824\n",
            "Epoch [1/2], Step [13860/15231], Loss: 0.3090\n",
            "Epoch [1/2], Step [13880/15231], Loss: 0.3338\n",
            "Epoch [1/2], Step [13900/15231], Loss: 0.3214\n",
            "Epoch [1/2], Step [13920/15231], Loss: 0.2902\n",
            "Epoch [1/2], Step [13940/15231], Loss: 0.2596\n",
            "Epoch [1/2], Step [13960/15231], Loss: 0.2894\n",
            "Epoch [1/2], Step [13980/15231], Loss: 0.2945\n",
            "Epoch [1/2], Step [14000/15231], Loss: 0.2966\n",
            "Epoch [1/2], Step [14020/15231], Loss: 0.2762\n",
            "Epoch [1/2], Step [14040/15231], Loss: 0.2758\n",
            "Epoch [1/2], Step [14060/15231], Loss: 0.3270\n",
            "Epoch [1/2], Step [14080/15231], Loss: 0.2911\n",
            "Epoch [1/2], Step [14100/15231], Loss: 0.2974\n",
            "Epoch [1/2], Step [14120/15231], Loss: 0.2555\n",
            "Epoch [1/2], Step [14140/15231], Loss: 0.3192\n",
            "Epoch [1/2], Step [14160/15231], Loss: 0.2882\n",
            "Epoch [1/2], Step [14180/15231], Loss: 0.2931\n",
            "Epoch [1/2], Step [14200/15231], Loss: 0.3078\n",
            "Epoch [1/2], Step [14220/15231], Loss: 0.3029\n",
            "Epoch [1/2], Step [14240/15231], Loss: 0.2813\n",
            "Epoch [1/2], Step [14260/15231], Loss: 0.3059\n",
            "Epoch [1/2], Step [14280/15231], Loss: 0.2490\n",
            "Epoch [1/2], Step [14300/15231], Loss: 0.2891\n",
            "Epoch [1/2], Step [14320/15231], Loss: 0.2901\n",
            "Epoch [1/2], Step [14340/15231], Loss: 0.3157\n",
            "Epoch [1/2], Step [14360/15231], Loss: 0.3013\n",
            "Epoch [1/2], Step [14380/15231], Loss: 0.2622\n",
            "Epoch [1/2], Step [14400/15231], Loss: 0.3082\n",
            "Epoch [1/2], Step [14420/15231], Loss: 0.3084\n",
            "Epoch [1/2], Step [14440/15231], Loss: 0.3072\n",
            "Epoch [1/2], Step [14460/15231], Loss: 0.2693\n",
            "Epoch [1/2], Step [14480/15231], Loss: 0.3094\n",
            "Epoch [1/2], Step [14500/15231], Loss: 0.2744\n",
            "Epoch [1/2], Step [14520/15231], Loss: 0.2832\n",
            "Epoch [1/2], Step [14540/15231], Loss: 0.2995\n",
            "Epoch [1/2], Step [14560/15231], Loss: 0.2832\n",
            "Epoch [1/2], Step [14580/15231], Loss: 0.3183\n",
            "Epoch [1/2], Step [14600/15231], Loss: 0.3230\n",
            "Epoch [1/2], Step [14620/15231], Loss: 0.3112\n",
            "Epoch [1/2], Step [14640/15231], Loss: 0.3241\n",
            "Epoch [1/2], Step [14660/15231], Loss: 0.2987\n",
            "Epoch [1/2], Step [14680/15231], Loss: 0.2779\n",
            "Epoch [1/2], Step [14700/15231], Loss: 0.2790\n",
            "Epoch [1/2], Step [14720/15231], Loss: 0.2746\n",
            "Epoch [1/2], Step [14740/15231], Loss: 0.2932\n",
            "Epoch [1/2], Step [14760/15231], Loss: 0.3207\n",
            "Epoch [1/2], Step [14780/15231], Loss: 0.2936\n",
            "Epoch [1/2], Step [14800/15231], Loss: 0.3023\n",
            "Epoch [1/2], Step [14820/15231], Loss: 0.2796\n",
            "Epoch [1/2], Step [14840/15231], Loss: 0.3149\n",
            "Epoch [1/2], Step [14860/15231], Loss: 0.2767\n",
            "Epoch [1/2], Step [14880/15231], Loss: 0.3568\n",
            "Epoch [1/2], Step [14900/15231], Loss: 0.2807\n",
            "Epoch [1/2], Step [14920/15231], Loss: 0.2972\n",
            "Epoch [1/2], Step [14940/15231], Loss: 0.2969\n",
            "Epoch [1/2], Step [14960/15231], Loss: 0.2661\n",
            "Epoch [1/2], Step [14980/15231], Loss: 0.3144\n",
            "Epoch [1/2], Step [15000/15231], Loss: 0.3234\n",
            "Epoch [1/2], Step [15020/15231], Loss: 0.2835\n",
            "Epoch [1/2], Step [15040/15231], Loss: 0.3250\n",
            "Epoch [1/2], Step [15060/15231], Loss: 0.2961\n",
            "Epoch [1/2], Step [15080/15231], Loss: 0.2716\n",
            "Epoch [1/2], Step [15100/15231], Loss: 0.2896\n",
            "Epoch [1/2], Step [15120/15231], Loss: 0.2980\n",
            "Epoch [1/2], Step [15140/15231], Loss: 0.2306\n",
            "Epoch [1/2], Step [15160/15231], Loss: 0.2591\n",
            "Epoch [1/2], Step [15180/15231], Loss: 0.2945\n",
            "Epoch [1/2], Step [15200/15231], Loss: 0.2932\n",
            "Epoch [1/2], Step [15220/15231], Loss: 0.2548\n",
            "Epoch [1/2] | Train Loss: 0.3593 | Train Acc: 87.44% | Test Loss: 0.7125 | Test Acc: 73.00%\n",
            "Epoch [2/2], Step [20/15231], Loss: 0.2724\n",
            "Epoch [2/2], Step [40/15231], Loss: 0.3112\n",
            "Epoch [2/2], Step [60/15231], Loss: 0.2845\n",
            "Epoch [2/2], Step [80/15231], Loss: 0.2695\n",
            "Epoch [2/2], Step [100/15231], Loss: 0.3412\n",
            "Epoch [2/2], Step [120/15231], Loss: 0.2794\n",
            "Epoch [2/2], Step [140/15231], Loss: 0.3337\n",
            "Epoch [2/2], Step [160/15231], Loss: 0.2419\n",
            "Epoch [2/2], Step [180/15231], Loss: 0.2794\n",
            "Epoch [2/2], Step [200/15231], Loss: 0.2637\n",
            "Epoch [2/2], Step [220/15231], Loss: 0.2614\n",
            "Epoch [2/2], Step [240/15231], Loss: 0.2817\n",
            "Epoch [2/2], Step [260/15231], Loss: 0.3270\n",
            "Epoch [2/2], Step [280/15231], Loss: 0.2628\n",
            "Epoch [2/2], Step [300/15231], Loss: 0.3074\n",
            "Epoch [2/2], Step [320/15231], Loss: 0.2522\n",
            "Epoch [2/2], Step [340/15231], Loss: 0.2754\n",
            "Epoch [2/2], Step [360/15231], Loss: 0.2551\n",
            "Epoch [2/2], Step [380/15231], Loss: 0.2708\n",
            "Epoch [2/2], Step [400/15231], Loss: 0.2964\n",
            "Epoch [2/2], Step [420/15231], Loss: 0.2845\n",
            "Epoch [2/2], Step [440/15231], Loss: 0.2989\n",
            "Epoch [2/2], Step [460/15231], Loss: 0.2724\n",
            "Epoch [2/2], Step [480/15231], Loss: 0.2963\n",
            "Epoch [2/2], Step [500/15231], Loss: 0.2884\n",
            "Epoch [2/2], Step [520/15231], Loss: 0.2947\n",
            "Epoch [2/2], Step [540/15231], Loss: 0.3330\n",
            "Epoch [2/2], Step [560/15231], Loss: 0.2209\n",
            "Epoch [2/2], Step [580/15231], Loss: 0.3043\n",
            "Epoch [2/2], Step [600/15231], Loss: 0.2729\n",
            "Epoch [2/2], Step [620/15231], Loss: 0.2799\n",
            "Epoch [2/2], Step [640/15231], Loss: 0.2952\n",
            "Epoch [2/2], Step [660/15231], Loss: 0.3002\n",
            "Epoch [2/2], Step [680/15231], Loss: 0.2904\n",
            "Epoch [2/2], Step [700/15231], Loss: 0.2998\n",
            "Epoch [2/2], Step [720/15231], Loss: 0.2906\n",
            "Epoch [2/2], Step [740/15231], Loss: 0.2784\n",
            "Epoch [2/2], Step [760/15231], Loss: 0.2661\n",
            "Epoch [2/2], Step [780/15231], Loss: 0.2820\n",
            "Epoch [2/2], Step [800/15231], Loss: 0.2828\n",
            "Epoch [2/2], Step [820/15231], Loss: 0.2894\n",
            "Epoch [2/2], Step [840/15231], Loss: 0.2799\n",
            "Epoch [2/2], Step [860/15231], Loss: 0.2660\n",
            "Epoch [2/2], Step [880/15231], Loss: 0.2778\n",
            "Epoch [2/2], Step [900/15231], Loss: 0.2759\n",
            "Epoch [2/2], Step [920/15231], Loss: 0.3073\n",
            "Epoch [2/2], Step [940/15231], Loss: 0.3372\n",
            "Epoch [2/2], Step [960/15231], Loss: 0.2699\n",
            "Epoch [2/2], Step [980/15231], Loss: 0.2859\n",
            "Epoch [2/2], Step [1000/15231], Loss: 0.2975\n",
            "Epoch [2/2], Step [1020/15231], Loss: 0.2780\n",
            "Epoch [2/2], Step [1040/15231], Loss: 0.2805\n",
            "Epoch [2/2], Step [1060/15231], Loss: 0.2990\n",
            "Epoch [2/2], Step [1080/15231], Loss: 0.2858\n",
            "Epoch [2/2], Step [1100/15231], Loss: 0.3110\n",
            "Epoch [2/2], Step [1120/15231], Loss: 0.2816\n",
            "Epoch [2/2], Step [1140/15231], Loss: 0.2766\n",
            "Epoch [2/2], Step [1160/15231], Loss: 0.2854\n",
            "Epoch [2/2], Step [1180/15231], Loss: 0.3025\n",
            "Epoch [2/2], Step [1200/15231], Loss: 0.2822\n",
            "Epoch [2/2], Step [1220/15231], Loss: 0.3045\n",
            "Epoch [2/2], Step [1240/15231], Loss: 0.2616\n",
            "Epoch [2/2], Step [1260/15231], Loss: 0.2896\n",
            "Epoch [2/2], Step [1280/15231], Loss: 0.2450\n",
            "Epoch [2/2], Step [1300/15231], Loss: 0.2489\n",
            "Epoch [2/2], Step [1320/15231], Loss: 0.2868\n",
            "Epoch [2/2], Step [1340/15231], Loss: 0.2724\n",
            "Epoch [2/2], Step [1360/15231], Loss: 0.2999\n",
            "Epoch [2/2], Step [1380/15231], Loss: 0.2742\n",
            "Epoch [2/2], Step [1400/15231], Loss: 0.3008\n",
            "Epoch [2/2], Step [1420/15231], Loss: 0.2773\n",
            "Epoch [2/2], Step [1440/15231], Loss: 0.3025\n",
            "Epoch [2/2], Step [1460/15231], Loss: 0.2654\n",
            "Epoch [2/2], Step [1480/15231], Loss: 0.2652\n",
            "Epoch [2/2], Step [1500/15231], Loss: 0.2806\n",
            "Epoch [2/2], Step [1520/15231], Loss: 0.2695\n",
            "Epoch [2/2], Step [1540/15231], Loss: 0.2880\n",
            "Epoch [2/2], Step [1560/15231], Loss: 0.2975\n",
            "Epoch [2/2], Step [1580/15231], Loss: 0.2948\n",
            "Epoch [2/2], Step [1600/15231], Loss: 0.2555\n",
            "Epoch [2/2], Step [1620/15231], Loss: 0.3011\n",
            "Epoch [2/2], Step [1640/15231], Loss: 0.3148\n",
            "Epoch [2/2], Step [1660/15231], Loss: 0.2760\n",
            "Epoch [2/2], Step [1680/15231], Loss: 0.2849\n",
            "Epoch [2/2], Step [1700/15231], Loss: 0.2942\n",
            "Epoch [2/2], Step [1720/15231], Loss: 0.2992\n",
            "Epoch [2/2], Step [1740/15231], Loss: 0.2651\n",
            "Epoch [2/2], Step [1760/15231], Loss: 0.3129\n",
            "Epoch [2/2], Step [1780/15231], Loss: 0.2718\n",
            "Epoch [2/2], Step [1800/15231], Loss: 0.2569\n",
            "Epoch [2/2], Step [1820/15231], Loss: 0.2551\n",
            "Epoch [2/2], Step [1840/15231], Loss: 0.2956\n",
            "Epoch [2/2], Step [1860/15231], Loss: 0.3008\n",
            "Epoch [2/2], Step [1880/15231], Loss: 0.2929\n",
            "Epoch [2/2], Step [1900/15231], Loss: 0.3061\n",
            "Epoch [2/2], Step [1920/15231], Loss: 0.2992\n",
            "Epoch [2/2], Step [1940/15231], Loss: 0.2992\n",
            "Epoch [2/2], Step [1960/15231], Loss: 0.2748\n",
            "Epoch [2/2], Step [1980/15231], Loss: 0.2895\n",
            "Epoch [2/2], Step [2000/15231], Loss: 0.2687\n",
            "Epoch [2/2], Step [2020/15231], Loss: 0.3050\n",
            "Epoch [2/2], Step [2040/15231], Loss: 0.2960\n",
            "Epoch [2/2], Step [2060/15231], Loss: 0.3041\n",
            "Epoch [2/2], Step [2080/15231], Loss: 0.2847\n",
            "Epoch [2/2], Step [2100/15231], Loss: 0.2709\n",
            "Epoch [2/2], Step [2120/15231], Loss: 0.2630\n",
            "Epoch [2/2], Step [2140/15231], Loss: 0.2731\n",
            "Epoch [2/2], Step [2160/15231], Loss: 0.2914\n",
            "Epoch [2/2], Step [2180/15231], Loss: 0.2697\n",
            "Epoch [2/2], Step [2200/15231], Loss: 0.2778\n",
            "Epoch [2/2], Step [2220/15231], Loss: 0.2646\n",
            "Epoch [2/2], Step [2240/15231], Loss: 0.2992\n",
            "Epoch [2/2], Step [2260/15231], Loss: 0.2686\n",
            "Epoch [2/2], Step [2280/15231], Loss: 0.2967\n",
            "Epoch [2/2], Step [2300/15231], Loss: 0.2587\n",
            "Epoch [2/2], Step [2320/15231], Loss: 0.2446\n",
            "Epoch [2/2], Step [2340/15231], Loss: 0.3105\n",
            "Epoch [2/2], Step [2360/15231], Loss: 0.3080\n",
            "Epoch [2/2], Step [2380/15231], Loss: 0.2841\n",
            "Epoch [2/2], Step [2400/15231], Loss: 0.2919\n",
            "Epoch [2/2], Step [2420/15231], Loss: 0.2984\n",
            "Epoch [2/2], Step [2440/15231], Loss: 0.2839\n",
            "Epoch [2/2], Step [2460/15231], Loss: 0.2579\n",
            "Epoch [2/2], Step [2480/15231], Loss: 0.2700\n",
            "Epoch [2/2], Step [2500/15231], Loss: 0.2846\n",
            "Epoch [2/2], Step [2520/15231], Loss: 0.2635\n",
            "Epoch [2/2], Step [2540/15231], Loss: 0.2789\n",
            "Epoch [2/2], Step [2560/15231], Loss: 0.2615\n",
            "Epoch [2/2], Step [2580/15231], Loss: 0.2440\n",
            "Epoch [2/2], Step [2600/15231], Loss: 0.2862\n",
            "Epoch [2/2], Step [2620/15231], Loss: 0.2674\n",
            "Epoch [2/2], Step [2640/15231], Loss: 0.3046\n",
            "Epoch [2/2], Step [2660/15231], Loss: 0.3146\n",
            "Epoch [2/2], Step [2680/15231], Loss: 0.2922\n",
            "Epoch [2/2], Step [2700/15231], Loss: 0.2715\n",
            "Epoch [2/2], Step [2720/15231], Loss: 0.3088\n",
            "Epoch [2/2], Step [2740/15231], Loss: 0.2634\n",
            "Epoch [2/2], Step [2760/15231], Loss: 0.2765\n",
            "Epoch [2/2], Step [2780/15231], Loss: 0.2642\n",
            "Epoch [2/2], Step [2800/15231], Loss: 0.2906\n",
            "Epoch [2/2], Step [2820/15231], Loss: 0.2230\n",
            "Epoch [2/2], Step [2840/15231], Loss: 0.2918\n",
            "Epoch [2/2], Step [2860/15231], Loss: 0.2599\n",
            "Epoch [2/2], Step [2880/15231], Loss: 0.2614\n",
            "Epoch [2/2], Step [2900/15231], Loss: 0.2521\n",
            "Epoch [2/2], Step [2920/15231], Loss: 0.3012\n",
            "Epoch [2/2], Step [2940/15231], Loss: 0.2540\n",
            "Epoch [2/2], Step [2960/15231], Loss: 0.2786\n",
            "Epoch [2/2], Step [2980/15231], Loss: 0.2683\n",
            "Epoch [2/2], Step [3000/15231], Loss: 0.2845\n",
            "Epoch [2/2], Step [3020/15231], Loss: 0.2590\n",
            "Epoch [2/2], Step [3040/15231], Loss: 0.2792\n",
            "Epoch [2/2], Step [3060/15231], Loss: 0.2807\n",
            "Epoch [2/2], Step [3080/15231], Loss: 0.2650\n",
            "Epoch [2/2], Step [3100/15231], Loss: 0.2970\n",
            "Epoch [2/2], Step [3120/15231], Loss: 0.2662\n",
            "Epoch [2/2], Step [3140/15231], Loss: 0.2419\n",
            "Epoch [2/2], Step [3160/15231], Loss: 0.2782\n",
            "Epoch [2/2], Step [3180/15231], Loss: 0.2534\n",
            "Epoch [2/2], Step [3200/15231], Loss: 0.2543\n",
            "Epoch [2/2], Step [3220/15231], Loss: 0.2687\n",
            "Epoch [2/2], Step [3240/15231], Loss: 0.2498\n",
            "Epoch [2/2], Step [3260/15231], Loss: 0.2649\n",
            "Epoch [2/2], Step [3280/15231], Loss: 0.2672\n",
            "Epoch [2/2], Step [3300/15231], Loss: 0.2617\n",
            "Epoch [2/2], Step [3320/15231], Loss: 0.2603\n",
            "Epoch [2/2], Step [3340/15231], Loss: 0.2950\n",
            "Epoch [2/2], Step [3360/15231], Loss: 0.3009\n",
            "Epoch [2/2], Step [3380/15231], Loss: 0.2888\n",
            "Epoch [2/2], Step [3400/15231], Loss: 0.2742\n",
            "Epoch [2/2], Step [3420/15231], Loss: 0.2480\n",
            "Epoch [2/2], Step [3440/15231], Loss: 0.2740\n",
            "Epoch [2/2], Step [3460/15231], Loss: 0.2615\n",
            "Epoch [2/2], Step [3480/15231], Loss: 0.2991\n",
            "Epoch [2/2], Step [3500/15231], Loss: 0.2842\n",
            "Epoch [2/2], Step [3520/15231], Loss: 0.2863\n",
            "Epoch [2/2], Step [3540/15231], Loss: 0.2951\n",
            "Epoch [2/2], Step [3560/15231], Loss: 0.2618\n",
            "Epoch [2/2], Step [3580/15231], Loss: 0.2964\n",
            "Epoch [2/2], Step [3600/15231], Loss: 0.3021\n",
            "Epoch [2/2], Step [3620/15231], Loss: 0.2918\n",
            "Epoch [2/2], Step [3640/15231], Loss: 0.2928\n",
            "Epoch [2/2], Step [3660/15231], Loss: 0.2692\n",
            "Epoch [2/2], Step [3680/15231], Loss: 0.2772\n",
            "Epoch [2/2], Step [3700/15231], Loss: 0.3108\n",
            "Epoch [2/2], Step [3720/15231], Loss: 0.2743\n",
            "Epoch [2/2], Step [3740/15231], Loss: 0.2375\n",
            "Epoch [2/2], Step [3760/15231], Loss: 0.2889\n",
            "Epoch [2/2], Step [3780/15231], Loss: 0.2885\n",
            "Epoch [2/2], Step [3800/15231], Loss: 0.2594\n",
            "Epoch [2/2], Step [3820/15231], Loss: 0.2782\n",
            "Epoch [2/2], Step [3840/15231], Loss: 0.3106\n",
            "Epoch [2/2], Step [3860/15231], Loss: 0.2742\n",
            "Epoch [2/2], Step [3880/15231], Loss: 0.3003\n",
            "Epoch [2/2], Step [3900/15231], Loss: 0.2947\n",
            "Epoch [2/2], Step [3920/15231], Loss: 0.2693\n",
            "Epoch [2/2], Step [3940/15231], Loss: 0.2910\n",
            "Epoch [2/2], Step [3960/15231], Loss: 0.2684\n",
            "Epoch [2/2], Step [3980/15231], Loss: 0.2943\n",
            "Epoch [2/2], Step [4000/15231], Loss: 0.2845\n",
            "Epoch [2/2], Step [4020/15231], Loss: 0.3072\n",
            "Epoch [2/2], Step [4040/15231], Loss: 0.2538\n",
            "Epoch [2/2], Step [4060/15231], Loss: 0.2567\n",
            "Epoch [2/2], Step [4080/15231], Loss: 0.2897\n",
            "Epoch [2/2], Step [4100/15231], Loss: 0.3006\n",
            "Epoch [2/2], Step [4120/15231], Loss: 0.2937\n",
            "Epoch [2/2], Step [4140/15231], Loss: 0.3016\n",
            "Epoch [2/2], Step [4160/15231], Loss: 0.2751\n",
            "Epoch [2/2], Step [4180/15231], Loss: 0.2783\n",
            "Epoch [2/2], Step [4200/15231], Loss: 0.2864\n",
            "Epoch [2/2], Step [4220/15231], Loss: 0.2698\n",
            "Epoch [2/2], Step [4240/15231], Loss: 0.2903\n",
            "Epoch [2/2], Step [4260/15231], Loss: 0.2852\n",
            "Epoch [2/2], Step [4280/15231], Loss: 0.2841\n",
            "Epoch [2/2], Step [4300/15231], Loss: 0.2656\n",
            "Epoch [2/2], Step [4320/15231], Loss: 0.2864\n",
            "Epoch [2/2], Step [4340/15231], Loss: 0.2818\n",
            "Epoch [2/2], Step [4360/15231], Loss: 0.2872\n",
            "Epoch [2/2], Step [4380/15231], Loss: 0.3069\n",
            "Epoch [2/2], Step [4400/15231], Loss: 0.3019\n",
            "Epoch [2/2], Step [4420/15231], Loss: 0.2886\n",
            "Epoch [2/2], Step [4440/15231], Loss: 0.2756\n",
            "Epoch [2/2], Step [4460/15231], Loss: 0.2915\n",
            "Epoch [2/2], Step [4480/15231], Loss: 0.2463\n",
            "Epoch [2/2], Step [4500/15231], Loss: 0.2965\n",
            "Epoch [2/2], Step [4520/15231], Loss: 0.3134\n",
            "Epoch [2/2], Step [4540/15231], Loss: 0.2941\n",
            "Epoch [2/2], Step [4560/15231], Loss: 0.2616\n",
            "Epoch [2/2], Step [4580/15231], Loss: 0.2621\n",
            "Epoch [2/2], Step [4600/15231], Loss: 0.2597\n",
            "Epoch [2/2], Step [4620/15231], Loss: 0.2636\n",
            "Epoch [2/2], Step [4640/15231], Loss: 0.2911\n",
            "Epoch [2/2], Step [4660/15231], Loss: 0.2703\n",
            "Epoch [2/2], Step [4680/15231], Loss: 0.2908\n",
            "Epoch [2/2], Step [4700/15231], Loss: 0.2696\n",
            "Epoch [2/2], Step [4720/15231], Loss: 0.2856\n",
            "Epoch [2/2], Step [4740/15231], Loss: 0.2816\n",
            "Epoch [2/2], Step [4760/15231], Loss: 0.2796\n",
            "Epoch [2/2], Step [4780/15231], Loss: 0.2842\n",
            "Epoch [2/2], Step [4800/15231], Loss: 0.2859\n",
            "Epoch [2/2], Step [4820/15231], Loss: 0.2941\n",
            "Epoch [2/2], Step [4840/15231], Loss: 0.2630\n",
            "Epoch [2/2], Step [4860/15231], Loss: 0.2864\n",
            "Epoch [2/2], Step [4880/15231], Loss: 0.2716\n",
            "Epoch [2/2], Step [4900/15231], Loss: 0.2939\n",
            "Epoch [2/2], Step [4920/15231], Loss: 0.2822\n",
            "Epoch [2/2], Step [4940/15231], Loss: 0.2851\n",
            "Epoch [2/2], Step [4960/15231], Loss: 0.2798\n",
            "Epoch [2/2], Step [4980/15231], Loss: 0.3111\n",
            "Epoch [2/2], Step [5000/15231], Loss: 0.2692\n",
            "Epoch [2/2], Step [5020/15231], Loss: 0.2530\n",
            "Epoch [2/2], Step [5040/15231], Loss: 0.2549\n",
            "Epoch [2/2], Step [5060/15231], Loss: 0.2980\n",
            "Epoch [2/2], Step [5080/15231], Loss: 0.2760\n",
            "Epoch [2/2], Step [5100/15231], Loss: 0.2712\n",
            "Epoch [2/2], Step [5120/15231], Loss: 0.2480\n",
            "Epoch [2/2], Step [5140/15231], Loss: 0.2785\n",
            "Epoch [2/2], Step [5160/15231], Loss: 0.2691\n",
            "Epoch [2/2], Step [5180/15231], Loss: 0.2537\n",
            "Epoch [2/2], Step [5200/15231], Loss: 0.2711\n",
            "Epoch [2/2], Step [5220/15231], Loss: 0.2617\n",
            "Epoch [2/2], Step [5240/15231], Loss: 0.2524\n",
            "Epoch [2/2], Step [5260/15231], Loss: 0.2715\n",
            "Epoch [2/2], Step [5280/15231], Loss: 0.2749\n",
            "Epoch [2/2], Step [5300/15231], Loss: 0.2645\n",
            "Epoch [2/2], Step [5320/15231], Loss: 0.2762\n",
            "Epoch [2/2], Step [5340/15231], Loss: 0.2564\n",
            "Epoch [2/2], Step [5360/15231], Loss: 0.2768\n",
            "Epoch [2/2], Step [5380/15231], Loss: 0.2990\n",
            "Epoch [2/2], Step [5400/15231], Loss: 0.2894\n",
            "Epoch [2/2], Step [5420/15231], Loss: 0.2619\n",
            "Epoch [2/2], Step [5440/15231], Loss: 0.2717\n",
            "Epoch [2/2], Step [5460/15231], Loss: 0.2806\n",
            "Epoch [2/2], Step [5480/15231], Loss: 0.2761\n",
            "Epoch [2/2], Step [5500/15231], Loss: 0.2736\n",
            "Epoch [2/2], Step [5520/15231], Loss: 0.2765\n",
            "Epoch [2/2], Step [5540/15231], Loss: 0.3221\n",
            "Epoch [2/2], Step [5560/15231], Loss: 0.2985\n",
            "Epoch [2/2], Step [5580/15231], Loss: 0.3032\n",
            "Epoch [2/2], Step [5600/15231], Loss: 0.2663\n",
            "Epoch [2/2], Step [5620/15231], Loss: 0.3240\n",
            "Epoch [2/2], Step [5640/15231], Loss: 0.3089\n",
            "Epoch [2/2], Step [5660/15231], Loss: 0.2415\n",
            "Epoch [2/2], Step [5680/15231], Loss: 0.2808\n",
            "Epoch [2/2], Step [5700/15231], Loss: 0.2545\n",
            "Epoch [2/2], Step [5720/15231], Loss: 0.2599\n",
            "Epoch [2/2], Step [5740/15231], Loss: 0.3164\n",
            "Epoch [2/2], Step [5760/15231], Loss: 0.2817\n",
            "Epoch [2/2], Step [5780/15231], Loss: 0.2864\n",
            "Epoch [2/2], Step [5800/15231], Loss: 0.2604\n",
            "Epoch [2/2], Step [5820/15231], Loss: 0.2737\n",
            "Epoch [2/2], Step [5840/15231], Loss: 0.2564\n",
            "Epoch [2/2], Step [5860/15231], Loss: 0.2511\n",
            "Epoch [2/2], Step [5880/15231], Loss: 0.2602\n",
            "Epoch [2/2], Step [5900/15231], Loss: 0.2578\n",
            "Epoch [2/2], Step [5920/15231], Loss: 0.2794\n",
            "Epoch [2/2], Step [5940/15231], Loss: 0.3199\n",
            "Epoch [2/2], Step [5960/15231], Loss: 0.2658\n",
            "Epoch [2/2], Step [5980/15231], Loss: 0.3008\n",
            "Epoch [2/2], Step [6000/15231], Loss: 0.2793\n",
            "Epoch [2/2], Step [6020/15231], Loss: 0.2590\n",
            "Epoch [2/2], Step [6040/15231], Loss: 0.2844\n",
            "Epoch [2/2], Step [6060/15231], Loss: 0.2761\n",
            "Epoch [2/2], Step [6080/15231], Loss: 0.2895\n",
            "Epoch [2/2], Step [6100/15231], Loss: 0.2727\n",
            "Epoch [2/2], Step [6120/15231], Loss: 0.2662\n",
            "Epoch [2/2], Step [6140/15231], Loss: 0.2521\n",
            "Epoch [2/2], Step [6160/15231], Loss: 0.2714\n",
            "Epoch [2/2], Step [6180/15231], Loss: 0.2835\n",
            "Epoch [2/2], Step [6200/15231], Loss: 0.2955\n",
            "Epoch [2/2], Step [6220/15231], Loss: 0.2901\n",
            "Epoch [2/2], Step [6240/15231], Loss: 0.2843\n",
            "Epoch [2/2], Step [6260/15231], Loss: 0.2733\n",
            "Epoch [2/2], Step [6280/15231], Loss: 0.2768\n",
            "Epoch [2/2], Step [6300/15231], Loss: 0.2602\n",
            "Epoch [2/2], Step [6320/15231], Loss: 0.2876\n",
            "Epoch [2/2], Step [6340/15231], Loss: 0.2717\n",
            "Epoch [2/2], Step [6360/15231], Loss: 0.2681\n",
            "Epoch [2/2], Step [6380/15231], Loss: 0.2969\n",
            "Epoch [2/2], Step [6400/15231], Loss: 0.2641\n",
            "Epoch [2/2], Step [6420/15231], Loss: 0.2408\n",
            "Epoch [2/2], Step [6440/15231], Loss: 0.2519\n",
            "Epoch [2/2], Step [6460/15231], Loss: 0.2758\n",
            "Epoch [2/2], Step [6480/15231], Loss: 0.2295\n",
            "Epoch [2/2], Step [6500/15231], Loss: 0.2722\n",
            "Epoch [2/2], Step [6520/15231], Loss: 0.2688\n",
            "Epoch [2/2], Step [6540/15231], Loss: 0.2835\n",
            "Epoch [2/2], Step [6560/15231], Loss: 0.2323\n",
            "Epoch [2/2], Step [6580/15231], Loss: 0.2386\n",
            "Epoch [2/2], Step [6600/15231], Loss: 0.2869\n",
            "Epoch [2/2], Step [6620/15231], Loss: 0.2489\n",
            "Epoch [2/2], Step [6640/15231], Loss: 0.2367\n",
            "Epoch [2/2], Step [6660/15231], Loss: 0.2780\n",
            "Epoch [2/2], Step [6680/15231], Loss: 0.2776\n",
            "Epoch [2/2], Step [6700/15231], Loss: 0.2364\n",
            "Epoch [2/2], Step [6720/15231], Loss: 0.2635\n",
            "Epoch [2/2], Step [6740/15231], Loss: 0.2407\n",
            "Epoch [2/2], Step [6760/15231], Loss: 0.2806\n",
            "Epoch [2/2], Step [6780/15231], Loss: 0.2673\n",
            "Epoch [2/2], Step [6800/15231], Loss: 0.2759\n",
            "Epoch [2/2], Step [6820/15231], Loss: 0.3029\n",
            "Epoch [2/2], Step [6840/15231], Loss: 0.2531\n",
            "Epoch [2/2], Step [6860/15231], Loss: 0.2818\n",
            "Epoch [2/2], Step [6880/15231], Loss: 0.2699\n",
            "Epoch [2/2], Step [6900/15231], Loss: 0.2813\n",
            "Epoch [2/2], Step [6920/15231], Loss: 0.2982\n",
            "Epoch [2/2], Step [6940/15231], Loss: 0.3099\n",
            "Epoch [2/2], Step [6960/15231], Loss: 0.2631\n",
            "Epoch [2/2], Step [6980/15231], Loss: 0.3305\n",
            "Epoch [2/2], Step [7000/15231], Loss: 0.2875\n",
            "Epoch [2/2], Step [7020/15231], Loss: 0.2972\n",
            "Epoch [2/2], Step [7040/15231], Loss: 0.2591\n",
            "Epoch [2/2], Step [7060/15231], Loss: 0.2886\n",
            "Epoch [2/2], Step [7080/15231], Loss: 0.2848\n",
            "Epoch [2/2], Step [7100/15231], Loss: 0.2839\n",
            "Epoch [2/2], Step [7120/15231], Loss: 0.2821\n",
            "Epoch [2/2], Step [7140/15231], Loss: 0.2714\n",
            "Epoch [2/2], Step [7160/15231], Loss: 0.2750\n",
            "Epoch [2/2], Step [7180/15231], Loss: 0.2797\n",
            "Epoch [2/2], Step [7200/15231], Loss: 0.2556\n",
            "Epoch [2/2], Step [7220/15231], Loss: 0.3041\n",
            "Epoch [2/2], Step [7240/15231], Loss: 0.2644\n",
            "Epoch [2/2], Step [7260/15231], Loss: 0.2713\n",
            "Epoch [2/2], Step [7280/15231], Loss: 0.2540\n",
            "Epoch [2/2], Step [7300/15231], Loss: 0.2889\n",
            "Epoch [2/2], Step [7320/15231], Loss: 0.2720\n",
            "Epoch [2/2], Step [7340/15231], Loss: 0.2859\n",
            "Epoch [2/2], Step [7360/15231], Loss: 0.2644\n",
            "Epoch [2/2], Step [7380/15231], Loss: 0.2668\n",
            "Epoch [2/2], Step [7400/15231], Loss: 0.2799\n",
            "Epoch [2/2], Step [7420/15231], Loss: 0.2709\n",
            "Epoch [2/2], Step [7440/15231], Loss: 0.2840\n",
            "Epoch [2/2], Step [7460/15231], Loss: 0.2832\n",
            "Epoch [2/2], Step [7480/15231], Loss: 0.2768\n",
            "Epoch [2/2], Step [7500/15231], Loss: 0.2883\n",
            "Epoch [2/2], Step [7520/15231], Loss: 0.2625\n",
            "Epoch [2/2], Step [7540/15231], Loss: 0.2925\n",
            "Epoch [2/2], Step [7560/15231], Loss: 0.2547\n",
            "Epoch [2/2], Step [7580/15231], Loss: 0.2378\n",
            "Epoch [2/2], Step [7600/15231], Loss: 0.2510\n",
            "Epoch [2/2], Step [7620/15231], Loss: 0.2769\n",
            "Epoch [2/2], Step [7640/15231], Loss: 0.2419\n",
            "Epoch [2/2], Step [7660/15231], Loss: 0.2718\n",
            "Epoch [2/2], Step [7680/15231], Loss: 0.2562\n",
            "Epoch [2/2], Step [7700/15231], Loss: 0.2636\n",
            "Epoch [2/2], Step [7720/15231], Loss: 0.2592\n",
            "Epoch [2/2], Step [7740/15231], Loss: 0.2642\n",
            "Epoch [2/2], Step [7760/15231], Loss: 0.2844\n",
            "Epoch [2/2], Step [7780/15231], Loss: 0.2719\n",
            "Epoch [2/2], Step [7800/15231], Loss: 0.2360\n",
            "Epoch [2/2], Step [7820/15231], Loss: 0.2928\n",
            "Epoch [2/2], Step [7840/15231], Loss: 0.2929\n",
            "Epoch [2/2], Step [7860/15231], Loss: 0.2358\n",
            "Epoch [2/2], Step [7880/15231], Loss: 0.2656\n",
            "Epoch [2/2], Step [7900/15231], Loss: 0.2747\n",
            "Epoch [2/2], Step [7920/15231], Loss: 0.2778\n",
            "Epoch [2/2], Step [7940/15231], Loss: 0.2598\n",
            "Epoch [2/2], Step [7960/15231], Loss: 0.2329\n",
            "Epoch [2/2], Step [7980/15231], Loss: 0.2515\n",
            "Epoch [2/2], Step [8000/15231], Loss: 0.2861\n",
            "Epoch [2/2], Step [8020/15231], Loss: 0.2653\n",
            "Epoch [2/2], Step [8040/15231], Loss: 0.2434\n",
            "Epoch [2/2], Step [8060/15231], Loss: 0.2978\n",
            "Epoch [2/2], Step [8080/15231], Loss: 0.2788\n",
            "Epoch [2/2], Step [8100/15231], Loss: 0.2710\n",
            "Epoch [2/2], Step [8120/15231], Loss: 0.2574\n",
            "Epoch [2/2], Step [8140/15231], Loss: 0.2806\n",
            "Epoch [2/2], Step [8160/15231], Loss: 0.2816\n",
            "Epoch [2/2], Step [8180/15231], Loss: 0.2395\n",
            "Epoch [2/2], Step [8200/15231], Loss: 0.2490\n",
            "Epoch [2/2], Step [8220/15231], Loss: 0.2878\n",
            "Epoch [2/2], Step [8240/15231], Loss: 0.3069\n",
            "Epoch [2/2], Step [8260/15231], Loss: 0.2671\n",
            "Epoch [2/2], Step [8280/15231], Loss: 0.2910\n",
            "Epoch [2/2], Step [8300/15231], Loss: 0.2575\n",
            "Epoch [2/2], Step [8320/15231], Loss: 0.2769\n",
            "Epoch [2/2], Step [8340/15231], Loss: 0.2479\n",
            "Epoch [2/2], Step [8360/15231], Loss: 0.2767\n",
            "Epoch [2/2], Step [8380/15231], Loss: 0.2773\n",
            "Epoch [2/2], Step [8400/15231], Loss: 0.2494\n",
            "Epoch [2/2], Step [8420/15231], Loss: 0.2754\n",
            "Epoch [2/2], Step [8440/15231], Loss: 0.2732\n",
            "Epoch [2/2], Step [8460/15231], Loss: 0.2512\n",
            "Epoch [2/2], Step [8480/15231], Loss: 0.2887\n",
            "Epoch [2/2], Step [8500/15231], Loss: 0.2860\n",
            "Epoch [2/2], Step [8520/15231], Loss: 0.2846\n",
            "Epoch [2/2], Step [8540/15231], Loss: 0.2586\n",
            "Epoch [2/2], Step [8560/15231], Loss: 0.2626\n",
            "Epoch [2/2], Step [8580/15231], Loss: 0.2658\n",
            "Epoch [2/2], Step [8600/15231], Loss: 0.2428\n",
            "Epoch [2/2], Step [8620/15231], Loss: 0.2844\n",
            "Epoch [2/2], Step [8640/15231], Loss: 0.2596\n",
            "Epoch [2/2], Step [8660/15231], Loss: 0.2855\n",
            "Epoch [2/2], Step [8680/15231], Loss: 0.2570\n",
            "Epoch [2/2], Step [8700/15231], Loss: 0.2916\n",
            "Epoch [2/2], Step [8720/15231], Loss: 0.3013\n",
            "Epoch [2/2], Step [8740/15231], Loss: 0.2792\n",
            "Epoch [2/2], Step [8760/15231], Loss: 0.2996\n",
            "Epoch [2/2], Step [8780/15231], Loss: 0.3048\n",
            "Epoch [2/2], Step [8800/15231], Loss: 0.2781\n",
            "Epoch [2/2], Step [8820/15231], Loss: 0.2685\n",
            "Epoch [2/2], Step [8840/15231], Loss: 0.2721\n",
            "Epoch [2/2], Step [8860/15231], Loss: 0.2623\n",
            "Epoch [2/2], Step [8880/15231], Loss: 0.2550\n",
            "Epoch [2/2], Step [8900/15231], Loss: 0.2753\n",
            "Epoch [2/2], Step [8920/15231], Loss: 0.2494\n",
            "Epoch [2/2], Step [8940/15231], Loss: 0.2426\n",
            "Epoch [2/2], Step [8960/15231], Loss: 0.2555\n",
            "Epoch [2/2], Step [8980/15231], Loss: 0.2366\n",
            "Epoch [2/2], Step [9000/15231], Loss: 0.2894\n",
            "Epoch [2/2], Step [9020/15231], Loss: 0.2878\n",
            "Epoch [2/2], Step [9040/15231], Loss: 0.2546\n",
            "Epoch [2/2], Step [9060/15231], Loss: 0.3023\n",
            "Epoch [2/2], Step [9080/15231], Loss: 0.2531\n",
            "Epoch [2/2], Step [9100/15231], Loss: 0.2587\n",
            "Epoch [2/2], Step [9120/15231], Loss: 0.2509\n",
            "Epoch [2/2], Step [9140/15231], Loss: 0.2358\n",
            "Epoch [2/2], Step [9160/15231], Loss: 0.2441\n",
            "Epoch [2/2], Step [9180/15231], Loss: 0.2410\n",
            "Epoch [2/2], Step [9200/15231], Loss: 0.2999\n",
            "Epoch [2/2], Step [9220/15231], Loss: 0.2326\n",
            "Epoch [2/2], Step [9240/15231], Loss: 0.2981\n",
            "Epoch [2/2], Step [9260/15231], Loss: 0.2922\n",
            "Epoch [2/2], Step [9280/15231], Loss: 0.2415\n",
            "Epoch [2/2], Step [9300/15231], Loss: 0.2652\n",
            "Epoch [2/2], Step [9320/15231], Loss: 0.2721\n",
            "Epoch [2/2], Step [9340/15231], Loss: 0.2339\n",
            "Epoch [2/2], Step [9360/15231], Loss: 0.2728\n",
            "Epoch [2/2], Step [9380/15231], Loss: 0.2677\n",
            "Epoch [2/2], Step [9400/15231], Loss: 0.2580\n",
            "Epoch [2/2], Step [9420/15231], Loss: 0.2564\n",
            "Epoch [2/2], Step [9440/15231], Loss: 0.2481\n",
            "Epoch [2/2], Step [9460/15231], Loss: 0.2662\n",
            "Epoch [2/2], Step [9480/15231], Loss: 0.2648\n",
            "Epoch [2/2], Step [9500/15231], Loss: 0.2913\n",
            "Epoch [2/2], Step [9520/15231], Loss: 0.2766\n",
            "Epoch [2/2], Step [9540/15231], Loss: 0.2510\n",
            "Epoch [2/2], Step [9560/15231], Loss: 0.2571\n",
            "Epoch [2/2], Step [9580/15231], Loss: 0.2462\n",
            "Epoch [2/2], Step [9600/15231], Loss: 0.2831\n",
            "Epoch [2/2], Step [9620/15231], Loss: 0.2785\n",
            "Epoch [2/2], Step [9640/15231], Loss: 0.2419\n",
            "Epoch [2/2], Step [9660/15231], Loss: 0.2497\n",
            "Epoch [2/2], Step [9680/15231], Loss: 0.2779\n",
            "Epoch [2/2], Step [9700/15231], Loss: 0.3056\n",
            "Epoch [2/2], Step [9720/15231], Loss: 0.2564\n",
            "Epoch [2/2], Step [9740/15231], Loss: 0.2446\n",
            "Epoch [2/2], Step [9760/15231], Loss: 0.2288\n",
            "Epoch [2/2], Step [9780/15231], Loss: 0.2602\n",
            "Epoch [2/2], Step [9800/15231], Loss: 0.2393\n",
            "Epoch [2/2], Step [9820/15231], Loss: 0.3036\n",
            "Epoch [2/2], Step [9840/15231], Loss: 0.2644\n",
            "Epoch [2/2], Step [9860/15231], Loss: 0.2579\n",
            "Epoch [2/2], Step [9880/15231], Loss: 0.2731\n",
            "Epoch [2/2], Step [9900/15231], Loss: 0.2724\n",
            "Epoch [2/2], Step [9920/15231], Loss: 0.2357\n",
            "Epoch [2/2], Step [9940/15231], Loss: 0.3056\n",
            "Epoch [2/2], Step [9960/15231], Loss: 0.2506\n",
            "Epoch [2/2], Step [9980/15231], Loss: 0.2488\n",
            "Epoch [2/2], Step [10000/15231], Loss: 0.2612\n",
            "Epoch [2/2], Step [10020/15231], Loss: 0.3050\n",
            "Epoch [2/2], Step [10040/15231], Loss: 0.2640\n",
            "Epoch [2/2], Step [10060/15231], Loss: 0.2394\n",
            "Epoch [2/2], Step [10080/15231], Loss: 0.2475\n",
            "Epoch [2/2], Step [10100/15231], Loss: 0.2635\n",
            "Epoch [2/2], Step [10120/15231], Loss: 0.2393\n",
            "Epoch [2/2], Step [10140/15231], Loss: 0.2545\n",
            "Epoch [2/2], Step [10160/15231], Loss: 0.2666\n",
            "Epoch [2/2], Step [10180/15231], Loss: 0.2842\n",
            "Epoch [2/2], Step [10200/15231], Loss: 0.2906\n",
            "Epoch [2/2], Step [10220/15231], Loss: 0.3130\n",
            "Epoch [2/2], Step [10240/15231], Loss: 0.2459\n",
            "Epoch [2/2], Step [10260/15231], Loss: 0.2804\n",
            "Epoch [2/2], Step [10280/15231], Loss: 0.2446\n",
            "Epoch [2/2], Step [10300/15231], Loss: 0.2328\n",
            "Epoch [2/2], Step [10320/15231], Loss: 0.2546\n",
            "Epoch [2/2], Step [10340/15231], Loss: 0.2974\n",
            "Epoch [2/2], Step [10360/15231], Loss: 0.2754\n",
            "Epoch [2/2], Step [10380/15231], Loss: 0.2602\n",
            "Epoch [2/2], Step [10400/15231], Loss: 0.2572\n",
            "Epoch [2/2], Step [10420/15231], Loss: 0.2819\n",
            "Epoch [2/2], Step [10440/15231], Loss: 0.2293\n",
            "Epoch [2/2], Step [10460/15231], Loss: 0.2732\n",
            "Epoch [2/2], Step [10480/15231], Loss: 0.2863\n",
            "Epoch [2/2], Step [10500/15231], Loss: 0.2498\n",
            "Epoch [2/2], Step [10520/15231], Loss: 0.2917\n",
            "Epoch [2/2], Step [10540/15231], Loss: 0.2758\n",
            "Epoch [2/2], Step [10560/15231], Loss: 0.2613\n",
            "Epoch [2/2], Step [10580/15231], Loss: 0.2325\n",
            "Epoch [2/2], Step [10600/15231], Loss: 0.2360\n",
            "Epoch [2/2], Step [10620/15231], Loss: 0.2341\n",
            "Epoch [2/2], Step [10640/15231], Loss: 0.2579\n",
            "Epoch [2/2], Step [10660/15231], Loss: 0.2464\n",
            "Epoch [2/2], Step [10680/15231], Loss: 0.2865\n",
            "Epoch [2/2], Step [10700/15231], Loss: 0.2731\n",
            "Epoch [2/2], Step [10720/15231], Loss: 0.2504\n",
            "Epoch [2/2], Step [10740/15231], Loss: 0.2935\n",
            "Epoch [2/2], Step [10760/15231], Loss: 0.2651\n",
            "Epoch [2/2], Step [10780/15231], Loss: 0.2287\n",
            "Epoch [2/2], Step [10800/15231], Loss: 0.2845\n",
            "Epoch [2/2], Step [10820/15231], Loss: 0.2761\n",
            "Epoch [2/2], Step [10840/15231], Loss: 0.2703\n",
            "Epoch [2/2], Step [10860/15231], Loss: 0.2881\n",
            "Epoch [2/2], Step [10880/15231], Loss: 0.2635\n",
            "Epoch [2/2], Step [10900/15231], Loss: 0.2312\n",
            "Epoch [2/2], Step [10920/15231], Loss: 0.2756\n",
            "Epoch [2/2], Step [10940/15231], Loss: 0.2489\n",
            "Epoch [2/2], Step [10960/15231], Loss: 0.2564\n",
            "Epoch [2/2], Step [10980/15231], Loss: 0.2457\n",
            "Epoch [2/2], Step [11000/15231], Loss: 0.2899\n",
            "Epoch [2/2], Step [11020/15231], Loss: 0.3048\n",
            "Epoch [2/2], Step [11040/15231], Loss: 0.2524\n",
            "Epoch [2/2], Step [11060/15231], Loss: 0.2813\n",
            "Epoch [2/2], Step [11080/15231], Loss: 0.2805\n",
            "Epoch [2/2], Step [11100/15231], Loss: 0.2724\n",
            "Epoch [2/2], Step [11120/15231], Loss: 0.2549\n",
            "Epoch [2/2], Step [11140/15231], Loss: 0.2483\n",
            "Epoch [2/2], Step [11160/15231], Loss: 0.2835\n",
            "Epoch [2/2], Step [11180/15231], Loss: 0.2993\n",
            "Epoch [2/2], Step [11200/15231], Loss: 0.2593\n",
            "Epoch [2/2], Step [11220/15231], Loss: 0.2791\n",
            "Epoch [2/2], Step [11240/15231], Loss: 0.2356\n",
            "Epoch [2/2], Step [11260/15231], Loss: 0.2707\n",
            "Epoch [2/2], Step [11280/15231], Loss: 0.2223\n",
            "Epoch [2/2], Step [11300/15231], Loss: 0.2855\n",
            "Epoch [2/2], Step [11320/15231], Loss: 0.2766\n",
            "Epoch [2/2], Step [11340/15231], Loss: 0.2573\n",
            "Epoch [2/2], Step [11360/15231], Loss: 0.2736\n",
            "Epoch [2/2], Step [11380/15231], Loss: 0.2611\n",
            "Epoch [2/2], Step [11400/15231], Loss: 0.2635\n",
            "Epoch [2/2], Step [11420/15231], Loss: 0.2558\n",
            "Epoch [2/2], Step [11440/15231], Loss: 0.2598\n",
            "Epoch [2/2], Step [11460/15231], Loss: 0.2895\n",
            "Epoch [2/2], Step [11480/15231], Loss: 0.2533\n",
            "Epoch [2/2], Step [11500/15231], Loss: 0.2570\n",
            "Epoch [2/2], Step [11520/15231], Loss: 0.2368\n",
            "Epoch [2/2], Step [11540/15231], Loss: 0.2095\n",
            "Epoch [2/2], Step [11560/15231], Loss: 0.2661\n",
            "Epoch [2/2], Step [11580/15231], Loss: 0.2538\n",
            "Epoch [2/2], Step [11600/15231], Loss: 0.2421\n",
            "Epoch [2/2], Step [11620/15231], Loss: 0.2304\n",
            "Epoch [2/2], Step [11640/15231], Loss: 0.2345\n",
            "Epoch [2/2], Step [11660/15231], Loss: 0.2611\n",
            "Epoch [2/2], Step [11680/15231], Loss: 0.2642\n",
            "Epoch [2/2], Step [11700/15231], Loss: 0.2717\n",
            "Epoch [2/2], Step [11720/15231], Loss: 0.2925\n",
            "Epoch [2/2], Step [11740/15231], Loss: 0.2632\n",
            "Epoch [2/2], Step [11760/15231], Loss: 0.2702\n",
            "Epoch [2/2], Step [11780/15231], Loss: 0.2282\n",
            "Epoch [2/2], Step [11800/15231], Loss: 0.2411\n",
            "Epoch [2/2], Step [11820/15231], Loss: 0.2304\n",
            "Epoch [2/2], Step [11840/15231], Loss: 0.2324\n",
            "Epoch [2/2], Step [11860/15231], Loss: 0.2831\n",
            "Epoch [2/2], Step [11880/15231], Loss: 0.2781\n",
            "Epoch [2/2], Step [11900/15231], Loss: 0.2438\n",
            "Epoch [2/2], Step [11920/15231], Loss: 0.2798\n",
            "Epoch [2/2], Step [11940/15231], Loss: 0.2616\n",
            "Epoch [2/2], Step [11960/15231], Loss: 0.2365\n",
            "Epoch [2/2], Step [11980/15231], Loss: 0.2864\n",
            "Epoch [2/2], Step [12000/15231], Loss: 0.2549\n",
            "Epoch [2/2], Step [12020/15231], Loss: 0.2362\n",
            "Epoch [2/2], Step [12040/15231], Loss: 0.2487\n",
            "Epoch [2/2], Step [12060/15231], Loss: 0.2841\n",
            "Epoch [2/2], Step [12080/15231], Loss: 0.2797\n",
            "Epoch [2/2], Step [12100/15231], Loss: 0.2623\n",
            "Epoch [2/2], Step [12120/15231], Loss: 0.2414\n",
            "Epoch [2/2], Step [12140/15231], Loss: 0.2498\n",
            "Epoch [2/2], Step [12160/15231], Loss: 0.2790\n",
            "Epoch [2/2], Step [12180/15231], Loss: 0.2614\n",
            "Epoch [2/2], Step [12200/15231], Loss: 0.2664\n",
            "Epoch [2/2], Step [12220/15231], Loss: 0.2185\n",
            "Epoch [2/2], Step [12240/15231], Loss: 0.2327\n",
            "Epoch [2/2], Step [12260/15231], Loss: 0.2437\n",
            "Epoch [2/2], Step [12280/15231], Loss: 0.2406\n",
            "Epoch [2/2], Step [12300/15231], Loss: 0.2680\n",
            "Epoch [2/2], Step [12320/15231], Loss: 0.2431\n",
            "Epoch [2/2], Step [12340/15231], Loss: 0.2758\n",
            "Epoch [2/2], Step [12360/15231], Loss: 0.2709\n",
            "Epoch [2/2], Step [12380/15231], Loss: 0.2197\n",
            "Epoch [2/2], Step [12400/15231], Loss: 0.2575\n",
            "Epoch [2/2], Step [12420/15231], Loss: 0.2444\n",
            "Epoch [2/2], Step [12440/15231], Loss: 0.2856\n",
            "Epoch [2/2], Step [12460/15231], Loss: 0.2447\n",
            "Epoch [2/2], Step [12480/15231], Loss: 0.2659\n",
            "Epoch [2/2], Step [12500/15231], Loss: 0.2347\n",
            "Epoch [2/2], Step [12520/15231], Loss: 0.2761\n",
            "Epoch [2/2], Step [12540/15231], Loss: 0.2163\n",
            "Epoch [2/2], Step [12560/15231], Loss: 0.2864\n",
            "Epoch [2/2], Step [12580/15231], Loss: 0.2375\n",
            "Epoch [2/2], Step [12600/15231], Loss: 0.2574\n",
            "Epoch [2/2], Step [12620/15231], Loss: 0.2384\n",
            "Epoch [2/2], Step [12640/15231], Loss: 0.2273\n",
            "Epoch [2/2], Step [12660/15231], Loss: 0.2674\n",
            "Epoch [2/2], Step [12680/15231], Loss: 0.2576\n",
            "Epoch [2/2], Step [12700/15231], Loss: 0.2487\n",
            "Epoch [2/2], Step [12720/15231], Loss: 0.2615\n",
            "Epoch [2/2], Step [12740/15231], Loss: 0.2843\n",
            "Epoch [2/2], Step [12760/15231], Loss: 0.2572\n",
            "Epoch [2/2], Step [12780/15231], Loss: 0.2442\n",
            "Epoch [2/2], Step [12800/15231], Loss: 0.2723\n",
            "Epoch [2/2], Step [12820/15231], Loss: 0.2195\n",
            "Epoch [2/2], Step [12840/15231], Loss: 0.2384\n",
            "Epoch [2/2], Step [12860/15231], Loss: 0.2866\n",
            "Epoch [2/2], Step [12880/15231], Loss: 0.2880\n",
            "Epoch [2/2], Step [12900/15231], Loss: 0.2876\n",
            "Epoch [2/2], Step [12920/15231], Loss: 0.2421\n",
            "Epoch [2/2], Step [12940/15231], Loss: 0.2841\n",
            "Epoch [2/2], Step [12960/15231], Loss: 0.2520\n",
            "Epoch [2/2], Step [12980/15231], Loss: 0.2847\n",
            "Epoch [2/2], Step [13000/15231], Loss: 0.2515\n",
            "Epoch [2/2], Step [13020/15231], Loss: 0.2568\n",
            "Epoch [2/2], Step [13040/15231], Loss: 0.2616\n",
            "Epoch [2/2], Step [13060/15231], Loss: 0.2569\n",
            "Epoch [2/2], Step [13080/15231], Loss: 0.2646\n",
            "Epoch [2/2], Step [13100/15231], Loss: 0.2416\n",
            "Epoch [2/2], Step [13120/15231], Loss: 0.2453\n",
            "Epoch [2/2], Step [13140/15231], Loss: 0.2338\n",
            "Epoch [2/2], Step [13160/15231], Loss: 0.2403\n",
            "Epoch [2/2], Step [13180/15231], Loss: 0.2772\n",
            "Epoch [2/2], Step [13200/15231], Loss: 0.2409\n",
            "Epoch [2/2], Step [13220/15231], Loss: 0.2600\n",
            "Epoch [2/2], Step [13240/15231], Loss: 0.2649\n",
            "Epoch [2/2], Step [13260/15231], Loss: 0.2746\n",
            "Epoch [2/2], Step [13280/15231], Loss: 0.2752\n",
            "Epoch [2/2], Step [13300/15231], Loss: 0.2517\n",
            "Epoch [2/2], Step [13320/15231], Loss: 0.2617\n",
            "Epoch [2/2], Step [13340/15231], Loss: 0.2758\n",
            "Epoch [2/2], Step [13360/15231], Loss: 0.2608\n",
            "Epoch [2/2], Step [13380/15231], Loss: 0.2489\n",
            "Epoch [2/2], Step [13400/15231], Loss: 0.2509\n",
            "Epoch [2/2], Step [13420/15231], Loss: 0.2780\n",
            "Epoch [2/2], Step [13440/15231], Loss: 0.2344\n",
            "Epoch [2/2], Step [13460/15231], Loss: 0.2562\n",
            "Epoch [2/2], Step [13480/15231], Loss: 0.2686\n",
            "Epoch [2/2], Step [13500/15231], Loss: 0.2478\n",
            "Epoch [2/2], Step [13520/15231], Loss: 0.3079\n",
            "Epoch [2/2], Step [13540/15231], Loss: 0.2724\n",
            "Epoch [2/2], Step [13560/15231], Loss: 0.2371\n",
            "Epoch [2/2], Step [13580/15231], Loss: 0.2415\n",
            "Epoch [2/2], Step [13600/15231], Loss: 0.2312\n",
            "Epoch [2/2], Step [13620/15231], Loss: 0.2420\n",
            "Epoch [2/2], Step [13640/15231], Loss: 0.2311\n",
            "Epoch [2/2], Step [13660/15231], Loss: 0.2597\n",
            "Epoch [2/2], Step [13680/15231], Loss: 0.2371\n",
            "Epoch [2/2], Step [13700/15231], Loss: 0.2537\n",
            "Epoch [2/2], Step [13720/15231], Loss: 0.2528\n",
            "Epoch [2/2], Step [13740/15231], Loss: 0.2305\n",
            "Epoch [2/2], Step [13760/15231], Loss: 0.2593\n",
            "Epoch [2/2], Step [13780/15231], Loss: 0.2703\n",
            "Epoch [2/2], Step [13800/15231], Loss: 0.2593\n",
            "Epoch [2/2], Step [13820/15231], Loss: 0.2587\n",
            "Epoch [2/2], Step [13840/15231], Loss: 0.2777\n",
            "Epoch [2/2], Step [13860/15231], Loss: 0.2606\n",
            "Epoch [2/2], Step [13880/15231], Loss: 0.2398\n",
            "Epoch [2/2], Step [13900/15231], Loss: 0.2590\n",
            "Epoch [2/2], Step [13920/15231], Loss: 0.2762\n",
            "Epoch [2/2], Step [13940/15231], Loss: 0.2552\n",
            "Epoch [2/2], Step [13960/15231], Loss: 0.2281\n",
            "Epoch [2/2], Step [13980/15231], Loss: 0.2863\n",
            "Epoch [2/2], Step [14000/15231], Loss: 0.2186\n",
            "Epoch [2/2], Step [14020/15231], Loss: 0.2533\n",
            "Epoch [2/2], Step [14040/15231], Loss: 0.2528\n",
            "Epoch [2/2], Step [14060/15231], Loss: 0.2623\n",
            "Epoch [2/2], Step [14080/15231], Loss: 0.2595\n",
            "Epoch [2/2], Step [14100/15231], Loss: 0.2439\n",
            "Epoch [2/2], Step [14120/15231], Loss: 0.2748\n",
            "Epoch [2/2], Step [14140/15231], Loss: 0.3041\n",
            "Epoch [2/2], Step [14160/15231], Loss: 0.2498\n",
            "Epoch [2/2], Step [14180/15231], Loss: 0.2814\n",
            "Epoch [2/2], Step [14200/15231], Loss: 0.2396\n",
            "Epoch [2/2], Step [14220/15231], Loss: 0.2572\n",
            "Epoch [2/2], Step [14240/15231], Loss: 0.2548\n",
            "Epoch [2/2], Step [14260/15231], Loss: 0.2410\n",
            "Epoch [2/2], Step [14280/15231], Loss: 0.2607\n",
            "Epoch [2/2], Step [14300/15231], Loss: 0.2615\n",
            "Epoch [2/2], Step [14320/15231], Loss: 0.2512\n",
            "Epoch [2/2], Step [14340/15231], Loss: 0.2420\n",
            "Epoch [2/2], Step [14360/15231], Loss: 0.2502\n",
            "Epoch [2/2], Step [14380/15231], Loss: 0.2826\n",
            "Epoch [2/2], Step [14400/15231], Loss: 0.2526\n",
            "Epoch [2/2], Step [14420/15231], Loss: 0.2202\n",
            "Epoch [2/2], Step [14440/15231], Loss: 0.2267\n",
            "Epoch [2/2], Step [14460/15231], Loss: 0.2382\n",
            "Epoch [2/2], Step [14480/15231], Loss: 0.2314\n",
            "Epoch [2/2], Step [14500/15231], Loss: 0.2399\n",
            "Epoch [2/2], Step [14520/15231], Loss: 0.2327\n",
            "Epoch [2/2], Step [14540/15231], Loss: 0.2531\n",
            "Epoch [2/2], Step [14560/15231], Loss: 0.2509\n",
            "Epoch [2/2], Step [14580/15231], Loss: 0.2748\n",
            "Epoch [2/2], Step [14600/15231], Loss: 0.2516\n",
            "Epoch [2/2], Step [14620/15231], Loss: 0.2422\n",
            "Epoch [2/2], Step [14640/15231], Loss: 0.2709\n",
            "Epoch [2/2], Step [14660/15231], Loss: 0.2831\n",
            "Epoch [2/2], Step [14680/15231], Loss: 0.2668\n",
            "Epoch [2/2], Step [14700/15231], Loss: 0.2609\n",
            "Epoch [2/2], Step [14720/15231], Loss: 0.2408\n",
            "Epoch [2/2], Step [14740/15231], Loss: 0.2529\n",
            "Epoch [2/2], Step [14760/15231], Loss: 0.2410\n",
            "Epoch [2/2], Step [14780/15231], Loss: 0.2633\n",
            "Epoch [2/2], Step [14800/15231], Loss: 0.2781\n",
            "Epoch [2/2], Step [14820/15231], Loss: 0.2442\n",
            "Epoch [2/2], Step [14840/15231], Loss: 0.2727\n",
            "Epoch [2/2], Step [14860/15231], Loss: 0.2655\n",
            "Epoch [2/2], Step [14880/15231], Loss: 0.2600\n",
            "Epoch [2/2], Step [14900/15231], Loss: 0.2499\n",
            "Epoch [2/2], Step [14920/15231], Loss: 0.2046\n",
            "Epoch [2/2], Step [14940/15231], Loss: 0.2702\n",
            "Epoch [2/2], Step [14960/15231], Loss: 0.2852\n",
            "Epoch [2/2], Step [14980/15231], Loss: 0.2781\n",
            "Epoch [2/2], Step [15000/15231], Loss: 0.2653\n",
            "Epoch [2/2], Step [15020/15231], Loss: 0.2428\n",
            "Epoch [2/2], Step [15040/15231], Loss: 0.2195\n",
            "Epoch [2/2], Step [15060/15231], Loss: 0.2598\n",
            "Epoch [2/2], Step [15080/15231], Loss: 0.2643\n",
            "Epoch [2/2], Step [15100/15231], Loss: 0.2932\n",
            "Epoch [2/2], Step [15120/15231], Loss: 0.2454\n",
            "Epoch [2/2], Step [15140/15231], Loss: 0.2308\n",
            "Epoch [2/2], Step [15160/15231], Loss: 0.2675\n",
            "Epoch [2/2], Step [15180/15231], Loss: 0.2604\n",
            "Epoch [2/2], Step [15200/15231], Loss: 0.2794\n",
            "Epoch [2/2], Step [15220/15231], Loss: 0.2356\n",
            "Epoch [2/2] | Train Loss: 0.2699 | Train Acc: 90.54% | Test Loss: 0.6466 | Test Acc: 77.30%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁███████████████</td></tr><tr><td>test_accuracy per epoch</td><td>▁█</td></tr><tr><td>train_accuracy per epoch</td><td>▁█</td></tr><tr><td>train_loss</td><td>█▆▆▄▄▄▄▂▃▃▃▂▂▃▂▂▂▃▂▂▂▂▂▂▂▂▂▁▁▂▁▂▂▁▁▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_accuracy per epoch</td><td>77.3</td></tr><tr><td>train_accuracy per epoch</td><td>90.5431</td></tr><tr><td>train_loss</td><td>0.23558</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet-3 with data aug (2 epochs)</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/uxe2kc6f' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/uxe2kc6f</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT</a><br>Synced 5 W&B file(s), 2 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250404_141931-uxe2kc6f/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW0zauKLk4em"
      },
      "source": [
        "## Best architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid search"
      ],
      "metadata": {
        "id": "JFfx0txDOhHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------\n",
        "# Data Loading and Augmentation Function\n",
        "# ---------------------------\n",
        "def create_dataloaders(n_augmented_images, batch_size):\n",
        "    # Download the OCTMNIST dataset without transforms to get raw PIL images.\n",
        "    raw_train_dataset = OCTMNIST(split='train', transform=None, download=True)\n",
        "    test_dataset = OCTMNIST(split='test', transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    ]), download=True)\n",
        "\n",
        "    print(\"Raw Train dataset size:\", len(raw_train_dataset))\n",
        "    print(\"Test dataset size:\", len(test_dataset))\n",
        "\n",
        "    # Define the augmentation transform.\n",
        "    augmentation_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])\n",
        "\n",
        "    # Augment every image in the raw training dataset.\n",
        "    augmented_images = []\n",
        "    augmented_labels = []\n",
        "    for i in range(len(raw_train_dataset)):\n",
        "        img, label = raw_train_dataset[i]\n",
        "        for _ in range(n_augmented_images):\n",
        "            aug_img = augmentation_transform(img)\n",
        "            augmented_images.append(aug_img)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "    print(\"Augmented Train dataset size:\", len(augmented_images))\n",
        "    augmented_train_dataset = TensorDataset(\n",
        "        torch.stack(augmented_images),\n",
        "        torch.from_numpy(np.array(augmented_labels))\n",
        "    )\n",
        "\n",
        "    # Create DataLoaders.\n",
        "    train_loader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# ---------------------------\n",
        "# Visualization Function (Optional)\n",
        "# ---------------------------\n",
        "def imshow(img, title=None):\n",
        "    img = img / 2 + 0.5  # Unnormalize.\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# Training Function with Checkpoint Saving and Print Statements\n",
        "# ---------------------------\n",
        "def train_model(model, config, run_name, train_loader, test_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
        "    num_epochs = config[\"num_epochs\"]\n",
        "\n",
        "    # Create checkpoint directory if it doesn't exist.\n",
        "    checkpoint_dir = \"checkpoints\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\n=== Epoch {epoch+1}/{num_epochs} ===\")\n",
        "\n",
        "        # Training phase.\n",
        "        model.train()\n",
        "        train_loss_epoch = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for i, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_epoch += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += targets.size(0)\n",
        "            train_correct += (predicted == targets).sum().item()\n",
        "\n",
        "            print(f\"[Batch {i+1}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_train_loss = train_loss_epoch / len(train_loader)\n",
        "        train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "        # Evaluation phase.\n",
        "        model.eval()\n",
        "        test_loss_epoch = 0.0\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                test_loss_epoch += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                test_total += targets.size(0)\n",
        "                test_correct += (predicted == targets).sum().item()\n",
        "\n",
        "        avg_test_loss = test_loss_epoch / len(test_loader)\n",
        "        test_accuracy = 100 * test_correct / test_total\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Test  Loss: {avg_test_loss:.4f} | Test  Acc: {test_accuracy:.2f}%\")\n",
        "\n",
        "        # Save model checkpoint.\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"{run_name}_epoch{epoch+1}.pth\")\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f\"Saved checkpoint: {checkpoint_path}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Grid Search Execution\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Define grid search parameter lists.\n",
        "    grid_search_params = {\n",
        "        \"num_epochs\": [3, 5],\n",
        "        \"lr\": [0.001, 0.0005],\n",
        "        \"batch_size\": [32, 64],\n",
        "        \"num_blocks\": [3, 5],\n",
        "        \"base_channels\": [32, 64],\n",
        "        \"kernel_size\": [3, 5],\n",
        "        \"n_augmented_images\": [5, 10]\n",
        "    }\n",
        "\n",
        "    # Iterate over every combination of the grid search parameters.\n",
        "    for num_epochs in grid_search_params[\"num_epochs\"]:\n",
        "        for lr in grid_search_params[\"lr\"]:\n",
        "            for bs in grid_search_params[\"batch_size\"]:\n",
        "                for num_blocks in grid_search_params[\"num_blocks\"]:\n",
        "                    for base_channels in grid_search_params[\"base_channels\"]:\n",
        "                        for kernel_size in grid_search_params[\"kernel_size\"]:\n",
        "                            for n_aug in grid_search_params[\"n_augmented_images\"]:\n",
        "                                # Define configuration for this run.\n",
        "                                config = {\n",
        "                                    \"in_channels\": 1,\n",
        "                                    \"num_epochs\": num_epochs,\n",
        "                                    \"lr\": lr,\n",
        "                                    \"batch_size\": bs,\n",
        "                                    \"num_blocks\": num_blocks,\n",
        "                                    \"base_channels\": base_channels,\n",
        "                                    \"kernel_size\": kernel_size,\n",
        "                                    \"activation\": \"LeakyReLU\",  # Can be logged or used to choose activation.\n",
        "                                    \"use_batchnorm\": True,\n",
        "                                    \"num_classes\": 4\n",
        "                                }\n",
        "\n",
        "                                run_name = (f\"epochs{num_epochs}_lr{lr}_bs{bs}_blocks{num_blocks}\"\n",
        "                                            f\"_base{base_channels}_kernel{kernel_size}_aug{n_aug}\")\n",
        "                                print(f\"\\nStarting run: {run_name}\")\n",
        "\n",
        "                                # Create data loaders for this configuration.\n",
        "                                train_loader, test_loader = create_dataloaders(n_augmented_images=n_aug, batch_size=bs)\n",
        "\n",
        "                                # (Optional) Visualize a few augmented training samples.\n",
        "                                dataiter = iter(train_loader)\n",
        "                                images, labels = next(dataiter)\n",
        "                                imshow(torchvision.utils.make_grid(images[:2]), title=f\"Sample Augmented OCTMNIST Images ({run_name})\")\n",
        "                                print(\"Labels:\", labels[:2])\n",
        "\n",
        "                                # Create a new model instance.\n",
        "                                model = CustomResNet(\n",
        "                                    in_channels=config['in_channels'],\n",
        "                                    num_blocks=config[\"num_blocks\"],\n",
        "                                    base_channels=config[\"base_channels\"],\n",
        "                                    kernel_size=config[\"kernel_size\"],\n",
        "                                    activation=nn.LeakyReLU(0.1, inplace=True),\n",
        "                                    use_batchnorm=config[\"use_batchnorm\"],\n",
        "                                    num_classes=config[\"num_classes\"]\n",
        "                                )\n",
        "                                print(model)\n",
        "\n",
        "                                # Train the model with the current configuration.\n",
        "                                train_model(model, config, run_name, train_loader, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "id": "AW0gvsTXOgyj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "6sP9HxoVtO9W",
        "G3TY4ytRkcEx",
        "_YUhZm4xuNUG",
        "ZVDGdQcqmNhu",
        "B1jxpGATE4s-",
        "y0BgAVIg63PA",
        "h-jiL1fPEy3-"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}