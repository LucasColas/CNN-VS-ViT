{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasColas/CNN-VS-ViT/blob/main/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet"
      ],
      "metadata": {
        "id": "tD6I13Z_s6ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install medmnist"
      ],
      "metadata": {
        "id": "wNX1kWxxuk39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGYvDa6_yfVI",
        "outputId": "103097b3-ff9c-4124-dcd5-a02d9b7fe71a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.24.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from medmnist import OCTMNIST  # Import the OCTMNIST dataset\n",
        "import wandb"
      ],
      "metadata": {
        "id": "dNe-jwnUuUwN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g4wnjaRxqNx",
        "outputId": "6226f39a-e6e9-4f72-9c88-ce831bf741bb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture"
      ],
      "metadata": {
        "id": "LxroXd-atMKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResidualBlock"
      ],
      "metadata": {
        "id": "47tSPB0PtFAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3,\n",
        "                 activation=nn.ReLU(inplace=True), use_batchnorm=True):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        padding = kernel_size // 2  # To keep spatial dimensions constant\n",
        "\n",
        "        # First convolution layer\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                               padding=padding, bias=not use_batchnorm)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels) if use_batchnorm else nn.Identity()\n",
        "\n",
        "        # Second convolution layer\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size,\n",
        "                               padding=padding, bias=not use_batchnorm)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels) if use_batchnorm else nn.Identity()\n",
        "\n",
        "        # Define the activation function\n",
        "        self.activation = activation\n",
        "\n",
        "        # If input and output channels differ, use a 1x1 conv to match dimensions\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.activation(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += shortcut\n",
        "        out = self.activation(out)\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jF2GPxJNs4zi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet"
      ],
      "metadata": {
        "id": "6sP9HxoVtO9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomResNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=3,         # Input channels (e.g., 3 for RGB images)\n",
        "                 num_blocks=4,          # Number of residual blocks\n",
        "                 base_channels=64,      # Number of channels for the first block\n",
        "                 kernel_size=3,         # Kernel size for convolutions\n",
        "                 activation=nn.ReLU(inplace=True),  # Activation function\n",
        "                 use_batchnorm=True,    # Whether to use BatchNorm\n",
        "                 num_classes=10         # Number of classes for final output\n",
        "                 ):\n",
        "        super(CustomResNet, self).__init__()\n",
        "\n",
        "        self.initial_conv = nn.Conv2d(in_channels, base_channels, kernel_size=kernel_size,\n",
        "                                      padding=kernel_size//2, bias=not use_batchnorm)\n",
        "        self.initial_bn = nn.BatchNorm2d(base_channels) if use_batchnorm else nn.Identity()\n",
        "        self.activation = activation\n",
        "\n",
        "        # Create a sequential container for residual blocks.\n",
        "        layers = []\n",
        "        # First block: input channels = base_channels, output channels = base_channels\n",
        "        for _ in range(num_blocks):\n",
        "            layers.append(ResidualBlock(base_channels, base_channels,\n",
        "                                        kernel_size=kernel_size,\n",
        "                                        activation=activation,\n",
        "                                        use_batchnorm=use_batchnorm))\n",
        "        self.residual_layers = nn.Sequential(*layers)\n",
        "\n",
        "        # Global average pooling and a final linear classifier.\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(base_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial_conv(x)\n",
        "        x = self.initial_bn(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.residual_layers(x)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "CGjqNFzitOow"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = CustomResNet(\n",
        "        in_channels=3,\n",
        "        num_blocks=5,               # You can choose how many blocks\n",
        "        base_channels=64,\n",
        "        kernel_size=3,              # Kernel size can be adjusted\n",
        "        activation=nn.LeakyReLU(0.1, inplace=True),  # You can choose any activation\n",
        "        use_batchnorm=True,         # Toggle batch normalization\n",
        "        num_classes=1000            # For example, for ImageNet classification\n",
        "    )\n",
        "\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "\n",
        "\n",
        "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "output = model(dummy_input)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvS7mOYitYeA",
        "outputId": "6bafe6e4-dcbf-42e0-d0c5-1a6b4c0f9193"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "CustomResNet(\n",
            "  (initial_conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (initial_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  (residual_layers): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (4): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "  )\n",
            "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=64, out_features=1000, bias=True)\n",
            ")\n",
            "Output shape: torch.Size([1, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "_YUhZm4xuNUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize for a single channel (grayscale)\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Download the OCTMNIST dataset.\n",
        "train_dataset = OCTMNIST(split='train', transform=transform, download=True)\n",
        "test_dataset  = OCTMNIST(split='test', transform=transform, download=True)\n",
        "\n",
        "print(\"Train dataset size:\", len(train_dataset))\n",
        "print(\"Test dataset size:\", len(test_dataset))\n",
        "\n",
        "# Create data loaders.\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Visualize a few training samples\n",
        "def imshow(img, title=None):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "imshow(torchvision.utils.make_grid(images[:2]), title=\"Sample OCTMNIST Images\")\n",
        "print(labels[:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "56d-4mfzufmN",
        "outputId": "29a7129d-2cd9-4050-dda7-0deb0ef61823"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 97477\n",
            "Test dataset size: 1000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1],\n",
            "        [3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CustomResNet(\n",
        "    in_channels=1,\n",
        "    num_blocks=5,               # adjust number of blocks as desired\n",
        "    base_channels=64,\n",
        "    kernel_size=3,\n",
        "    activation=nn.LeakyReLU(0.1, inplace=True),  # choose your activation function\n",
        "    use_batchnorm=True,\n",
        "    num_classes=4               # OCTMNIST has 4 classes\n",
        ")\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 5  # adjust as necessary\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 20 == 19:  # print every 20 mini-batches\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/20:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "# Evaluation on test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRBseAzTu7U7",
        "outputId": "87eb86ae-3c46-4e38-bd38-c5f296bade67"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomResNet(\n",
            "  (initial_conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (initial_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  (residual_layers): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (4): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "  )\n",
            "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Epoch [1/5], Step [20/1524], Loss: 0.9820\n",
            "Epoch [1/5], Step [40/1524], Loss: 0.7545\n",
            "Epoch [1/5], Step [60/1524], Loss: 0.7776\n",
            "Epoch [1/5], Step [80/1524], Loss: 0.7208\n",
            "Epoch [1/5], Step [100/1524], Loss: 0.6921\n",
            "Epoch [1/5], Step [120/1524], Loss: 0.6211\n",
            "Epoch [1/5], Step [140/1524], Loss: 0.6242\n",
            "Epoch [1/5], Step [160/1524], Loss: 0.6119\n",
            "Epoch [1/5], Step [180/1524], Loss: 0.6272\n",
            "Epoch [1/5], Step [200/1524], Loss: 0.5823\n",
            "Epoch [1/5], Step [220/1524], Loss: 0.5832\n",
            "Epoch [1/5], Step [240/1524], Loss: 0.5438\n",
            "Epoch [1/5], Step [260/1524], Loss: 0.5417\n",
            "Epoch [1/5], Step [280/1524], Loss: 0.5433\n",
            "Epoch [1/5], Step [300/1524], Loss: 0.5134\n",
            "Epoch [1/5], Step [320/1524], Loss: 0.5574\n",
            "Epoch [1/5], Step [340/1524], Loss: 0.5189\n",
            "Epoch [1/5], Step [360/1524], Loss: 0.5195\n",
            "Epoch [1/5], Step [380/1524], Loss: 0.4882\n",
            "Epoch [1/5], Step [400/1524], Loss: 0.5696\n",
            "Epoch [1/5], Step [420/1524], Loss: 0.4952\n",
            "Epoch [1/5], Step [440/1524], Loss: 0.4925\n",
            "Epoch [1/5], Step [460/1524], Loss: 0.5114\n",
            "Epoch [1/5], Step [480/1524], Loss: 0.5073\n",
            "Epoch [1/5], Step [500/1524], Loss: 0.4545\n",
            "Epoch [1/5], Step [520/1524], Loss: 0.4752\n",
            "Epoch [1/5], Step [540/1524], Loss: 0.4511\n",
            "Epoch [1/5], Step [560/1524], Loss: 0.4408\n",
            "Epoch [1/5], Step [580/1524], Loss: 0.4763\n",
            "Epoch [1/5], Step [600/1524], Loss: 0.4921\n",
            "Epoch [1/5], Step [620/1524], Loss: 0.4517\n",
            "Epoch [1/5], Step [640/1524], Loss: 0.4592\n",
            "Epoch [1/5], Step [660/1524], Loss: 0.4493\n",
            "Epoch [1/5], Step [680/1524], Loss: 0.4572\n",
            "Epoch [1/5], Step [700/1524], Loss: 0.4463\n",
            "Epoch [1/5], Step [720/1524], Loss: 0.4434\n",
            "Epoch [1/5], Step [740/1524], Loss: 0.4408\n",
            "Epoch [1/5], Step [760/1524], Loss: 0.4433\n",
            "Epoch [1/5], Step [780/1524], Loss: 0.3980\n",
            "Epoch [1/5], Step [800/1524], Loss: 0.4026\n",
            "Epoch [1/5], Step [820/1524], Loss: 0.3937\n",
            "Epoch [1/5], Step [840/1524], Loss: 0.3869\n",
            "Epoch [1/5], Step [860/1524], Loss: 0.4023\n",
            "Epoch [1/5], Step [880/1524], Loss: 0.3886\n",
            "Epoch [1/5], Step [900/1524], Loss: 0.3812\n",
            "Epoch [1/5], Step [920/1524], Loss: 0.3929\n",
            "Epoch [1/5], Step [940/1524], Loss: 0.4276\n",
            "Epoch [1/5], Step [960/1524], Loss: 0.3894\n",
            "Epoch [1/5], Step [980/1524], Loss: 0.3878\n",
            "Epoch [1/5], Step [1000/1524], Loss: 0.4151\n",
            "Epoch [1/5], Step [1020/1524], Loss: 0.4301\n",
            "Epoch [1/5], Step [1040/1524], Loss: 0.3753\n",
            "Epoch [1/5], Step [1060/1524], Loss: 0.3723\n",
            "Epoch [1/5], Step [1080/1524], Loss: 0.3702\n",
            "Epoch [1/5], Step [1100/1524], Loss: 0.3588\n",
            "Epoch [1/5], Step [1120/1524], Loss: 0.3267\n",
            "Epoch [1/5], Step [1140/1524], Loss: 0.4106\n",
            "Epoch [1/5], Step [1160/1524], Loss: 0.3642\n",
            "Epoch [1/5], Step [1180/1524], Loss: 0.3947\n",
            "Epoch [1/5], Step [1200/1524], Loss: 0.4039\n",
            "Epoch [1/5], Step [1220/1524], Loss: 0.3585\n",
            "Epoch [1/5], Step [1240/1524], Loss: 0.3790\n",
            "Epoch [1/5], Step [1260/1524], Loss: 0.3396\n",
            "Epoch [1/5], Step [1280/1524], Loss: 0.3252\n",
            "Epoch [1/5], Step [1300/1524], Loss: 0.3718\n",
            "Epoch [1/5], Step [1320/1524], Loss: 0.3777\n",
            "Epoch [1/5], Step [1340/1524], Loss: 0.4118\n",
            "Epoch [1/5], Step [1360/1524], Loss: 0.3209\n",
            "Epoch [1/5], Step [1380/1524], Loss: 0.3549\n",
            "Epoch [1/5], Step [1400/1524], Loss: 0.3611\n",
            "Epoch [1/5], Step [1420/1524], Loss: 0.3539\n",
            "Epoch [1/5], Step [1440/1524], Loss: 0.3668\n",
            "Epoch [1/5], Step [1460/1524], Loss: 0.3336\n",
            "Epoch [1/5], Step [1480/1524], Loss: 0.3997\n",
            "Epoch [1/5], Step [1500/1524], Loss: 0.3508\n",
            "Epoch [1/5], Step [1520/1524], Loss: 0.3739\n",
            "Epoch [2/5], Step [20/1524], Loss: 0.3818\n",
            "Epoch [2/5], Step [40/1524], Loss: 0.3387\n",
            "Epoch [2/5], Step [60/1524], Loss: 0.2972\n",
            "Epoch [2/5], Step [80/1524], Loss: 0.3676\n",
            "Epoch [2/5], Step [100/1524], Loss: 0.3363\n",
            "Epoch [2/5], Step [120/1524], Loss: 0.3277\n",
            "Epoch [2/5], Step [140/1524], Loss: 0.3452\n",
            "Epoch [2/5], Step [160/1524], Loss: 0.3281\n",
            "Epoch [2/5], Step [180/1524], Loss: 0.3278\n",
            "Epoch [2/5], Step [200/1524], Loss: 0.3318\n",
            "Epoch [2/5], Step [220/1524], Loss: 0.3626\n",
            "Epoch [2/5], Step [240/1524], Loss: 0.2792\n",
            "Epoch [2/5], Step [260/1524], Loss: 0.2888\n",
            "Epoch [2/5], Step [280/1524], Loss: 0.3654\n",
            "Epoch [2/5], Step [300/1524], Loss: 0.3184\n",
            "Epoch [2/5], Step [320/1524], Loss: 0.3083\n",
            "Epoch [2/5], Step [340/1524], Loss: 0.3074\n",
            "Epoch [2/5], Step [360/1524], Loss: 0.3114\n",
            "Epoch [2/5], Step [380/1524], Loss: 0.3439\n",
            "Epoch [2/5], Step [400/1524], Loss: 0.3183\n",
            "Epoch [2/5], Step [420/1524], Loss: 0.3403\n",
            "Epoch [2/5], Step [440/1524], Loss: 0.3226\n",
            "Epoch [2/5], Step [460/1524], Loss: 0.3049\n",
            "Epoch [2/5], Step [480/1524], Loss: 0.3455\n",
            "Epoch [2/5], Step [500/1524], Loss: 0.3349\n",
            "Epoch [2/5], Step [520/1524], Loss: 0.3399\n",
            "Epoch [2/5], Step [540/1524], Loss: 0.2916\n",
            "Epoch [2/5], Step [560/1524], Loss: 0.3064\n",
            "Epoch [2/5], Step [580/1524], Loss: 0.3259\n",
            "Epoch [2/5], Step [600/1524], Loss: 0.2953\n",
            "Epoch [2/5], Step [620/1524], Loss: 0.3223\n",
            "Epoch [2/5], Step [640/1524], Loss: 0.3462\n",
            "Epoch [2/5], Step [660/1524], Loss: 0.3355\n",
            "Epoch [2/5], Step [680/1524], Loss: 0.3122\n",
            "Epoch [2/5], Step [700/1524], Loss: 0.2844\n",
            "Epoch [2/5], Step [720/1524], Loss: 0.3271\n",
            "Epoch [2/5], Step [740/1524], Loss: 0.3219\n",
            "Epoch [2/5], Step [760/1524], Loss: 0.3181\n",
            "Epoch [2/5], Step [780/1524], Loss: 0.3162\n",
            "Epoch [2/5], Step [800/1524], Loss: 0.3239\n",
            "Epoch [2/5], Step [820/1524], Loss: 0.2774\n",
            "Epoch [2/5], Step [840/1524], Loss: 0.3283\n",
            "Epoch [2/5], Step [860/1524], Loss: 0.3158\n",
            "Epoch [2/5], Step [880/1524], Loss: 0.2972\n",
            "Epoch [2/5], Step [900/1524], Loss: 0.2919\n",
            "Epoch [2/5], Step [920/1524], Loss: 0.3024\n",
            "Epoch [2/5], Step [940/1524], Loss: 0.3196\n",
            "Epoch [2/5], Step [960/1524], Loss: 0.3454\n",
            "Epoch [2/5], Step [980/1524], Loss: 0.3172\n",
            "Epoch [2/5], Step [1000/1524], Loss: 0.3096\n",
            "Epoch [2/5], Step [1020/1524], Loss: 0.3038\n",
            "Epoch [2/5], Step [1040/1524], Loss: 0.3565\n",
            "Epoch [2/5], Step [1060/1524], Loss: 0.2914\n",
            "Epoch [2/5], Step [1080/1524], Loss: 0.3352\n",
            "Epoch [2/5], Step [1100/1524], Loss: 0.2872\n",
            "Epoch [2/5], Step [1120/1524], Loss: 0.2999\n",
            "Epoch [2/5], Step [1140/1524], Loss: 0.2945\n",
            "Epoch [2/5], Step [1160/1524], Loss: 0.3317\n",
            "Epoch [2/5], Step [1180/1524], Loss: 0.3591\n",
            "Epoch [2/5], Step [1200/1524], Loss: 0.3045\n",
            "Epoch [2/5], Step [1220/1524], Loss: 0.3292\n",
            "Epoch [2/5], Step [1240/1524], Loss: 0.2931\n",
            "Epoch [2/5], Step [1260/1524], Loss: 0.3129\n",
            "Epoch [2/5], Step [1280/1524], Loss: 0.3198\n",
            "Epoch [2/5], Step [1300/1524], Loss: 0.2969\n",
            "Epoch [2/5], Step [1320/1524], Loss: 0.3065\n",
            "Epoch [2/5], Step [1340/1524], Loss: 0.2858\n",
            "Epoch [2/5], Step [1360/1524], Loss: 0.2955\n",
            "Epoch [2/5], Step [1380/1524], Loss: 0.2926\n",
            "Epoch [2/5], Step [1400/1524], Loss: 0.3154\n",
            "Epoch [2/5], Step [1420/1524], Loss: 0.2781\n",
            "Epoch [2/5], Step [1440/1524], Loss: 0.3104\n",
            "Epoch [2/5], Step [1460/1524], Loss: 0.2964\n",
            "Epoch [2/5], Step [1480/1524], Loss: 0.3054\n",
            "Epoch [2/5], Step [1500/1524], Loss: 0.2943\n",
            "Epoch [2/5], Step [1520/1524], Loss: 0.3602\n",
            "Epoch [3/5], Step [20/1524], Loss: 0.2627\n",
            "Epoch [3/5], Step [40/1524], Loss: 0.2789\n",
            "Epoch [3/5], Step [60/1524], Loss: 0.3255\n",
            "Epoch [3/5], Step [80/1524], Loss: 0.3213\n",
            "Epoch [3/5], Step [100/1524], Loss: 0.2991\n",
            "Epoch [3/5], Step [120/1524], Loss: 0.2793\n",
            "Epoch [3/5], Step [140/1524], Loss: 0.3053\n",
            "Epoch [3/5], Step [160/1524], Loss: 0.3080\n",
            "Epoch [3/5], Step [180/1524], Loss: 0.2851\n",
            "Epoch [3/5], Step [200/1524], Loss: 0.3318\n",
            "Epoch [3/5], Step [220/1524], Loss: 0.2576\n",
            "Epoch [3/5], Step [240/1524], Loss: 0.2493\n",
            "Epoch [3/5], Step [260/1524], Loss: 0.3001\n",
            "Epoch [3/5], Step [280/1524], Loss: 0.3058\n",
            "Epoch [3/5], Step [300/1524], Loss: 0.2792\n",
            "Epoch [3/5], Step [320/1524], Loss: 0.3160\n",
            "Epoch [3/5], Step [340/1524], Loss: 0.2568\n",
            "Epoch [3/5], Step [360/1524], Loss: 0.2962\n",
            "Epoch [3/5], Step [380/1524], Loss: 0.2993\n",
            "Epoch [3/5], Step [400/1524], Loss: 0.2987\n",
            "Epoch [3/5], Step [420/1524], Loss: 0.2778\n",
            "Epoch [3/5], Step [440/1524], Loss: 0.2911\n",
            "Epoch [3/5], Step [460/1524], Loss: 0.2818\n",
            "Epoch [3/5], Step [480/1524], Loss: 0.2643\n",
            "Epoch [3/5], Step [500/1524], Loss: 0.2871\n",
            "Epoch [3/5], Step [520/1524], Loss: 0.2750\n",
            "Epoch [3/5], Step [540/1524], Loss: 0.2953\n",
            "Epoch [3/5], Step [560/1524], Loss: 0.2810\n",
            "Epoch [3/5], Step [580/1524], Loss: 0.2970\n",
            "Epoch [3/5], Step [600/1524], Loss: 0.2885\n",
            "Epoch [3/5], Step [620/1524], Loss: 0.2592\n",
            "Epoch [3/5], Step [640/1524], Loss: 0.2933\n",
            "Epoch [3/5], Step [660/1524], Loss: 0.2741\n",
            "Epoch [3/5], Step [680/1524], Loss: 0.2505\n",
            "Epoch [3/5], Step [700/1524], Loss: 0.2494\n",
            "Epoch [3/5], Step [720/1524], Loss: 0.2959\n",
            "Epoch [3/5], Step [740/1524], Loss: 0.2652\n",
            "Epoch [3/5], Step [760/1524], Loss: 0.2704\n",
            "Epoch [3/5], Step [780/1524], Loss: 0.2611\n",
            "Epoch [3/5], Step [800/1524], Loss: 0.2784\n",
            "Epoch [3/5], Step [820/1524], Loss: 0.2956\n",
            "Epoch [3/5], Step [840/1524], Loss: 0.3502\n",
            "Epoch [3/5], Step [860/1524], Loss: 0.2980\n",
            "Epoch [3/5], Step [880/1524], Loss: 0.2991\n",
            "Epoch [3/5], Step [900/1524], Loss: 0.2950\n",
            "Epoch [3/5], Step [920/1524], Loss: 0.2398\n",
            "Epoch [3/5], Step [940/1524], Loss: 0.2881\n",
            "Epoch [3/5], Step [960/1524], Loss: 0.2965\n",
            "Epoch [3/5], Step [980/1524], Loss: 0.3225\n",
            "Epoch [3/5], Step [1000/1524], Loss: 0.2792\n",
            "Epoch [3/5], Step [1020/1524], Loss: 0.3052\n",
            "Epoch [3/5], Step [1040/1524], Loss: 0.2928\n",
            "Epoch [3/5], Step [1060/1524], Loss: 0.2755\n",
            "Epoch [3/5], Step [1080/1524], Loss: 0.3037\n",
            "Epoch [3/5], Step [1100/1524], Loss: 0.2704\n",
            "Epoch [3/5], Step [1120/1524], Loss: 0.3036\n",
            "Epoch [3/5], Step [1140/1524], Loss: 0.3426\n",
            "Epoch [3/5], Step [1160/1524], Loss: 0.2778\n",
            "Epoch [3/5], Step [1180/1524], Loss: 0.3062\n",
            "Epoch [3/5], Step [1200/1524], Loss: 0.2733\n",
            "Epoch [3/5], Step [1220/1524], Loss: 0.2685\n",
            "Epoch [3/5], Step [1240/1524], Loss: 0.2917\n",
            "Epoch [3/5], Step [1260/1524], Loss: 0.2823\n",
            "Epoch [3/5], Step [1280/1524], Loss: 0.2691\n",
            "Epoch [3/5], Step [1300/1524], Loss: 0.2950\n",
            "Epoch [3/5], Step [1320/1524], Loss: 0.3167\n",
            "Epoch [3/5], Step [1340/1524], Loss: 0.2899\n",
            "Epoch [3/5], Step [1360/1524], Loss: 0.2620\n",
            "Epoch [3/5], Step [1380/1524], Loss: 0.3063\n",
            "Epoch [3/5], Step [1400/1524], Loss: 0.3054\n",
            "Epoch [3/5], Step [1420/1524], Loss: 0.2848\n",
            "Epoch [3/5], Step [1440/1524], Loss: 0.3009\n",
            "Epoch [3/5], Step [1460/1524], Loss: 0.2903\n",
            "Epoch [3/5], Step [1480/1524], Loss: 0.2797\n",
            "Epoch [3/5], Step [1500/1524], Loss: 0.3053\n",
            "Epoch [3/5], Step [1520/1524], Loss: 0.2828\n",
            "Epoch [4/5], Step [20/1524], Loss: 0.2948\n",
            "Epoch [4/5], Step [40/1524], Loss: 0.2738\n",
            "Epoch [4/5], Step [60/1524], Loss: 0.2828\n",
            "Epoch [4/5], Step [80/1524], Loss: 0.2891\n",
            "Epoch [4/5], Step [100/1524], Loss: 0.2818\n",
            "Epoch [4/5], Step [120/1524], Loss: 0.2987\n",
            "Epoch [4/5], Step [140/1524], Loss: 0.2773\n",
            "Epoch [4/5], Step [160/1524], Loss: 0.2683\n",
            "Epoch [4/5], Step [180/1524], Loss: 0.3010\n",
            "Epoch [4/5], Step [200/1524], Loss: 0.2987\n",
            "Epoch [4/5], Step [220/1524], Loss: 0.2431\n",
            "Epoch [4/5], Step [240/1524], Loss: 0.3111\n",
            "Epoch [4/5], Step [260/1524], Loss: 0.2471\n",
            "Epoch [4/5], Step [280/1524], Loss: 0.2630\n",
            "Epoch [4/5], Step [300/1524], Loss: 0.2912\n",
            "Epoch [4/5], Step [320/1524], Loss: 0.2342\n",
            "Epoch [4/5], Step [340/1524], Loss: 0.2677\n",
            "Epoch [4/5], Step [360/1524], Loss: 0.2556\n",
            "Epoch [4/5], Step [380/1524], Loss: 0.2746\n",
            "Epoch [4/5], Step [400/1524], Loss: 0.2937\n",
            "Epoch [4/5], Step [420/1524], Loss: 0.2860\n",
            "Epoch [4/5], Step [440/1524], Loss: 0.2903\n",
            "Epoch [4/5], Step [460/1524], Loss: 0.2639\n",
            "Epoch [4/5], Step [480/1524], Loss: 0.2955\n",
            "Epoch [4/5], Step [500/1524], Loss: 0.3016\n",
            "Epoch [4/5], Step [520/1524], Loss: 0.2198\n",
            "Epoch [4/5], Step [540/1524], Loss: 0.2951\n",
            "Epoch [4/5], Step [560/1524], Loss: 0.2740\n",
            "Epoch [4/5], Step [580/1524], Loss: 0.2597\n",
            "Epoch [4/5], Step [600/1524], Loss: 0.3210\n",
            "Epoch [4/5], Step [620/1524], Loss: 0.2682\n",
            "Epoch [4/5], Step [640/1524], Loss: 0.2831\n",
            "Epoch [4/5], Step [660/1524], Loss: 0.3104\n",
            "Epoch [4/5], Step [680/1524], Loss: 0.2834\n",
            "Epoch [4/5], Step [700/1524], Loss: 0.2494\n",
            "Epoch [4/5], Step [720/1524], Loss: 0.2592\n",
            "Epoch [4/5], Step [740/1524], Loss: 0.2865\n",
            "Epoch [4/5], Step [760/1524], Loss: 0.3073\n",
            "Epoch [4/5], Step [780/1524], Loss: 0.3121\n",
            "Epoch [4/5], Step [800/1524], Loss: 0.2633\n",
            "Epoch [4/5], Step [820/1524], Loss: 0.2601\n",
            "Epoch [4/5], Step [840/1524], Loss: 0.2558\n",
            "Epoch [4/5], Step [860/1524], Loss: 0.2826\n",
            "Epoch [4/5], Step [880/1524], Loss: 0.2774\n",
            "Epoch [4/5], Step [900/1524], Loss: 0.2375\n",
            "Epoch [4/5], Step [920/1524], Loss: 0.2717\n",
            "Epoch [4/5], Step [940/1524], Loss: 0.2584\n",
            "Epoch [4/5], Step [960/1524], Loss: 0.2458\n",
            "Epoch [4/5], Step [980/1524], Loss: 0.2694\n",
            "Epoch [4/5], Step [1000/1524], Loss: 0.2566\n",
            "Epoch [4/5], Step [1020/1524], Loss: 0.2851\n",
            "Epoch [4/5], Step [1040/1524], Loss: 0.2648\n",
            "Epoch [4/5], Step [1060/1524], Loss: 0.2420\n",
            "Epoch [4/5], Step [1080/1524], Loss: 0.2405\n",
            "Epoch [4/5], Step [1100/1524], Loss: 0.2865\n",
            "Epoch [4/5], Step [1120/1524], Loss: 0.2387\n",
            "Epoch [4/5], Step [1140/1524], Loss: 0.2957\n",
            "Epoch [4/5], Step [1160/1524], Loss: 0.2529\n",
            "Epoch [4/5], Step [1180/1524], Loss: 0.2712\n",
            "Epoch [4/5], Step [1200/1524], Loss: 0.2638\n",
            "Epoch [4/5], Step [1220/1524], Loss: 0.2689\n",
            "Epoch [4/5], Step [1240/1524], Loss: 0.2517\n",
            "Epoch [4/5], Step [1260/1524], Loss: 0.2428\n",
            "Epoch [4/5], Step [1280/1524], Loss: 0.2547\n",
            "Epoch [4/5], Step [1300/1524], Loss: 0.2975\n",
            "Epoch [4/5], Step [1320/1524], Loss: 0.2753\n",
            "Epoch [4/5], Step [1340/1524], Loss: 0.2513\n",
            "Epoch [4/5], Step [1360/1524], Loss: 0.2441\n",
            "Epoch [4/5], Step [1380/1524], Loss: 0.2559\n",
            "Epoch [4/5], Step [1400/1524], Loss: 0.2658\n",
            "Epoch [4/5], Step [1420/1524], Loss: 0.2523\n",
            "Epoch [4/5], Step [1440/1524], Loss: 0.2761\n",
            "Epoch [4/5], Step [1460/1524], Loss: 0.2989\n",
            "Epoch [4/5], Step [1480/1524], Loss: 0.2717\n",
            "Epoch [4/5], Step [1500/1524], Loss: 0.2449\n",
            "Epoch [4/5], Step [1520/1524], Loss: 0.2702\n",
            "Epoch [5/5], Step [20/1524], Loss: 0.2599\n",
            "Epoch [5/5], Step [40/1524], Loss: 0.2739\n",
            "Epoch [5/5], Step [60/1524], Loss: 0.2740\n",
            "Epoch [5/5], Step [80/1524], Loss: 0.2735\n",
            "Epoch [5/5], Step [100/1524], Loss: 0.2745\n",
            "Epoch [5/5], Step [120/1524], Loss: 0.2738\n",
            "Epoch [5/5], Step [140/1524], Loss: 0.2778\n",
            "Epoch [5/5], Step [160/1524], Loss: 0.2702\n",
            "Epoch [5/5], Step [180/1524], Loss: 0.2475\n",
            "Epoch [5/5], Step [200/1524], Loss: 0.2654\n",
            "Epoch [5/5], Step [220/1524], Loss: 0.2335\n",
            "Epoch [5/5], Step [240/1524], Loss: 0.2508\n",
            "Epoch [5/5], Step [260/1524], Loss: 0.2479\n",
            "Epoch [5/5], Step [280/1524], Loss: 0.2321\n",
            "Epoch [5/5], Step [300/1524], Loss: 0.3003\n",
            "Epoch [5/5], Step [320/1524], Loss: 0.2553\n",
            "Epoch [5/5], Step [340/1524], Loss: 0.2414\n",
            "Epoch [5/5], Step [360/1524], Loss: 0.3066\n",
            "Epoch [5/5], Step [380/1524], Loss: 0.2741\n",
            "Epoch [5/5], Step [400/1524], Loss: 0.2528\n",
            "Epoch [5/5], Step [420/1524], Loss: 0.2405\n",
            "Epoch [5/5], Step [440/1524], Loss: 0.2637\n",
            "Epoch [5/5], Step [460/1524], Loss: 0.2684\n",
            "Epoch [5/5], Step [480/1524], Loss: 0.2856\n",
            "Epoch [5/5], Step [500/1524], Loss: 0.2399\n",
            "Epoch [5/5], Step [520/1524], Loss: 0.3148\n",
            "Epoch [5/5], Step [540/1524], Loss: 0.2638\n",
            "Epoch [5/5], Step [560/1524], Loss: 0.2654\n",
            "Epoch [5/5], Step [580/1524], Loss: 0.2752\n",
            "Epoch [5/5], Step [600/1524], Loss: 0.2544\n",
            "Epoch [5/5], Step [620/1524], Loss: 0.2663\n",
            "Epoch [5/5], Step [640/1524], Loss: 0.2763\n",
            "Epoch [5/5], Step [660/1524], Loss: 0.2521\n",
            "Epoch [5/5], Step [680/1524], Loss: 0.2719\n",
            "Epoch [5/5], Step [700/1524], Loss: 0.2471\n",
            "Epoch [5/5], Step [720/1524], Loss: 0.2735\n",
            "Epoch [5/5], Step [740/1524], Loss: 0.2624\n",
            "Epoch [5/5], Step [760/1524], Loss: 0.2379\n",
            "Epoch [5/5], Step [780/1524], Loss: 0.2469\n",
            "Epoch [5/5], Step [800/1524], Loss: 0.2621\n",
            "Epoch [5/5], Step [820/1524], Loss: 0.2301\n",
            "Epoch [5/5], Step [840/1524], Loss: 0.2757\n",
            "Epoch [5/5], Step [860/1524], Loss: 0.2376\n",
            "Epoch [5/5], Step [880/1524], Loss: 0.2568\n",
            "Epoch [5/5], Step [900/1524], Loss: 0.2441\n",
            "Epoch [5/5], Step [920/1524], Loss: 0.2410\n",
            "Epoch [5/5], Step [940/1524], Loss: 0.2543\n",
            "Epoch [5/5], Step [960/1524], Loss: 0.2665\n",
            "Epoch [5/5], Step [980/1524], Loss: 0.2585\n",
            "Epoch [5/5], Step [1000/1524], Loss: 0.2476\n",
            "Epoch [5/5], Step [1020/1524], Loss: 0.2447\n",
            "Epoch [5/5], Step [1040/1524], Loss: 0.2742\n",
            "Epoch [5/5], Step [1060/1524], Loss: 0.2630\n",
            "Epoch [5/5], Step [1080/1524], Loss: 0.2407\n",
            "Epoch [5/5], Step [1100/1524], Loss: 0.2860\n",
            "Epoch [5/5], Step [1120/1524], Loss: 0.2523\n",
            "Epoch [5/5], Step [1140/1524], Loss: 0.2557\n",
            "Epoch [5/5], Step [1160/1524], Loss: 0.3032\n",
            "Epoch [5/5], Step [1180/1524], Loss: 0.2632\n",
            "Epoch [5/5], Step [1200/1524], Loss: 0.2893\n",
            "Epoch [5/5], Step [1220/1524], Loss: 0.2526\n",
            "Epoch [5/5], Step [1240/1524], Loss: 0.2425\n",
            "Epoch [5/5], Step [1260/1524], Loss: 0.2380\n",
            "Epoch [5/5], Step [1280/1524], Loss: 0.2596\n",
            "Epoch [5/5], Step [1300/1524], Loss: 0.2448\n",
            "Epoch [5/5], Step [1320/1524], Loss: 0.2409\n",
            "Epoch [5/5], Step [1340/1524], Loss: 0.2237\n",
            "Epoch [5/5], Step [1360/1524], Loss: 0.2492\n",
            "Epoch [5/5], Step [1380/1524], Loss: 0.2534\n",
            "Epoch [5/5], Step [1400/1524], Loss: 0.2832\n",
            "Epoch [5/5], Step [1420/1524], Loss: 0.2345\n",
            "Epoch [5/5], Step [1440/1524], Loss: 0.2789\n",
            "Epoch [5/5], Step [1460/1524], Loss: 0.2716\n",
            "Epoch [5/5], Step [1480/1524], Loss: 0.2396\n",
            "Epoch [5/5], Step [1500/1524], Loss: 0.2279\n",
            "Epoch [5/5], Step [1520/1524], Loss: 0.2281\n",
            "Test Accuracy: 76.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, config):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
        "    num_epochs = config[\"num_epochs\"]\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 20 == 19:  # Log every 20 mini-batches\n",
        "                avg_loss = running_loss / 20\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {avg_loss:.4f}\")\n",
        "                wandb.log({\"epoch\": epoch+1, \"loss\": avg_loss})\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Evaluate on the test set after each epoch\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += targets.size(0)\n",
        "                correct += (predicted == targets).sum().item()\n",
        "        test_accuracy = 100 * correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Test Accuracy: {test_accuracy:.2f}%\")\n",
        "        wandb.log({\"epoch\": epoch+1, \"test_accuracy\": test_accuracy})\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TTT8r-Alz7tg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "\n",
        "# Configuration dictionary for wandb\n",
        "config = {\n",
        "    \"in_channels\": 1,\n",
        "    \"num_epochs\": 5,            # Adjust number of epochs as necessary\n",
        "    \"lr\": 0.001,                # Learning rate\n",
        "    \"batch_size\": 64,           # Batch size (should match your DataLoader)\n",
        "    \"num_blocks\": 5,            # Number of residual blocks in the network\n",
        "    \"base_channels\": 64,        # Number of filters in the first block\n",
        "    \"kernel_size\": 3,           # Kernel size for convolutions\n",
        "    \"activation\": \"LeakyReLU\",  # Name of the activation function\n",
        "    \"use_batchnorm\": True,      # Whether to use batch normalization\n",
        "    \"num_classes\": 4            # OCTMNIST has 4 classes\n",
        "}\n",
        "\n",
        "# Initialize wandb run and train the model\n",
        "with wandb.init(\n",
        "    config=config,\n",
        "    project='CNN VS ViT',   # Title of your project\n",
        "    group='Test',           # Group name for your run\n",
        "    save_code=True,\n",
        "    name='simple resnet',\n",
        "):\n",
        "    # Create the model with custom configuration\n",
        "    model = CustomResNet(\n",
        "        in_channels=config['in_channels'],  # OCTMNIST images are single-channel if you have preprocessed accordingly\n",
        "        num_blocks=config[\"num_blocks\"],\n",
        "        base_channels=config[\"base_channels\"],\n",
        "        kernel_size=config[\"kernel_size\"],\n",
        "        activation=nn.LeakyReLU(0.1, inplace=True),\n",
        "        use_batchnorm=config[\"use_batchnorm\"],\n",
        "        num_classes=config[\"num_classes\"]\n",
        "    )\n",
        "    print(model)\n",
        "\n",
        "    # Train the model (train_loader and test_loader should be defined before)\n",
        "    train_model(model, config)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NKnhU1T-0I_w",
        "outputId": "f11837aa-17bd-48c3-994f-3261c6096dbc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250326_141738-wmu52fie</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/wmu52fie' target=\"_blank\">simple resnet</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/wmu52fie' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/wmu52fie</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomResNet(\n",
            "  (initial_conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (initial_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  (residual_layers): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (4): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "  )\n",
            "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Epoch [1/5], Step [20/1524], Loss: 0.9758\n",
            "Epoch [1/5], Step [40/1524], Loss: 0.8532\n",
            "Epoch [1/5], Step [60/1524], Loss: 0.7885\n",
            "Epoch [1/5], Step [80/1524], Loss: 0.7502\n",
            "Epoch [1/5], Step [100/1524], Loss: 0.7057\n",
            "Epoch [1/5], Step [120/1524], Loss: 0.6730\n",
            "Epoch [1/5], Step [140/1524], Loss: 0.6199\n",
            "Epoch [1/5], Step [160/1524], Loss: 0.6281\n",
            "Epoch [1/5], Step [180/1524], Loss: 0.5954\n",
            "Epoch [1/5], Step [200/1524], Loss: 0.5740\n",
            "Epoch [1/5], Step [220/1524], Loss: 0.6317\n",
            "Epoch [1/5], Step [240/1524], Loss: 0.5776\n",
            "Epoch [1/5], Step [260/1524], Loss: 0.5775\n",
            "Epoch [1/5], Step [280/1524], Loss: 0.6076\n",
            "Epoch [1/5], Step [300/1524], Loss: 0.5982\n",
            "Epoch [1/5], Step [320/1524], Loss: 0.5743\n",
            "Epoch [1/5], Step [340/1524], Loss: 0.5542\n",
            "Epoch [1/5], Step [360/1524], Loss: 0.5547\n",
            "Epoch [1/5], Step [380/1524], Loss: 0.5286\n",
            "Epoch [1/5], Step [400/1524], Loss: 0.5321\n",
            "Epoch [1/5], Step [420/1524], Loss: 0.5112\n",
            "Epoch [1/5], Step [440/1524], Loss: 0.5566\n",
            "Epoch [1/5], Step [460/1524], Loss: 0.5449\n",
            "Epoch [1/5], Step [480/1524], Loss: 0.4563\n",
            "Epoch [1/5], Step [500/1524], Loss: 0.4914\n",
            "Epoch [1/5], Step [520/1524], Loss: 0.5293\n",
            "Epoch [1/5], Step [540/1524], Loss: 0.5184\n",
            "Epoch [1/5], Step [560/1524], Loss: 0.4633\n",
            "Epoch [1/5], Step [580/1524], Loss: 0.4617\n",
            "Epoch [1/5], Step [600/1524], Loss: 0.4834\n",
            "Epoch [1/5], Step [620/1524], Loss: 0.4586\n",
            "Epoch [1/5], Step [640/1524], Loss: 0.5070\n",
            "Epoch [1/5], Step [660/1524], Loss: 0.4345\n",
            "Epoch [1/5], Step [680/1524], Loss: 0.4455\n",
            "Epoch [1/5], Step [700/1524], Loss: 0.4336\n",
            "Epoch [1/5], Step [720/1524], Loss: 0.4897\n",
            "Epoch [1/5], Step [740/1524], Loss: 0.4561\n",
            "Epoch [1/5], Step [760/1524], Loss: 0.4278\n",
            "Epoch [1/5], Step [780/1524], Loss: 0.4689\n",
            "Epoch [1/5], Step [800/1524], Loss: 0.4528\n",
            "Epoch [1/5], Step [820/1524], Loss: 0.4211\n",
            "Epoch [1/5], Step [840/1524], Loss: 0.3853\n",
            "Epoch [1/5], Step [860/1524], Loss: 0.4113\n",
            "Epoch [1/5], Step [880/1524], Loss: 0.3975\n",
            "Epoch [1/5], Step [900/1524], Loss: 0.3527\n",
            "Epoch [1/5], Step [920/1524], Loss: 0.3984\n",
            "Epoch [1/5], Step [940/1524], Loss: 0.4317\n",
            "Epoch [1/5], Step [960/1524], Loss: 0.4340\n",
            "Epoch [1/5], Step [980/1524], Loss: 0.3967\n",
            "Epoch [1/5], Step [1000/1524], Loss: 0.4328\n",
            "Epoch [1/5], Step [1020/1524], Loss: 0.3796\n",
            "Epoch [1/5], Step [1040/1524], Loss: 0.4148\n",
            "Epoch [1/5], Step [1060/1524], Loss: 0.4057\n",
            "Epoch [1/5], Step [1080/1524], Loss: 0.4241\n",
            "Epoch [1/5], Step [1100/1524], Loss: 0.4103\n",
            "Epoch [1/5], Step [1120/1524], Loss: 0.3800\n",
            "Epoch [1/5], Step [1140/1524], Loss: 0.3811\n",
            "Epoch [1/5], Step [1160/1524], Loss: 0.3977\n",
            "Epoch [1/5], Step [1180/1524], Loss: 0.3762\n",
            "Epoch [1/5], Step [1200/1524], Loss: 0.3469\n",
            "Epoch [1/5], Step [1220/1524], Loss: 0.3459\n",
            "Epoch [1/5], Step [1240/1524], Loss: 0.3650\n",
            "Epoch [1/5], Step [1260/1524], Loss: 0.3646\n",
            "Epoch [1/5], Step [1280/1524], Loss: 0.3738\n",
            "Epoch [1/5], Step [1300/1524], Loss: 0.3114\n",
            "Epoch [1/5], Step [1320/1524], Loss: 0.3740\n",
            "Epoch [1/5], Step [1340/1524], Loss: 0.3393\n",
            "Epoch [1/5], Step [1360/1524], Loss: 0.3476\n",
            "Epoch [1/5], Step [1380/1524], Loss: 0.3211\n",
            "Epoch [1/5], Step [1400/1524], Loss: 0.3648\n",
            "Epoch [1/5], Step [1420/1524], Loss: 0.3981\n",
            "Epoch [1/5], Step [1440/1524], Loss: 0.3554\n",
            "Epoch [1/5], Step [1460/1524], Loss: 0.3728\n",
            "Epoch [1/5], Step [1480/1524], Loss: 0.3141\n",
            "Epoch [1/5], Step [1500/1524], Loss: 0.3244\n",
            "Epoch [1/5], Step [1520/1524], Loss: 0.3458\n",
            "Epoch [1/5] Test Accuracy: 44.10%\n",
            "Epoch [2/5], Step [20/1524], Loss: 0.3898\n",
            "Epoch [2/5], Step [40/1524], Loss: 0.3446\n",
            "Epoch [2/5], Step [60/1524], Loss: 0.3339\n",
            "Epoch [2/5], Step [80/1524], Loss: 0.3329\n",
            "Epoch [2/5], Step [100/1524], Loss: 0.3475\n",
            "Epoch [2/5], Step [120/1524], Loss: 0.3199\n",
            "Epoch [2/5], Step [140/1524], Loss: 0.3124\n",
            "Epoch [2/5], Step [160/1524], Loss: 0.3675\n",
            "Epoch [2/5], Step [180/1524], Loss: 0.3518\n",
            "Epoch [2/5], Step [200/1524], Loss: 0.2911\n",
            "Epoch [2/5], Step [220/1524], Loss: 0.3580\n",
            "Epoch [2/5], Step [240/1524], Loss: 0.3378\n",
            "Epoch [2/5], Step [260/1524], Loss: 0.3556\n",
            "Epoch [2/5], Step [280/1524], Loss: 0.3562\n",
            "Epoch [2/5], Step [300/1524], Loss: 0.3361\n",
            "Epoch [2/5], Step [320/1524], Loss: 0.3577\n",
            "Epoch [2/5], Step [340/1524], Loss: 0.3454\n",
            "Epoch [2/5], Step [360/1524], Loss: 0.3379\n",
            "Epoch [2/5], Step [380/1524], Loss: 0.3863\n",
            "Epoch [2/5], Step [400/1524], Loss: 0.3560\n",
            "Epoch [2/5], Step [420/1524], Loss: 0.3558\n",
            "Epoch [2/5], Step [440/1524], Loss: 0.3346\n",
            "Epoch [2/5], Step [460/1524], Loss: 0.3293\n",
            "Epoch [2/5], Step [480/1524], Loss: 0.3057\n",
            "Epoch [2/5], Step [500/1524], Loss: 0.3199\n",
            "Epoch [2/5], Step [520/1524], Loss: 0.3272\n",
            "Epoch [2/5], Step [540/1524], Loss: 0.2852\n",
            "Epoch [2/5], Step [560/1524], Loss: 0.3524\n",
            "Epoch [2/5], Step [580/1524], Loss: 0.3058\n",
            "Epoch [2/5], Step [600/1524], Loss: 0.2935\n",
            "Epoch [2/5], Step [620/1524], Loss: 0.2973\n",
            "Epoch [2/5], Step [640/1524], Loss: 0.3265\n",
            "Epoch [2/5], Step [660/1524], Loss: 0.3102\n",
            "Epoch [2/5], Step [680/1524], Loss: 0.3521\n",
            "Epoch [2/5], Step [700/1524], Loss: 0.3457\n",
            "Epoch [2/5], Step [720/1524], Loss: 0.3127\n",
            "Epoch [2/5], Step [740/1524], Loss: 0.3203\n",
            "Epoch [2/5], Step [760/1524], Loss: 0.3393\n",
            "Epoch [2/5], Step [780/1524], Loss: 0.3149\n",
            "Epoch [2/5], Step [800/1524], Loss: 0.2742\n",
            "Epoch [2/5], Step [820/1524], Loss: 0.2751\n",
            "Epoch [2/5], Step [840/1524], Loss: 0.2997\n",
            "Epoch [2/5], Step [860/1524], Loss: 0.3146\n",
            "Epoch [2/5], Step [880/1524], Loss: 0.3283\n",
            "Epoch [2/5], Step [900/1524], Loss: 0.3336\n",
            "Epoch [2/5], Step [920/1524], Loss: 0.3241\n",
            "Epoch [2/5], Step [940/1524], Loss: 0.3415\n",
            "Epoch [2/5], Step [960/1524], Loss: 0.3122\n",
            "Epoch [2/5], Step [980/1524], Loss: 0.3464\n",
            "Epoch [2/5], Step [1000/1524], Loss: 0.3126\n",
            "Epoch [2/5], Step [1020/1524], Loss: 0.3564\n",
            "Epoch [2/5], Step [1040/1524], Loss: 0.3203\n",
            "Epoch [2/5], Step [1060/1524], Loss: 0.3542\n",
            "Epoch [2/5], Step [1080/1524], Loss: 0.3326\n",
            "Epoch [2/5], Step [1100/1524], Loss: 0.3082\n",
            "Epoch [2/5], Step [1120/1524], Loss: 0.3069\n",
            "Epoch [2/5], Step [1140/1524], Loss: 0.3150\n",
            "Epoch [2/5], Step [1160/1524], Loss: 0.3026\n",
            "Epoch [2/5], Step [1180/1524], Loss: 0.3466\n",
            "Epoch [2/5], Step [1200/1524], Loss: 0.3188\n",
            "Epoch [2/5], Step [1220/1524], Loss: 0.3281\n",
            "Epoch [2/5], Step [1240/1524], Loss: 0.3001\n",
            "Epoch [2/5], Step [1260/1524], Loss: 0.3111\n",
            "Epoch [2/5], Step [1280/1524], Loss: 0.3116\n",
            "Epoch [2/5], Step [1300/1524], Loss: 0.2818\n",
            "Epoch [2/5], Step [1320/1524], Loss: 0.2908\n",
            "Epoch [2/5], Step [1340/1524], Loss: 0.2951\n",
            "Epoch [2/5], Step [1360/1524], Loss: 0.2960\n",
            "Epoch [2/5], Step [1380/1524], Loss: 0.3511\n",
            "Epoch [2/5], Step [1400/1524], Loss: 0.2923\n",
            "Epoch [2/5], Step [1420/1524], Loss: 0.3012\n",
            "Epoch [2/5], Step [1440/1524], Loss: 0.3340\n",
            "Epoch [2/5], Step [1460/1524], Loss: 0.3154\n",
            "Epoch [2/5], Step [1480/1524], Loss: 0.2896\n",
            "Epoch [2/5], Step [1500/1524], Loss: 0.3277\n",
            "Epoch [2/5], Step [1520/1524], Loss: 0.3137\n",
            "Epoch [2/5] Test Accuracy: 74.50%\n",
            "Epoch [3/5], Step [20/1524], Loss: 0.3275\n",
            "Epoch [3/5], Step [40/1524], Loss: 0.3168\n",
            "Epoch [3/5], Step [60/1524], Loss: 0.3410\n",
            "Epoch [3/5], Step [80/1524], Loss: 0.2756\n",
            "Epoch [3/5], Step [100/1524], Loss: 0.2888\n",
            "Epoch [3/5], Step [120/1524], Loss: 0.2789\n",
            "Epoch [3/5], Step [140/1524], Loss: 0.3163\n",
            "Epoch [3/5], Step [160/1524], Loss: 0.2962\n",
            "Epoch [3/5], Step [180/1524], Loss: 0.2933\n",
            "Epoch [3/5], Step [200/1524], Loss: 0.3296\n",
            "Epoch [3/5], Step [220/1524], Loss: 0.3036\n",
            "Epoch [3/5], Step [240/1524], Loss: 0.2948\n",
            "Epoch [3/5], Step [260/1524], Loss: 0.2984\n",
            "Epoch [3/5], Step [280/1524], Loss: 0.3167\n",
            "Epoch [3/5], Step [300/1524], Loss: 0.2651\n",
            "Epoch [3/5], Step [320/1524], Loss: 0.2820\n",
            "Epoch [3/5], Step [340/1524], Loss: 0.2991\n",
            "Epoch [3/5], Step [360/1524], Loss: 0.3226\n",
            "Epoch [3/5], Step [380/1524], Loss: 0.2716\n",
            "Epoch [3/5], Step [400/1524], Loss: 0.2743\n",
            "Epoch [3/5], Step [420/1524], Loss: 0.2758\n",
            "Epoch [3/5], Step [440/1524], Loss: 0.3039\n",
            "Epoch [3/5], Step [460/1524], Loss: 0.2998\n",
            "Epoch [3/5], Step [480/1524], Loss: 0.3133\n",
            "Epoch [3/5], Step [500/1524], Loss: 0.3151\n",
            "Epoch [3/5], Step [520/1524], Loss: 0.2996\n",
            "Epoch [3/5], Step [540/1524], Loss: 0.3155\n",
            "Epoch [3/5], Step [560/1524], Loss: 0.2952\n",
            "Epoch [3/5], Step [580/1524], Loss: 0.2842\n",
            "Epoch [3/5], Step [600/1524], Loss: 0.2851\n",
            "Epoch [3/5], Step [620/1524], Loss: 0.2880\n",
            "Epoch [3/5], Step [640/1524], Loss: 0.2767\n",
            "Epoch [3/5], Step [660/1524], Loss: 0.3327\n",
            "Epoch [3/5], Step [680/1524], Loss: 0.3034\n",
            "Epoch [3/5], Step [700/1524], Loss: 0.2679\n",
            "Epoch [3/5], Step [720/1524], Loss: 0.2920\n",
            "Epoch [3/5], Step [740/1524], Loss: 0.2676\n",
            "Epoch [3/5], Step [760/1524], Loss: 0.2906\n",
            "Epoch [3/5], Step [780/1524], Loss: 0.2472\n",
            "Epoch [3/5], Step [800/1524], Loss: 0.3019\n",
            "Epoch [3/5], Step [820/1524], Loss: 0.2962\n",
            "Epoch [3/5], Step [840/1524], Loss: 0.3248\n",
            "Epoch [3/5], Step [860/1524], Loss: 0.3040\n",
            "Epoch [3/5], Step [880/1524], Loss: 0.2914\n",
            "Epoch [3/5], Step [900/1524], Loss: 0.2942\n",
            "Epoch [3/5], Step [920/1524], Loss: 0.3015\n",
            "Epoch [3/5], Step [940/1524], Loss: 0.2834\n",
            "Epoch [3/5], Step [960/1524], Loss: 0.3205\n",
            "Epoch [3/5], Step [980/1524], Loss: 0.3216\n",
            "Epoch [3/5], Step [1000/1524], Loss: 0.3579\n",
            "Epoch [3/5], Step [1020/1524], Loss: 0.2759\n",
            "Epoch [3/5], Step [1040/1524], Loss: 0.2864\n",
            "Epoch [3/5], Step [1060/1524], Loss: 0.3003\n",
            "Epoch [3/5], Step [1080/1524], Loss: 0.2524\n",
            "Epoch [3/5], Step [1100/1524], Loss: 0.2710\n",
            "Epoch [3/5], Step [1120/1524], Loss: 0.2934\n",
            "Epoch [3/5], Step [1140/1524], Loss: 0.3000\n",
            "Epoch [3/5], Step [1160/1524], Loss: 0.2846\n",
            "Epoch [3/5], Step [1180/1524], Loss: 0.2986\n",
            "Epoch [3/5], Step [1200/1524], Loss: 0.2903\n",
            "Epoch [3/5], Step [1220/1524], Loss: 0.2999\n",
            "Epoch [3/5], Step [1240/1524], Loss: 0.3145\n",
            "Epoch [3/5], Step [1260/1524], Loss: 0.2692\n",
            "Epoch [3/5], Step [1280/1524], Loss: 0.2742\n",
            "Epoch [3/5], Step [1300/1524], Loss: 0.2772\n",
            "Epoch [3/5], Step [1320/1524], Loss: 0.2803\n",
            "Epoch [3/5], Step [1340/1524], Loss: 0.2961\n",
            "Epoch [3/5], Step [1360/1524], Loss: 0.2726\n",
            "Epoch [3/5], Step [1380/1524], Loss: 0.2917\n",
            "Epoch [3/5], Step [1400/1524], Loss: 0.2994\n",
            "Epoch [3/5], Step [1420/1524], Loss: 0.2817\n",
            "Epoch [3/5], Step [1440/1524], Loss: 0.2933\n",
            "Epoch [3/5], Step [1460/1524], Loss: 0.2956\n",
            "Epoch [3/5], Step [1480/1524], Loss: 0.3021\n",
            "Epoch [3/5], Step [1500/1524], Loss: 0.2674\n",
            "Epoch [3/5], Step [1520/1524], Loss: 0.2742\n",
            "Epoch [3/5] Test Accuracy: 81.70%\n",
            "Epoch [4/5], Step [20/1524], Loss: 0.2803\n",
            "Epoch [4/5], Step [40/1524], Loss: 0.3305\n",
            "Epoch [4/5], Step [60/1524], Loss: 0.2657\n",
            "Epoch [4/5], Step [80/1524], Loss: 0.2744\n",
            "Epoch [4/5], Step [100/1524], Loss: 0.2768\n",
            "Epoch [4/5], Step [120/1524], Loss: 0.3091\n",
            "Epoch [4/5], Step [140/1524], Loss: 0.2657\n",
            "Epoch [4/5], Step [160/1524], Loss: 0.2546\n",
            "Epoch [4/5], Step [180/1524], Loss: 0.2832\n",
            "Epoch [4/5], Step [200/1524], Loss: 0.2570\n",
            "Epoch [4/5], Step [220/1524], Loss: 0.2747\n",
            "Epoch [4/5], Step [240/1524], Loss: 0.2291\n",
            "Epoch [4/5], Step [260/1524], Loss: 0.2987\n",
            "Epoch [4/5], Step [280/1524], Loss: 0.2891\n",
            "Epoch [4/5], Step [300/1524], Loss: 0.2420\n",
            "Epoch [4/5], Step [320/1524], Loss: 0.2283\n",
            "Epoch [4/5], Step [340/1524], Loss: 0.2643\n",
            "Epoch [4/5], Step [360/1524], Loss: 0.2656\n",
            "Epoch [4/5], Step [380/1524], Loss: 0.2889\n",
            "Epoch [4/5], Step [400/1524], Loss: 0.2493\n",
            "Epoch [4/5], Step [420/1524], Loss: 0.3126\n",
            "Epoch [4/5], Step [440/1524], Loss: 0.3170\n",
            "Epoch [4/5], Step [460/1524], Loss: 0.3101\n",
            "Epoch [4/5], Step [480/1524], Loss: 0.2914\n",
            "Epoch [4/5], Step [500/1524], Loss: 0.2820\n",
            "Epoch [4/5], Step [520/1524], Loss: 0.2753\n",
            "Epoch [4/5], Step [540/1524], Loss: 0.2569\n",
            "Epoch [4/5], Step [560/1524], Loss: 0.2797\n",
            "Epoch [4/5], Step [580/1524], Loss: 0.2859\n",
            "Epoch [4/5], Step [600/1524], Loss: 0.2650\n",
            "Epoch [4/5], Step [620/1524], Loss: 0.2525\n",
            "Epoch [4/5], Step [640/1524], Loss: 0.2792\n",
            "Epoch [4/5], Step [660/1524], Loss: 0.2762\n",
            "Epoch [4/5], Step [680/1524], Loss: 0.2872\n",
            "Epoch [4/5], Step [700/1524], Loss: 0.2866\n",
            "Epoch [4/5], Step [720/1524], Loss: 0.2472\n",
            "Epoch [4/5], Step [740/1524], Loss: 0.2706\n",
            "Epoch [4/5], Step [760/1524], Loss: 0.2693\n",
            "Epoch [4/5], Step [780/1524], Loss: 0.2422\n",
            "Epoch [4/5], Step [800/1524], Loss: 0.2368\n",
            "Epoch [4/5], Step [820/1524], Loss: 0.2461\n",
            "Epoch [4/5], Step [840/1524], Loss: 0.2556\n",
            "Epoch [4/5], Step [860/1524], Loss: 0.2844\n",
            "Epoch [4/5], Step [880/1524], Loss: 0.2640\n",
            "Epoch [4/5], Step [900/1524], Loss: 0.3068\n",
            "Epoch [4/5], Step [920/1524], Loss: 0.2514\n",
            "Epoch [4/5], Step [940/1524], Loss: 0.2632\n",
            "Epoch [4/5], Step [960/1524], Loss: 0.2729\n",
            "Epoch [4/5], Step [980/1524], Loss: 0.3074\n",
            "Epoch [4/5], Step [1000/1524], Loss: 0.2718\n",
            "Epoch [4/5], Step [1020/1524], Loss: 0.2777\n",
            "Epoch [4/5], Step [1040/1524], Loss: 0.2883\n",
            "Epoch [4/5], Step [1060/1524], Loss: 0.2702\n",
            "Epoch [4/5], Step [1080/1524], Loss: 0.2471\n",
            "Epoch [4/5], Step [1100/1524], Loss: 0.2984\n",
            "Epoch [4/5], Step [1120/1524], Loss: 0.2798\n",
            "Epoch [4/5], Step [1140/1524], Loss: 0.3165\n",
            "Epoch [4/5], Step [1160/1524], Loss: 0.2915\n",
            "Epoch [4/5], Step [1180/1524], Loss: 0.2775\n",
            "Epoch [4/5], Step [1200/1524], Loss: 0.2492\n",
            "Epoch [4/5], Step [1220/1524], Loss: 0.2778\n",
            "Epoch [4/5], Step [1240/1524], Loss: 0.2589\n",
            "Epoch [4/5], Step [1260/1524], Loss: 0.3031\n",
            "Epoch [4/5], Step [1280/1524], Loss: 0.2928\n",
            "Epoch [4/5], Step [1300/1524], Loss: 0.2941\n",
            "Epoch [4/5], Step [1320/1524], Loss: 0.2741\n",
            "Epoch [4/5], Step [1340/1524], Loss: 0.2686\n",
            "Epoch [4/5], Step [1360/1524], Loss: 0.2680\n",
            "Epoch [4/5], Step [1380/1524], Loss: 0.2883\n",
            "Epoch [4/5], Step [1400/1524], Loss: 0.3128\n",
            "Epoch [4/5], Step [1420/1524], Loss: 0.2614\n",
            "Epoch [4/5], Step [1440/1524], Loss: 0.2582\n",
            "Epoch [4/5], Step [1460/1524], Loss: 0.2683\n",
            "Epoch [4/5], Step [1480/1524], Loss: 0.2778\n",
            "Epoch [4/5], Step [1500/1524], Loss: 0.2620\n",
            "Epoch [4/5], Step [1520/1524], Loss: 0.2817\n",
            "Epoch [4/5] Test Accuracy: 72.20%\n",
            "Epoch [5/5], Step [20/1524], Loss: 0.2889\n",
            "Epoch [5/5], Step [40/1524], Loss: 0.2706\n",
            "Epoch [5/5], Step [60/1524], Loss: 0.2668\n",
            "Epoch [5/5], Step [80/1524], Loss: 0.2434\n",
            "Epoch [5/5], Step [100/1524], Loss: 0.2541\n",
            "Epoch [5/5], Step [120/1524], Loss: 0.2672\n",
            "Epoch [5/5], Step [140/1524], Loss: 0.2158\n",
            "Epoch [5/5], Step [160/1524], Loss: 0.3133\n",
            "Epoch [5/5], Step [180/1524], Loss: 0.2844\n",
            "Epoch [5/5], Step [200/1524], Loss: 0.2477\n",
            "Epoch [5/5], Step [220/1524], Loss: 0.2521\n",
            "Epoch [5/5], Step [240/1524], Loss: 0.2571\n",
            "Epoch [5/5], Step [260/1524], Loss: 0.2796\n",
            "Epoch [5/5], Step [280/1524], Loss: 0.2639\n",
            "Epoch [5/5], Step [300/1524], Loss: 0.2581\n",
            "Epoch [5/5], Step [320/1524], Loss: 0.2907\n",
            "Epoch [5/5], Step [340/1524], Loss: 0.2640\n",
            "Epoch [5/5], Step [360/1524], Loss: 0.2350\n",
            "Epoch [5/5], Step [380/1524], Loss: 0.2994\n",
            "Epoch [5/5], Step [400/1524], Loss: 0.2754\n",
            "Epoch [5/5], Step [420/1524], Loss: 0.2401\n",
            "Epoch [5/5], Step [440/1524], Loss: 0.2918\n",
            "Epoch [5/5], Step [460/1524], Loss: 0.2899\n",
            "Epoch [5/5], Step [480/1524], Loss: 0.2552\n",
            "Epoch [5/5], Step [500/1524], Loss: 0.2518\n",
            "Epoch [5/5], Step [520/1524], Loss: 0.2710\n",
            "Epoch [5/5], Step [540/1524], Loss: 0.2792\n",
            "Epoch [5/5], Step [560/1524], Loss: 0.2628\n",
            "Epoch [5/5], Step [580/1524], Loss: 0.2458\n",
            "Epoch [5/5], Step [600/1524], Loss: 0.2620\n",
            "Epoch [5/5], Step [620/1524], Loss: 0.2554\n",
            "Epoch [5/5], Step [640/1524], Loss: 0.2672\n",
            "Epoch [5/5], Step [660/1524], Loss: 0.2509\n",
            "Epoch [5/5], Step [680/1524], Loss: 0.2562\n",
            "Epoch [5/5], Step [700/1524], Loss: 0.2672\n",
            "Epoch [5/5], Step [720/1524], Loss: 0.2851\n",
            "Epoch [5/5], Step [740/1524], Loss: 0.2670\n",
            "Epoch [5/5], Step [760/1524], Loss: 0.2598\n",
            "Epoch [5/5], Step [780/1524], Loss: 0.2356\n",
            "Epoch [5/5], Step [800/1524], Loss: 0.2294\n",
            "Epoch [5/5], Step [820/1524], Loss: 0.2582\n",
            "Epoch [5/5], Step [840/1524], Loss: 0.2409\n",
            "Epoch [5/5], Step [860/1524], Loss: 0.2708\n",
            "Epoch [5/5], Step [880/1524], Loss: 0.2446\n",
            "Epoch [5/5], Step [900/1524], Loss: 0.2738\n",
            "Epoch [5/5], Step [920/1524], Loss: 0.2549\n",
            "Epoch [5/5], Step [940/1524], Loss: 0.2593\n",
            "Epoch [5/5], Step [960/1524], Loss: 0.2623\n",
            "Epoch [5/5], Step [980/1524], Loss: 0.2409\n",
            "Epoch [5/5], Step [1000/1524], Loss: 0.2604\n",
            "Epoch [5/5], Step [1020/1524], Loss: 0.2481\n",
            "Epoch [5/5], Step [1040/1524], Loss: 0.2215\n",
            "Epoch [5/5], Step [1060/1524], Loss: 0.2615\n",
            "Epoch [5/5], Step [1080/1524], Loss: 0.2689\n",
            "Epoch [5/5], Step [1100/1524], Loss: 0.2466\n",
            "Epoch [5/5], Step [1120/1524], Loss: 0.2717\n",
            "Epoch [5/5], Step [1140/1524], Loss: 0.2725\n",
            "Epoch [5/5], Step [1160/1524], Loss: 0.2767\n",
            "Epoch [5/5], Step [1180/1524], Loss: 0.3093\n",
            "Epoch [5/5], Step [1200/1524], Loss: 0.2660\n",
            "Epoch [5/5], Step [1220/1524], Loss: 0.2398\n",
            "Epoch [5/5], Step [1240/1524], Loss: 0.2506\n",
            "Epoch [5/5], Step [1260/1524], Loss: 0.2565\n",
            "Epoch [5/5], Step [1280/1524], Loss: 0.2644\n",
            "Epoch [5/5], Step [1300/1524], Loss: 0.2221\n",
            "Epoch [5/5], Step [1320/1524], Loss: 0.2722\n",
            "Epoch [5/5], Step [1340/1524], Loss: 0.2453\n",
            "Epoch [5/5], Step [1360/1524], Loss: 0.2598\n",
            "Epoch [5/5], Step [1380/1524], Loss: 0.2558\n",
            "Epoch [5/5], Step [1400/1524], Loss: 0.2235\n",
            "Epoch [5/5], Step [1420/1524], Loss: 0.2459\n",
            "Epoch [5/5], Step [1440/1524], Loss: 0.2746\n",
            "Epoch [5/5], Step [1460/1524], Loss: 0.2544\n",
            "Epoch [5/5], Step [1480/1524], Loss: 0.2670\n",
            "Epoch [5/5], Step [1500/1524], Loss: 0.3039\n",
            "Epoch [5/5], Step [1520/1524], Loss: 0.2418\n",
            "Epoch [5/5] Test Accuracy: 70.90%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▃▅▅▅▆▆▆▆▆▆▆▆▆▆██████████</td></tr><tr><td>loss</td><td>█▅▅▅▄▃▂▃▂▃▂▂▂▂▂▁▂▂▁▂▂▂▂▂▂▁▂▂▁▂▁▁▂▁▁▂▁▁▂▁</td></tr><tr><td>test_accuracy</td><td>▁▇█▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>loss</td><td>0.24185</td></tr><tr><td>test_accuracy</td><td>70.9</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">simple resnet</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/wmu52fie' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/wmu52fie</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250326_141738-wmu52fie/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO\n",
        "\n",
        "\n",
        "*   Bigger architecture(s)\n",
        "*   Optuna / Grid Search\n",
        "\n"
      ],
      "metadata": {
        "id": "vgXYJQkH0jU_"
      }
    }
  ]
}