{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasColas/CNN-VS-ViT/blob/main/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet"
      ],
      "metadata": {
        "id": "tD6I13Z_s6ad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "cTcoNazCpaIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download packages / dataset"
      ],
      "metadata": {
        "id": "ugBidLUTpbaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install medmnist"
      ],
      "metadata": {
        "id": "wNX1kWxxuk39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2304ccf-1dd5-44b0-8408-b6d7f17857e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: medmnist in /usr/local/lib/python3.11/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from medmnist) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from medmnist) (4.67.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from medmnist) (11.1.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.7.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.21.0+cu124)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->medmnist) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (1.14.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (2025.3.13)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medmnist) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medmnist) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (4.13.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->medmnist) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->medmnist) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGYvDa6_yfVI",
        "outputId": "8b12847f-f94b-44f0-b008-68e271451630"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.24.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import"
      ],
      "metadata": {
        "id": "au4ree9vpfF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from medmnist import OCTMNIST  # Import the OCTMNIST dataset\n",
        "import wandb\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "dNe-jwnUuUwN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g4wnjaRxqNx",
        "outputId": "b1ad75c2-de89-478f-cd52-ebacbe886ad6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture"
      ],
      "metadata": {
        "id": "LxroXd-atMKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResidualBlock"
      ],
      "metadata": {
        "id": "47tSPB0PtFAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3,\n",
        "                 activation=nn.ReLU(inplace=True), use_batchnorm=True):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        padding = kernel_size // 2  # To keep spatial dimensions constant\n",
        "\n",
        "        # First convolution layer\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                               padding=padding, bias=not use_batchnorm)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels) if use_batchnorm else nn.Identity()\n",
        "\n",
        "        # Second convolution layer\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size,\n",
        "                               padding=padding, bias=not use_batchnorm)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels) if use_batchnorm else nn.Identity()\n",
        "\n",
        "        # Define the activation function\n",
        "        self.activation = activation\n",
        "\n",
        "        # If input and output channels differ, use a 1x1 conv to match dimensions\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.activation(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += shortcut\n",
        "        out = self.activation(out)\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jF2GPxJNs4zi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet"
      ],
      "metadata": {
        "id": "6sP9HxoVtO9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomResNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=3,         # Input channels (e.g., 3 for RGB images)\n",
        "                 num_blocks=4,          # Number of residual blocks\n",
        "                 base_channels=64,      # Number of channels for the first block\n",
        "                 kernel_size=3,         # Kernel size for convolutions\n",
        "                 activation=nn.ReLU(inplace=True),  # Activation function\n",
        "                 use_batchnorm=True,    # Whether to use BatchNorm\n",
        "                 num_classes=10         # Number of classes for final output\n",
        "                 ):\n",
        "        super(CustomResNet, self).__init__()\n",
        "\n",
        "        self.initial_conv = nn.Conv2d(in_channels, base_channels, kernel_size=kernel_size,\n",
        "                                      padding=kernel_size//2, bias=not use_batchnorm)\n",
        "        self.initial_bn = nn.BatchNorm2d(base_channels) if use_batchnorm else nn.Identity()\n",
        "        self.activation = activation\n",
        "\n",
        "        # Create a sequential container for residual blocks.\n",
        "        layers = []\n",
        "        # First block: input channels = base_channels, output channels = base_channels\n",
        "        for _ in range(num_blocks):\n",
        "            layers.append(ResidualBlock(base_channels, base_channels,\n",
        "                                        kernel_size=kernel_size,\n",
        "                                        activation=activation,\n",
        "                                        use_batchnorm=use_batchnorm))\n",
        "        self.residual_layers = nn.Sequential(*layers)\n",
        "\n",
        "        # Global average pooling and a final linear classifier.\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(base_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial_conv(x)\n",
        "        x = self.initial_bn(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.residual_layers(x)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "CGjqNFzitOow"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = CustomResNet(\n",
        "        in_channels=3,\n",
        "        num_blocks=5,               # You can choose how many blocks\n",
        "        base_channels=64,\n",
        "        kernel_size=3,              # Kernel size can be adjusted\n",
        "        activation=nn.LeakyReLU(0.1, inplace=True),  # You can choose any activation\n",
        "        use_batchnorm=True,         # Toggle batch normalization\n",
        "        num_classes=1000            # For example, for ImageNet classification\n",
        "    )\n",
        "\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "\n",
        "\n",
        "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "output = model(dummy_input)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvS7mOYitYeA",
        "outputId": "ee9fee87-9bb8-4149-bf42-2ae2ff0352af"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "CustomResNet(\n",
            "  (initial_conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (initial_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  (residual_layers): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (4): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "  )\n",
            "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=64, out_features=1000, bias=True)\n",
            ")\n",
            "Output shape: torch.Size([1, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing and data augmentation"
      ],
      "metadata": {
        "id": "G3TY4ytRkcEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
        "    transforms.RandomRotation(10),      # Randomly rotate images by +/- 10 degrees\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize for a single channel (grayscale)\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "# Download the OCTMNIST dataset.\n",
        "train_dataset = OCTMNIST(split='train', transform=transform_train, download=True)\n",
        "test_dataset  = OCTMNIST(split='test', transform=transform_train, download=True)\n",
        "\n",
        "print(\"Train dataset size:\", len(train_dataset))\n",
        "print(\"Test dataset size:\", len(test_dataset))\n",
        "\n",
        "# Create data loaders.\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Visualize a few training samples\n",
        "def imshow(img, title=None):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "imshow(torchvision.utils.make_grid(images[:2]), title=\"Sample OCTMNIST Images\")\n",
        "print(labels[:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "KTGP5IGVkenI",
        "outputId": "45c9b51e-73d3-44cf-9b02-a25b510c3384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 97477\n",
            "Test dataset size: 1000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3],\n",
            "        [3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "_YUhZm4xuNUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, config, run_name):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
        "    num_epochs = config[\"num_epochs\"]\n",
        "\n",
        "    # For logging the confusion matrix at the end of training\n",
        "    all_test_targets = []\n",
        "    all_test_preds = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # --------------------\n",
        "        # Training Phase\n",
        "        # --------------------\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        train_loss_epoch = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for i, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            train_loss_epoch += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += targets.size(0)\n",
        "            train_correct += (predicted == targets).sum().item()\n",
        "\n",
        "            if i % 20 == 19:  # Log every 20 mini-batches\n",
        "                avg_loss = running_loss / 20\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {avg_loss:.4f}\")\n",
        "                # Log training loss with epoch as x-axis value\n",
        "                wandb.log({\"epoch\": epoch+1, \"train_loss\": avg_loss})\n",
        "                running_loss = 0.0\n",
        "\n",
        "        avg_train_loss = train_loss_epoch / len(train_loader)\n",
        "        train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "        # --------------------\n",
        "        # Evaluation Phase\n",
        "        # --------------------\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        test_loss_epoch = 0.0\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "        with torch.no_grad():\n",
        "            for i, (inputs, targets) in enumerate(test_loader):\n",
        "                inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                test_loss_epoch += loss.item()\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                test_total += targets.size(0)\n",
        "                test_correct += (predicted == targets).sum().item()\n",
        "\n",
        "                # Accumulate predictions and true labels for confusion matrix\n",
        "                all_test_targets.extend(targets.cpu().numpy())\n",
        "                all_test_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "\n",
        "\n",
        "        avg_test_loss = test_loss_epoch / len(test_loader)\n",
        "        test_accuracy = 100 * test_correct / test_total\n",
        "\n",
        "        # Log epoch-level metrics so that train and test losses/accuracies appear on the same plots.\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_accuracy per epoch\": train_accuracy,\n",
        "            \"test_accuracy per epoch\": test_accuracy,\n",
        "        })\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}% | Test Loss: {avg_test_loss:.4f} | Test Acc: {test_accuracy:.2f}%\")\n",
        "\n",
        "    # --------------------\n",
        "    # Confusion Matrix Logging\n",
        "    # --------------------\n",
        "    class_names = [\"CNV\", \"DME\", \"DRUSEN\", \"NORMAL\"]  # Adjust if necessary\n",
        "    cm = confusion_matrix(all_test_targets, all_test_preds)\n",
        "    wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "                probs=None,\n",
        "                y_true=np.array(all_test_targets),\n",
        "                preds=np.array(all_test_preds),\n",
        "                class_names=class_names)})\n",
        "\n",
        "    # Optionally, log a matplotlib figure of the confusion matrix.\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.set(xticks=np.arange(len(class_names)),\n",
        "           yticks=np.arange(len(class_names)),\n",
        "           xticklabels=class_names,\n",
        "           yticklabels=class_names,\n",
        "           title=\"Confusion Matrix\",\n",
        "           ylabel=\"True label\",\n",
        "           xlabel=\"Predicted label\")\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], 'd'),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    wandb.log({\"confusion_matrix_fig\" + run_name: fig})\n",
        "    plt.close(fig)"
      ],
      "metadata": {
        "id": "TTT8r-Alz7tg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "\n",
        "# Configuration dictionary for wandb\n",
        "config = {\n",
        "    \"in_channels\": 1,\n",
        "    \"num_epochs\": 5,            # Adjust number of epochs as necessary\n",
        "    \"lr\": 0.001,                # Learning rate\n",
        "    \"batch_size\": 64,           # Batch size (should match your DataLoader)\n",
        "    \"num_blocks\": 5,            # Number of residual blocks in the network\n",
        "    \"base_channels\": 64,        # Number of filters in the first block\n",
        "    \"kernel_size\": 3,           # Kernel size for convolutions\n",
        "    \"activation\": \"LeakyReLU\",  # Name of the activation function\n",
        "    \"use_batchnorm\": True,      # Whether to use batch normalization\n",
        "    \"num_classes\": 4            # OCTMNIST has 4 classes\n",
        "}\n",
        "\n",
        "# Initialize wandb run and train the model\n",
        "with wandb.init(\n",
        "    config=config,\n",
        "    project='CNN VS ViT',   # Title of your project\n",
        "    group='Test',           # Group name for your run\n",
        "    save_code=True,\n",
        "    name='resnet',\n",
        "):\n",
        "    # Create the model with custom configuration\n",
        "    model = CustomResNet(\n",
        "        in_channels=config['in_channels'],  # OCTMNIST images are single-channel if you have preprocessed accordingly\n",
        "        num_blocks=config[\"num_blocks\"],\n",
        "        base_channels=config[\"base_channels\"],\n",
        "        kernel_size=config[\"kernel_size\"],\n",
        "        activation=nn.LeakyReLU(0.1, inplace=True),\n",
        "        use_batchnorm=config[\"use_batchnorm\"],\n",
        "        num_classes=config[\"num_classes\"]\n",
        "    )\n",
        "    print(model)\n",
        "\n",
        "    # Train the model (train_loader and test_loader should be defined before)\n",
        "    train_model(model, config, \"simple resnet\")\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NKnhU1T-0I_w",
        "outputId": "8e0ec4fb-e255-42ec-adf5-a13122605357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250326_152629-7y5vimi7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/7y5vimi7' target=\"_blank\">resnet</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/7y5vimi7' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/7y5vimi7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomResNet(\n",
            "  (initial_conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (initial_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  (residual_layers): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (4): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "  )\n",
            "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Epoch [1/5], Step [20/1524], Loss: 1.0469\n",
            "Epoch [1/5], Step [40/1524], Loss: 0.7758\n",
            "Epoch [1/5], Step [60/1524], Loss: 0.7367\n",
            "Epoch [1/5], Step [80/1524], Loss: 0.7143\n",
            "Epoch [1/5], Step [100/1524], Loss: 0.6602\n",
            "Epoch [1/5], Step [120/1524], Loss: 0.6646\n",
            "Epoch [1/5], Step [140/1524], Loss: 0.6057\n",
            "Epoch [1/5], Step [160/1524], Loss: 0.6625\n",
            "Epoch [1/5], Step [180/1524], Loss: 0.6321\n",
            "Epoch [1/5], Step [200/1524], Loss: 0.6129\n",
            "Epoch [1/5], Step [220/1524], Loss: 0.5938\n",
            "Epoch [1/5], Step [240/1524], Loss: 0.5582\n",
            "Epoch [1/5], Step [260/1524], Loss: 0.6086\n",
            "Epoch [1/5], Step [280/1524], Loss: 0.5347\n",
            "Epoch [1/5], Step [300/1524], Loss: 0.5168\n",
            "Epoch [1/5], Step [320/1524], Loss: 0.4943\n",
            "Epoch [1/5], Step [340/1524], Loss: 0.5322\n",
            "Epoch [1/5], Step [360/1524], Loss: 0.5070\n",
            "Epoch [1/5], Step [380/1524], Loss: 0.5578\n",
            "Epoch [1/5], Step [400/1524], Loss: 0.5476\n",
            "Epoch [1/5], Step [420/1524], Loss: 0.5391\n",
            "Epoch [1/5], Step [440/1524], Loss: 0.4875\n",
            "Epoch [1/5], Step [460/1524], Loss: 0.5097\n",
            "Epoch [1/5], Step [480/1524], Loss: 0.4902\n",
            "Epoch [1/5], Step [500/1524], Loss: 0.4904\n",
            "Epoch [1/5], Step [520/1524], Loss: 0.4892\n",
            "Epoch [1/5], Step [540/1524], Loss: 0.4614\n",
            "Epoch [1/5], Step [560/1524], Loss: 0.4954\n",
            "Epoch [1/5], Step [580/1524], Loss: 0.4979\n",
            "Epoch [1/5], Step [600/1524], Loss: 0.4689\n",
            "Epoch [1/5], Step [620/1524], Loss: 0.5299\n",
            "Epoch [1/5], Step [640/1524], Loss: 0.4420\n",
            "Epoch [1/5], Step [660/1524], Loss: 0.4642\n",
            "Epoch [1/5], Step [680/1524], Loss: 0.4737\n",
            "Epoch [1/5], Step [700/1524], Loss: 0.4687\n",
            "Epoch [1/5], Step [720/1524], Loss: 0.4418\n",
            "Epoch [1/5], Step [740/1524], Loss: 0.4663\n",
            "Epoch [1/5], Step [760/1524], Loss: 0.4721\n",
            "Epoch [1/5], Step [780/1524], Loss: 0.4726\n",
            "Epoch [1/5], Step [800/1524], Loss: 0.3812\n",
            "Epoch [1/5], Step [820/1524], Loss: 0.4354\n",
            "Epoch [1/5], Step [840/1524], Loss: 0.4527\n",
            "Epoch [1/5], Step [860/1524], Loss: 0.4423\n",
            "Epoch [1/5], Step [880/1524], Loss: 0.4353\n",
            "Epoch [1/5], Step [900/1524], Loss: 0.4308\n",
            "Epoch [1/5], Step [920/1524], Loss: 0.3994\n",
            "Epoch [1/5], Step [940/1524], Loss: 0.3775\n",
            "Epoch [1/5], Step [960/1524], Loss: 0.4395\n",
            "Epoch [1/5], Step [980/1524], Loss: 0.4047\n",
            "Epoch [1/5], Step [1000/1524], Loss: 0.4029\n",
            "Epoch [1/5], Step [1020/1524], Loss: 0.3950\n",
            "Epoch [1/5], Step [1040/1524], Loss: 0.3525\n",
            "Epoch [1/5], Step [1060/1524], Loss: 0.3765\n",
            "Epoch [1/5], Step [1080/1524], Loss: 0.3543\n",
            "Epoch [1/5], Step [1100/1524], Loss: 0.3630\n",
            "Epoch [1/5], Step [1120/1524], Loss: 0.4079\n",
            "Epoch [1/5], Step [1140/1524], Loss: 0.3728\n",
            "Epoch [1/5], Step [1160/1524], Loss: 0.4039\n",
            "Epoch [1/5], Step [1180/1524], Loss: 0.3740\n",
            "Epoch [1/5], Step [1200/1524], Loss: 0.3296\n",
            "Epoch [1/5], Step [1220/1524], Loss: 0.3825\n",
            "Epoch [1/5], Step [1240/1524], Loss: 0.3594\n",
            "Epoch [1/5], Step [1260/1524], Loss: 0.3861\n",
            "Epoch [1/5], Step [1280/1524], Loss: 0.3489\n",
            "Epoch [1/5], Step [1300/1524], Loss: 0.3588\n",
            "Epoch [1/5], Step [1320/1524], Loss: 0.3782\n",
            "Epoch [1/5], Step [1340/1524], Loss: 0.3600\n",
            "Epoch [1/5], Step [1360/1524], Loss: 0.3719\n",
            "Epoch [1/5], Step [1380/1524], Loss: 0.3700\n",
            "Epoch [1/5], Step [1400/1524], Loss: 0.3510\n",
            "Epoch [1/5], Step [1420/1524], Loss: 0.3271\n",
            "Epoch [1/5], Step [1440/1524], Loss: 0.3356\n",
            "Epoch [1/5], Step [1460/1524], Loss: 0.3369\n",
            "Epoch [1/5], Step [1480/1524], Loss: 0.3746\n",
            "Epoch [1/5], Step [1500/1524], Loss: 0.3660\n",
            "Epoch [1/5], Step [1520/1524], Loss: 0.3463\n",
            "Epoch [1/5] | Train Loss: 0.4713 | Train Acc: 83.65% | Test Loss: 0.5404 | Test Acc: 82.10%\n",
            "Epoch [2/5], Step [20/1524], Loss: 0.3699\n",
            "Epoch [2/5], Step [40/1524], Loss: 0.3936\n",
            "Epoch [2/5], Step [60/1524], Loss: 0.3406\n",
            "Epoch [2/5], Step [80/1524], Loss: 0.3389\n",
            "Epoch [2/5], Step [100/1524], Loss: 0.3500\n",
            "Epoch [2/5], Step [120/1524], Loss: 0.3494\n",
            "Epoch [2/5], Step [140/1524], Loss: 0.3141\n",
            "Epoch [2/5], Step [160/1524], Loss: 0.3458\n",
            "Epoch [2/5], Step [180/1524], Loss: 0.3742\n",
            "Epoch [2/5], Step [200/1524], Loss: 0.3445\n",
            "Epoch [2/5], Step [220/1524], Loss: 0.3442\n",
            "Epoch [2/5], Step [240/1524], Loss: 0.3066\n",
            "Epoch [2/5], Step [260/1524], Loss: 0.3560\n",
            "Epoch [2/5], Step [280/1524], Loss: 0.3390\n",
            "Epoch [2/5], Step [300/1524], Loss: 0.3584\n",
            "Epoch [2/5], Step [320/1524], Loss: 0.3477\n",
            "Epoch [2/5], Step [340/1524], Loss: 0.3351\n",
            "Epoch [2/5], Step [360/1524], Loss: 0.3482\n",
            "Epoch [2/5], Step [380/1524], Loss: 0.3804\n",
            "Epoch [2/5], Step [400/1524], Loss: 0.3096\n",
            "Epoch [2/5], Step [420/1524], Loss: 0.3359\n",
            "Epoch [2/5], Step [440/1524], Loss: 0.3447\n",
            "Epoch [2/5], Step [460/1524], Loss: 0.3315\n",
            "Epoch [2/5], Step [480/1524], Loss: 0.3520\n",
            "Epoch [2/5], Step [500/1524], Loss: 0.3525\n",
            "Epoch [2/5], Step [520/1524], Loss: 0.2972\n",
            "Epoch [2/5], Step [540/1524], Loss: 0.3399\n",
            "Epoch [2/5], Step [560/1524], Loss: 0.3266\n",
            "Epoch [2/5], Step [580/1524], Loss: 0.3225\n",
            "Epoch [2/5], Step [600/1524], Loss: 0.3326\n",
            "Epoch [2/5], Step [620/1524], Loss: 0.2872\n",
            "Epoch [2/5], Step [640/1524], Loss: 0.3326\n",
            "Epoch [2/5], Step [660/1524], Loss: 0.3286\n",
            "Epoch [2/5], Step [680/1524], Loss: 0.2783\n",
            "Epoch [2/5], Step [700/1524], Loss: 0.3376\n",
            "Epoch [2/5], Step [720/1524], Loss: 0.3037\n",
            "Epoch [2/5], Step [740/1524], Loss: 0.3138\n",
            "Epoch [2/5], Step [760/1524], Loss: 0.3043\n",
            "Epoch [2/5], Step [780/1524], Loss: 0.3325\n",
            "Epoch [2/5], Step [800/1524], Loss: 0.3535\n",
            "Epoch [2/5], Step [820/1524], Loss: 0.3084\n",
            "Epoch [2/5], Step [840/1524], Loss: 0.3255\n",
            "Epoch [2/5], Step [860/1524], Loss: 0.3530\n",
            "Epoch [2/5], Step [880/1524], Loss: 0.3146\n",
            "Epoch [2/5], Step [900/1524], Loss: 0.3051\n",
            "Epoch [2/5], Step [920/1524], Loss: 0.3246\n",
            "Epoch [2/5], Step [940/1524], Loss: 0.2860\n",
            "Epoch [2/5], Step [960/1524], Loss: 0.2889\n",
            "Epoch [2/5], Step [980/1524], Loss: 0.3211\n",
            "Epoch [2/5], Step [1000/1524], Loss: 0.3000\n",
            "Epoch [2/5], Step [1020/1524], Loss: 0.3501\n",
            "Epoch [2/5], Step [1040/1524], Loss: 0.2950\n",
            "Epoch [2/5], Step [1060/1524], Loss: 0.2915\n",
            "Epoch [2/5], Step [1080/1524], Loss: 0.3229\n",
            "Epoch [2/5], Step [1100/1524], Loss: 0.3233\n",
            "Epoch [2/5], Step [1120/1524], Loss: 0.2818\n",
            "Epoch [2/5], Step [1140/1524], Loss: 0.3426\n",
            "Epoch [2/5], Step [1160/1524], Loss: 0.3165\n",
            "Epoch [2/5], Step [1180/1524], Loss: 0.3461\n",
            "Epoch [2/5], Step [1200/1524], Loss: 0.3029\n",
            "Epoch [2/5], Step [1220/1524], Loss: 0.2787\n",
            "Epoch [2/5], Step [1240/1524], Loss: 0.3007\n",
            "Epoch [2/5], Step [1260/1524], Loss: 0.3123\n",
            "Epoch [2/5], Step [1280/1524], Loss: 0.3385\n",
            "Epoch [2/5], Step [1300/1524], Loss: 0.3340\n",
            "Epoch [2/5], Step [1320/1524], Loss: 0.3165\n",
            "Epoch [2/5], Step [1340/1524], Loss: 0.2989\n",
            "Epoch [2/5], Step [1360/1524], Loss: 0.2968\n",
            "Epoch [2/5], Step [1380/1524], Loss: 0.2794\n",
            "Epoch [2/5], Step [1400/1524], Loss: 0.3058\n",
            "Epoch [2/5], Step [1420/1524], Loss: 0.3477\n",
            "Epoch [2/5], Step [1440/1524], Loss: 0.3275\n",
            "Epoch [2/5], Step [1460/1524], Loss: 0.2721\n",
            "Epoch [2/5], Step [1480/1524], Loss: 0.2807\n",
            "Epoch [2/5], Step [1500/1524], Loss: 0.2961\n",
            "Epoch [2/5], Step [1520/1524], Loss: 0.3006\n",
            "Epoch [2/5] | Train Loss: 0.3240 | Train Acc: 88.87% | Test Loss: 0.7028 | Test Acc: 72.10%\n",
            "Epoch [3/5], Step [20/1524], Loss: 0.2826\n",
            "Epoch [3/5], Step [40/1524], Loss: 0.2852\n",
            "Epoch [3/5], Step [60/1524], Loss: 0.2886\n",
            "Epoch [3/5], Step [80/1524], Loss: 0.3055\n",
            "Epoch [3/5], Step [100/1524], Loss: 0.3235\n",
            "Epoch [3/5], Step [120/1524], Loss: 0.3385\n",
            "Epoch [3/5], Step [140/1524], Loss: 0.3012\n",
            "Epoch [3/5], Step [160/1524], Loss: 0.3303\n",
            "Epoch [3/5], Step [180/1524], Loss: 0.3143\n",
            "Epoch [3/5], Step [200/1524], Loss: 0.3228\n",
            "Epoch [3/5], Step [220/1524], Loss: 0.2662\n",
            "Epoch [3/5], Step [240/1524], Loss: 0.2818\n",
            "Epoch [3/5], Step [260/1524], Loss: 0.2875\n",
            "Epoch [3/5], Step [280/1524], Loss: 0.2969\n",
            "Epoch [3/5], Step [300/1524], Loss: 0.3025\n",
            "Epoch [3/5], Step [320/1524], Loss: 0.3060\n",
            "Epoch [3/5], Step [340/1524], Loss: 0.3036\n",
            "Epoch [3/5], Step [360/1524], Loss: 0.2889\n",
            "Epoch [3/5], Step [380/1524], Loss: 0.2953\n",
            "Epoch [3/5], Step [400/1524], Loss: 0.2894\n",
            "Epoch [3/5], Step [420/1524], Loss: 0.3248\n",
            "Epoch [3/5], Step [440/1524], Loss: 0.3235\n",
            "Epoch [3/5], Step [460/1524], Loss: 0.3001\n",
            "Epoch [3/5], Step [480/1524], Loss: 0.2833\n",
            "Epoch [3/5], Step [500/1524], Loss: 0.2796\n",
            "Epoch [3/5], Step [520/1524], Loss: 0.2638\n",
            "Epoch [3/5], Step [540/1524], Loss: 0.2653\n",
            "Epoch [3/5], Step [560/1524], Loss: 0.2754\n",
            "Epoch [3/5], Step [580/1524], Loss: 0.2875\n",
            "Epoch [3/5], Step [600/1524], Loss: 0.3226\n",
            "Epoch [3/5], Step [620/1524], Loss: 0.2991\n",
            "Epoch [3/5], Step [640/1524], Loss: 0.3200\n",
            "Epoch [3/5], Step [660/1524], Loss: 0.3261\n",
            "Epoch [3/5], Step [680/1524], Loss: 0.3129\n",
            "Epoch [3/5], Step [700/1524], Loss: 0.2535\n",
            "Epoch [3/5], Step [720/1524], Loss: 0.3212\n",
            "Epoch [3/5], Step [740/1524], Loss: 0.2582\n",
            "Epoch [3/5], Step [760/1524], Loss: 0.3314\n",
            "Epoch [3/5], Step [780/1524], Loss: 0.2917\n",
            "Epoch [3/5], Step [800/1524], Loss: 0.2988\n",
            "Epoch [3/5], Step [820/1524], Loss: 0.2646\n",
            "Epoch [3/5], Step [840/1524], Loss: 0.2860\n",
            "Epoch [3/5], Step [860/1524], Loss: 0.2647\n",
            "Epoch [3/5], Step [880/1524], Loss: 0.2648\n",
            "Epoch [3/5], Step [900/1524], Loss: 0.3242\n",
            "Epoch [3/5], Step [920/1524], Loss: 0.2430\n",
            "Epoch [3/5], Step [940/1524], Loss: 0.3059\n",
            "Epoch [3/5], Step [960/1524], Loss: 0.2854\n",
            "Epoch [3/5], Step [980/1524], Loss: 0.3073\n",
            "Epoch [3/5], Step [1000/1524], Loss: 0.2837\n",
            "Epoch [3/5], Step [1020/1524], Loss: 0.2458\n",
            "Epoch [3/5], Step [1040/1524], Loss: 0.2819\n",
            "Epoch [3/5], Step [1060/1524], Loss: 0.2859\n",
            "Epoch [3/5], Step [1080/1524], Loss: 0.2876\n",
            "Epoch [3/5], Step [1100/1524], Loss: 0.3105\n",
            "Epoch [3/5], Step [1120/1524], Loss: 0.2890\n",
            "Epoch [3/5], Step [1140/1524], Loss: 0.3089\n",
            "Epoch [3/5], Step [1160/1524], Loss: 0.2967\n",
            "Epoch [3/5], Step [1180/1524], Loss: 0.2748\n",
            "Epoch [3/5], Step [1200/1524], Loss: 0.2655\n",
            "Epoch [3/5], Step [1220/1524], Loss: 0.2508\n",
            "Epoch [3/5], Step [1240/1524], Loss: 0.3088\n",
            "Epoch [3/5], Step [1260/1524], Loss: 0.3188\n",
            "Epoch [3/5], Step [1280/1524], Loss: 0.2788\n",
            "Epoch [3/5], Step [1300/1524], Loss: 0.3233\n",
            "Epoch [3/5], Step [1320/1524], Loss: 0.3002\n",
            "Epoch [3/5], Step [1340/1524], Loss: 0.2766\n",
            "Epoch [3/5], Step [1360/1524], Loss: 0.3181\n",
            "Epoch [3/5], Step [1380/1524], Loss: 0.2987\n",
            "Epoch [3/5], Step [1400/1524], Loss: 0.3397\n",
            "Epoch [3/5], Step [1420/1524], Loss: 0.3038\n",
            "Epoch [3/5], Step [1440/1524], Loss: 0.2950\n",
            "Epoch [3/5], Step [1460/1524], Loss: 0.2982\n",
            "Epoch [3/5], Step [1480/1524], Loss: 0.3408\n",
            "Epoch [3/5], Step [1500/1524], Loss: 0.3018\n",
            "Epoch [3/5], Step [1520/1524], Loss: 0.2710\n",
            "Epoch [3/5] | Train Loss: 0.2955 | Train Acc: 89.83% | Test Loss: 1.0337 | Test Acc: 69.20%\n",
            "Epoch [4/5], Step [20/1524], Loss: 0.2715\n",
            "Epoch [4/5], Step [40/1524], Loss: 0.3095\n",
            "Epoch [4/5], Step [60/1524], Loss: 0.2700\n",
            "Epoch [4/5], Step [80/1524], Loss: 0.2806\n",
            "Epoch [4/5], Step [100/1524], Loss: 0.3208\n",
            "Epoch [4/5], Step [120/1524], Loss: 0.2649\n",
            "Epoch [4/5], Step [140/1524], Loss: 0.3062\n",
            "Epoch [4/5], Step [160/1524], Loss: 0.2493\n",
            "Epoch [4/5], Step [180/1524], Loss: 0.2692\n",
            "Epoch [4/5], Step [200/1524], Loss: 0.2964\n",
            "Epoch [4/5], Step [220/1524], Loss: 0.3120\n",
            "Epoch [4/5], Step [240/1524], Loss: 0.3275\n",
            "Epoch [4/5], Step [260/1524], Loss: 0.2802\n",
            "Epoch [4/5], Step [280/1524], Loss: 0.2556\n",
            "Epoch [4/5], Step [300/1524], Loss: 0.2224\n",
            "Epoch [4/5], Step [320/1524], Loss: 0.2481\n",
            "Epoch [4/5], Step [340/1524], Loss: 0.2873\n",
            "Epoch [4/5], Step [360/1524], Loss: 0.2732\n",
            "Epoch [4/5], Step [380/1524], Loss: 0.3189\n",
            "Epoch [4/5], Step [400/1524], Loss: 0.2780\n",
            "Epoch [4/5], Step [420/1524], Loss: 0.2742\n",
            "Epoch [4/5], Step [440/1524], Loss: 0.2680\n",
            "Epoch [4/5], Step [460/1524], Loss: 0.3182\n",
            "Epoch [4/5], Step [480/1524], Loss: 0.2927\n",
            "Epoch [4/5], Step [500/1524], Loss: 0.2772\n",
            "Epoch [4/5], Step [520/1524], Loss: 0.2766\n",
            "Epoch [4/5], Step [540/1524], Loss: 0.2515\n",
            "Epoch [4/5], Step [560/1524], Loss: 0.2648\n",
            "Epoch [4/5], Step [580/1524], Loss: 0.2937\n",
            "Epoch [4/5], Step [600/1524], Loss: 0.2465\n",
            "Epoch [4/5], Step [620/1524], Loss: 0.2783\n",
            "Epoch [4/5], Step [640/1524], Loss: 0.2616\n",
            "Epoch [4/5], Step [660/1524], Loss: 0.2882\n",
            "Epoch [4/5], Step [680/1524], Loss: 0.2810\n",
            "Epoch [4/5], Step [700/1524], Loss: 0.2843\n",
            "Epoch [4/5], Step [720/1524], Loss: 0.2607\n",
            "Epoch [4/5], Step [740/1524], Loss: 0.3110\n",
            "Epoch [4/5], Step [760/1524], Loss: 0.2655\n",
            "Epoch [4/5], Step [780/1524], Loss: 0.2533\n",
            "Epoch [4/5], Step [800/1524], Loss: 0.2209\n",
            "Epoch [4/5], Step [820/1524], Loss: 0.2514\n",
            "Epoch [4/5], Step [840/1524], Loss: 0.2379\n",
            "Epoch [4/5], Step [860/1524], Loss: 0.2405\n",
            "Epoch [4/5], Step [880/1524], Loss: 0.2822\n",
            "Epoch [4/5], Step [900/1524], Loss: 0.2673\n",
            "Epoch [4/5], Step [920/1524], Loss: 0.2806\n",
            "Epoch [4/5], Step [940/1524], Loss: 0.2645\n",
            "Epoch [4/5], Step [960/1524], Loss: 0.2951\n",
            "Epoch [4/5], Step [980/1524], Loss: 0.3007\n",
            "Epoch [4/5], Step [1000/1524], Loss: 0.2797\n",
            "Epoch [4/5], Step [1020/1524], Loss: 0.2593\n",
            "Epoch [4/5], Step [1040/1524], Loss: 0.2813\n",
            "Epoch [4/5], Step [1060/1524], Loss: 0.2893\n",
            "Epoch [4/5], Step [1080/1524], Loss: 0.2879\n",
            "Epoch [4/5], Step [1100/1524], Loss: 0.2840\n",
            "Epoch [4/5], Step [1120/1524], Loss: 0.2812\n",
            "Epoch [4/5], Step [1140/1524], Loss: 0.2711\n",
            "Epoch [4/5], Step [1160/1524], Loss: 0.2937\n",
            "Epoch [4/5], Step [1180/1524], Loss: 0.3109\n",
            "Epoch [4/5], Step [1200/1524], Loss: 0.2770\n",
            "Epoch [4/5], Step [1220/1524], Loss: 0.2544\n",
            "Epoch [4/5], Step [1240/1524], Loss: 0.3012\n",
            "Epoch [4/5], Step [1260/1524], Loss: 0.2579\n",
            "Epoch [4/5], Step [1280/1524], Loss: 0.2944\n",
            "Epoch [4/5], Step [1300/1524], Loss: 0.2553\n",
            "Epoch [4/5], Step [1320/1524], Loss: 0.2929\n",
            "Epoch [4/5], Step [1340/1524], Loss: 0.3003\n",
            "Epoch [4/5], Step [1360/1524], Loss: 0.3130\n",
            "Epoch [4/5], Step [1380/1524], Loss: 0.2823\n",
            "Epoch [4/5], Step [1400/1524], Loss: 0.2888\n",
            "Epoch [4/5], Step [1420/1524], Loss: 0.2647\n",
            "Epoch [4/5], Step [1440/1524], Loss: 0.2571\n",
            "Epoch [4/5], Step [1460/1524], Loss: 0.2810\n",
            "Epoch [4/5], Step [1480/1524], Loss: 0.2381\n",
            "Epoch [4/5], Step [1500/1524], Loss: 0.2647\n",
            "Epoch [4/5], Step [1520/1524], Loss: 0.2633\n",
            "Epoch [4/5] | Train Loss: 0.2769 | Train Acc: 90.45% | Test Loss: 0.4963 | Test Acc: 81.00%\n",
            "Epoch [5/5], Step [20/1524], Loss: 0.2535\n",
            "Epoch [5/5], Step [40/1524], Loss: 0.2604\n",
            "Epoch [5/5], Step [60/1524], Loss: 0.2788\n",
            "Epoch [5/5], Step [80/1524], Loss: 0.2743\n",
            "Epoch [5/5], Step [100/1524], Loss: 0.2221\n",
            "Epoch [5/5], Step [120/1524], Loss: 0.2496\n",
            "Epoch [5/5], Step [140/1524], Loss: 0.2807\n",
            "Epoch [5/5], Step [160/1524], Loss: 0.2685\n",
            "Epoch [5/5], Step [180/1524], Loss: 0.2564\n",
            "Epoch [5/5], Step [200/1524], Loss: 0.2941\n",
            "Epoch [5/5], Step [220/1524], Loss: 0.2359\n",
            "Epoch [5/5], Step [240/1524], Loss: 0.2548\n",
            "Epoch [5/5], Step [260/1524], Loss: 0.2381\n",
            "Epoch [5/5], Step [280/1524], Loss: 0.2534\n",
            "Epoch [5/5], Step [300/1524], Loss: 0.2612\n",
            "Epoch [5/5], Step [320/1524], Loss: 0.2770\n",
            "Epoch [5/5], Step [340/1524], Loss: 0.2523\n",
            "Epoch [5/5], Step [360/1524], Loss: 0.2560\n",
            "Epoch [5/5], Step [380/1524], Loss: 0.2457\n",
            "Epoch [5/5], Step [400/1524], Loss: 0.2710\n",
            "Epoch [5/5], Step [420/1524], Loss: 0.2778\n",
            "Epoch [5/5], Step [440/1524], Loss: 0.2275\n",
            "Epoch [5/5], Step [460/1524], Loss: 0.2954\n",
            "Epoch [5/5], Step [480/1524], Loss: 0.2355\n",
            "Epoch [5/5], Step [500/1524], Loss: 0.3002\n",
            "Epoch [5/5], Step [520/1524], Loss: 0.2996\n",
            "Epoch [5/5], Step [540/1524], Loss: 0.2460\n",
            "Epoch [5/5], Step [560/1524], Loss: 0.3036\n",
            "Epoch [5/5], Step [580/1524], Loss: 0.2724\n",
            "Epoch [5/5], Step [600/1524], Loss: 0.2409\n",
            "Epoch [5/5], Step [620/1524], Loss: 0.2468\n",
            "Epoch [5/5], Step [640/1524], Loss: 0.2598\n",
            "Epoch [5/5], Step [660/1524], Loss: 0.2660\n",
            "Epoch [5/5], Step [680/1524], Loss: 0.2879\n",
            "Epoch [5/5], Step [700/1524], Loss: 0.2441\n",
            "Epoch [5/5], Step [720/1524], Loss: 0.2342\n",
            "Epoch [5/5], Step [740/1524], Loss: 0.3107\n",
            "Epoch [5/5], Step [760/1524], Loss: 0.2623\n",
            "Epoch [5/5], Step [780/1524], Loss: 0.2987\n",
            "Epoch [5/5], Step [800/1524], Loss: 0.2516\n",
            "Epoch [5/5], Step [820/1524], Loss: 0.2521\n",
            "Epoch [5/5], Step [840/1524], Loss: 0.3262\n",
            "Epoch [5/5], Step [860/1524], Loss: 0.2922\n",
            "Epoch [5/5], Step [880/1524], Loss: 0.2434\n",
            "Epoch [5/5], Step [900/1524], Loss: 0.2469\n",
            "Epoch [5/5], Step [920/1524], Loss: 0.2782\n",
            "Epoch [5/5], Step [940/1524], Loss: 0.3140\n",
            "Epoch [5/5], Step [960/1524], Loss: 0.2596\n",
            "Epoch [5/5], Step [980/1524], Loss: 0.2525\n",
            "Epoch [5/5], Step [1000/1524], Loss: 0.2879\n",
            "Epoch [5/5], Step [1020/1524], Loss: 0.2898\n",
            "Epoch [5/5], Step [1040/1524], Loss: 0.3009\n",
            "Epoch [5/5], Step [1060/1524], Loss: 0.2774\n",
            "Epoch [5/5], Step [1080/1524], Loss: 0.2743\n",
            "Epoch [5/5], Step [1100/1524], Loss: 0.2565\n",
            "Epoch [5/5], Step [1120/1524], Loss: 0.2526\n",
            "Epoch [5/5], Step [1140/1524], Loss: 0.2684\n",
            "Epoch [5/5], Step [1160/1524], Loss: 0.2715\n",
            "Epoch [5/5], Step [1180/1524], Loss: 0.3001\n",
            "Epoch [5/5], Step [1200/1524], Loss: 0.2570\n",
            "Epoch [5/5], Step [1220/1524], Loss: 0.2471\n",
            "Epoch [5/5], Step [1240/1524], Loss: 0.2536\n",
            "Epoch [5/5], Step [1260/1524], Loss: 0.2695\n",
            "Epoch [5/5], Step [1280/1524], Loss: 0.2581\n",
            "Epoch [5/5], Step [1300/1524], Loss: 0.2953\n",
            "Epoch [5/5], Step [1320/1524], Loss: 0.2668\n",
            "Epoch [5/5], Step [1340/1524], Loss: 0.2532\n",
            "Epoch [5/5], Step [1360/1524], Loss: 0.2455\n",
            "Epoch [5/5], Step [1380/1524], Loss: 0.2534\n",
            "Epoch [5/5], Step [1400/1524], Loss: 0.2544\n",
            "Epoch [5/5], Step [1420/1524], Loss: 0.3079\n",
            "Epoch [5/5], Step [1440/1524], Loss: 0.2726\n",
            "Epoch [5/5], Step [1460/1524], Loss: 0.2496\n",
            "Epoch [5/5], Step [1480/1524], Loss: 0.2463\n",
            "Epoch [5/5], Step [1500/1524], Loss: 0.2475\n",
            "Epoch [5/5], Step [1520/1524], Loss: 0.2486\n",
            "Epoch [5/5] | Train Loss: 0.2659 | Train Acc: 90.79% | Test Loss: 0.8034 | Test Acc: 71.00%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▃▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆█████</td></tr><tr><td>test_accuracy per epoch</td><td>█▃▁▇▂</td></tr><tr><td>train_accuracy per epoch</td><td>▁▆▇██</td></tr><tr><td>train_loss</td><td>█▆▆▅▅▄▃▄▃▃▃▃▃▃▃▂▃▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▂▁▁▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>test_accuracy per epoch</td><td>71</td></tr><tr><td>train_accuracy per epoch</td><td>90.79475</td></tr><tr><td>train_loss</td><td>0.24865</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/7y5vimi7' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/7y5vimi7</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT</a><br>Synced 5 W&B file(s), 2 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250326_152629-7y5vimi7/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments\n",
        "\n",
        "TODO\n",
        "\n",
        "\n",
        "*   Bigger architecture(s)\n",
        "*   Optuna / Grid Search\n",
        "\n"
      ],
      "metadata": {
        "id": "vgXYJQkH0jU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Without data augmentation"
      ],
      "metadata": {
        "id": "ZVDGdQcqmNhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "\n",
        "# Configuration dictionary for wandb\n",
        "config = {\n",
        "    \"in_channels\": 1,\n",
        "    \"num_epochs\": 20,            # Adjust number of epochs as necessary\n",
        "    \"lr\": 0.001,                # Learning rate\n",
        "    \"batch_size\": 64,           # Batch size (should match your DataLoader)\n",
        "    \"num_blocks\": 18,            # Number of residual blocks in the network\n",
        "    \"base_channels\": 64,        # Number of filters in the first block\n",
        "    \"kernel_size\": 3,           # Kernel size for convolutions\n",
        "    \"activation\": \"LeakyReLU\",  # Name of the activation function\n",
        "    \"use_batchnorm\": True,      # Whether to use batch normalization\n",
        "    \"num_classes\": 4            # OCTMNIST has 4 classes\n",
        "}\n",
        "\n",
        "# Initialize wandb run and train the model\n",
        "with wandb.init(\n",
        "    config=config,\n",
        "    project='CNN VS ViT',   # Title of your project\n",
        "    group='Test',           # Group name for your run\n",
        "    save_code=True,\n",
        "    name='resnet-18',\n",
        "):\n",
        "    # Create the model with custom configuration\n",
        "    model = CustomResNet(\n",
        "        in_channels=config['in_channels'],  # OCTMNIST images are single-channel if you have preprocessed accordingly\n",
        "        num_blocks=config[\"num_blocks\"],\n",
        "        base_channels=config[\"base_channels\"],\n",
        "        kernel_size=config[\"kernel_size\"],\n",
        "        activation=nn.LeakyReLU(0.1, inplace=True),\n",
        "        use_batchnorm=config[\"use_batchnorm\"],\n",
        "        num_classes=config[\"num_classes\"]\n",
        "    )\n",
        "    print(model)\n",
        "\n",
        "    # Train the model (train_loader and test_loader should be defined before)\n",
        "    train_model(model, config, \"simple resnet\")\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X8TYaSsYj3oB",
        "outputId": "460d54c1-77b3-46ec-f40e-da5a9e0c3589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukyoda-13\u001b[0m (\u001b[33mlukyoda-13-polytechnique-montr-al\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250401_134654-ecu3dq4v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ecu3dq4v' target=\"_blank\">resnet-18</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ecu3dq4v' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ecu3dq4v</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomResNet(\n",
            "  (initial_conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (initial_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  (residual_layers): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (4): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (5): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (6): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (7): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (8): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (9): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (10): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (11): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (12): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (13): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (14): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (15): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (16): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (17): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "  )\n",
            "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Epoch [1/20], Step [20/1524], Loss: 1.0445\n",
            "Epoch [1/20], Step [40/1524], Loss: 0.7904\n",
            "Epoch [1/20], Step [60/1524], Loss: 0.7567\n",
            "Epoch [1/20], Step [80/1524], Loss: 0.7216\n",
            "Epoch [1/20], Step [100/1524], Loss: 0.6913\n",
            "Epoch [1/20], Step [120/1524], Loss: 0.6692\n",
            "Epoch [1/20], Step [140/1524], Loss: 0.6825\n",
            "Epoch [1/20], Step [160/1524], Loss: 0.6800\n",
            "Epoch [1/20], Step [180/1524], Loss: 0.6552\n",
            "Epoch [1/20], Step [200/1524], Loss: 0.6643\n",
            "Epoch [1/20], Step [220/1524], Loss: 0.6282\n",
            "Epoch [1/20], Step [240/1524], Loss: 0.5994\n",
            "Epoch [1/20], Step [260/1524], Loss: 0.6591\n",
            "Epoch [1/20], Step [280/1524], Loss: 0.6343\n",
            "Epoch [1/20], Step [300/1524], Loss: 0.5681\n",
            "Epoch [1/20], Step [320/1524], Loss: 0.5701\n",
            "Epoch [1/20], Step [340/1524], Loss: 0.5850\n",
            "Epoch [1/20], Step [360/1524], Loss: 0.5688\n",
            "Epoch [1/20], Step [380/1524], Loss: 0.6099\n",
            "Epoch [1/20], Step [400/1524], Loss: 0.6114\n",
            "Epoch [1/20], Step [420/1524], Loss: 0.5840\n",
            "Epoch [1/20], Step [440/1524], Loss: 0.5549\n",
            "Epoch [1/20], Step [460/1524], Loss: 0.5563\n",
            "Epoch [1/20], Step [480/1524], Loss: 0.5390\n",
            "Epoch [1/20], Step [500/1524], Loss: 0.5589\n",
            "Epoch [1/20], Step [520/1524], Loss: 0.5717\n",
            "Epoch [1/20], Step [540/1524], Loss: 0.5810\n",
            "Epoch [1/20], Step [560/1524], Loss: 0.4853\n",
            "Epoch [1/20], Step [580/1524], Loss: 0.4996\n",
            "Epoch [1/20], Step [600/1524], Loss: 0.5135\n",
            "Epoch [1/20], Step [620/1524], Loss: 0.5555\n",
            "Epoch [1/20], Step [640/1524], Loss: 0.5290\n",
            "Epoch [1/20], Step [660/1524], Loss: 0.5407\n",
            "Epoch [1/20], Step [680/1524], Loss: 0.4976\n",
            "Epoch [1/20], Step [700/1524], Loss: 0.5125\n",
            "Epoch [1/20], Step [720/1524], Loss: 0.5670\n",
            "Epoch [1/20], Step [740/1524], Loss: 0.4773\n",
            "Epoch [1/20], Step [760/1524], Loss: 0.5087\n",
            "Epoch [1/20], Step [780/1524], Loss: 0.4872\n",
            "Epoch [1/20], Step [800/1524], Loss: 0.5371\n",
            "Epoch [1/20], Step [820/1524], Loss: 0.4722\n",
            "Epoch [1/20], Step [840/1524], Loss: 0.5103\n",
            "Epoch [1/20], Step [860/1524], Loss: 0.5352\n",
            "Epoch [1/20], Step [880/1524], Loss: 0.4660\n",
            "Epoch [1/20], Step [900/1524], Loss: 0.4669\n",
            "Epoch [1/20], Step [920/1524], Loss: 0.4337\n",
            "Epoch [1/20], Step [940/1524], Loss: 0.5497\n",
            "Epoch [1/20], Step [960/1524], Loss: 0.4556\n",
            "Epoch [1/20], Step [980/1524], Loss: 0.5053\n",
            "Epoch [1/20], Step [1000/1524], Loss: 0.4239\n",
            "Epoch [1/20], Step [1020/1524], Loss: 0.4835\n",
            "Epoch [1/20], Step [1040/1524], Loss: 0.4871\n",
            "Epoch [1/20], Step [1060/1524], Loss: 0.5214\n",
            "Epoch [1/20], Step [1080/1524], Loss: 0.4825\n",
            "Epoch [1/20], Step [1100/1524], Loss: 0.4712\n",
            "Epoch [1/20], Step [1120/1524], Loss: 0.4712\n",
            "Epoch [1/20], Step [1140/1524], Loss: 0.5223\n",
            "Epoch [1/20], Step [1160/1524], Loss: 0.3877\n",
            "Epoch [1/20], Step [1180/1524], Loss: 0.4439\n",
            "Epoch [1/20], Step [1200/1524], Loss: 0.4740\n",
            "Epoch [1/20], Step [1220/1524], Loss: 0.4368\n",
            "Epoch [1/20], Step [1240/1524], Loss: 0.4455\n",
            "Epoch [1/20], Step [1260/1524], Loss: 0.4672\n",
            "Epoch [1/20], Step [1280/1524], Loss: 0.4628\n",
            "Epoch [1/20], Step [1300/1524], Loss: 0.4782\n",
            "Epoch [1/20], Step [1320/1524], Loss: 0.4360\n",
            "Epoch [1/20], Step [1340/1524], Loss: 0.4298\n",
            "Epoch [1/20], Step [1360/1524], Loss: 0.4188\n",
            "Epoch [1/20], Step [1380/1524], Loss: 0.4750\n",
            "Epoch [1/20], Step [1400/1524], Loss: 0.4580\n",
            "Epoch [1/20], Step [1420/1524], Loss: 0.4050\n",
            "Epoch [1/20], Step [1440/1524], Loss: 0.4451\n",
            "Epoch [1/20], Step [1460/1524], Loss: 0.4409\n",
            "Epoch [1/20], Step [1480/1524], Loss: 0.4480\n",
            "Epoch [1/20], Step [1500/1524], Loss: 0.4444\n",
            "Epoch [1/20], Step [1520/1524], Loss: 0.4048\n",
            "Epoch [1/20] | Train Loss: 0.5353 | Train Acc: 81.43% | Test Loss: 1.1714 | Test Acc: 53.10%\n",
            "Epoch [2/20], Step [20/1524], Loss: 0.4617\n",
            "Epoch [2/20], Step [40/1524], Loss: 0.4617\n",
            "Epoch [2/20], Step [60/1524], Loss: 0.4550\n",
            "Epoch [2/20], Step [80/1524], Loss: 0.4150\n",
            "Epoch [2/20], Step [100/1524], Loss: 0.4260\n",
            "Epoch [2/20], Step [120/1524], Loss: 0.4337\n",
            "Epoch [2/20], Step [140/1524], Loss: 0.4155\n",
            "Epoch [2/20], Step [160/1524], Loss: 0.4468\n",
            "Epoch [2/20], Step [180/1524], Loss: 0.4257\n",
            "Epoch [2/20], Step [200/1524], Loss: 0.4352\n",
            "Epoch [2/20], Step [220/1524], Loss: 0.4134\n",
            "Epoch [2/20], Step [240/1524], Loss: 0.4504\n",
            "Epoch [2/20], Step [260/1524], Loss: 0.3730\n",
            "Epoch [2/20], Step [280/1524], Loss: 0.3915\n",
            "Epoch [2/20], Step [300/1524], Loss: 0.4264\n",
            "Epoch [2/20], Step [320/1524], Loss: 0.4228\n",
            "Epoch [2/20], Step [340/1524], Loss: 0.3996\n",
            "Epoch [2/20], Step [360/1524], Loss: 0.4346\n",
            "Epoch [2/20], Step [380/1524], Loss: 0.4071\n",
            "Epoch [2/20], Step [400/1524], Loss: 0.4120\n",
            "Epoch [2/20], Step [420/1524], Loss: 0.4892\n",
            "Epoch [2/20], Step [440/1524], Loss: 0.4033\n",
            "Epoch [2/20], Step [460/1524], Loss: 0.3992\n",
            "Epoch [2/20], Step [480/1524], Loss: 0.4158\n",
            "Epoch [2/20], Step [500/1524], Loss: 0.4133\n",
            "Epoch [2/20], Step [520/1524], Loss: 0.4477\n",
            "Epoch [2/20], Step [540/1524], Loss: 0.3841\n",
            "Epoch [2/20], Step [560/1524], Loss: 0.4389\n",
            "Epoch [2/20], Step [580/1524], Loss: 0.3902\n",
            "Epoch [2/20], Step [600/1524], Loss: 0.3519\n",
            "Epoch [2/20], Step [620/1524], Loss: 0.4053\n",
            "Epoch [2/20], Step [640/1524], Loss: 0.3873\n",
            "Epoch [2/20], Step [660/1524], Loss: 0.4244\n",
            "Epoch [2/20], Step [680/1524], Loss: 0.4155\n",
            "Epoch [2/20], Step [700/1524], Loss: 0.3916\n",
            "Epoch [2/20], Step [720/1524], Loss: 0.4041\n",
            "Epoch [2/20], Step [740/1524], Loss: 0.3835\n",
            "Epoch [2/20], Step [760/1524], Loss: 0.3897\n",
            "Epoch [2/20], Step [780/1524], Loss: 0.4071\n",
            "Epoch [2/20], Step [800/1524], Loss: 0.4204\n",
            "Epoch [2/20], Step [820/1524], Loss: 0.3855\n",
            "Epoch [2/20], Step [840/1524], Loss: 0.4152\n",
            "Epoch [2/20], Step [860/1524], Loss: 0.4137\n",
            "Epoch [2/20], Step [880/1524], Loss: 0.3604\n",
            "Epoch [2/20], Step [900/1524], Loss: 0.3994\n",
            "Epoch [2/20], Step [920/1524], Loss: 0.3774\n",
            "Epoch [2/20], Step [940/1524], Loss: 0.3746\n",
            "Epoch [2/20], Step [960/1524], Loss: 0.4043\n",
            "Epoch [2/20], Step [980/1524], Loss: 0.3624\n",
            "Epoch [2/20], Step [1000/1524], Loss: 0.4285\n",
            "Epoch [2/20], Step [1020/1524], Loss: 0.3978\n",
            "Epoch [2/20], Step [1040/1524], Loss: 0.3504\n",
            "Epoch [2/20], Step [1060/1524], Loss: 0.3868\n",
            "Epoch [2/20], Step [1080/1524], Loss: 0.3846\n",
            "Epoch [2/20], Step [1100/1524], Loss: 0.4357\n",
            "Epoch [2/20], Step [1120/1524], Loss: 0.3630\n",
            "Epoch [2/20], Step [1140/1524], Loss: 0.4120\n",
            "Epoch [2/20], Step [1160/1524], Loss: 0.3894\n",
            "Epoch [2/20], Step [1180/1524], Loss: 0.4188\n",
            "Epoch [2/20], Step [1200/1524], Loss: 0.3780\n",
            "Epoch [2/20], Step [1220/1524], Loss: 0.3805\n",
            "Epoch [2/20], Step [1240/1524], Loss: 0.3754\n",
            "Epoch [2/20], Step [1260/1524], Loss: 0.3788\n",
            "Epoch [2/20], Step [1280/1524], Loss: 0.3682\n",
            "Epoch [2/20], Step [1300/1524], Loss: 0.3592\n",
            "Epoch [2/20], Step [1320/1524], Loss: 0.3353\n",
            "Epoch [2/20], Step [1340/1524], Loss: 0.3775\n",
            "Epoch [2/20], Step [1360/1524], Loss: 0.3844\n",
            "Epoch [2/20], Step [1380/1524], Loss: 0.4026\n",
            "Epoch [2/20], Step [1400/1524], Loss: 0.3743\n",
            "Epoch [2/20], Step [1420/1524], Loss: 0.3770\n",
            "Epoch [2/20], Step [1440/1524], Loss: 0.3825\n",
            "Epoch [2/20], Step [1460/1524], Loss: 0.3834\n",
            "Epoch [2/20], Step [1480/1524], Loss: 0.3743\n",
            "Epoch [2/20], Step [1500/1524], Loss: 0.3747\n",
            "Epoch [2/20], Step [1520/1524], Loss: 0.3628\n",
            "Epoch [2/20] | Train Loss: 0.4012 | Train Acc: 86.12% | Test Loss: 0.9355 | Test Acc: 68.10%\n",
            "Epoch [3/20], Step [20/1524], Loss: 0.4255\n",
            "Epoch [3/20], Step [40/1524], Loss: 0.3710\n",
            "Epoch [3/20], Step [60/1524], Loss: 0.3541\n",
            "Epoch [3/20], Step [80/1524], Loss: 0.3488\n",
            "Epoch [3/20], Step [100/1524], Loss: 0.3802\n",
            "Epoch [3/20], Step [120/1524], Loss: 0.4504\n",
            "Epoch [3/20], Step [140/1524], Loss: 0.4213\n",
            "Epoch [3/20], Step [160/1524], Loss: 0.3642\n",
            "Epoch [3/20], Step [180/1524], Loss: 0.3667\n",
            "Epoch [3/20], Step [200/1524], Loss: 0.3785\n",
            "Epoch [3/20], Step [220/1524], Loss: 0.3719\n",
            "Epoch [3/20], Step [240/1524], Loss: 0.3930\n",
            "Epoch [3/20], Step [260/1524], Loss: 0.3247\n",
            "Epoch [3/20], Step [280/1524], Loss: 0.3810\n",
            "Epoch [3/20], Step [300/1524], Loss: 0.3682\n",
            "Epoch [3/20], Step [320/1524], Loss: 0.3595\n",
            "Epoch [3/20], Step [340/1524], Loss: 0.3534\n",
            "Epoch [3/20], Step [360/1524], Loss: 0.4039\n",
            "Epoch [3/20], Step [380/1524], Loss: 0.3993\n",
            "Epoch [3/20], Step [400/1524], Loss: 0.3845\n",
            "Epoch [3/20], Step [420/1524], Loss: 0.3408\n",
            "Epoch [3/20], Step [440/1524], Loss: 0.3522\n",
            "Epoch [3/20], Step [460/1524], Loss: 0.3513\n",
            "Epoch [3/20], Step [480/1524], Loss: 0.3839\n",
            "Epoch [3/20], Step [500/1524], Loss: 0.3653\n",
            "Epoch [3/20], Step [520/1524], Loss: 0.3700\n",
            "Epoch [3/20], Step [540/1524], Loss: 0.3918\n",
            "Epoch [3/20], Step [560/1524], Loss: 0.3590\n",
            "Epoch [3/20], Step [580/1524], Loss: 0.3734\n",
            "Epoch [3/20], Step [600/1524], Loss: 0.3872\n",
            "Epoch [3/20], Step [620/1524], Loss: 0.3752\n",
            "Epoch [3/20], Step [640/1524], Loss: 0.3808\n",
            "Epoch [3/20], Step [660/1524], Loss: 0.3682\n",
            "Epoch [3/20], Step [680/1524], Loss: 0.3336\n",
            "Epoch [3/20], Step [700/1524], Loss: 0.3487\n",
            "Epoch [3/20], Step [720/1524], Loss: 0.3703\n",
            "Epoch [3/20], Step [740/1524], Loss: 0.3718\n",
            "Epoch [3/20], Step [760/1524], Loss: 0.3601\n",
            "Epoch [3/20], Step [780/1524], Loss: 0.3868\n",
            "Epoch [3/20], Step [800/1524], Loss: 0.3440\n",
            "Epoch [3/20], Step [820/1524], Loss: 0.3635\n",
            "Epoch [3/20], Step [840/1524], Loss: 0.3754\n",
            "Epoch [3/20], Step [860/1524], Loss: 0.3558\n",
            "Epoch [3/20], Step [880/1524], Loss: 0.3349\n",
            "Epoch [3/20], Step [900/1524], Loss: 0.3574\n",
            "Epoch [3/20], Step [920/1524], Loss: 0.3307\n",
            "Epoch [3/20], Step [940/1524], Loss: 0.3293\n",
            "Epoch [3/20], Step [960/1524], Loss: 0.3855\n",
            "Epoch [3/20], Step [980/1524], Loss: 0.3534\n",
            "Epoch [3/20], Step [1000/1524], Loss: 0.3515\n",
            "Epoch [3/20], Step [1020/1524], Loss: 0.3653\n",
            "Epoch [3/20], Step [1040/1524], Loss: 0.3697\n",
            "Epoch [3/20], Step [1060/1524], Loss: 0.2841\n",
            "Epoch [3/20], Step [1080/1524], Loss: 0.3292\n",
            "Epoch [3/20], Step [1100/1524], Loss: 0.3652\n",
            "Epoch [3/20], Step [1120/1524], Loss: 0.3241\n",
            "Epoch [3/20], Step [1140/1524], Loss: 0.3200\n",
            "Epoch [3/20], Step [1160/1524], Loss: 0.3495\n",
            "Epoch [3/20], Step [1180/1524], Loss: 0.3477\n",
            "Epoch [3/20], Step [1200/1524], Loss: 0.3486\n",
            "Epoch [3/20], Step [1220/1524], Loss: 0.3557\n",
            "Epoch [3/20], Step [1240/1524], Loss: 0.3384\n",
            "Epoch [3/20], Step [1260/1524], Loss: 0.3269\n",
            "Epoch [3/20], Step [1280/1524], Loss: 0.3533\n",
            "Epoch [3/20], Step [1300/1524], Loss: 0.3522\n",
            "Epoch [3/20], Step [1320/1524], Loss: 0.3674\n",
            "Epoch [3/20], Step [1340/1524], Loss: 0.3768\n",
            "Epoch [3/20], Step [1360/1524], Loss: 0.3621\n",
            "Epoch [3/20], Step [1380/1524], Loss: 0.3518\n",
            "Epoch [3/20], Step [1400/1524], Loss: 0.3034\n",
            "Epoch [3/20], Step [1420/1524], Loss: 0.3679\n",
            "Epoch [3/20], Step [1440/1524], Loss: 0.3700\n",
            "Epoch [3/20], Step [1460/1524], Loss: 0.3795\n",
            "Epoch [3/20], Step [1480/1524], Loss: 0.3222\n",
            "Epoch [3/20], Step [1500/1524], Loss: 0.3496\n",
            "Epoch [3/20], Step [1520/1524], Loss: 0.3333\n",
            "Epoch [3/20] | Train Loss: 0.3615 | Train Acc: 87.48% | Test Loss: 0.6059 | Test Acc: 76.70%\n",
            "Epoch [4/20], Step [20/1524], Loss: 0.3902\n",
            "Epoch [4/20], Step [40/1524], Loss: 0.3417\n",
            "Epoch [4/20], Step [60/1524], Loss: 0.3697\n",
            "Epoch [4/20], Step [80/1524], Loss: 0.3948\n",
            "Epoch [4/20], Step [100/1524], Loss: 0.3627\n",
            "Epoch [4/20], Step [120/1524], Loss: 0.3477\n",
            "Epoch [4/20], Step [140/1524], Loss: 0.2917\n",
            "Epoch [4/20], Step [160/1524], Loss: 0.3372\n",
            "Epoch [4/20], Step [180/1524], Loss: 0.3180\n",
            "Epoch [4/20], Step [200/1524], Loss: 0.3140\n",
            "Epoch [4/20], Step [220/1524], Loss: 0.3421\n",
            "Epoch [4/20], Step [240/1524], Loss: 0.3530\n",
            "Epoch [4/20], Step [260/1524], Loss: 0.3283\n",
            "Epoch [4/20], Step [280/1524], Loss: 0.3447\n",
            "Epoch [4/20], Step [300/1524], Loss: 0.3584\n",
            "Epoch [4/20], Step [320/1524], Loss: 0.3407\n",
            "Epoch [4/20], Step [340/1524], Loss: 0.3388\n",
            "Epoch [4/20], Step [360/1524], Loss: 0.3222\n",
            "Epoch [4/20], Step [380/1524], Loss: 0.3609\n",
            "Epoch [4/20], Step [400/1524], Loss: 0.3224\n",
            "Epoch [4/20], Step [420/1524], Loss: 0.3297\n",
            "Epoch [4/20], Step [440/1524], Loss: 0.3601\n",
            "Epoch [4/20], Step [460/1524], Loss: 0.3744\n",
            "Epoch [4/20], Step [480/1524], Loss: 0.3617\n",
            "Epoch [4/20], Step [500/1524], Loss: 0.3518\n",
            "Epoch [4/20], Step [520/1524], Loss: 0.3365\n",
            "Epoch [4/20], Step [540/1524], Loss: 0.3270\n",
            "Epoch [4/20], Step [560/1524], Loss: 0.3262\n",
            "Epoch [4/20], Step [580/1524], Loss: 0.3113\n",
            "Epoch [4/20], Step [600/1524], Loss: 0.2851\n",
            "Epoch [4/20], Step [620/1524], Loss: 0.2774\n",
            "Epoch [4/20], Step [640/1524], Loss: 0.3196\n",
            "Epoch [4/20], Step [660/1524], Loss: 0.3173\n",
            "Epoch [4/20], Step [680/1524], Loss: 0.3264\n",
            "Epoch [4/20], Step [700/1524], Loss: 0.3576\n",
            "Epoch [4/20], Step [720/1524], Loss: 0.2944\n",
            "Epoch [4/20], Step [740/1524], Loss: 0.3597\n",
            "Epoch [4/20], Step [760/1524], Loss: 0.3757\n",
            "Epoch [4/20], Step [780/1524], Loss: 0.3399\n",
            "Epoch [4/20], Step [800/1524], Loss: 0.3425\n",
            "Epoch [4/20], Step [820/1524], Loss: 0.3274\n",
            "Epoch [4/20], Step [840/1524], Loss: 0.3397\n",
            "Epoch [4/20], Step [860/1524], Loss: 0.3017\n",
            "Epoch [4/20], Step [880/1524], Loss: 0.3079\n",
            "Epoch [4/20], Step [900/1524], Loss: 0.3145\n",
            "Epoch [4/20], Step [920/1524], Loss: 0.3379\n",
            "Epoch [4/20], Step [940/1524], Loss: 0.3604\n",
            "Epoch [4/20], Step [960/1524], Loss: 0.3349\n",
            "Epoch [4/20], Step [980/1524], Loss: 0.3719\n",
            "Epoch [4/20], Step [1000/1524], Loss: 0.3486\n",
            "Epoch [4/20], Step [1020/1524], Loss: 0.3673\n",
            "Epoch [4/20], Step [1040/1524], Loss: 0.3369\n",
            "Epoch [4/20], Step [1060/1524], Loss: 0.3511\n",
            "Epoch [4/20], Step [1080/1524], Loss: 0.3142\n",
            "Epoch [4/20], Step [1100/1524], Loss: 0.3043\n",
            "Epoch [4/20], Step [1120/1524], Loss: 0.3426\n",
            "Epoch [4/20], Step [1140/1524], Loss: 0.3033\n",
            "Epoch [4/20], Step [1160/1524], Loss: 0.3347\n",
            "Epoch [4/20], Step [1180/1524], Loss: 0.3448\n",
            "Epoch [4/20], Step [1200/1524], Loss: 0.3527\n",
            "Epoch [4/20], Step [1220/1524], Loss: 0.3295\n",
            "Epoch [4/20], Step [1240/1524], Loss: 0.3472\n",
            "Epoch [4/20], Step [1260/1524], Loss: 0.3683\n",
            "Epoch [4/20], Step [1280/1524], Loss: 0.3451\n",
            "Epoch [4/20], Step [1300/1524], Loss: 0.3543\n",
            "Epoch [4/20], Step [1320/1524], Loss: 0.3445\n",
            "Epoch [4/20], Step [1340/1524], Loss: 0.3079\n",
            "Epoch [4/20], Step [1360/1524], Loss: 0.2786\n",
            "Epoch [4/20], Step [1380/1524], Loss: 0.3165\n",
            "Epoch [4/20], Step [1400/1524], Loss: 0.3481\n",
            "Epoch [4/20], Step [1420/1524], Loss: 0.2926\n",
            "Epoch [4/20], Step [1440/1524], Loss: 0.3319\n",
            "Epoch [4/20], Step [1460/1524], Loss: 0.3292\n",
            "Epoch [4/20], Step [1480/1524], Loss: 0.3113\n",
            "Epoch [4/20], Step [1500/1524], Loss: 0.3057\n",
            "Epoch [4/20], Step [1520/1524], Loss: 0.3015\n",
            "Epoch [4/20] | Train Loss: 0.3347 | Train Acc: 88.42% | Test Loss: 0.8541 | Test Acc: 72.90%\n",
            "Epoch [5/20], Step [20/1524], Loss: 0.3479\n",
            "Epoch [5/20], Step [40/1524], Loss: 0.3026\n",
            "Epoch [5/20], Step [60/1524], Loss: 0.2806\n",
            "Epoch [5/20], Step [80/1524], Loss: 0.3112\n",
            "Epoch [5/20], Step [100/1524], Loss: 0.3324\n",
            "Epoch [5/20], Step [120/1524], Loss: 0.3287\n",
            "Epoch [5/20], Step [140/1524], Loss: 0.3220\n",
            "Epoch [5/20], Step [160/1524], Loss: 0.3062\n",
            "Epoch [5/20], Step [180/1524], Loss: 0.2933\n",
            "Epoch [5/20], Step [200/1524], Loss: 0.2935\n",
            "Epoch [5/20], Step [220/1524], Loss: 0.3280\n",
            "Epoch [5/20], Step [240/1524], Loss: 0.2894\n",
            "Epoch [5/20], Step [260/1524], Loss: 0.2867\n",
            "Epoch [5/20], Step [280/1524], Loss: 0.3013\n",
            "Epoch [5/20], Step [300/1524], Loss: 0.3340\n",
            "Epoch [5/20], Step [320/1524], Loss: 0.3173\n",
            "Epoch [5/20], Step [340/1524], Loss: 0.3420\n",
            "Epoch [5/20], Step [360/1524], Loss: 0.3596\n",
            "Epoch [5/20], Step [380/1524], Loss: 0.3390\n",
            "Epoch [5/20], Step [400/1524], Loss: 0.2903\n",
            "Epoch [5/20], Step [420/1524], Loss: 0.3140\n",
            "Epoch [5/20], Step [440/1524], Loss: 0.3230\n",
            "Epoch [5/20], Step [460/1524], Loss: 0.3129\n",
            "Epoch [5/20], Step [480/1524], Loss: 0.3178\n",
            "Epoch [5/20], Step [500/1524], Loss: 0.3136\n",
            "Epoch [5/20], Step [520/1524], Loss: 0.2880\n",
            "Epoch [5/20], Step [540/1524], Loss: 0.3089\n",
            "Epoch [5/20], Step [560/1524], Loss: 0.3131\n",
            "Epoch [5/20], Step [580/1524], Loss: 0.3112\n",
            "Epoch [5/20], Step [600/1524], Loss: 0.2989\n",
            "Epoch [5/20], Step [620/1524], Loss: 0.3197\n",
            "Epoch [5/20], Step [640/1524], Loss: 0.3488\n",
            "Epoch [5/20], Step [660/1524], Loss: 0.3271\n",
            "Epoch [5/20], Step [680/1524], Loss: 0.3261\n",
            "Epoch [5/20], Step [700/1524], Loss: 0.3186\n",
            "Epoch [5/20], Step [720/1524], Loss: 0.3366\n",
            "Epoch [5/20], Step [740/1524], Loss: 0.3367\n",
            "Epoch [5/20], Step [760/1524], Loss: 0.3064\n",
            "Epoch [5/20], Step [780/1524], Loss: 0.2979\n",
            "Epoch [5/20], Step [800/1524], Loss: 0.3337\n",
            "Epoch [5/20], Step [820/1524], Loss: 0.3646\n",
            "Epoch [5/20], Step [840/1524], Loss: 0.3518\n",
            "Epoch [5/20], Step [860/1524], Loss: 0.3644\n",
            "Epoch [5/20], Step [880/1524], Loss: 0.3460\n",
            "Epoch [5/20], Step [900/1524], Loss: 0.3186\n",
            "Epoch [5/20], Step [920/1524], Loss: 0.3187\n",
            "Epoch [5/20], Step [940/1524], Loss: 0.2896\n",
            "Epoch [5/20], Step [960/1524], Loss: 0.3566\n",
            "Epoch [5/20], Step [980/1524], Loss: 0.3061\n",
            "Epoch [5/20], Step [1000/1524], Loss: 0.3466\n",
            "Epoch [5/20], Step [1020/1524], Loss: 0.2799\n",
            "Epoch [5/20], Step [1040/1524], Loss: 0.2855\n",
            "Epoch [5/20], Step [1060/1524], Loss: 0.3201\n",
            "Epoch [5/20], Step [1080/1524], Loss: 0.3353\n",
            "Epoch [5/20], Step [1100/1524], Loss: 0.3694\n",
            "Epoch [5/20], Step [1120/1524], Loss: 0.3363\n",
            "Epoch [5/20], Step [1140/1524], Loss: 0.3417\n",
            "Epoch [5/20], Step [1160/1524], Loss: 0.3258\n",
            "Epoch [5/20], Step [1180/1524], Loss: 0.3149\n",
            "Epoch [5/20], Step [1200/1524], Loss: 0.2667\n",
            "Epoch [5/20], Step [1220/1524], Loss: 0.3467\n",
            "Epoch [5/20], Step [1240/1524], Loss: 0.3467\n",
            "Epoch [5/20], Step [1260/1524], Loss: 0.2699\n",
            "Epoch [5/20], Step [1280/1524], Loss: 0.3239\n",
            "Epoch [5/20], Step [1300/1524], Loss: 0.3202\n",
            "Epoch [5/20], Step [1320/1524], Loss: 0.3348\n",
            "Epoch [5/20], Step [1340/1524], Loss: 0.3365\n",
            "Epoch [5/20], Step [1360/1524], Loss: 0.2637\n",
            "Epoch [5/20], Step [1380/1524], Loss: 0.3268\n",
            "Epoch [5/20], Step [1400/1524], Loss: 0.3192\n",
            "Epoch [5/20], Step [1420/1524], Loss: 0.2496\n",
            "Epoch [5/20], Step [1440/1524], Loss: 0.3029\n",
            "Epoch [5/20], Step [1460/1524], Loss: 0.3301\n",
            "Epoch [5/20], Step [1480/1524], Loss: 0.2926\n",
            "Epoch [5/20], Step [1500/1524], Loss: 0.3121\n",
            "Epoch [5/20], Step [1520/1524], Loss: 0.2943\n",
            "Epoch [5/20] | Train Loss: 0.3183 | Train Acc: 89.00% | Test Loss: 0.5498 | Test Acc: 77.90%\n",
            "Epoch [6/20], Step [20/1524], Loss: 0.3873\n",
            "Epoch [6/20], Step [40/1524], Loss: 0.3233\n",
            "Epoch [6/20], Step [60/1524], Loss: 0.3569\n",
            "Epoch [6/20], Step [80/1524], Loss: 0.3257\n",
            "Epoch [6/20], Step [100/1524], Loss: 0.3182\n",
            "Epoch [6/20], Step [120/1524], Loss: 0.3453\n",
            "Epoch [6/20], Step [140/1524], Loss: 0.2819\n",
            "Epoch [6/20], Step [160/1524], Loss: 0.3117\n",
            "Epoch [6/20], Step [180/1524], Loss: 0.2738\n",
            "Epoch [6/20], Step [200/1524], Loss: 0.3032\n",
            "Epoch [6/20], Step [220/1524], Loss: 0.2897\n",
            "Epoch [6/20], Step [240/1524], Loss: 0.3690\n",
            "Epoch [6/20], Step [260/1524], Loss: 0.3043\n",
            "Epoch [6/20], Step [280/1524], Loss: 0.2894\n",
            "Epoch [6/20], Step [300/1524], Loss: 0.3163\n",
            "Epoch [6/20], Step [320/1524], Loss: 0.3468\n",
            "Epoch [6/20], Step [340/1524], Loss: 0.3132\n",
            "Epoch [6/20], Step [360/1524], Loss: 0.2971\n",
            "Epoch [6/20], Step [380/1524], Loss: 0.3135\n",
            "Epoch [6/20], Step [400/1524], Loss: 0.3386\n",
            "Epoch [6/20], Step [420/1524], Loss: 0.2978\n",
            "Epoch [6/20], Step [440/1524], Loss: 0.2906\n",
            "Epoch [6/20], Step [460/1524], Loss: 0.3139\n",
            "Epoch [6/20], Step [480/1524], Loss: 0.2802\n",
            "Epoch [6/20], Step [500/1524], Loss: 0.2852\n",
            "Epoch [6/20], Step [520/1524], Loss: 0.2866\n",
            "Epoch [6/20], Step [540/1524], Loss: 0.3236\n",
            "Epoch [6/20], Step [560/1524], Loss: 0.2883\n",
            "Epoch [6/20], Step [580/1524], Loss: 0.2811\n",
            "Epoch [6/20], Step [600/1524], Loss: 0.2879\n",
            "Epoch [6/20], Step [620/1524], Loss: 0.3213\n",
            "Epoch [6/20], Step [640/1524], Loss: 0.3174\n",
            "Epoch [6/20], Step [660/1524], Loss: 0.3410\n",
            "Epoch [6/20], Step [680/1524], Loss: 0.3465\n",
            "Epoch [6/20], Step [700/1524], Loss: 0.2931\n",
            "Epoch [6/20], Step [720/1524], Loss: 0.2706\n",
            "Epoch [6/20], Step [740/1524], Loss: 0.2927\n",
            "Epoch [6/20], Step [760/1524], Loss: 0.3447\n",
            "Epoch [6/20], Step [780/1524], Loss: 0.2881\n",
            "Epoch [6/20], Step [800/1524], Loss: 0.2473\n",
            "Epoch [6/20], Step [820/1524], Loss: 0.3152\n",
            "Epoch [6/20], Step [840/1524], Loss: 0.2825\n",
            "Epoch [6/20], Step [860/1524], Loss: 0.3172\n",
            "Epoch [6/20], Step [880/1524], Loss: 0.3256\n",
            "Epoch [6/20], Step [900/1524], Loss: 0.3105\n",
            "Epoch [6/20], Step [920/1524], Loss: 0.3105\n",
            "Epoch [6/20], Step [940/1524], Loss: 0.3163\n",
            "Epoch [6/20], Step [960/1524], Loss: 0.3060\n",
            "Epoch [6/20], Step [980/1524], Loss: 0.3319\n",
            "Epoch [6/20], Step [1000/1524], Loss: 0.2822\n",
            "Epoch [6/20], Step [1020/1524], Loss: 0.2905\n",
            "Epoch [6/20], Step [1040/1524], Loss: 0.3197\n",
            "Epoch [6/20], Step [1060/1524], Loss: 0.3137\n",
            "Epoch [6/20], Step [1080/1524], Loss: 0.2816\n",
            "Epoch [6/20], Step [1100/1524], Loss: 0.3089\n",
            "Epoch [6/20], Step [1120/1524], Loss: 0.3127\n",
            "Epoch [6/20], Step [1140/1524], Loss: 0.2953\n",
            "Epoch [6/20], Step [1160/1524], Loss: 0.3008\n",
            "Epoch [6/20], Step [1180/1524], Loss: 0.3166\n",
            "Epoch [6/20], Step [1200/1524], Loss: 0.2991\n",
            "Epoch [6/20], Step [1220/1524], Loss: 0.2982\n",
            "Epoch [6/20], Step [1240/1524], Loss: 0.3357\n",
            "Epoch [6/20], Step [1260/1524], Loss: 0.2887\n",
            "Epoch [6/20], Step [1280/1524], Loss: 0.2758\n",
            "Epoch [6/20], Step [1300/1524], Loss: 0.2887\n",
            "Epoch [6/20], Step [1320/1524], Loss: 0.2857\n",
            "Epoch [6/20], Step [1340/1524], Loss: 0.3215\n",
            "Epoch [6/20], Step [1360/1524], Loss: 0.3039\n",
            "Epoch [6/20], Step [1380/1524], Loss: 0.3238\n",
            "Epoch [6/20], Step [1400/1524], Loss: 0.3023\n",
            "Epoch [6/20], Step [1420/1524], Loss: 0.2982\n",
            "Epoch [6/20], Step [1440/1524], Loss: 0.3105\n",
            "Epoch [6/20], Step [1460/1524], Loss: 0.3253\n",
            "Epoch [6/20], Step [1480/1524], Loss: 0.3010\n",
            "Epoch [6/20], Step [1500/1524], Loss: 0.2953\n",
            "Epoch [6/20], Step [1520/1524], Loss: 0.3137\n",
            "Epoch [6/20] | Train Loss: 0.3088 | Train Acc: 89.27% | Test Loss: 0.7924 | Test Acc: 69.70%\n",
            "Epoch [7/20], Step [20/1524], Loss: 0.3615\n",
            "Epoch [7/20], Step [40/1524], Loss: 0.3208\n",
            "Epoch [7/20], Step [60/1524], Loss: 0.2914\n",
            "Epoch [7/20], Step [80/1524], Loss: 0.2901\n",
            "Epoch [7/20], Step [100/1524], Loss: 0.2844\n",
            "Epoch [7/20], Step [120/1524], Loss: 0.3190\n",
            "Epoch [7/20], Step [140/1524], Loss: 0.3154\n",
            "Epoch [7/20], Step [160/1524], Loss: 0.2749\n",
            "Epoch [7/20], Step [180/1524], Loss: 0.3194\n",
            "Epoch [7/20], Step [200/1524], Loss: 0.3219\n",
            "Epoch [7/20], Step [220/1524], Loss: 0.2832\n",
            "Epoch [7/20], Step [240/1524], Loss: 0.2655\n",
            "Epoch [7/20], Step [260/1524], Loss: 0.3179\n",
            "Epoch [7/20], Step [280/1524], Loss: 0.3217\n",
            "Epoch [7/20], Step [300/1524], Loss: 0.2714\n",
            "Epoch [7/20], Step [320/1524], Loss: 0.2569\n",
            "Epoch [7/20], Step [340/1524], Loss: 0.2892\n",
            "Epoch [7/20], Step [360/1524], Loss: 0.3503\n",
            "Epoch [7/20], Step [380/1524], Loss: 0.2649\n",
            "Epoch [7/20], Step [400/1524], Loss: 0.3093\n",
            "Epoch [7/20], Step [420/1524], Loss: 0.2738\n",
            "Epoch [7/20], Step [440/1524], Loss: 0.2801\n",
            "Epoch [7/20], Step [460/1524], Loss: 0.2708\n",
            "Epoch [7/20], Step [480/1524], Loss: 0.3045\n",
            "Epoch [7/20], Step [500/1524], Loss: 0.2875\n",
            "Epoch [7/20], Step [520/1524], Loss: 0.3268\n",
            "Epoch [7/20], Step [540/1524], Loss: 0.3278\n",
            "Epoch [7/20], Step [560/1524], Loss: 0.2572\n",
            "Epoch [7/20], Step [580/1524], Loss: 0.3043\n",
            "Epoch [7/20], Step [600/1524], Loss: 0.2717\n",
            "Epoch [7/20], Step [620/1524], Loss: 0.2969\n",
            "Epoch [7/20], Step [640/1524], Loss: 0.3156\n",
            "Epoch [7/20], Step [660/1524], Loss: 0.2791\n",
            "Epoch [7/20], Step [680/1524], Loss: 0.2974\n",
            "Epoch [7/20], Step [700/1524], Loss: 0.2759\n",
            "Epoch [7/20], Step [720/1524], Loss: 0.2954\n",
            "Epoch [7/20], Step [740/1524], Loss: 0.2688\n",
            "Epoch [7/20], Step [760/1524], Loss: 0.2546\n",
            "Epoch [7/20], Step [780/1524], Loss: 0.2938\n",
            "Epoch [7/20], Step [800/1524], Loss: 0.2645\n",
            "Epoch [7/20], Step [820/1524], Loss: 0.3130\n",
            "Epoch [7/20], Step [840/1524], Loss: 0.2891\n",
            "Epoch [7/20], Step [860/1524], Loss: 0.3211\n",
            "Epoch [7/20], Step [880/1524], Loss: 0.2963\n",
            "Epoch [7/20], Step [900/1524], Loss: 0.2884\n",
            "Epoch [7/20], Step [920/1524], Loss: 0.3208\n",
            "Epoch [7/20], Step [940/1524], Loss: 0.3168\n",
            "Epoch [7/20], Step [960/1524], Loss: 0.2935\n",
            "Epoch [7/20], Step [980/1524], Loss: 0.2959\n",
            "Epoch [7/20], Step [1000/1524], Loss: 0.3173\n",
            "Epoch [7/20], Step [1020/1524], Loss: 0.2948\n",
            "Epoch [7/20], Step [1040/1524], Loss: 0.2668\n",
            "Epoch [7/20], Step [1060/1524], Loss: 0.2973\n",
            "Epoch [7/20], Step [1080/1524], Loss: 0.3026\n",
            "Epoch [7/20], Step [1100/1524], Loss: 0.3217\n",
            "Epoch [7/20], Step [1120/1524], Loss: 0.3040\n",
            "Epoch [7/20], Step [1140/1524], Loss: 0.3023\n",
            "Epoch [7/20], Step [1160/1524], Loss: 0.3175\n",
            "Epoch [7/20], Step [1180/1524], Loss: 0.3120\n",
            "Epoch [7/20], Step [1200/1524], Loss: 0.3144\n",
            "Epoch [7/20], Step [1220/1524], Loss: 0.2824\n",
            "Epoch [7/20], Step [1240/1524], Loss: 0.2762\n",
            "Epoch [7/20], Step [1260/1524], Loss: 0.2698\n",
            "Epoch [7/20], Step [1280/1524], Loss: 0.2579\n",
            "Epoch [7/20], Step [1300/1524], Loss: 0.3020\n",
            "Epoch [7/20], Step [1320/1524], Loss: 0.2707\n",
            "Epoch [7/20], Step [1340/1524], Loss: 0.3056\n",
            "Epoch [7/20], Step [1360/1524], Loss: 0.3216\n",
            "Epoch [7/20], Step [1380/1524], Loss: 0.2905\n",
            "Epoch [7/20], Step [1400/1524], Loss: 0.2845\n",
            "Epoch [7/20], Step [1420/1524], Loss: 0.3243\n",
            "Epoch [7/20], Step [1440/1524], Loss: 0.3258\n",
            "Epoch [7/20], Step [1460/1524], Loss: 0.2786\n",
            "Epoch [7/20], Step [1480/1524], Loss: 0.2933\n",
            "Epoch [7/20], Step [1500/1524], Loss: 0.3240\n",
            "Epoch [7/20], Step [1520/1524], Loss: 0.3088\n",
            "Epoch [7/20] | Train Loss: 0.2974 | Train Acc: 89.76% | Test Loss: 0.7464 | Test Acc: 71.30%\n",
            "Epoch [8/20], Step [20/1524], Loss: 0.2939\n",
            "Epoch [8/20], Step [40/1524], Loss: 0.3348\n",
            "Epoch [8/20], Step [60/1524], Loss: 0.2916\n",
            "Epoch [8/20], Step [80/1524], Loss: 0.2673\n",
            "Epoch [8/20], Step [100/1524], Loss: 0.2612\n",
            "Epoch [8/20], Step [120/1524], Loss: 0.2787\n",
            "Epoch [8/20], Step [140/1524], Loss: 0.2571\n",
            "Epoch [8/20], Step [160/1524], Loss: 0.2796\n",
            "Epoch [8/20], Step [180/1524], Loss: 0.3126\n",
            "Epoch [8/20], Step [200/1524], Loss: 0.3036\n",
            "Epoch [8/20], Step [220/1524], Loss: 0.2989\n",
            "Epoch [8/20], Step [240/1524], Loss: 0.3035\n",
            "Epoch [8/20], Step [260/1524], Loss: 0.3080\n",
            "Epoch [8/20], Step [280/1524], Loss: 0.2799\n",
            "Epoch [8/20], Step [300/1524], Loss: 0.2739\n",
            "Epoch [8/20], Step [320/1524], Loss: 0.2973\n",
            "Epoch [8/20], Step [340/1524], Loss: 0.3109\n",
            "Epoch [8/20], Step [360/1524], Loss: 0.2862\n",
            "Epoch [8/20], Step [380/1524], Loss: 0.2716\n",
            "Epoch [8/20], Step [400/1524], Loss: 0.3129\n",
            "Epoch [8/20], Step [420/1524], Loss: 0.3010\n",
            "Epoch [8/20], Step [440/1524], Loss: 0.2862\n",
            "Epoch [8/20], Step [460/1524], Loss: 0.3048\n",
            "Epoch [8/20], Step [480/1524], Loss: 0.3074\n",
            "Epoch [8/20], Step [500/1524], Loss: 0.2794\n",
            "Epoch [8/20], Step [520/1524], Loss: 0.3646\n",
            "Epoch [8/20], Step [540/1524], Loss: 0.3107\n",
            "Epoch [8/20], Step [560/1524], Loss: 0.2777\n",
            "Epoch [8/20], Step [580/1524], Loss: 0.3286\n",
            "Epoch [8/20], Step [600/1524], Loss: 0.2891\n",
            "Epoch [8/20], Step [620/1524], Loss: 0.2985\n",
            "Epoch [8/20], Step [640/1524], Loss: 0.3135\n",
            "Epoch [8/20], Step [660/1524], Loss: 0.2940\n",
            "Epoch [8/20], Step [680/1524], Loss: 0.3211\n",
            "Epoch [8/20], Step [700/1524], Loss: 0.2738\n",
            "Epoch [8/20], Step [720/1524], Loss: 0.3054\n",
            "Epoch [8/20], Step [740/1524], Loss: 0.2907\n",
            "Epoch [8/20], Step [760/1524], Loss: 0.2569\n",
            "Epoch [8/20], Step [780/1524], Loss: 0.2979\n",
            "Epoch [8/20], Step [800/1524], Loss: 0.2938\n",
            "Epoch [8/20], Step [820/1524], Loss: 0.2748\n",
            "Epoch [8/20], Step [840/1524], Loss: 0.2641\n",
            "Epoch [8/20], Step [860/1524], Loss: 0.3292\n",
            "Epoch [8/20], Step [880/1524], Loss: 0.2641\n",
            "Epoch [8/20], Step [900/1524], Loss: 0.2886\n",
            "Epoch [8/20], Step [920/1524], Loss: 0.2782\n",
            "Epoch [8/20], Step [940/1524], Loss: 0.3065\n",
            "Epoch [8/20], Step [960/1524], Loss: 0.2926\n",
            "Epoch [8/20], Step [980/1524], Loss: 0.2729\n",
            "Epoch [8/20], Step [1000/1524], Loss: 0.2746\n",
            "Epoch [8/20], Step [1020/1524], Loss: 0.2690\n",
            "Epoch [8/20], Step [1040/1524], Loss: 0.2900\n",
            "Epoch [8/20], Step [1060/1524], Loss: 0.2903\n",
            "Epoch [8/20], Step [1080/1524], Loss: 0.2624\n",
            "Epoch [8/20], Step [1100/1524], Loss: 0.2862\n",
            "Epoch [8/20], Step [1120/1524], Loss: 0.2622\n",
            "Epoch [8/20], Step [1140/1524], Loss: 0.2988\n",
            "Epoch [8/20], Step [1160/1524], Loss: 0.2895\n",
            "Epoch [8/20], Step [1180/1524], Loss: 0.2562\n",
            "Epoch [8/20], Step [1200/1524], Loss: 0.2797\n",
            "Epoch [8/20], Step [1220/1524], Loss: 0.3214\n",
            "Epoch [8/20], Step [1240/1524], Loss: 0.2943\n",
            "Epoch [8/20], Step [1260/1524], Loss: 0.2531\n",
            "Epoch [8/20], Step [1280/1524], Loss: 0.2738\n",
            "Epoch [8/20], Step [1300/1524], Loss: 0.2812\n",
            "Epoch [8/20], Step [1320/1524], Loss: 0.3063\n",
            "Epoch [8/20], Step [1340/1524], Loss: 0.3228\n",
            "Epoch [8/20], Step [1360/1524], Loss: 0.2817\n",
            "Epoch [8/20], Step [1380/1524], Loss: 0.2706\n",
            "Epoch [8/20], Step [1400/1524], Loss: 0.2650\n",
            "Epoch [8/20], Step [1420/1524], Loss: 0.3146\n",
            "Epoch [8/20], Step [1440/1524], Loss: 0.2568\n",
            "Epoch [8/20], Step [1460/1524], Loss: 0.2860\n",
            "Epoch [8/20], Step [1480/1524], Loss: 0.2974\n",
            "Epoch [8/20], Step [1500/1524], Loss: 0.2843\n",
            "Epoch [8/20], Step [1520/1524], Loss: 0.2737\n",
            "Epoch [8/20] | Train Loss: 0.2897 | Train Acc: 90.00% | Test Loss: 0.5969 | Test Acc: 78.20%\n",
            "Epoch [9/20], Step [20/1524], Loss: 0.2917\n",
            "Epoch [9/20], Step [40/1524], Loss: 0.2736\n",
            "Epoch [9/20], Step [60/1524], Loss: 0.2778\n",
            "Epoch [9/20], Step [80/1524], Loss: 0.3008\n",
            "Epoch [9/20], Step [100/1524], Loss: 0.2785\n",
            "Epoch [9/20], Step [120/1524], Loss: 0.3138\n",
            "Epoch [9/20], Step [140/1524], Loss: 0.3244\n",
            "Epoch [9/20], Step [160/1524], Loss: 0.2904\n",
            "Epoch [9/20], Step [180/1524], Loss: 0.2638\n",
            "Epoch [9/20], Step [200/1524], Loss: 0.2826\n",
            "Epoch [9/20], Step [220/1524], Loss: 0.2910\n",
            "Epoch [9/20], Step [240/1524], Loss: 0.2687\n",
            "Epoch [9/20], Step [260/1524], Loss: 0.3064\n",
            "Epoch [9/20], Step [280/1524], Loss: 0.2721\n",
            "Epoch [9/20], Step [300/1524], Loss: 0.2807\n",
            "Epoch [9/20], Step [320/1524], Loss: 0.2682\n",
            "Epoch [9/20], Step [340/1524], Loss: 0.2734\n",
            "Epoch [9/20], Step [360/1524], Loss: 0.2656\n",
            "Epoch [9/20], Step [380/1524], Loss: 0.2605\n",
            "Epoch [9/20], Step [400/1524], Loss: 0.2945\n",
            "Epoch [9/20], Step [420/1524], Loss: 0.2689\n",
            "Epoch [9/20], Step [440/1524], Loss: 0.2555\n",
            "Epoch [9/20], Step [460/1524], Loss: 0.2624\n",
            "Epoch [9/20], Step [480/1524], Loss: 0.2735\n",
            "Epoch [9/20], Step [500/1524], Loss: 0.2507\n",
            "Epoch [9/20], Step [520/1524], Loss: 0.3008\n",
            "Epoch [9/20], Step [540/1524], Loss: 0.2792\n",
            "Epoch [9/20], Step [560/1524], Loss: 0.3216\n",
            "Epoch [9/20], Step [580/1524], Loss: 0.2890\n",
            "Epoch [9/20], Step [600/1524], Loss: 0.2699\n",
            "Epoch [9/20], Step [620/1524], Loss: 0.2522\n",
            "Epoch [9/20], Step [640/1524], Loss: 0.3049\n",
            "Epoch [9/20], Step [660/1524], Loss: 0.2903\n",
            "Epoch [9/20], Step [680/1524], Loss: 0.2567\n",
            "Epoch [9/20], Step [700/1524], Loss: 0.2727\n",
            "Epoch [9/20], Step [720/1524], Loss: 0.2707\n",
            "Epoch [9/20], Step [740/1524], Loss: 0.3380\n",
            "Epoch [9/20], Step [760/1524], Loss: 0.2957\n",
            "Epoch [9/20], Step [780/1524], Loss: 0.3090\n",
            "Epoch [9/20], Step [800/1524], Loss: 0.2597\n",
            "Epoch [9/20], Step [820/1524], Loss: 0.2731\n",
            "Epoch [9/20], Step [840/1524], Loss: 0.3106\n",
            "Epoch [9/20], Step [860/1524], Loss: 0.2649\n",
            "Epoch [9/20], Step [880/1524], Loss: 0.3023\n",
            "Epoch [9/20], Step [900/1524], Loss: 0.3210\n",
            "Epoch [9/20], Step [920/1524], Loss: 0.2856\n",
            "Epoch [9/20], Step [940/1524], Loss: 0.2787\n",
            "Epoch [9/20], Step [960/1524], Loss: 0.2441\n",
            "Epoch [9/20], Step [980/1524], Loss: 0.2827\n",
            "Epoch [9/20], Step [1000/1524], Loss: 0.2945\n",
            "Epoch [9/20], Step [1020/1524], Loss: 0.2879\n",
            "Epoch [9/20], Step [1040/1524], Loss: 0.2706\n",
            "Epoch [9/20], Step [1060/1524], Loss: 0.2968\n",
            "Epoch [9/20], Step [1080/1524], Loss: 0.2556\n",
            "Epoch [9/20], Step [1100/1524], Loss: 0.2578\n",
            "Epoch [9/20], Step [1120/1524], Loss: 0.2943\n",
            "Epoch [9/20], Step [1140/1524], Loss: 0.2887\n",
            "Epoch [9/20], Step [1160/1524], Loss: 0.2733\n",
            "Epoch [9/20], Step [1180/1524], Loss: 0.2846\n",
            "Epoch [9/20], Step [1200/1524], Loss: 0.3199\n",
            "Epoch [9/20], Step [1220/1524], Loss: 0.2759\n",
            "Epoch [9/20], Step [1240/1524], Loss: 0.2932\n",
            "Epoch [9/20], Step [1260/1524], Loss: 0.3134\n",
            "Epoch [9/20], Step [1280/1524], Loss: 0.2315\n",
            "Epoch [9/20], Step [1300/1524], Loss: 0.3070\n",
            "Epoch [9/20], Step [1320/1524], Loss: 0.2964\n",
            "Epoch [9/20], Step [1340/1524], Loss: 0.2678\n",
            "Epoch [9/20], Step [1360/1524], Loss: 0.2664\n",
            "Epoch [9/20], Step [1380/1524], Loss: 0.2555\n",
            "Epoch [9/20], Step [1400/1524], Loss: 0.2796\n",
            "Epoch [9/20], Step [1420/1524], Loss: 0.2477\n",
            "Epoch [9/20], Step [1440/1524], Loss: 0.2539\n",
            "Epoch [9/20], Step [1460/1524], Loss: 0.2548\n",
            "Epoch [9/20], Step [1480/1524], Loss: 0.2678\n",
            "Epoch [9/20], Step [1500/1524], Loss: 0.2549\n",
            "Epoch [9/20], Step [1520/1524], Loss: 0.3259\n",
            "Epoch [9/20] | Train Loss: 0.2818 | Train Acc: 90.26% | Test Loss: 1.0943 | Test Acc: 70.40%\n",
            "Epoch [10/20], Step [20/1524], Loss: 0.3201\n",
            "Epoch [10/20], Step [40/1524], Loss: 0.2926\n",
            "Epoch [10/20], Step [60/1524], Loss: 0.2966\n",
            "Epoch [10/20], Step [80/1524], Loss: 0.3246\n",
            "Epoch [10/20], Step [100/1524], Loss: 0.2604\n",
            "Epoch [10/20], Step [120/1524], Loss: 0.3032\n",
            "Epoch [10/20], Step [140/1524], Loss: 0.2649\n",
            "Epoch [10/20], Step [160/1524], Loss: 0.3090\n",
            "Epoch [10/20], Step [180/1524], Loss: 0.2621\n",
            "Epoch [10/20], Step [200/1524], Loss: 0.2454\n",
            "Epoch [10/20], Step [220/1524], Loss: 0.2891\n",
            "Epoch [10/20], Step [240/1524], Loss: 0.2698\n",
            "Epoch [10/20], Step [260/1524], Loss: 0.2422\n",
            "Epoch [10/20], Step [280/1524], Loss: 0.2952\n",
            "Epoch [10/20], Step [300/1524], Loss: 0.2335\n",
            "Epoch [10/20], Step [320/1524], Loss: 0.2747\n",
            "Epoch [10/20], Step [340/1524], Loss: 0.2613\n",
            "Epoch [10/20], Step [360/1524], Loss: 0.2914\n",
            "Epoch [10/20], Step [380/1524], Loss: 0.2732\n",
            "Epoch [10/20], Step [400/1524], Loss: 0.2656\n",
            "Epoch [10/20], Step [420/1524], Loss: 0.2888\n",
            "Epoch [10/20], Step [440/1524], Loss: 0.3066\n",
            "Epoch [10/20], Step [460/1524], Loss: 0.2630\n",
            "Epoch [10/20], Step [480/1524], Loss: 0.2411\n",
            "Epoch [10/20], Step [500/1524], Loss: 0.2459\n",
            "Epoch [10/20], Step [520/1524], Loss: 0.2487\n",
            "Epoch [10/20], Step [540/1524], Loss: 0.2740\n",
            "Epoch [10/20], Step [560/1524], Loss: 0.2642\n",
            "Epoch [10/20], Step [580/1524], Loss: 0.2665\n",
            "Epoch [10/20], Step [600/1524], Loss: 0.2315\n",
            "Epoch [10/20], Step [620/1524], Loss: 0.2967\n",
            "Epoch [10/20], Step [640/1524], Loss: 0.2604\n",
            "Epoch [10/20], Step [660/1524], Loss: 0.2662\n",
            "Epoch [10/20], Step [680/1524], Loss: 0.2781\n",
            "Epoch [10/20], Step [700/1524], Loss: 0.2717\n",
            "Epoch [10/20], Step [720/1524], Loss: 0.2877\n",
            "Epoch [10/20], Step [740/1524], Loss: 0.2810\n",
            "Epoch [10/20], Step [760/1524], Loss: 0.3167\n",
            "Epoch [10/20], Step [780/1524], Loss: 0.2937\n",
            "Epoch [10/20], Step [800/1524], Loss: 0.3026\n",
            "Epoch [10/20], Step [820/1524], Loss: 0.2880\n",
            "Epoch [10/20], Step [840/1524], Loss: 0.3019\n",
            "Epoch [10/20], Step [860/1524], Loss: 0.2868\n",
            "Epoch [10/20], Step [880/1524], Loss: 0.2851\n",
            "Epoch [10/20], Step [900/1524], Loss: 0.2550\n",
            "Epoch [10/20], Step [920/1524], Loss: 0.2675\n",
            "Epoch [10/20], Step [940/1524], Loss: 0.3092\n",
            "Epoch [10/20], Step [960/1524], Loss: 0.2860\n",
            "Epoch [10/20], Step [980/1524], Loss: 0.2720\n",
            "Epoch [10/20], Step [1000/1524], Loss: 0.2736\n",
            "Epoch [10/20], Step [1020/1524], Loss: 0.2661\n",
            "Epoch [10/20], Step [1040/1524], Loss: 0.2552\n",
            "Epoch [10/20], Step [1060/1524], Loss: 0.2344\n",
            "Epoch [10/20], Step [1080/1524], Loss: 0.2735\n",
            "Epoch [10/20], Step [1100/1524], Loss: 0.2803\n",
            "Epoch [10/20], Step [1120/1524], Loss: 0.2727\n",
            "Epoch [10/20], Step [1140/1524], Loss: 0.2753\n",
            "Epoch [10/20], Step [1160/1524], Loss: 0.2938\n",
            "Epoch [10/20], Step [1180/1524], Loss: 0.2666\n",
            "Epoch [10/20], Step [1200/1524], Loss: 0.2716\n",
            "Epoch [10/20], Step [1220/1524], Loss: 0.2747\n",
            "Epoch [10/20], Step [1240/1524], Loss: 0.2943\n",
            "Epoch [10/20], Step [1260/1524], Loss: 0.2783\n",
            "Epoch [10/20], Step [1280/1524], Loss: 0.2484\n",
            "Epoch [10/20], Step [1300/1524], Loss: 0.2753\n",
            "Epoch [10/20], Step [1320/1524], Loss: 0.3244\n",
            "Epoch [10/20], Step [1340/1524], Loss: 0.2462\n",
            "Epoch [10/20], Step [1360/1524], Loss: 0.2796\n",
            "Epoch [10/20], Step [1380/1524], Loss: 0.2393\n",
            "Epoch [10/20], Step [1400/1524], Loss: 0.2258\n",
            "Epoch [10/20], Step [1420/1524], Loss: 0.2926\n",
            "Epoch [10/20], Step [1440/1524], Loss: 0.2841\n",
            "Epoch [10/20], Step [1460/1524], Loss: 0.2913\n",
            "Epoch [10/20], Step [1480/1524], Loss: 0.2988\n",
            "Epoch [10/20], Step [1500/1524], Loss: 0.2885\n",
            "Epoch [10/20], Step [1520/1524], Loss: 0.3041\n",
            "Epoch [10/20] | Train Loss: 0.2776 | Train Acc: 90.47% | Test Loss: 0.6262 | Test Acc: 76.10%\n",
            "Epoch [11/20], Step [20/1524], Loss: 0.3457\n",
            "Epoch [11/20], Step [40/1524], Loss: 0.2550\n",
            "Epoch [11/20], Step [60/1524], Loss: 0.2600\n",
            "Epoch [11/20], Step [80/1524], Loss: 0.2743\n",
            "Epoch [11/20], Step [100/1524], Loss: 0.3014\n",
            "Epoch [11/20], Step [120/1524], Loss: 0.2841\n",
            "Epoch [11/20], Step [140/1524], Loss: 0.3265\n",
            "Epoch [11/20], Step [160/1524], Loss: 0.2629\n",
            "Epoch [11/20], Step [180/1524], Loss: 0.2741\n",
            "Epoch [11/20], Step [200/1524], Loss: 0.2848\n",
            "Epoch [11/20], Step [220/1524], Loss: 0.2621\n",
            "Epoch [11/20], Step [240/1524], Loss: 0.2800\n",
            "Epoch [11/20], Step [260/1524], Loss: 0.2584\n",
            "Epoch [11/20], Step [280/1524], Loss: 0.2829\n",
            "Epoch [11/20], Step [300/1524], Loss: 0.2776\n",
            "Epoch [11/20], Step [320/1524], Loss: 0.2978\n",
            "Epoch [11/20], Step [340/1524], Loss: 0.2466\n",
            "Epoch [11/20], Step [360/1524], Loss: 0.2802\n",
            "Epoch [11/20], Step [380/1524], Loss: 0.3038\n",
            "Epoch [11/20], Step [400/1524], Loss: 0.2758\n",
            "Epoch [11/20], Step [420/1524], Loss: 0.2808\n",
            "Epoch [11/20], Step [440/1524], Loss: 0.2479\n",
            "Epoch [11/20], Step [460/1524], Loss: 0.2465\n",
            "Epoch [11/20], Step [480/1524], Loss: 0.2617\n",
            "Epoch [11/20], Step [500/1524], Loss: 0.2380\n",
            "Epoch [11/20], Step [520/1524], Loss: 0.2668\n",
            "Epoch [11/20], Step [540/1524], Loss: 0.2378\n",
            "Epoch [11/20], Step [560/1524], Loss: 0.2492\n",
            "Epoch [11/20], Step [580/1524], Loss: 0.2624\n",
            "Epoch [11/20], Step [600/1524], Loss: 0.2732\n",
            "Epoch [11/20], Step [620/1524], Loss: 0.2445\n",
            "Epoch [11/20], Step [640/1524], Loss: 0.2770\n",
            "Epoch [11/20], Step [660/1524], Loss: 0.2524\n",
            "Epoch [11/20], Step [680/1524], Loss: 0.2783\n",
            "Epoch [11/20], Step [700/1524], Loss: 0.2595\n",
            "Epoch [11/20], Step [720/1524], Loss: 0.2499\n",
            "Epoch [11/20], Step [740/1524], Loss: 0.2777\n",
            "Epoch [11/20], Step [760/1524], Loss: 0.2660\n",
            "Epoch [11/20], Step [780/1524], Loss: 0.2763\n",
            "Epoch [11/20], Step [800/1524], Loss: 0.2788\n",
            "Epoch [11/20], Step [820/1524], Loss: 0.2434\n",
            "Epoch [11/20], Step [840/1524], Loss: 0.2810\n",
            "Epoch [11/20], Step [860/1524], Loss: 0.2279\n",
            "Epoch [11/20], Step [880/1524], Loss: 0.2920\n",
            "Epoch [11/20], Step [900/1524], Loss: 0.2669\n",
            "Epoch [11/20], Step [920/1524], Loss: 0.2873\n",
            "Epoch [11/20], Step [940/1524], Loss: 0.3256\n",
            "Epoch [11/20], Step [960/1524], Loss: 0.2552\n",
            "Epoch [11/20], Step [980/1524], Loss: 0.2670\n",
            "Epoch [11/20], Step [1000/1524], Loss: 0.2905\n",
            "Epoch [11/20], Step [1020/1524], Loss: 0.2958\n",
            "Epoch [11/20], Step [1040/1524], Loss: 0.2459\n",
            "Epoch [11/20], Step [1060/1524], Loss: 0.2946\n",
            "Epoch [11/20], Step [1080/1524], Loss: 0.2708\n",
            "Epoch [11/20], Step [1100/1524], Loss: 0.2664\n",
            "Epoch [11/20], Step [1120/1524], Loss: 0.2547\n",
            "Epoch [11/20], Step [1140/1524], Loss: 0.3039\n",
            "Epoch [11/20], Step [1160/1524], Loss: 0.2554\n",
            "Epoch [11/20], Step [1180/1524], Loss: 0.2279\n",
            "Epoch [11/20], Step [1200/1524], Loss: 0.2570\n",
            "Epoch [11/20], Step [1220/1524], Loss: 0.2775\n",
            "Epoch [11/20], Step [1240/1524], Loss: 0.2568\n",
            "Epoch [11/20], Step [1260/1524], Loss: 0.2857\n",
            "Epoch [11/20], Step [1280/1524], Loss: 0.2587\n",
            "Epoch [11/20], Step [1300/1524], Loss: 0.2793\n",
            "Epoch [11/20], Step [1320/1524], Loss: 0.2812\n",
            "Epoch [11/20], Step [1340/1524], Loss: 0.2623\n",
            "Epoch [11/20], Step [1360/1524], Loss: 0.2647\n",
            "Epoch [11/20], Step [1380/1524], Loss: 0.2947\n",
            "Epoch [11/20], Step [1400/1524], Loss: 0.2883\n",
            "Epoch [11/20], Step [1420/1524], Loss: 0.2414\n",
            "Epoch [11/20], Step [1440/1524], Loss: 0.2877\n",
            "Epoch [11/20], Step [1460/1524], Loss: 0.2782\n",
            "Epoch [11/20], Step [1480/1524], Loss: 0.2676\n",
            "Epoch [11/20], Step [1500/1524], Loss: 0.3001\n",
            "Epoch [11/20], Step [1520/1524], Loss: 0.2384\n",
            "Epoch [11/20] | Train Loss: 0.2719 | Train Acc: 90.58% | Test Loss: 0.9012 | Test Acc: 70.80%\n",
            "Epoch [12/20], Step [20/1524], Loss: 0.2846\n",
            "Epoch [12/20], Step [40/1524], Loss: 0.2773\n",
            "Epoch [12/20], Step [60/1524], Loss: 0.2870\n",
            "Epoch [12/20], Step [80/1524], Loss: 0.2526\n",
            "Epoch [12/20], Step [100/1524], Loss: 0.2466\n",
            "Epoch [12/20], Step [120/1524], Loss: 0.2666\n",
            "Epoch [12/20], Step [140/1524], Loss: 0.2767\n",
            "Epoch [12/20], Step [160/1524], Loss: 0.2888\n",
            "Epoch [12/20], Step [180/1524], Loss: 0.2719\n",
            "Epoch [12/20], Step [200/1524], Loss: 0.2734\n",
            "Epoch [12/20], Step [220/1524], Loss: 0.2741\n",
            "Epoch [12/20], Step [240/1524], Loss: 0.2336\n",
            "Epoch [12/20], Step [260/1524], Loss: 0.2817\n",
            "Epoch [12/20], Step [280/1524], Loss: 0.3006\n",
            "Epoch [12/20], Step [300/1524], Loss: 0.2548\n",
            "Epoch [12/20], Step [320/1524], Loss: 0.2674\n",
            "Epoch [12/20], Step [340/1524], Loss: 0.2546\n",
            "Epoch [12/20], Step [360/1524], Loss: 0.2950\n",
            "Epoch [12/20], Step [380/1524], Loss: 0.2763\n",
            "Epoch [12/20], Step [400/1524], Loss: 0.2658\n",
            "Epoch [12/20], Step [420/1524], Loss: 0.2654\n",
            "Epoch [12/20], Step [440/1524], Loss: 0.2687\n",
            "Epoch [12/20], Step [460/1524], Loss: 0.2786\n",
            "Epoch [12/20], Step [480/1524], Loss: 0.2701\n",
            "Epoch [12/20], Step [500/1524], Loss: 0.2579\n",
            "Epoch [12/20], Step [520/1524], Loss: 0.2344\n",
            "Epoch [12/20], Step [540/1524], Loss: 0.2665\n",
            "Epoch [12/20], Step [560/1524], Loss: 0.2505\n",
            "Epoch [12/20], Step [580/1524], Loss: 0.2940\n",
            "Epoch [12/20], Step [600/1524], Loss: 0.2601\n",
            "Epoch [12/20], Step [620/1524], Loss: 0.2685\n",
            "Epoch [12/20], Step [640/1524], Loss: 0.2485\n",
            "Epoch [12/20], Step [660/1524], Loss: 0.2650\n",
            "Epoch [12/20], Step [680/1524], Loss: 0.2676\n",
            "Epoch [12/20], Step [700/1524], Loss: 0.2846\n",
            "Epoch [12/20], Step [720/1524], Loss: 0.2531\n",
            "Epoch [12/20], Step [740/1524], Loss: 0.2655\n",
            "Epoch [12/20], Step [760/1524], Loss: 0.2583\n",
            "Epoch [12/20], Step [780/1524], Loss: 0.2540\n",
            "Epoch [12/20], Step [800/1524], Loss: 0.2227\n",
            "Epoch [12/20], Step [820/1524], Loss: 0.2488\n",
            "Epoch [12/20], Step [840/1524], Loss: 0.2771\n",
            "Epoch [12/20], Step [860/1524], Loss: 0.2758\n",
            "Epoch [12/20], Step [880/1524], Loss: 0.2605\n",
            "Epoch [12/20], Step [900/1524], Loss: 0.2683\n",
            "Epoch [12/20], Step [920/1524], Loss: 0.3027\n",
            "Epoch [12/20], Step [940/1524], Loss: 0.2826\n",
            "Epoch [12/20], Step [960/1524], Loss: 0.2508\n",
            "Epoch [12/20], Step [980/1524], Loss: 0.2465\n",
            "Epoch [12/20], Step [1000/1524], Loss: 0.2268\n",
            "Epoch [12/20], Step [1020/1524], Loss: 0.2714\n",
            "Epoch [12/20], Step [1040/1524], Loss: 0.2700\n",
            "Epoch [12/20], Step [1060/1524], Loss: 0.2499\n",
            "Epoch [12/20], Step [1080/1524], Loss: 0.2629\n",
            "Epoch [12/20], Step [1100/1524], Loss: 0.2548\n",
            "Epoch [12/20], Step [1120/1524], Loss: 0.2374\n",
            "Epoch [12/20], Step [1140/1524], Loss: 0.2920\n",
            "Epoch [12/20], Step [1160/1524], Loss: 0.2759\n",
            "Epoch [12/20], Step [1180/1524], Loss: 0.2799\n",
            "Epoch [12/20], Step [1200/1524], Loss: 0.2496\n",
            "Epoch [12/20], Step [1220/1524], Loss: 0.2380\n",
            "Epoch [12/20], Step [1240/1524], Loss: 0.2341\n",
            "Epoch [12/20], Step [1260/1524], Loss: 0.2527\n",
            "Epoch [12/20], Step [1280/1524], Loss: 0.2473\n",
            "Epoch [12/20], Step [1300/1524], Loss: 0.2588\n",
            "Epoch [12/20], Step [1320/1524], Loss: 0.2696\n",
            "Epoch [12/20], Step [1340/1524], Loss: 0.2313\n",
            "Epoch [12/20], Step [1360/1524], Loss: 0.2787\n",
            "Epoch [12/20], Step [1380/1524], Loss: 0.3002\n",
            "Epoch [12/20], Step [1400/1524], Loss: 0.2714\n",
            "Epoch [12/20], Step [1420/1524], Loss: 0.2644\n",
            "Epoch [12/20], Step [1440/1524], Loss: 0.2681\n",
            "Epoch [12/20], Step [1460/1524], Loss: 0.2535\n",
            "Epoch [12/20], Step [1480/1524], Loss: 0.3043\n",
            "Epoch [12/20], Step [1500/1524], Loss: 0.2667\n",
            "Epoch [12/20], Step [1520/1524], Loss: 0.2528\n",
            "Epoch [12/20] | Train Loss: 0.2660 | Train Acc: 90.80% | Test Loss: 0.5971 | Test Acc: 76.80%\n",
            "Epoch [13/20], Step [20/1524], Loss: 0.2965\n",
            "Epoch [13/20], Step [40/1524], Loss: 0.2977\n",
            "Epoch [13/20], Step [60/1524], Loss: 0.2816\n",
            "Epoch [13/20], Step [80/1524], Loss: 0.3192\n",
            "Epoch [13/20], Step [100/1524], Loss: 0.2942\n",
            "Epoch [13/20], Step [120/1524], Loss: 0.2838\n",
            "Epoch [13/20], Step [140/1524], Loss: 0.2436\n",
            "Epoch [13/20], Step [160/1524], Loss: 0.3248\n",
            "Epoch [13/20], Step [180/1524], Loss: 0.2199\n",
            "Epoch [13/20], Step [200/1524], Loss: 0.2484\n",
            "Epoch [13/20], Step [220/1524], Loss: 0.2639\n",
            "Epoch [13/20], Step [240/1524], Loss: 0.2703\n",
            "Epoch [13/20], Step [260/1524], Loss: 0.2631\n",
            "Epoch [13/20], Step [280/1524], Loss: 0.2568\n",
            "Epoch [13/20], Step [300/1524], Loss: 0.2471\n",
            "Epoch [13/20], Step [320/1524], Loss: 0.2996\n",
            "Epoch [13/20], Step [340/1524], Loss: 0.2540\n",
            "Epoch [13/20], Step [360/1524], Loss: 0.2502\n",
            "Epoch [13/20], Step [380/1524], Loss: 0.2530\n",
            "Epoch [13/20], Step [400/1524], Loss: 0.2496\n",
            "Epoch [13/20], Step [420/1524], Loss: 0.2620\n",
            "Epoch [13/20], Step [440/1524], Loss: 0.2619\n",
            "Epoch [13/20], Step [460/1524], Loss: 0.2296\n",
            "Epoch [13/20], Step [480/1524], Loss: 0.2658\n",
            "Epoch [13/20], Step [500/1524], Loss: 0.2595\n",
            "Epoch [13/20], Step [520/1524], Loss: 0.2600\n",
            "Epoch [13/20], Step [540/1524], Loss: 0.2968\n",
            "Epoch [13/20], Step [560/1524], Loss: 0.2743\n",
            "Epoch [13/20], Step [580/1524], Loss: 0.2585\n",
            "Epoch [13/20], Step [600/1524], Loss: 0.2536\n",
            "Epoch [13/20], Step [620/1524], Loss: 0.2770\n",
            "Epoch [13/20], Step [640/1524], Loss: 0.2607\n",
            "Epoch [13/20], Step [660/1524], Loss: 0.2728\n",
            "Epoch [13/20], Step [680/1524], Loss: 0.2512\n",
            "Epoch [13/20], Step [700/1524], Loss: 0.2551\n",
            "Epoch [13/20], Step [720/1524], Loss: 0.2774\n",
            "Epoch [13/20], Step [740/1524], Loss: 0.2425\n",
            "Epoch [13/20], Step [760/1524], Loss: 0.2610\n",
            "Epoch [13/20], Step [780/1524], Loss: 0.2608\n",
            "Epoch [13/20], Step [800/1524], Loss: 0.2603\n",
            "Epoch [13/20], Step [820/1524], Loss: 0.2840\n",
            "Epoch [13/20], Step [840/1524], Loss: 0.2690\n",
            "Epoch [13/20], Step [860/1524], Loss: 0.2462\n",
            "Epoch [13/20], Step [880/1524], Loss: 0.2428\n",
            "Epoch [13/20], Step [900/1524], Loss: 0.2676\n",
            "Epoch [13/20], Step [920/1524], Loss: 0.2536\n",
            "Epoch [13/20], Step [940/1524], Loss: 0.2581\n",
            "Epoch [13/20], Step [960/1524], Loss: 0.2644\n",
            "Epoch [13/20], Step [980/1524], Loss: 0.2640\n",
            "Epoch [13/20], Step [1000/1524], Loss: 0.2541\n",
            "Epoch [13/20], Step [1020/1524], Loss: 0.2400\n",
            "Epoch [13/20], Step [1040/1524], Loss: 0.2293\n",
            "Epoch [13/20], Step [1060/1524], Loss: 0.2537\n",
            "Epoch [13/20], Step [1080/1524], Loss: 0.2839\n",
            "Epoch [13/20], Step [1100/1524], Loss: 0.2490\n",
            "Epoch [13/20], Step [1120/1524], Loss: 0.2755\n",
            "Epoch [13/20], Step [1140/1524], Loss: 0.2588\n",
            "Epoch [13/20], Step [1160/1524], Loss: 0.2648\n",
            "Epoch [13/20], Step [1180/1524], Loss: 0.2312\n",
            "Epoch [13/20], Step [1200/1524], Loss: 0.2649\n",
            "Epoch [13/20], Step [1220/1524], Loss: 0.2868\n",
            "Epoch [13/20], Step [1240/1524], Loss: 0.2342\n",
            "Epoch [13/20], Step [1260/1524], Loss: 0.2360\n",
            "Epoch [13/20], Step [1280/1524], Loss: 0.2631\n",
            "Epoch [13/20], Step [1300/1524], Loss: 0.2453\n",
            "Epoch [13/20], Step [1320/1524], Loss: 0.2718\n",
            "Epoch [13/20], Step [1340/1524], Loss: 0.2565\n",
            "Epoch [13/20], Step [1360/1524], Loss: 0.2607\n",
            "Epoch [13/20], Step [1380/1524], Loss: 0.2502\n",
            "Epoch [13/20], Step [1400/1524], Loss: 0.2643\n",
            "Epoch [13/20], Step [1420/1524], Loss: 0.2758\n",
            "Epoch [13/20], Step [1440/1524], Loss: 0.3150\n",
            "Epoch [13/20], Step [1460/1524], Loss: 0.2649\n",
            "Epoch [13/20], Step [1480/1524], Loss: 0.2703\n",
            "Epoch [13/20], Step [1500/1524], Loss: 0.2553\n",
            "Epoch [13/20], Step [1520/1524], Loss: 0.2600\n",
            "Epoch [13/20] | Train Loss: 0.2637 | Train Acc: 90.83% | Test Loss: 0.5516 | Test Acc: 77.70%\n",
            "Epoch [14/20], Step [20/1524], Loss: 0.2469\n",
            "Epoch [14/20], Step [40/1524], Loss: 0.2567\n",
            "Epoch [14/20], Step [60/1524], Loss: 0.2445\n",
            "Epoch [14/20], Step [80/1524], Loss: 0.2498\n",
            "Epoch [14/20], Step [100/1524], Loss: 0.2360\n",
            "Epoch [14/20], Step [120/1524], Loss: 0.2098\n",
            "Epoch [14/20], Step [140/1524], Loss: 0.2357\n",
            "Epoch [14/20], Step [160/1524], Loss: 0.2813\n",
            "Epoch [14/20], Step [180/1524], Loss: 0.2524\n",
            "Epoch [14/20], Step [200/1524], Loss: 0.2552\n",
            "Epoch [14/20], Step [220/1524], Loss: 0.2440\n",
            "Epoch [14/20], Step [240/1524], Loss: 0.2557\n",
            "Epoch [14/20], Step [260/1524], Loss: 0.2515\n",
            "Epoch [14/20], Step [280/1524], Loss: 0.2624\n",
            "Epoch [14/20], Step [300/1524], Loss: 0.2438\n",
            "Epoch [14/20], Step [320/1524], Loss: 0.2603\n",
            "Epoch [14/20], Step [340/1524], Loss: 0.2539\n",
            "Epoch [14/20], Step [360/1524], Loss: 0.2260\n",
            "Epoch [14/20], Step [380/1524], Loss: 0.2590\n",
            "Epoch [14/20], Step [400/1524], Loss: 0.2490\n",
            "Epoch [14/20], Step [420/1524], Loss: 0.2450\n",
            "Epoch [14/20], Step [440/1524], Loss: 0.3020\n",
            "Epoch [14/20], Step [460/1524], Loss: 0.2999\n",
            "Epoch [14/20], Step [480/1524], Loss: 0.2570\n",
            "Epoch [14/20], Step [500/1524], Loss: 0.2557\n",
            "Epoch [14/20], Step [520/1524], Loss: 0.2615\n",
            "Epoch [14/20], Step [540/1524], Loss: 0.2342\n",
            "Epoch [14/20], Step [560/1524], Loss: 0.2291\n",
            "Epoch [14/20], Step [580/1524], Loss: 0.2719\n",
            "Epoch [14/20], Step [600/1524], Loss: 0.2424\n",
            "Epoch [14/20], Step [620/1524], Loss: 0.2430\n",
            "Epoch [14/20], Step [640/1524], Loss: 0.2723\n",
            "Epoch [14/20], Step [660/1524], Loss: 0.2379\n",
            "Epoch [14/20], Step [680/1524], Loss: 0.2609\n",
            "Epoch [14/20], Step [700/1524], Loss: 0.2762\n",
            "Epoch [14/20], Step [720/1524], Loss: 0.2433\n",
            "Epoch [14/20], Step [740/1524], Loss: 0.2711\n",
            "Epoch [14/20], Step [760/1524], Loss: 0.2490\n",
            "Epoch [14/20], Step [780/1524], Loss: 0.2300\n",
            "Epoch [14/20], Step [800/1524], Loss: 0.2498\n",
            "Epoch [14/20], Step [820/1524], Loss: 0.2691\n",
            "Epoch [14/20], Step [840/1524], Loss: 0.2581\n",
            "Epoch [14/20], Step [860/1524], Loss: 0.2818\n",
            "Epoch [14/20], Step [880/1524], Loss: 0.2485\n",
            "Epoch [14/20], Step [900/1524], Loss: 0.2893\n",
            "Epoch [14/20], Step [920/1524], Loss: 0.2443\n",
            "Epoch [14/20], Step [940/1524], Loss: 0.2631\n",
            "Epoch [14/20], Step [960/1524], Loss: 0.2385\n",
            "Epoch [14/20], Step [980/1524], Loss: 0.2598\n",
            "Epoch [14/20], Step [1000/1524], Loss: 0.3055\n",
            "Epoch [14/20], Step [1020/1524], Loss: 0.2801\n",
            "Epoch [14/20], Step [1040/1524], Loss: 0.2445\n",
            "Epoch [14/20], Step [1060/1524], Loss: 0.2586\n",
            "Epoch [14/20], Step [1080/1524], Loss: 0.2816\n",
            "Epoch [14/20], Step [1100/1524], Loss: 0.2956\n",
            "Epoch [14/20], Step [1120/1524], Loss: 0.2387\n",
            "Epoch [14/20], Step [1140/1524], Loss: 0.2168\n",
            "Epoch [14/20], Step [1160/1524], Loss: 0.2734\n",
            "Epoch [14/20], Step [1180/1524], Loss: 0.2283\n",
            "Epoch [14/20], Step [1200/1524], Loss: 0.2430\n",
            "Epoch [14/20], Step [1220/1524], Loss: 0.2327\n",
            "Epoch [14/20], Step [1240/1524], Loss: 0.2662\n",
            "Epoch [14/20], Step [1260/1524], Loss: 0.2628\n",
            "Epoch [14/20], Step [1280/1524], Loss: 0.2612\n",
            "Epoch [14/20], Step [1300/1524], Loss: 0.2535\n",
            "Epoch [14/20], Step [1320/1524], Loss: 0.2737\n",
            "Epoch [14/20], Step [1340/1524], Loss: 0.2721\n",
            "Epoch [14/20], Step [1360/1524], Loss: 0.2539\n",
            "Epoch [14/20], Step [1380/1524], Loss: 0.2296\n",
            "Epoch [14/20], Step [1400/1524], Loss: 0.2618\n",
            "Epoch [14/20], Step [1420/1524], Loss: 0.2809\n",
            "Epoch [14/20], Step [1440/1524], Loss: 0.2388\n",
            "Epoch [14/20], Step [1460/1524], Loss: 0.2408\n",
            "Epoch [14/20], Step [1480/1524], Loss: 0.2907\n",
            "Epoch [14/20], Step [1500/1524], Loss: 0.2446\n",
            "Epoch [14/20], Step [1520/1524], Loss: 0.2583\n",
            "Epoch [14/20] | Train Loss: 0.2564 | Train Acc: 91.15% | Test Loss: 0.5275 | Test Acc: 80.30%\n",
            "Epoch [15/20], Step [20/1524], Loss: 0.2868\n",
            "Epoch [15/20], Step [40/1524], Loss: 0.2541\n",
            "Epoch [15/20], Step [60/1524], Loss: 0.2661\n",
            "Epoch [15/20], Step [80/1524], Loss: 0.2612\n",
            "Epoch [15/20], Step [100/1524], Loss: 0.2637\n",
            "Epoch [15/20], Step [120/1524], Loss: 0.2488\n",
            "Epoch [15/20], Step [140/1524], Loss: 0.2670\n",
            "Epoch [15/20], Step [160/1524], Loss: 0.2285\n",
            "Epoch [15/20], Step [180/1524], Loss: 0.2300\n",
            "Epoch [15/20], Step [200/1524], Loss: 0.2751\n",
            "Epoch [15/20], Step [220/1524], Loss: 0.2520\n",
            "Epoch [15/20], Step [240/1524], Loss: 0.2555\n",
            "Epoch [15/20], Step [260/1524], Loss: 0.2637\n",
            "Epoch [15/20], Step [280/1524], Loss: 0.2222\n",
            "Epoch [15/20], Step [300/1524], Loss: 0.2256\n",
            "Epoch [15/20], Step [320/1524], Loss: 0.2248\n",
            "Epoch [15/20], Step [340/1524], Loss: 0.2774\n",
            "Epoch [15/20], Step [360/1524], Loss: 0.2325\n",
            "Epoch [15/20], Step [380/1524], Loss: 0.2658\n",
            "Epoch [15/20], Step [400/1524], Loss: 0.2581\n",
            "Epoch [15/20], Step [420/1524], Loss: 0.2821\n",
            "Epoch [15/20], Step [440/1524], Loss: 0.2579\n",
            "Epoch [15/20], Step [460/1524], Loss: 0.2626\n",
            "Epoch [15/20], Step [480/1524], Loss: 0.2570\n",
            "Epoch [15/20], Step [500/1524], Loss: 0.2713\n",
            "Epoch [15/20], Step [520/1524], Loss: 0.2931\n",
            "Epoch [15/20], Step [540/1524], Loss: 0.2516\n",
            "Epoch [15/20], Step [560/1524], Loss: 0.2361\n",
            "Epoch [15/20], Step [580/1524], Loss: 0.2311\n",
            "Epoch [15/20], Step [600/1524], Loss: 0.2427\n",
            "Epoch [15/20], Step [620/1524], Loss: 0.2657\n",
            "Epoch [15/20], Step [640/1524], Loss: 0.2303\n",
            "Epoch [15/20], Step [660/1524], Loss: 0.2407\n",
            "Epoch [15/20], Step [680/1524], Loss: 0.2838\n",
            "Epoch [15/20], Step [700/1524], Loss: 0.2465\n",
            "Epoch [15/20], Step [720/1524], Loss: 0.2294\n",
            "Epoch [15/20], Step [740/1524], Loss: 0.2362\n",
            "Epoch [15/20], Step [760/1524], Loss: 0.2468\n",
            "Epoch [15/20], Step [780/1524], Loss: 0.2325\n",
            "Epoch [15/20], Step [800/1524], Loss: 0.2684\n",
            "Epoch [15/20], Step [820/1524], Loss: 0.2697\n",
            "Epoch [15/20], Step [840/1524], Loss: 0.2362\n",
            "Epoch [15/20], Step [860/1524], Loss: 0.2349\n",
            "Epoch [15/20], Step [880/1524], Loss: 0.2616\n",
            "Epoch [15/20], Step [900/1524], Loss: 0.2721\n",
            "Epoch [15/20], Step [920/1524], Loss: 0.2219\n",
            "Epoch [15/20], Step [940/1524], Loss: 0.2408\n",
            "Epoch [15/20], Step [960/1524], Loss: 0.2626\n",
            "Epoch [15/20], Step [980/1524], Loss: 0.2966\n",
            "Epoch [15/20], Step [1000/1524], Loss: 0.2663\n",
            "Epoch [15/20], Step [1020/1524], Loss: 0.2678\n",
            "Epoch [15/20], Step [1040/1524], Loss: 0.2318\n",
            "Epoch [15/20], Step [1060/1524], Loss: 0.2342\n",
            "Epoch [15/20], Step [1080/1524], Loss: 0.2653\n",
            "Epoch [15/20], Step [1100/1524], Loss: 0.2630\n",
            "Epoch [15/20], Step [1120/1524], Loss: 0.2692\n",
            "Epoch [15/20], Step [1140/1524], Loss: 0.2106\n",
            "Epoch [15/20], Step [1160/1524], Loss: 0.2470\n",
            "Epoch [15/20], Step [1180/1524], Loss: 0.2842\n",
            "Epoch [15/20], Step [1200/1524], Loss: 0.2144\n",
            "Epoch [15/20], Step [1220/1524], Loss: 0.2270\n",
            "Epoch [15/20], Step [1240/1524], Loss: 0.2241\n",
            "Epoch [15/20], Step [1260/1524], Loss: 0.2347\n",
            "Epoch [15/20], Step [1280/1524], Loss: 0.2754\n",
            "Epoch [15/20], Step [1300/1524], Loss: 0.2740\n",
            "Epoch [15/20], Step [1320/1524], Loss: 0.2772\n",
            "Epoch [15/20], Step [1340/1524], Loss: 0.2377\n",
            "Epoch [15/20], Step [1360/1524], Loss: 0.2213\n",
            "Epoch [15/20], Step [1380/1524], Loss: 0.2368\n",
            "Epoch [15/20], Step [1400/1524], Loss: 0.2582\n",
            "Epoch [15/20], Step [1420/1524], Loss: 0.2615\n",
            "Epoch [15/20], Step [1440/1524], Loss: 0.2576\n",
            "Epoch [15/20], Step [1460/1524], Loss: 0.2657\n",
            "Epoch [15/20], Step [1480/1524], Loss: 0.2266\n",
            "Epoch [15/20], Step [1500/1524], Loss: 0.2535\n",
            "Epoch [15/20], Step [1520/1524], Loss: 0.2776\n",
            "Epoch [15/20] | Train Loss: 0.2525 | Train Acc: 91.15% | Test Loss: 0.6458 | Test Acc: 76.20%\n",
            "Epoch [16/20], Step [20/1524], Loss: 0.2394\n",
            "Epoch [16/20], Step [40/1524], Loss: 0.2469\n",
            "Epoch [16/20], Step [60/1524], Loss: 0.2388\n",
            "Epoch [16/20], Step [80/1524], Loss: 0.2414\n",
            "Epoch [16/20], Step [100/1524], Loss: 0.2506\n",
            "Epoch [16/20], Step [120/1524], Loss: 0.2660\n",
            "Epoch [16/20], Step [140/1524], Loss: 0.2441\n",
            "Epoch [16/20], Step [160/1524], Loss: 0.2436\n",
            "Epoch [16/20], Step [180/1524], Loss: 0.2259\n",
            "Epoch [16/20], Step [200/1524], Loss: 0.2469\n",
            "Epoch [16/20], Step [220/1524], Loss: 0.2249\n",
            "Epoch [16/20], Step [240/1524], Loss: 0.2657\n",
            "Epoch [16/20], Step [260/1524], Loss: 0.2567\n",
            "Epoch [16/20], Step [280/1524], Loss: 0.2693\n",
            "Epoch [16/20], Step [300/1524], Loss: 0.2297\n",
            "Epoch [16/20], Step [320/1524], Loss: 0.2865\n",
            "Epoch [16/20], Step [340/1524], Loss: 0.2815\n",
            "Epoch [16/20], Step [360/1524], Loss: 0.2596\n",
            "Epoch [16/20], Step [380/1524], Loss: 0.2327\n",
            "Epoch [16/20], Step [400/1524], Loss: 0.2797\n",
            "Epoch [16/20], Step [420/1524], Loss: 0.2832\n",
            "Epoch [16/20], Step [440/1524], Loss: 0.2595\n",
            "Epoch [16/20], Step [460/1524], Loss: 0.2359\n",
            "Epoch [16/20], Step [480/1524], Loss: 0.2297\n",
            "Epoch [16/20], Step [500/1524], Loss: 0.2621\n",
            "Epoch [16/20], Step [520/1524], Loss: 0.2539\n",
            "Epoch [16/20], Step [540/1524], Loss: 0.2646\n",
            "Epoch [16/20], Step [560/1524], Loss: 0.2402\n",
            "Epoch [16/20], Step [580/1524], Loss: 0.2586\n",
            "Epoch [16/20], Step [600/1524], Loss: 0.2514\n",
            "Epoch [16/20], Step [620/1524], Loss: 0.2646\n",
            "Epoch [16/20], Step [640/1524], Loss: 0.2305\n",
            "Epoch [16/20], Step [660/1524], Loss: 0.2495\n",
            "Epoch [16/20], Step [680/1524], Loss: 0.2560\n",
            "Epoch [16/20], Step [700/1524], Loss: 0.2493\n",
            "Epoch [16/20], Step [720/1524], Loss: 0.2181\n",
            "Epoch [16/20], Step [740/1524], Loss: 0.2395\n",
            "Epoch [16/20], Step [760/1524], Loss: 0.2379\n",
            "Epoch [16/20], Step [780/1524], Loss: 0.2257\n",
            "Epoch [16/20], Step [800/1524], Loss: 0.2097\n",
            "Epoch [16/20], Step [820/1524], Loss: 0.2422\n",
            "Epoch [16/20], Step [840/1524], Loss: 0.2566\n",
            "Epoch [16/20], Step [860/1524], Loss: 0.2462\n",
            "Epoch [16/20], Step [880/1524], Loss: 0.2409\n",
            "Epoch [16/20], Step [900/1524], Loss: 0.2810\n",
            "Epoch [16/20], Step [920/1524], Loss: 0.2398\n",
            "Epoch [16/20], Step [940/1524], Loss: 0.2587\n",
            "Epoch [16/20], Step [960/1524], Loss: 0.2549\n",
            "Epoch [16/20], Step [980/1524], Loss: 0.2514\n",
            "Epoch [16/20], Step [1000/1524], Loss: 0.2316\n",
            "Epoch [16/20], Step [1020/1524], Loss: 0.2610\n",
            "Epoch [16/20], Step [1040/1524], Loss: 0.2223\n",
            "Epoch [16/20], Step [1060/1524], Loss: 0.2567\n",
            "Epoch [16/20], Step [1080/1524], Loss: 0.2447\n",
            "Epoch [16/20], Step [1100/1524], Loss: 0.2407\n",
            "Epoch [16/20], Step [1120/1524], Loss: 0.2458\n",
            "Epoch [16/20], Step [1140/1524], Loss: 0.2338\n",
            "Epoch [16/20], Step [1160/1524], Loss: 0.2668\n",
            "Epoch [16/20], Step [1180/1524], Loss: 0.2275\n",
            "Epoch [16/20], Step [1200/1524], Loss: 0.2083\n",
            "Epoch [16/20], Step [1220/1524], Loss: 0.2359\n",
            "Epoch [16/20], Step [1240/1524], Loss: 0.2394\n",
            "Epoch [16/20], Step [1260/1524], Loss: 0.2564\n",
            "Epoch [16/20], Step [1280/1524], Loss: 0.2344\n",
            "Epoch [16/20], Step [1300/1524], Loss: 0.2471\n",
            "Epoch [16/20], Step [1320/1524], Loss: 0.2506\n",
            "Epoch [16/20], Step [1340/1524], Loss: 0.2425\n",
            "Epoch [16/20], Step [1360/1524], Loss: 0.2321\n",
            "Epoch [16/20], Step [1380/1524], Loss: 0.2637\n",
            "Epoch [16/20], Step [1400/1524], Loss: 0.2494\n",
            "Epoch [16/20], Step [1420/1524], Loss: 0.2433\n",
            "Epoch [16/20], Step [1440/1524], Loss: 0.2663\n",
            "Epoch [16/20], Step [1460/1524], Loss: 0.2823\n",
            "Epoch [16/20], Step [1480/1524], Loss: 0.2560\n",
            "Epoch [16/20], Step [1500/1524], Loss: 0.2421\n",
            "Epoch [16/20], Step [1520/1524], Loss: 0.2563\n",
            "Epoch [16/20] | Train Loss: 0.2490 | Train Acc: 91.29% | Test Loss: 0.7128 | Test Acc: 73.30%\n",
            "Epoch [17/20], Step [20/1524], Loss: 0.3015\n",
            "Epoch [17/20], Step [40/1524], Loss: 0.2910\n",
            "Epoch [17/20], Step [60/1524], Loss: 0.2444\n",
            "Epoch [17/20], Step [80/1524], Loss: 0.2266\n",
            "Epoch [17/20], Step [100/1524], Loss: 0.2258\n",
            "Epoch [17/20], Step [120/1524], Loss: 0.2371\n",
            "Epoch [17/20], Step [140/1524], Loss: 0.2343\n",
            "Epoch [17/20], Step [160/1524], Loss: 0.2398\n",
            "Epoch [17/20], Step [180/1524], Loss: 0.2409\n",
            "Epoch [17/20], Step [200/1524], Loss: 0.2580\n",
            "Epoch [17/20], Step [220/1524], Loss: 0.2647\n",
            "Epoch [17/20], Step [240/1524], Loss: 0.2409\n",
            "Epoch [17/20], Step [260/1524], Loss: 0.2434\n",
            "Epoch [17/20], Step [280/1524], Loss: 0.2460\n",
            "Epoch [17/20], Step [300/1524], Loss: 0.2241\n",
            "Epoch [17/20], Step [320/1524], Loss: 0.2464\n",
            "Epoch [17/20], Step [340/1524], Loss: 0.2609\n",
            "Epoch [17/20], Step [360/1524], Loss: 0.2460\n",
            "Epoch [17/20], Step [380/1524], Loss: 0.2601\n",
            "Epoch [17/20], Step [400/1524], Loss: 0.2378\n",
            "Epoch [17/20], Step [420/1524], Loss: 0.2472\n",
            "Epoch [17/20], Step [440/1524], Loss: 0.2273\n",
            "Epoch [17/20], Step [460/1524], Loss: 0.1950\n",
            "Epoch [17/20], Step [480/1524], Loss: 0.2779\n",
            "Epoch [17/20], Step [500/1524], Loss: 0.2686\n",
            "Epoch [17/20], Step [520/1524], Loss: 0.2388\n",
            "Epoch [17/20], Step [540/1524], Loss: 0.2601\n",
            "Epoch [17/20], Step [560/1524], Loss: 0.2595\n",
            "Epoch [17/20], Step [580/1524], Loss: 0.2450\n",
            "Epoch [17/20], Step [600/1524], Loss: 0.2373\n",
            "Epoch [17/20], Step [620/1524], Loss: 0.2542\n",
            "Epoch [17/20], Step [640/1524], Loss: 0.2288\n",
            "Epoch [17/20], Step [660/1524], Loss: 0.2286\n",
            "Epoch [17/20], Step [680/1524], Loss: 0.2427\n",
            "Epoch [17/20], Step [700/1524], Loss: 0.2812\n",
            "Epoch [17/20], Step [720/1524], Loss: 0.2570\n",
            "Epoch [17/20], Step [740/1524], Loss: 0.2299\n",
            "Epoch [17/20], Step [760/1524], Loss: 0.2328\n",
            "Epoch [17/20], Step [780/1524], Loss: 0.2436\n",
            "Epoch [17/20], Step [800/1524], Loss: 0.2761\n",
            "Epoch [17/20], Step [820/1524], Loss: 0.2282\n",
            "Epoch [17/20], Step [840/1524], Loss: 0.2704\n",
            "Epoch [17/20], Step [860/1524], Loss: 0.2257\n",
            "Epoch [17/20], Step [880/1524], Loss: 0.2408\n",
            "Epoch [17/20], Step [900/1524], Loss: 0.1905\n",
            "Epoch [17/20], Step [920/1524], Loss: 0.2791\n",
            "Epoch [17/20], Step [940/1524], Loss: 0.2258\n",
            "Epoch [17/20], Step [960/1524], Loss: 0.2498\n",
            "Epoch [17/20], Step [980/1524], Loss: 0.2449\n",
            "Epoch [17/20], Step [1000/1524], Loss: 0.2254\n",
            "Epoch [17/20], Step [1020/1524], Loss: 0.2393\n",
            "Epoch [17/20], Step [1040/1524], Loss: 0.2823\n",
            "Epoch [17/20], Step [1060/1524], Loss: 0.2409\n",
            "Epoch [17/20], Step [1080/1524], Loss: 0.2281\n",
            "Epoch [17/20], Step [1100/1524], Loss: 0.2194\n",
            "Epoch [17/20], Step [1120/1524], Loss: 0.2315\n",
            "Epoch [17/20], Step [1140/1524], Loss: 0.2355\n",
            "Epoch [17/20], Step [1160/1524], Loss: 0.2582\n",
            "Epoch [17/20], Step [1180/1524], Loss: 0.2543\n",
            "Epoch [17/20], Step [1200/1524], Loss: 0.2466\n",
            "Epoch [17/20], Step [1220/1524], Loss: 0.2272\n",
            "Epoch [17/20], Step [1240/1524], Loss: 0.2278\n",
            "Epoch [17/20], Step [1260/1524], Loss: 0.2528\n",
            "Epoch [17/20], Step [1280/1524], Loss: 0.2507\n",
            "Epoch [17/20], Step [1300/1524], Loss: 0.2394\n",
            "Epoch [17/20], Step [1320/1524], Loss: 0.2446\n",
            "Epoch [17/20], Step [1340/1524], Loss: 0.2705\n",
            "Epoch [17/20], Step [1360/1524], Loss: 0.2386\n",
            "Epoch [17/20], Step [1380/1524], Loss: 0.2124\n",
            "Epoch [17/20], Step [1400/1524], Loss: 0.2472\n",
            "Epoch [17/20], Step [1420/1524], Loss: 0.2434\n",
            "Epoch [17/20], Step [1440/1524], Loss: 0.2479\n",
            "Epoch [17/20], Step [1460/1524], Loss: 0.2417\n",
            "Epoch [17/20], Step [1480/1524], Loss: 0.2518\n",
            "Epoch [17/20], Step [1500/1524], Loss: 0.2522\n",
            "Epoch [17/20], Step [1520/1524], Loss: 0.2202\n",
            "Epoch [17/20] | Train Loss: 0.2452 | Train Acc: 91.42% | Test Loss: 0.6512 | Test Acc: 77.10%\n",
            "Epoch [18/20], Step [20/1524], Loss: 0.3227\n",
            "Epoch [18/20], Step [40/1524], Loss: 0.2511\n",
            "Epoch [18/20], Step [60/1524], Loss: 0.2629\n",
            "Epoch [18/20], Step [80/1524], Loss: 0.2451\n",
            "Epoch [18/20], Step [100/1524], Loss: 0.2488\n",
            "Epoch [18/20], Step [120/1524], Loss: 0.2237\n",
            "Epoch [18/20], Step [140/1524], Loss: 0.2690\n",
            "Epoch [18/20], Step [160/1524], Loss: 0.2604\n",
            "Epoch [18/20], Step [180/1524], Loss: 0.2433\n",
            "Epoch [18/20], Step [200/1524], Loss: 0.2208\n",
            "Epoch [18/20], Step [220/1524], Loss: 0.2535\n",
            "Epoch [18/20], Step [240/1524], Loss: 0.2196\n",
            "Epoch [18/20], Step [260/1524], Loss: 0.2278\n",
            "Epoch [18/20], Step [280/1524], Loss: 0.2979\n",
            "Epoch [18/20], Step [300/1524], Loss: 0.2294\n",
            "Epoch [18/20], Step [320/1524], Loss: 0.2343\n",
            "Epoch [18/20], Step [340/1524], Loss: 0.1991\n",
            "Epoch [18/20], Step [360/1524], Loss: 0.2248\n",
            "Epoch [18/20], Step [380/1524], Loss: 0.2206\n",
            "Epoch [18/20], Step [400/1524], Loss: 0.2645\n",
            "Epoch [18/20], Step [420/1524], Loss: 0.2401\n",
            "Epoch [18/20], Step [440/1524], Loss: 0.2417\n",
            "Epoch [18/20], Step [460/1524], Loss: 0.2634\n",
            "Epoch [18/20], Step [480/1524], Loss: 0.2522\n",
            "Epoch [18/20], Step [500/1524], Loss: 0.2258\n",
            "Epoch [18/20], Step [520/1524], Loss: 0.2334\n",
            "Epoch [18/20], Step [540/1524], Loss: 0.2612\n",
            "Epoch [18/20], Step [560/1524], Loss: 0.2361\n",
            "Epoch [18/20], Step [580/1524], Loss: 0.2748\n",
            "Epoch [18/20], Step [600/1524], Loss: 0.2446\n",
            "Epoch [18/20], Step [620/1524], Loss: 0.1993\n",
            "Epoch [18/20], Step [640/1524], Loss: 0.2232\n",
            "Epoch [18/20], Step [660/1524], Loss: 0.2180\n",
            "Epoch [18/20], Step [680/1524], Loss: 0.2440\n",
            "Epoch [18/20], Step [700/1524], Loss: 0.2363\n",
            "Epoch [18/20], Step [720/1524], Loss: 0.2228\n",
            "Epoch [18/20], Step [740/1524], Loss: 0.2547\n",
            "Epoch [18/20], Step [760/1524], Loss: 0.2543\n",
            "Epoch [18/20], Step [780/1524], Loss: 0.2639\n",
            "Epoch [18/20], Step [800/1524], Loss: 0.2000\n",
            "Epoch [18/20], Step [820/1524], Loss: 0.2467\n",
            "Epoch [18/20], Step [840/1524], Loss: 0.2236\n",
            "Epoch [18/20], Step [860/1524], Loss: 0.2568\n",
            "Epoch [18/20], Step [880/1524], Loss: 0.2488\n",
            "Epoch [18/20], Step [900/1524], Loss: 0.2171\n",
            "Epoch [18/20], Step [920/1524], Loss: 0.2683\n",
            "Epoch [18/20], Step [940/1524], Loss: 0.2347\n",
            "Epoch [18/20], Step [960/1524], Loss: 0.2769\n",
            "Epoch [18/20], Step [980/1524], Loss: 0.2635\n",
            "Epoch [18/20], Step [1000/1524], Loss: 0.2413\n",
            "Epoch [18/20], Step [1020/1524], Loss: 0.2170\n",
            "Epoch [18/20], Step [1040/1524], Loss: 0.2249\n",
            "Epoch [18/20], Step [1060/1524], Loss: 0.2410\n",
            "Epoch [18/20], Step [1080/1524], Loss: 0.2548\n",
            "Epoch [18/20], Step [1100/1524], Loss: 0.2393\n",
            "Epoch [18/20], Step [1120/1524], Loss: 0.2732\n",
            "Epoch [18/20], Step [1140/1524], Loss: 0.2293\n",
            "Epoch [18/20], Step [1160/1524], Loss: 0.2419\n",
            "Epoch [18/20], Step [1180/1524], Loss: 0.2060\n",
            "Epoch [18/20], Step [1200/1524], Loss: 0.2293\n",
            "Epoch [18/20], Step [1220/1524], Loss: 0.2577\n",
            "Epoch [18/20], Step [1240/1524], Loss: 0.2249\n",
            "Epoch [18/20], Step [1260/1524], Loss: 0.2490\n",
            "Epoch [18/20], Step [1280/1524], Loss: 0.2758\n",
            "Epoch [18/20], Step [1300/1524], Loss: 0.2508\n",
            "Epoch [18/20], Step [1320/1524], Loss: 0.2535\n",
            "Epoch [18/20], Step [1340/1524], Loss: 0.2568\n",
            "Epoch [18/20], Step [1360/1524], Loss: 0.2449\n",
            "Epoch [18/20], Step [1380/1524], Loss: 0.2283\n",
            "Epoch [18/20], Step [1400/1524], Loss: 0.2512\n",
            "Epoch [18/20], Step [1420/1524], Loss: 0.2501\n",
            "Epoch [18/20], Step [1440/1524], Loss: 0.2308\n",
            "Epoch [18/20], Step [1460/1524], Loss: 0.2344\n",
            "Epoch [18/20], Step [1480/1524], Loss: 0.2518\n",
            "Epoch [18/20], Step [1500/1524], Loss: 0.2193\n",
            "Epoch [18/20], Step [1520/1524], Loss: 0.2588\n",
            "Epoch [18/20] | Train Loss: 0.2437 | Train Acc: 91.46% | Test Loss: 0.7843 | Test Acc: 70.20%\n",
            "Epoch [19/20], Step [20/1524], Loss: 0.2574\n",
            "Epoch [19/20], Step [40/1524], Loss: 0.2879\n",
            "Epoch [19/20], Step [60/1524], Loss: 0.2317\n",
            "Epoch [19/20], Step [80/1524], Loss: 0.2363\n",
            "Epoch [19/20], Step [100/1524], Loss: 0.2586\n",
            "Epoch [19/20], Step [120/1524], Loss: 0.2676\n",
            "Epoch [19/20], Step [140/1524], Loss: 0.2403\n",
            "Epoch [19/20], Step [160/1524], Loss: 0.2463\n",
            "Epoch [19/20], Step [180/1524], Loss: 0.2361\n",
            "Epoch [19/20], Step [200/1524], Loss: 0.2341\n",
            "Epoch [19/20], Step [220/1524], Loss: 0.2371\n",
            "Epoch [19/20], Step [240/1524], Loss: 0.2095\n",
            "Epoch [19/20], Step [260/1524], Loss: 0.2400\n",
            "Epoch [19/20], Step [280/1524], Loss: 0.2904\n",
            "Epoch [19/20], Step [300/1524], Loss: 0.2500\n",
            "Epoch [19/20], Step [320/1524], Loss: 0.2449\n",
            "Epoch [19/20], Step [340/1524], Loss: 0.2669\n",
            "Epoch [19/20], Step [360/1524], Loss: 0.2314\n",
            "Epoch [19/20], Step [380/1524], Loss: 0.2259\n",
            "Epoch [19/20], Step [400/1524], Loss: 0.2505\n",
            "Epoch [19/20], Step [420/1524], Loss: 0.2643\n",
            "Epoch [19/20], Step [440/1524], Loss: 0.2426\n",
            "Epoch [19/20], Step [460/1524], Loss: 0.2572\n",
            "Epoch [19/20], Step [480/1524], Loss: 0.2183\n",
            "Epoch [19/20], Step [500/1524], Loss: 0.2181\n",
            "Epoch [19/20], Step [520/1524], Loss: 0.2182\n",
            "Epoch [19/20], Step [540/1524], Loss: 0.2224\n",
            "Epoch [19/20], Step [560/1524], Loss: 0.2591\n",
            "Epoch [19/20], Step [580/1524], Loss: 0.2401\n",
            "Epoch [19/20], Step [600/1524], Loss: 0.2010\n",
            "Epoch [19/20], Step [620/1524], Loss: 0.2276\n",
            "Epoch [19/20], Step [640/1524], Loss: 0.2481\n",
            "Epoch [19/20], Step [660/1524], Loss: 0.2167\n",
            "Epoch [19/20], Step [680/1524], Loss: 0.2258\n",
            "Epoch [19/20], Step [700/1524], Loss: 0.2103\n",
            "Epoch [19/20], Step [720/1524], Loss: 0.2351\n",
            "Epoch [19/20], Step [740/1524], Loss: 0.2431\n",
            "Epoch [19/20], Step [760/1524], Loss: 0.2199\n",
            "Epoch [19/20], Step [780/1524], Loss: 0.2338\n",
            "Epoch [19/20], Step [800/1524], Loss: 0.2467\n",
            "Epoch [19/20], Step [820/1524], Loss: 0.2415\n",
            "Epoch [19/20], Step [840/1524], Loss: 0.2441\n",
            "Epoch [19/20], Step [860/1524], Loss: 0.2026\n",
            "Epoch [19/20], Step [880/1524], Loss: 0.1979\n",
            "Epoch [19/20], Step [900/1524], Loss: 0.2243\n",
            "Epoch [19/20], Step [920/1524], Loss: 0.2448\n",
            "Epoch [19/20], Step [940/1524], Loss: 0.2404\n",
            "Epoch [19/20], Step [960/1524], Loss: 0.2176\n",
            "Epoch [19/20], Step [980/1524], Loss: 0.2208\n",
            "Epoch [19/20], Step [1000/1524], Loss: 0.1983\n",
            "Epoch [19/20], Step [1020/1524], Loss: 0.2305\n",
            "Epoch [19/20], Step [1040/1524], Loss: 0.2054\n",
            "Epoch [19/20], Step [1060/1524], Loss: 0.2303\n",
            "Epoch [19/20], Step [1080/1524], Loss: 0.2392\n",
            "Epoch [19/20], Step [1100/1524], Loss: 0.2217\n",
            "Epoch [19/20], Step [1120/1524], Loss: 0.2327\n",
            "Epoch [19/20], Step [1140/1524], Loss: 0.2421\n",
            "Epoch [19/20], Step [1160/1524], Loss: 0.2384\n",
            "Epoch [19/20], Step [1180/1524], Loss: 0.2553\n",
            "Epoch [19/20], Step [1200/1524], Loss: 0.2706\n",
            "Epoch [19/20], Step [1220/1524], Loss: 0.2509\n",
            "Epoch [19/20], Step [1240/1524], Loss: 0.2114\n",
            "Epoch [19/20], Step [1260/1524], Loss: 0.2116\n",
            "Epoch [19/20], Step [1280/1524], Loss: 0.2279\n",
            "Epoch [19/20], Step [1300/1524], Loss: 0.2222\n",
            "Epoch [19/20], Step [1320/1524], Loss: 0.2761\n",
            "Epoch [19/20], Step [1340/1524], Loss: 0.2356\n",
            "Epoch [19/20], Step [1360/1524], Loss: 0.2391\n",
            "Epoch [19/20], Step [1380/1524], Loss: 0.2249\n",
            "Epoch [19/20], Step [1400/1524], Loss: 0.2437\n",
            "Epoch [19/20], Step [1420/1524], Loss: 0.2282\n",
            "Epoch [19/20], Step [1440/1524], Loss: 0.2369\n",
            "Epoch [19/20], Step [1460/1524], Loss: 0.2456\n",
            "Epoch [19/20], Step [1480/1524], Loss: 0.2292\n",
            "Epoch [19/20], Step [1500/1524], Loss: 0.2647\n",
            "Epoch [19/20], Step [1520/1524], Loss: 0.2427\n",
            "Epoch [19/20] | Train Loss: 0.2366 | Train Acc: 91.83% | Test Loss: 1.2493 | Test Acc: 63.70%\n",
            "Epoch [20/20], Step [20/1524], Loss: 0.2532\n",
            "Epoch [20/20], Step [40/1524], Loss: 0.2377\n",
            "Epoch [20/20], Step [60/1524], Loss: 0.2365\n",
            "Epoch [20/20], Step [80/1524], Loss: 0.2441\n",
            "Epoch [20/20], Step [100/1524], Loss: 0.2312\n",
            "Epoch [20/20], Step [120/1524], Loss: 0.2224\n",
            "Epoch [20/20], Step [140/1524], Loss: 0.2189\n",
            "Epoch [20/20], Step [160/1524], Loss: 0.2406\n",
            "Epoch [20/20], Step [180/1524], Loss: 0.2349\n",
            "Epoch [20/20], Step [200/1524], Loss: 0.2048\n",
            "Epoch [20/20], Step [220/1524], Loss: 0.2415\n",
            "Epoch [20/20], Step [240/1524], Loss: 0.2346\n",
            "Epoch [20/20], Step [260/1524], Loss: 0.2172\n",
            "Epoch [20/20], Step [280/1524], Loss: 0.2255\n",
            "Epoch [20/20], Step [300/1524], Loss: 0.2605\n",
            "Epoch [20/20], Step [320/1524], Loss: 0.2522\n",
            "Epoch [20/20], Step [340/1524], Loss: 0.2261\n",
            "Epoch [20/20], Step [360/1524], Loss: 0.2450\n",
            "Epoch [20/20], Step [380/1524], Loss: 0.2264\n",
            "Epoch [20/20], Step [400/1524], Loss: 0.2202\n",
            "Epoch [20/20], Step [420/1524], Loss: 0.2328\n",
            "Epoch [20/20], Step [440/1524], Loss: 0.2162\n",
            "Epoch [20/20], Step [460/1524], Loss: 0.2036\n",
            "Epoch [20/20], Step [480/1524], Loss: 0.2644\n",
            "Epoch [20/20], Step [500/1524], Loss: 0.2368\n",
            "Epoch [20/20], Step [520/1524], Loss: 0.2014\n",
            "Epoch [20/20], Step [540/1524], Loss: 0.2195\n",
            "Epoch [20/20], Step [560/1524], Loss: 0.2751\n",
            "Epoch [20/20], Step [580/1524], Loss: 0.2225\n",
            "Epoch [20/20], Step [600/1524], Loss: 0.2347\n",
            "Epoch [20/20], Step [620/1524], Loss: 0.2445\n",
            "Epoch [20/20], Step [640/1524], Loss: 0.2365\n",
            "Epoch [20/20], Step [660/1524], Loss: 0.2582\n",
            "Epoch [20/20], Step [680/1524], Loss: 0.2038\n",
            "Epoch [20/20], Step [700/1524], Loss: 0.2193\n",
            "Epoch [20/20], Step [720/1524], Loss: 0.2371\n",
            "Epoch [20/20], Step [740/1524], Loss: 0.2459\n",
            "Epoch [20/20], Step [760/1524], Loss: 0.2436\n",
            "Epoch [20/20], Step [780/1524], Loss: 0.2341\n",
            "Epoch [20/20], Step [800/1524], Loss: 0.2365\n",
            "Epoch [20/20], Step [820/1524], Loss: 0.2048\n",
            "Epoch [20/20], Step [840/1524], Loss: 0.2396\n",
            "Epoch [20/20], Step [860/1524], Loss: 0.2684\n",
            "Epoch [20/20], Step [880/1524], Loss: 0.2578\n",
            "Epoch [20/20], Step [900/1524], Loss: 0.2076\n",
            "Epoch [20/20], Step [920/1524], Loss: 0.2395\n",
            "Epoch [20/20], Step [940/1524], Loss: 0.2280\n",
            "Epoch [20/20], Step [960/1524], Loss: 0.2211\n",
            "Epoch [20/20], Step [980/1524], Loss: 0.2370\n",
            "Epoch [20/20], Step [1000/1524], Loss: 0.2191\n",
            "Epoch [20/20], Step [1020/1524], Loss: 0.2304\n",
            "Epoch [20/20], Step [1040/1524], Loss: 0.2069\n",
            "Epoch [20/20], Step [1060/1524], Loss: 0.2067\n",
            "Epoch [20/20], Step [1080/1524], Loss: 0.2651\n",
            "Epoch [20/20], Step [1100/1524], Loss: 0.2253\n",
            "Epoch [20/20], Step [1120/1524], Loss: 0.2437\n",
            "Epoch [20/20], Step [1140/1524], Loss: 0.2524\n",
            "Epoch [20/20], Step [1160/1524], Loss: 0.2367\n",
            "Epoch [20/20], Step [1180/1524], Loss: 0.2615\n",
            "Epoch [20/20], Step [1200/1524], Loss: 0.2350\n",
            "Epoch [20/20], Step [1220/1524], Loss: 0.2069\n",
            "Epoch [20/20], Step [1240/1524], Loss: 0.2006\n",
            "Epoch [20/20], Step [1260/1524], Loss: 0.2546\n",
            "Epoch [20/20], Step [1280/1524], Loss: 0.2197\n",
            "Epoch [20/20], Step [1300/1524], Loss: 0.2842\n",
            "Epoch [20/20], Step [1320/1524], Loss: 0.2537\n",
            "Epoch [20/20], Step [1340/1524], Loss: 0.2101\n",
            "Epoch [20/20], Step [1360/1524], Loss: 0.2140\n",
            "Epoch [20/20], Step [1380/1524], Loss: 0.2657\n",
            "Epoch [20/20], Step [1400/1524], Loss: 0.2583\n",
            "Epoch [20/20], Step [1420/1524], Loss: 0.2461\n",
            "Epoch [20/20], Step [1440/1524], Loss: 0.2403\n",
            "Epoch [20/20], Step [1460/1524], Loss: 0.2009\n",
            "Epoch [20/20], Step [1480/1524], Loss: 0.2390\n",
            "Epoch [20/20], Step [1500/1524], Loss: 0.2415\n",
            "Epoch [20/20], Step [1520/1524], Loss: 0.2564\n",
            "Epoch [20/20] | Train Loss: 0.2346 | Train Acc: 91.75% | Test Loss: 0.5399 | Test Acc: 78.80%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇████</td></tr><tr><td>test_accuracy per epoch</td><td>▁▅▇▆▇▅▆▇▅▇▆▇▇█▇▆▇▅▄█</td></tr><tr><td>train_accuracy per epoch</td><td>▁▄▅▆▆▆▇▇▇▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>█▅▅▅▄▂▄▃▄▂▃▃▂▄▃▂▃▃▂▂▃▂▃▂▂▂▂▂▂▁▁▂▂▂▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>test_accuracy per epoch</td><td>78.8</td></tr><tr><td>train_accuracy per epoch</td><td>91.75395</td></tr><tr><td>train_loss</td><td>0.25645</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet-18</strong> at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ecu3dq4v' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ecu3dq4v</a><br> View project at: <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT</a><br>Synced 5 W&B file(s), 2 media file(s), 5 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250401_134654-ecu3dq4v/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With data augmentation"
      ],
      "metadata": {
        "id": "9jVPUW1Hm6Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the OCTMNIST dataset without transforms to get raw PIL images.\n",
        "raw_train_dataset = OCTMNIST(split='train', transform=None, download=True)\n",
        "test_dataset  = OCTMNIST(split='test', transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "]), download=True)\n",
        "\n",
        "print(\"Raw Train dataset size:\", len(raw_train_dataset))\n",
        "print(\"Test dataset size:\", len(test_dataset))\n",
        "\n",
        "# Define an augmentation transform.\n",
        "augmentation_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # Random horizontal flip.\n",
        "    transforms.RandomRotation(10),        # Random rotation of +/- 10 degrees.\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize for a single channel (grayscale)\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Augment every image in the raw_train_dataset 10 times.\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "\n",
        "for i in range(len(raw_train_dataset)):\n",
        "    img, label = raw_train_dataset[i]\n",
        "    for _ in range(10):\n",
        "        aug_img = augmentation_transform(img)\n",
        "        augmented_images.append(aug_img)\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "print(\"Augmented Train dataset size:\", len(augmented_images))\n",
        "# Convert lists into a dataset.\n",
        "augmented_train_dataset = TensorDataset(\n",
        "    torch.stack(augmented_images),\n",
        "    torch.from_numpy(np.array(augmented_labels))\n",
        ")\n",
        "\n",
        "# Create data loaders.\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# (Optional) Visualize a few augmented training samples.\n",
        "def imshow(img, title=None):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display the first two images from the augmented training loader.\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "imshow(torchvision.utils.make_grid(images[:2]), title=\"Sample Augmented OCTMNIST Images\")\n",
        "print(\"Labels:\", labels[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "NQDXosASm83t",
        "outputId": "3d84a644-6f44-4eaf-f6c0-ccbe036977c8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Train dataset size: 97477\n",
            "Test dataset size: 1000\n",
            "Augmented Train dataset size: 974770\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: tensor([[1],\n",
            "        [3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online\n",
        "\n",
        "name = \"resnet-18 with data aug\"\n",
        "\n",
        "# Configuration dictionary for wandb\n",
        "config = {\n",
        "    \"in_channels\": 1,\n",
        "    \"num_epochs\": 2,            # Adjust number of epochs as necessary\n",
        "    \"lr\": 0.001,                # Learning rate\n",
        "    \"batch_size\": 64,           # Batch size (should match your DataLoader)\n",
        "    \"num_blocks\": 18,            # Number of residual blocks in the network\n",
        "    \"base_channels\": 64,        # Number of filters in the first block\n",
        "    \"kernel_size\": 3,           # Kernel size for convolutions\n",
        "    \"activation\": \"LeakyReLU\",  # Name of the activation function\n",
        "    \"use_batchnorm\": True,      # Whether to use batch normalization\n",
        "    \"num_classes\": 4            # OCTMNIST has 4 classes\n",
        "}\n",
        "\n",
        "# Initialize wandb run and train the model\n",
        "with wandb.init(\n",
        "    config=config,\n",
        "    project='CNN VS ViT',   # Title of your project\n",
        "    group='Test',           # Group name for your run\n",
        "    save_code=True,\n",
        "    name=name,\n",
        "):\n",
        "    # Create the model with custom configuration\n",
        "    model = CustomResNet(\n",
        "        in_channels=config['in_channels'],  # OCTMNIST images are single-channel if you have preprocessed accordingly\n",
        "        num_blocks=config[\"num_blocks\"],\n",
        "        base_channels=config[\"base_channels\"],\n",
        "        kernel_size=config[\"kernel_size\"],\n",
        "        activation=nn.LeakyReLU(0.1, inplace=True),\n",
        "        use_batchnorm=config[\"use_batchnorm\"],\n",
        "        num_classes=config[\"num_classes\"]\n",
        "    )\n",
        "    print(model)\n",
        "\n",
        "    # Train the model (train_loader and test_loader should be defined before)\n",
        "    train_model(model, config, name)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "46XGHko71CDk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c8cec021-dfe3-4712-e868-c0475d15e2a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250401_151515-ztcosh2o</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ztcosh2o' target=\"_blank\">resnet-18 with data aug</a></strong> to <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ztcosh2o' target=\"_blank\">https://wandb.ai/lukyoda-13-polytechnique-montr-al/CNN%20VS%20ViT/runs/ztcosh2o</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomResNet(\n",
            "  (initial_conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (initial_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  (residual_layers): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (4): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (5): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (6): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (7): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (8): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (9): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (10): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (11): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (12): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (13): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (14): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (15): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (16): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (17): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "  )\n",
            "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Epoch [1/2], Step [20/15231], Loss: 1.0894\n",
            "Epoch [1/2], Step [40/15231], Loss: 0.7958\n",
            "Epoch [1/2], Step [60/15231], Loss: 0.7313\n",
            "Epoch [1/2], Step [80/15231], Loss: 0.7298\n",
            "Epoch [1/2], Step [100/15231], Loss: 0.6521\n",
            "Epoch [1/2], Step [120/15231], Loss: 0.6686\n",
            "Epoch [1/2], Step [140/15231], Loss: 0.6347\n",
            "Epoch [1/2], Step [160/15231], Loss: 0.6655\n",
            "Epoch [1/2], Step [180/15231], Loss: 0.6291\n",
            "Epoch [1/2], Step [200/15231], Loss: 0.6334\n",
            "Epoch [1/2], Step [220/15231], Loss: 0.5863\n",
            "Epoch [1/2], Step [240/15231], Loss: 0.5550\n",
            "Epoch [1/2], Step [260/15231], Loss: 0.5826\n",
            "Epoch [1/2], Step [280/15231], Loss: 0.6068\n",
            "Epoch [1/2], Step [300/15231], Loss: 0.5706\n",
            "Epoch [1/2], Step [320/15231], Loss: 0.5658\n",
            "Epoch [1/2], Step [340/15231], Loss: 0.5668\n",
            "Epoch [1/2], Step [360/15231], Loss: 0.5667\n",
            "Epoch [1/2], Step [380/15231], Loss: 0.5449\n",
            "Epoch [1/2], Step [400/15231], Loss: 0.5106\n",
            "Epoch [1/2], Step [420/15231], Loss: 0.5649\n",
            "Epoch [1/2], Step [440/15231], Loss: 0.5401\n",
            "Epoch [1/2], Step [460/15231], Loss: 0.5092\n",
            "Epoch [1/2], Step [480/15231], Loss: 0.5397\n",
            "Epoch [1/2], Step [500/15231], Loss: 0.5321\n",
            "Epoch [1/2], Step [520/15231], Loss: 0.5353\n",
            "Epoch [1/2], Step [540/15231], Loss: 0.4934\n",
            "Epoch [1/2], Step [560/15231], Loss: 0.4756\n",
            "Epoch [1/2], Step [580/15231], Loss: 0.5178\n",
            "Epoch [1/2], Step [600/15231], Loss: 0.4797\n",
            "Epoch [1/2], Step [620/15231], Loss: 0.4731\n",
            "Epoch [1/2], Step [640/15231], Loss: 0.4597\n",
            "Epoch [1/2], Step [660/15231], Loss: 0.5244\n",
            "Epoch [1/2], Step [680/15231], Loss: 0.4991\n",
            "Epoch [1/2], Step [700/15231], Loss: 0.5021\n",
            "Epoch [1/2], Step [720/15231], Loss: 0.4645\n",
            "Epoch [1/2], Step [740/15231], Loss: 0.5277\n",
            "Epoch [1/2], Step [760/15231], Loss: 0.4933\n",
            "Epoch [1/2], Step [780/15231], Loss: 0.4808\n",
            "Epoch [1/2], Step [800/15231], Loss: 0.4464\n",
            "Epoch [1/2], Step [820/15231], Loss: 0.4988\n",
            "Epoch [1/2], Step [840/15231], Loss: 0.4742\n",
            "Epoch [1/2], Step [860/15231], Loss: 0.5062\n",
            "Epoch [1/2], Step [880/15231], Loss: 0.5091\n",
            "Epoch [1/2], Step [900/15231], Loss: 0.5150\n",
            "Epoch [1/2], Step [920/15231], Loss: 0.5146\n",
            "Epoch [1/2], Step [940/15231], Loss: 0.4548\n",
            "Epoch [1/2], Step [960/15231], Loss: 0.4384\n",
            "Epoch [1/2], Step [980/15231], Loss: 0.4718\n",
            "Epoch [1/2], Step [1000/15231], Loss: 0.4830\n",
            "Epoch [1/2], Step [1020/15231], Loss: 0.4749\n",
            "Epoch [1/2], Step [1040/15231], Loss: 0.4590\n",
            "Epoch [1/2], Step [1060/15231], Loss: 0.4410\n",
            "Epoch [1/2], Step [1080/15231], Loss: 0.4263\n",
            "Epoch [1/2], Step [1100/15231], Loss: 0.4502\n",
            "Epoch [1/2], Step [1120/15231], Loss: 0.4181\n",
            "Epoch [1/2], Step [1140/15231], Loss: 0.4399\n",
            "Epoch [1/2], Step [1160/15231], Loss: 0.4255\n",
            "Epoch [1/2], Step [1180/15231], Loss: 0.4160\n",
            "Epoch [1/2], Step [1200/15231], Loss: 0.4164\n",
            "Epoch [1/2], Step [1220/15231], Loss: 0.4132\n",
            "Epoch [1/2], Step [1240/15231], Loss: 0.4335\n",
            "Epoch [1/2], Step [1260/15231], Loss: 0.4542\n",
            "Epoch [1/2], Step [1280/15231], Loss: 0.3743\n",
            "Epoch [1/2], Step [1300/15231], Loss: 0.4264\n",
            "Epoch [1/2], Step [1320/15231], Loss: 0.4566\n",
            "Epoch [1/2], Step [1340/15231], Loss: 0.4226\n",
            "Epoch [1/2], Step [1360/15231], Loss: 0.4525\n",
            "Epoch [1/2], Step [1380/15231], Loss: 0.4414\n",
            "Epoch [1/2], Step [1400/15231], Loss: 0.4644\n",
            "Epoch [1/2], Step [1420/15231], Loss: 0.3893\n",
            "Epoch [1/2], Step [1440/15231], Loss: 0.4577\n",
            "Epoch [1/2], Step [1460/15231], Loss: 0.4287\n",
            "Epoch [1/2], Step [1480/15231], Loss: 0.4082\n",
            "Epoch [1/2], Step [1500/15231], Loss: 0.4098\n",
            "Epoch [1/2], Step [1520/15231], Loss: 0.3959\n",
            "Epoch [1/2], Step [1540/15231], Loss: 0.4206\n",
            "Epoch [1/2], Step [1560/15231], Loss: 0.4173\n",
            "Epoch [1/2], Step [1580/15231], Loss: 0.4437\n",
            "Epoch [1/2], Step [1600/15231], Loss: 0.4335\n",
            "Epoch [1/2], Step [1620/15231], Loss: 0.4101\n",
            "Epoch [1/2], Step [1640/15231], Loss: 0.4214\n",
            "Epoch [1/2], Step [1660/15231], Loss: 0.4121\n",
            "Epoch [1/2], Step [1680/15231], Loss: 0.4194\n",
            "Epoch [1/2], Step [1700/15231], Loss: 0.4011\n",
            "Epoch [1/2], Step [1720/15231], Loss: 0.4583\n",
            "Epoch [1/2], Step [1740/15231], Loss: 0.3890\n",
            "Epoch [1/2], Step [1760/15231], Loss: 0.4274\n",
            "Epoch [1/2], Step [1780/15231], Loss: 0.4425\n",
            "Epoch [1/2], Step [1800/15231], Loss: 0.4380\n",
            "Epoch [1/2], Step [1820/15231], Loss: 0.4098\n",
            "Epoch [1/2], Step [1840/15231], Loss: 0.4076\n",
            "Epoch [1/2], Step [1860/15231], Loss: 0.4359\n",
            "Epoch [1/2], Step [1880/15231], Loss: 0.3706\n",
            "Epoch [1/2], Step [1900/15231], Loss: 0.4036\n",
            "Epoch [1/2], Step [1920/15231], Loss: 0.4041\n",
            "Epoch [1/2], Step [1940/15231], Loss: 0.3880\n",
            "Epoch [1/2], Step [1960/15231], Loss: 0.4234\n",
            "Epoch [1/2], Step [1980/15231], Loss: 0.3904\n",
            "Epoch [1/2], Step [2000/15231], Loss: 0.4218\n",
            "Epoch [1/2], Step [2020/15231], Loss: 0.3835\n",
            "Epoch [1/2], Step [2040/15231], Loss: 0.3864\n",
            "Epoch [1/2], Step [2060/15231], Loss: 0.3870\n",
            "Epoch [1/2], Step [2080/15231], Loss: 0.4066\n",
            "Epoch [1/2], Step [2100/15231], Loss: 0.4165\n",
            "Epoch [1/2], Step [2120/15231], Loss: 0.3872\n",
            "Epoch [1/2], Step [2140/15231], Loss: 0.4007\n",
            "Epoch [1/2], Step [2160/15231], Loss: 0.3770\n",
            "Epoch [1/2], Step [2180/15231], Loss: 0.3586\n",
            "Epoch [1/2], Step [2200/15231], Loss: 0.4227\n",
            "Epoch [1/2], Step [2220/15231], Loss: 0.3556\n",
            "Epoch [1/2], Step [2240/15231], Loss: 0.4224\n",
            "Epoch [1/2], Step [2260/15231], Loss: 0.3778\n",
            "Epoch [1/2], Step [2280/15231], Loss: 0.3888\n",
            "Epoch [1/2], Step [2300/15231], Loss: 0.3817\n",
            "Epoch [1/2], Step [2320/15231], Loss: 0.4010\n",
            "Epoch [1/2], Step [2340/15231], Loss: 0.3994\n",
            "Epoch [1/2], Step [2360/15231], Loss: 0.3847\n",
            "Epoch [1/2], Step [2380/15231], Loss: 0.3956\n",
            "Epoch [1/2], Step [2400/15231], Loss: 0.3894\n",
            "Epoch [1/2], Step [2420/15231], Loss: 0.3850\n",
            "Epoch [1/2], Step [2440/15231], Loss: 0.4109\n",
            "Epoch [1/2], Step [2460/15231], Loss: 0.3781\n",
            "Epoch [1/2], Step [2480/15231], Loss: 0.4058\n",
            "Epoch [1/2], Step [2500/15231], Loss: 0.4002\n",
            "Epoch [1/2], Step [2520/15231], Loss: 0.4025\n",
            "Epoch [1/2], Step [2540/15231], Loss: 0.3789\n",
            "Epoch [1/2], Step [2560/15231], Loss: 0.3849\n",
            "Epoch [1/2], Step [2580/15231], Loss: 0.3856\n",
            "Epoch [1/2], Step [2600/15231], Loss: 0.3861\n",
            "Epoch [1/2], Step [2620/15231], Loss: 0.3254\n",
            "Epoch [1/2], Step [2640/15231], Loss: 0.3649\n",
            "Epoch [1/2], Step [2660/15231], Loss: 0.3716\n",
            "Epoch [1/2], Step [2680/15231], Loss: 0.3718\n",
            "Epoch [1/2], Step [2700/15231], Loss: 0.4098\n",
            "Epoch [1/2], Step [2720/15231], Loss: 0.3754\n",
            "Epoch [1/2], Step [2740/15231], Loss: 0.3597\n",
            "Epoch [1/2], Step [2760/15231], Loss: 0.3782\n",
            "Epoch [1/2], Step [2780/15231], Loss: 0.3849\n",
            "Epoch [1/2], Step [2800/15231], Loss: 0.3852\n",
            "Epoch [1/2], Step [2820/15231], Loss: 0.4012\n",
            "Epoch [1/2], Step [2840/15231], Loss: 0.3787\n",
            "Epoch [1/2], Step [2860/15231], Loss: 0.3774\n",
            "Epoch [1/2], Step [2880/15231], Loss: 0.3912\n",
            "Epoch [1/2], Step [2900/15231], Loss: 0.3835\n",
            "Epoch [1/2], Step [2920/15231], Loss: 0.3390\n",
            "Epoch [1/2], Step [2940/15231], Loss: 0.3751\n",
            "Epoch [1/2], Step [2960/15231], Loss: 0.3450\n",
            "Epoch [1/2], Step [2980/15231], Loss: 0.3552\n",
            "Epoch [1/2], Step [3000/15231], Loss: 0.3898\n",
            "Epoch [1/2], Step [3020/15231], Loss: 0.3430\n",
            "Epoch [1/2], Step [3040/15231], Loss: 0.3662\n",
            "Epoch [1/2], Step [3060/15231], Loss: 0.3817\n",
            "Epoch [1/2], Step [3080/15231], Loss: 0.3820\n",
            "Epoch [1/2], Step [3100/15231], Loss: 0.3915\n",
            "Epoch [1/2], Step [3120/15231], Loss: 0.3609\n",
            "Epoch [1/2], Step [3140/15231], Loss: 0.3162\n",
            "Epoch [1/2], Step [3160/15231], Loss: 0.3487\n",
            "Epoch [1/2], Step [3180/15231], Loss: 0.3625\n",
            "Epoch [1/2], Step [3200/15231], Loss: 0.3547\n",
            "Epoch [1/2], Step [3220/15231], Loss: 0.3483\n",
            "Epoch [1/2], Step [3240/15231], Loss: 0.3639\n",
            "Epoch [1/2], Step [3260/15231], Loss: 0.3197\n",
            "Epoch [1/2], Step [3280/15231], Loss: 0.3702\n",
            "Epoch [1/2], Step [3300/15231], Loss: 0.3745\n",
            "Epoch [1/2], Step [3320/15231], Loss: 0.3505\n",
            "Epoch [1/2], Step [3340/15231], Loss: 0.3903\n",
            "Epoch [1/2], Step [3360/15231], Loss: 0.4011\n",
            "Epoch [1/2], Step [3380/15231], Loss: 0.3573\n",
            "Epoch [1/2], Step [3400/15231], Loss: 0.3901\n",
            "Epoch [1/2], Step [3420/15231], Loss: 0.3333\n",
            "Epoch [1/2], Step [3440/15231], Loss: 0.3410\n",
            "Epoch [1/2], Step [3460/15231], Loss: 0.3280\n",
            "Epoch [1/2], Step [3480/15231], Loss: 0.3616\n",
            "Epoch [1/2], Step [3500/15231], Loss: 0.3738\n",
            "Epoch [1/2], Step [3520/15231], Loss: 0.3743\n",
            "Epoch [1/2], Step [3540/15231], Loss: 0.3801\n",
            "Epoch [1/2], Step [3560/15231], Loss: 0.3449\n",
            "Epoch [1/2], Step [3580/15231], Loss: 0.3419\n",
            "Epoch [1/2], Step [3600/15231], Loss: 0.3916\n",
            "Epoch [1/2], Step [3620/15231], Loss: 0.3688\n",
            "Epoch [1/2], Step [3640/15231], Loss: 0.3505\n",
            "Epoch [1/2], Step [3660/15231], Loss: 0.3315\n",
            "Epoch [1/2], Step [3680/15231], Loss: 0.3575\n",
            "Epoch [1/2], Step [3700/15231], Loss: 0.3286\n",
            "Epoch [1/2], Step [3720/15231], Loss: 0.3763\n",
            "Epoch [1/2], Step [3740/15231], Loss: 0.3595\n",
            "Epoch [1/2], Step [3760/15231], Loss: 0.3570\n",
            "Epoch [1/2], Step [3780/15231], Loss: 0.3944\n",
            "Epoch [1/2], Step [3800/15231], Loss: 0.3797\n",
            "Epoch [1/2], Step [3820/15231], Loss: 0.3446\n",
            "Epoch [1/2], Step [3840/15231], Loss: 0.3705\n",
            "Epoch [1/2], Step [3860/15231], Loss: 0.3603\n",
            "Epoch [1/2], Step [3880/15231], Loss: 0.3243\n",
            "Epoch [1/2], Step [3900/15231], Loss: 0.3746\n",
            "Epoch [1/2], Step [3920/15231], Loss: 0.3851\n",
            "Epoch [1/2], Step [3940/15231], Loss: 0.3784\n",
            "Epoch [1/2], Step [3960/15231], Loss: 0.3331\n",
            "Epoch [1/2], Step [3980/15231], Loss: 0.3713\n",
            "Epoch [1/2], Step [4000/15231], Loss: 0.3940\n",
            "Epoch [1/2], Step [4020/15231], Loss: 0.3519\n",
            "Epoch [1/2], Step [4040/15231], Loss: 0.3207\n",
            "Epoch [1/2], Step [4060/15231], Loss: 0.3377\n",
            "Epoch [1/2], Step [4080/15231], Loss: 0.3732\n",
            "Epoch [1/2], Step [4100/15231], Loss: 0.3817\n",
            "Epoch [1/2], Step [4120/15231], Loss: 0.3027\n",
            "Epoch [1/2], Step [4140/15231], Loss: 0.3599\n",
            "Epoch [1/2], Step [4160/15231], Loss: 0.3587\n",
            "Epoch [1/2], Step [4180/15231], Loss: 0.3880\n",
            "Epoch [1/2], Step [4200/15231], Loss: 0.3533\n",
            "Epoch [1/2], Step [4220/15231], Loss: 0.3835\n",
            "Epoch [1/2], Step [4240/15231], Loss: 0.3402\n",
            "Epoch [1/2], Step [4260/15231], Loss: 0.3650\n",
            "Epoch [1/2], Step [4280/15231], Loss: 0.3479\n",
            "Epoch [1/2], Step [4300/15231], Loss: 0.3754\n",
            "Epoch [1/2], Step [4320/15231], Loss: 0.3387\n",
            "Epoch [1/2], Step [4340/15231], Loss: 0.3858\n",
            "Epoch [1/2], Step [4360/15231], Loss: 0.3670\n",
            "Epoch [1/2], Step [4380/15231], Loss: 0.3658\n",
            "Epoch [1/2], Step [4400/15231], Loss: 0.3075\n",
            "Epoch [1/2], Step [4420/15231], Loss: 0.3769\n",
            "Epoch [1/2], Step [4440/15231], Loss: 0.3733\n",
            "Epoch [1/2], Step [4460/15231], Loss: 0.3236\n",
            "Epoch [1/2], Step [4480/15231], Loss: 0.3446\n",
            "Epoch [1/2], Step [4500/15231], Loss: 0.3396\n",
            "Epoch [1/2], Step [4520/15231], Loss: 0.3424\n",
            "Epoch [1/2], Step [4540/15231], Loss: 0.3488\n",
            "Epoch [1/2], Step [4560/15231], Loss: 0.3757\n",
            "Epoch [1/2], Step [4580/15231], Loss: 0.3404\n",
            "Epoch [1/2], Step [4600/15231], Loss: 0.3258\n",
            "Epoch [1/2], Step [4620/15231], Loss: 0.3270\n",
            "Epoch [1/2], Step [4640/15231], Loss: 0.3681\n",
            "Epoch [1/2], Step [4660/15231], Loss: 0.3558\n",
            "Epoch [1/2], Step [4680/15231], Loss: 0.3443\n",
            "Epoch [1/2], Step [4700/15231], Loss: 0.3200\n",
            "Epoch [1/2], Step [4720/15231], Loss: 0.2953\n",
            "Epoch [1/2], Step [4740/15231], Loss: 0.3087\n",
            "Epoch [1/2], Step [4760/15231], Loss: 0.3387\n",
            "Epoch [1/2], Step [4780/15231], Loss: 0.3452\n",
            "Epoch [1/2], Step [4800/15231], Loss: 0.3622\n",
            "Epoch [1/2], Step [4820/15231], Loss: 0.3864\n",
            "Epoch [1/2], Step [4840/15231], Loss: 0.3681\n",
            "Epoch [1/2], Step [4860/15231], Loss: 0.3207\n",
            "Epoch [1/2], Step [4880/15231], Loss: 0.3614\n",
            "Epoch [1/2], Step [4900/15231], Loss: 0.3783\n",
            "Epoch [1/2], Step [4920/15231], Loss: 0.3367\n",
            "Epoch [1/2], Step [4940/15231], Loss: 0.3329\n",
            "Epoch [1/2], Step [4960/15231], Loss: 0.3722\n",
            "Epoch [1/2], Step [4980/15231], Loss: 0.3205\n",
            "Epoch [1/2], Step [5000/15231], Loss: 0.3261\n",
            "Epoch [1/2], Step [5020/15231], Loss: 0.2981\n",
            "Epoch [1/2], Step [5040/15231], Loss: 0.3242\n",
            "Epoch [1/2], Step [5060/15231], Loss: 0.3398\n",
            "Epoch [1/2], Step [5080/15231], Loss: 0.3210\n",
            "Epoch [1/2], Step [5100/15231], Loss: 0.3383\n",
            "Epoch [1/2], Step [5120/15231], Loss: 0.3353\n",
            "Epoch [1/2], Step [5140/15231], Loss: 0.3489\n",
            "Epoch [1/2], Step [5160/15231], Loss: 0.3285\n",
            "Epoch [1/2], Step [5180/15231], Loss: 0.3151\n",
            "Epoch [1/2], Step [5200/15231], Loss: 0.3627\n",
            "Epoch [1/2], Step [5220/15231], Loss: 0.3014\n",
            "Epoch [1/2], Step [5240/15231], Loss: 0.3123\n",
            "Epoch [1/2], Step [5260/15231], Loss: 0.3149\n",
            "Epoch [1/2], Step [5280/15231], Loss: 0.3008\n",
            "Epoch [1/2], Step [5300/15231], Loss: 0.2979\n",
            "Epoch [1/2], Step [5320/15231], Loss: 0.3532\n",
            "Epoch [1/2], Step [5340/15231], Loss: 0.3646\n",
            "Epoch [1/2], Step [5360/15231], Loss: 0.2875\n",
            "Epoch [1/2], Step [5380/15231], Loss: 0.2921\n",
            "Epoch [1/2], Step [5400/15231], Loss: 0.2805\n",
            "Epoch [1/2], Step [5420/15231], Loss: 0.3914\n",
            "Epoch [1/2], Step [5440/15231], Loss: 0.3466\n",
            "Epoch [1/2], Step [5460/15231], Loss: 0.3336\n",
            "Epoch [1/2], Step [5480/15231], Loss: 0.3726\n",
            "Epoch [1/2], Step [5500/15231], Loss: 0.3333\n",
            "Epoch [1/2], Step [5520/15231], Loss: 0.3512\n",
            "Epoch [1/2], Step [5540/15231], Loss: 0.3490\n",
            "Epoch [1/2], Step [5560/15231], Loss: 0.3460\n",
            "Epoch [1/2], Step [5580/15231], Loss: 0.3378\n",
            "Epoch [1/2], Step [5600/15231], Loss: 0.3263\n",
            "Epoch [1/2], Step [5620/15231], Loss: 0.3458\n",
            "Epoch [1/2], Step [5640/15231], Loss: 0.3352\n",
            "Epoch [1/2], Step [5660/15231], Loss: 0.3016\n",
            "Epoch [1/2], Step [5680/15231], Loss: 0.3319\n",
            "Epoch [1/2], Step [5700/15231], Loss: 0.3395\n",
            "Epoch [1/2], Step [5720/15231], Loss: 0.3142\n",
            "Epoch [1/2], Step [5740/15231], Loss: 0.3228\n",
            "Epoch [1/2], Step [5760/15231], Loss: 0.3357\n",
            "Epoch [1/2], Step [5780/15231], Loss: 0.3246\n",
            "Epoch [1/2], Step [5800/15231], Loss: 0.3146\n",
            "Epoch [1/2], Step [5820/15231], Loss: 0.3574\n",
            "Epoch [1/2], Step [5840/15231], Loss: 0.2985\n",
            "Epoch [1/2], Step [5860/15231], Loss: 0.3268\n",
            "Epoch [1/2], Step [5880/15231], Loss: 0.3196\n",
            "Epoch [1/2], Step [5900/15231], Loss: 0.2984\n",
            "Epoch [1/2], Step [5920/15231], Loss: 0.3285\n",
            "Epoch [1/2], Step [5940/15231], Loss: 0.3573\n",
            "Epoch [1/2], Step [5960/15231], Loss: 0.3046\n",
            "Epoch [1/2], Step [5980/15231], Loss: 0.3469\n",
            "Epoch [1/2], Step [6000/15231], Loss: 0.2750\n",
            "Epoch [1/2], Step [6020/15231], Loss: 0.3321\n",
            "Epoch [1/2], Step [6040/15231], Loss: 0.3356\n",
            "Epoch [1/2], Step [6060/15231], Loss: 0.3557\n",
            "Epoch [1/2], Step [6080/15231], Loss: 0.3508\n",
            "Epoch [1/2], Step [6100/15231], Loss: 0.2944\n",
            "Epoch [1/2], Step [6120/15231], Loss: 0.3376\n",
            "Epoch [1/2], Step [6140/15231], Loss: 0.3202\n",
            "Epoch [1/2], Step [6160/15231], Loss: 0.3110\n",
            "Epoch [1/2], Step [6180/15231], Loss: 0.3182\n",
            "Epoch [1/2], Step [6200/15231], Loss: 0.3371\n",
            "Epoch [1/2], Step [6220/15231], Loss: 0.3129\n",
            "Epoch [1/2], Step [6240/15231], Loss: 0.2783\n",
            "Epoch [1/2], Step [6260/15231], Loss: 0.2811\n",
            "Epoch [1/2], Step [6280/15231], Loss: 0.3326\n",
            "Epoch [1/2], Step [6300/15231], Loss: 0.3353\n",
            "Epoch [1/2], Step [6320/15231], Loss: 0.3004\n",
            "Epoch [1/2], Step [6340/15231], Loss: 0.3330\n",
            "Epoch [1/2], Step [6360/15231], Loss: 0.3121\n",
            "Epoch [1/2], Step [6380/15231], Loss: 0.2925\n",
            "Epoch [1/2], Step [6400/15231], Loss: 0.3084\n",
            "Epoch [1/2], Step [6420/15231], Loss: 0.2947\n",
            "Epoch [1/2], Step [6440/15231], Loss: 0.2959\n",
            "Epoch [1/2], Step [6460/15231], Loss: 0.3034\n",
            "Epoch [1/2], Step [6480/15231], Loss: 0.3290\n",
            "Epoch [1/2], Step [6500/15231], Loss: 0.3331\n",
            "Epoch [1/2], Step [6520/15231], Loss: 0.3277\n",
            "Epoch [1/2], Step [6540/15231], Loss: 0.3036\n",
            "Epoch [1/2], Step [6560/15231], Loss: 0.3740\n",
            "Epoch [1/2], Step [6580/15231], Loss: 0.3074\n",
            "Epoch [1/2], Step [6600/15231], Loss: 0.3007\n",
            "Epoch [1/2], Step [6620/15231], Loss: 0.3494\n",
            "Epoch [1/2], Step [6640/15231], Loss: 0.3292\n",
            "Epoch [1/2], Step [6660/15231], Loss: 0.3054\n",
            "Epoch [1/2], Step [6680/15231], Loss: 0.3593\n",
            "Epoch [1/2], Step [6700/15231], Loss: 0.3466\n",
            "Epoch [1/2], Step [6720/15231], Loss: 0.3000\n",
            "Epoch [1/2], Step [6740/15231], Loss: 0.3033\n",
            "Epoch [1/2], Step [6760/15231], Loss: 0.2989\n",
            "Epoch [1/2], Step [6780/15231], Loss: 0.2911\n",
            "Epoch [1/2], Step [6800/15231], Loss: 0.3466\n",
            "Epoch [1/2], Step [6820/15231], Loss: 0.3149\n",
            "Epoch [1/2], Step [6840/15231], Loss: 0.3196\n",
            "Epoch [1/2], Step [6860/15231], Loss: 0.3307\n",
            "Epoch [1/2], Step [6880/15231], Loss: 0.3134\n",
            "Epoch [1/2], Step [6900/15231], Loss: 0.3376\n",
            "Epoch [1/2], Step [6920/15231], Loss: 0.3230\n",
            "Epoch [1/2], Step [6940/15231], Loss: 0.3169\n",
            "Epoch [1/2], Step [6960/15231], Loss: 0.3784\n",
            "Epoch [1/2], Step [6980/15231], Loss: 0.3221\n",
            "Epoch [1/2], Step [7000/15231], Loss: 0.3302\n",
            "Epoch [1/2], Step [7020/15231], Loss: 0.2966\n",
            "Epoch [1/2], Step [7040/15231], Loss: 0.3119\n",
            "Epoch [1/2], Step [7060/15231], Loss: 0.2922\n",
            "Epoch [1/2], Step [7080/15231], Loss: 0.2950\n",
            "Epoch [1/2], Step [7100/15231], Loss: 0.2867\n",
            "Epoch [1/2], Step [7120/15231], Loss: 0.2730\n",
            "Epoch [1/2], Step [7140/15231], Loss: 0.2993\n",
            "Epoch [1/2], Step [7160/15231], Loss: 0.3109\n",
            "Epoch [1/2], Step [7180/15231], Loss: 0.3148\n",
            "Epoch [1/2], Step [7200/15231], Loss: 0.3350\n",
            "Epoch [1/2], Step [7220/15231], Loss: 0.3351\n",
            "Epoch [1/2], Step [7240/15231], Loss: 0.2831\n",
            "Epoch [1/2], Step [7260/15231], Loss: 0.3136\n",
            "Epoch [1/2], Step [7280/15231], Loss: 0.3386\n",
            "Epoch [1/2], Step [7300/15231], Loss: 0.2871\n",
            "Epoch [1/2], Step [7320/15231], Loss: 0.3350\n",
            "Epoch [1/2], Step [7340/15231], Loss: 0.3268\n",
            "Epoch [1/2], Step [7360/15231], Loss: 0.3148\n",
            "Epoch [1/2], Step [7380/15231], Loss: 0.3299\n",
            "Epoch [1/2], Step [7400/15231], Loss: 0.3083\n",
            "Epoch [1/2], Step [7420/15231], Loss: 0.2889\n",
            "Epoch [1/2], Step [7440/15231], Loss: 0.3067\n",
            "Epoch [1/2], Step [7460/15231], Loss: 0.3120\n",
            "Epoch [1/2], Step [7480/15231], Loss: 0.3254\n",
            "Epoch [1/2], Step [7500/15231], Loss: 0.3217\n",
            "Epoch [1/2], Step [7520/15231], Loss: 0.3574\n",
            "Epoch [1/2], Step [7540/15231], Loss: 0.3057\n",
            "Epoch [1/2], Step [7560/15231], Loss: 0.3108\n",
            "Epoch [1/2], Step [7580/15231], Loss: 0.2764\n",
            "Epoch [1/2], Step [7600/15231], Loss: 0.3195\n",
            "Epoch [1/2], Step [7620/15231], Loss: 0.3244\n",
            "Epoch [1/2], Step [7640/15231], Loss: 0.3049\n",
            "Epoch [1/2], Step [7660/15231], Loss: 0.2952\n",
            "Epoch [1/2], Step [7680/15231], Loss: 0.2586\n",
            "Epoch [1/2], Step [7700/15231], Loss: 0.3505\n",
            "Epoch [1/2], Step [7720/15231], Loss: 0.2696\n",
            "Epoch [1/2], Step [7740/15231], Loss: 0.3127\n",
            "Epoch [1/2], Step [7760/15231], Loss: 0.3260\n",
            "Epoch [1/2], Step [7780/15231], Loss: 0.2975\n",
            "Epoch [1/2], Step [7800/15231], Loss: 0.2822\n",
            "Epoch [1/2], Step [7820/15231], Loss: 0.3261\n",
            "Epoch [1/2], Step [7840/15231], Loss: 0.3484\n",
            "Epoch [1/2], Step [7860/15231], Loss: 0.3255\n",
            "Epoch [1/2], Step [7880/15231], Loss: 0.3044\n",
            "Epoch [1/2], Step [7900/15231], Loss: 0.3404\n",
            "Epoch [1/2], Step [7920/15231], Loss: 0.3059\n",
            "Epoch [1/2], Step [7940/15231], Loss: 0.3053\n",
            "Epoch [1/2], Step [7960/15231], Loss: 0.3335\n",
            "Epoch [1/2], Step [7980/15231], Loss: 0.3100\n",
            "Epoch [1/2], Step [8000/15231], Loss: 0.3011\n",
            "Epoch [1/2], Step [8020/15231], Loss: 0.3406\n",
            "Epoch [1/2], Step [8040/15231], Loss: 0.3164\n",
            "Epoch [1/2], Step [8060/15231], Loss: 0.2829\n",
            "Epoch [1/2], Step [8080/15231], Loss: 0.2551\n",
            "Epoch [1/2], Step [8100/15231], Loss: 0.3032\n",
            "Epoch [1/2], Step [8120/15231], Loss: 0.3104\n",
            "Epoch [1/2], Step [8140/15231], Loss: 0.3380\n",
            "Epoch [1/2], Step [8160/15231], Loss: 0.3270\n",
            "Epoch [1/2], Step [8180/15231], Loss: 0.3385\n",
            "Epoch [1/2], Step [8200/15231], Loss: 0.3137\n",
            "Epoch [1/2], Step [8220/15231], Loss: 0.3058\n",
            "Epoch [1/2], Step [8240/15231], Loss: 0.2968\n",
            "Epoch [1/2], Step [8260/15231], Loss: 0.2857\n",
            "Epoch [1/2], Step [8280/15231], Loss: 0.2949\n",
            "Epoch [1/2], Step [8300/15231], Loss: 0.2913\n",
            "Epoch [1/2], Step [8320/15231], Loss: 0.3241\n",
            "Epoch [1/2], Step [8340/15231], Loss: 0.3510\n",
            "Epoch [1/2], Step [8360/15231], Loss: 0.3411\n",
            "Epoch [1/2], Step [8380/15231], Loss: 0.2871\n",
            "Epoch [1/2], Step [8400/15231], Loss: 0.2702\n",
            "Epoch [1/2], Step [8420/15231], Loss: 0.3070\n",
            "Epoch [1/2], Step [8440/15231], Loss: 0.3313\n",
            "Epoch [1/2], Step [8460/15231], Loss: 0.3182\n",
            "Epoch [1/2], Step [8480/15231], Loss: 0.3334\n",
            "Epoch [1/2], Step [8500/15231], Loss: 0.2850\n",
            "Epoch [1/2], Step [8520/15231], Loss: 0.2873\n",
            "Epoch [1/2], Step [8540/15231], Loss: 0.3041\n",
            "Epoch [1/2], Step [8560/15231], Loss: 0.2902\n",
            "Epoch [1/2], Step [8580/15231], Loss: 0.3136\n",
            "Epoch [1/2], Step [8600/15231], Loss: 0.3209\n",
            "Epoch [1/2], Step [8620/15231], Loss: 0.2851\n",
            "Epoch [1/2], Step [8640/15231], Loss: 0.2713\n",
            "Epoch [1/2], Step [8660/15231], Loss: 0.2380\n",
            "Epoch [1/2], Step [8680/15231], Loss: 0.2894\n",
            "Epoch [1/2], Step [8700/15231], Loss: 0.2766\n",
            "Epoch [1/2], Step [8720/15231], Loss: 0.2959\n",
            "Epoch [1/2], Step [8740/15231], Loss: 0.2884\n",
            "Epoch [1/2], Step [8760/15231], Loss: 0.3116\n",
            "Epoch [1/2], Step [8780/15231], Loss: 0.2859\n",
            "Epoch [1/2], Step [8800/15231], Loss: 0.3314\n",
            "Epoch [1/2], Step [8820/15231], Loss: 0.2943\n",
            "Epoch [1/2], Step [8840/15231], Loss: 0.2892\n",
            "Epoch [1/2], Step [8860/15231], Loss: 0.3324\n",
            "Epoch [1/2], Step [8880/15231], Loss: 0.2960\n",
            "Epoch [1/2], Step [8900/15231], Loss: 0.3195\n",
            "Epoch [1/2], Step [8920/15231], Loss: 0.2903\n",
            "Epoch [1/2], Step [8940/15231], Loss: 0.3322\n",
            "Epoch [1/2], Step [8960/15231], Loss: 0.2887\n",
            "Epoch [1/2], Step [8980/15231], Loss: 0.2999\n",
            "Epoch [1/2], Step [9000/15231], Loss: 0.3009\n",
            "Epoch [1/2], Step [9020/15231], Loss: 0.2967\n",
            "Epoch [1/2], Step [9040/15231], Loss: 0.2524\n",
            "Epoch [1/2], Step [9060/15231], Loss: 0.2664\n",
            "Epoch [1/2], Step [9080/15231], Loss: 0.3022\n",
            "Epoch [1/2], Step [9100/15231], Loss: 0.2613\n",
            "Epoch [1/2], Step [9120/15231], Loss: 0.3186\n",
            "Epoch [1/2], Step [9140/15231], Loss: 0.2963\n",
            "Epoch [1/2], Step [9160/15231], Loss: 0.2797\n",
            "Epoch [1/2], Step [9180/15231], Loss: 0.3183\n",
            "Epoch [1/2], Step [9200/15231], Loss: 0.2865\n",
            "Epoch [1/2], Step [9220/15231], Loss: 0.2667\n",
            "Epoch [1/2], Step [9240/15231], Loss: 0.2976\n",
            "Epoch [1/2], Step [9260/15231], Loss: 0.3130\n",
            "Epoch [1/2], Step [9280/15231], Loss: 0.2705\n",
            "Epoch [1/2], Step [9300/15231], Loss: 0.3383\n",
            "Epoch [1/2], Step [9320/15231], Loss: 0.2751\n",
            "Epoch [1/2], Step [9340/15231], Loss: 0.3078\n",
            "Epoch [1/2], Step [9360/15231], Loss: 0.2768\n",
            "Epoch [1/2], Step [9380/15231], Loss: 0.3088\n",
            "Epoch [1/2], Step [9400/15231], Loss: 0.2830\n",
            "Epoch [1/2], Step [9420/15231], Loss: 0.3281\n",
            "Epoch [1/2], Step [9440/15231], Loss: 0.2973\n",
            "Epoch [1/2], Step [9460/15231], Loss: 0.2789\n",
            "Epoch [1/2], Step [9480/15231], Loss: 0.3039\n",
            "Epoch [1/2], Step [9500/15231], Loss: 0.3140\n",
            "Epoch [1/2], Step [9520/15231], Loss: 0.3014\n",
            "Epoch [1/2], Step [9540/15231], Loss: 0.2981\n",
            "Epoch [1/2], Step [9560/15231], Loss: 0.2589\n",
            "Epoch [1/2], Step [9580/15231], Loss: 0.3095\n",
            "Epoch [1/2], Step [9600/15231], Loss: 0.3077\n",
            "Epoch [1/2], Step [9620/15231], Loss: 0.2972\n",
            "Epoch [1/2], Step [9640/15231], Loss: 0.3001\n",
            "Epoch [1/2], Step [9660/15231], Loss: 0.3253\n",
            "Epoch [1/2], Step [9680/15231], Loss: 0.2982\n",
            "Epoch [1/2], Step [9700/15231], Loss: 0.3219\n",
            "Epoch [1/2], Step [9720/15231], Loss: 0.3321\n",
            "Epoch [1/2], Step [9740/15231], Loss: 0.2900\n",
            "Epoch [1/2], Step [9760/15231], Loss: 0.2863\n",
            "Epoch [1/2], Step [9780/15231], Loss: 0.2951\n",
            "Epoch [1/2], Step [9800/15231], Loss: 0.2738\n",
            "Epoch [1/2], Step [9820/15231], Loss: 0.2693\n",
            "Epoch [1/2], Step [9840/15231], Loss: 0.2882\n",
            "Epoch [1/2], Step [9860/15231], Loss: 0.2986\n",
            "Epoch [1/2], Step [9880/15231], Loss: 0.3142\n",
            "Epoch [1/2], Step [9900/15231], Loss: 0.2975\n",
            "Epoch [1/2], Step [9920/15231], Loss: 0.2952\n",
            "Epoch [1/2], Step [9940/15231], Loss: 0.2914\n",
            "Epoch [1/2], Step [9960/15231], Loss: 0.2832\n",
            "Epoch [1/2], Step [9980/15231], Loss: 0.3035\n",
            "Epoch [1/2], Step [10000/15231], Loss: 0.2580\n",
            "Epoch [1/2], Step [10020/15231], Loss: 0.2821\n",
            "Epoch [1/2], Step [10040/15231], Loss: 0.3064\n",
            "Epoch [1/2], Step [10060/15231], Loss: 0.2926\n",
            "Epoch [1/2], Step [10080/15231], Loss: 0.3076\n",
            "Epoch [1/2], Step [10100/15231], Loss: 0.3050\n",
            "Epoch [1/2], Step [10120/15231], Loss: 0.2778\n",
            "Epoch [1/2], Step [10140/15231], Loss: 0.2688\n",
            "Epoch [1/2], Step [10160/15231], Loss: 0.3379\n",
            "Epoch [1/2], Step [10180/15231], Loss: 0.3185\n",
            "Epoch [1/2], Step [10200/15231], Loss: 0.3103\n",
            "Epoch [1/2], Step [10220/15231], Loss: 0.2780\n",
            "Epoch [1/2], Step [10240/15231], Loss: 0.2903\n",
            "Epoch [1/2], Step [10260/15231], Loss: 0.2917\n",
            "Epoch [1/2], Step [10280/15231], Loss: 0.2785\n",
            "Epoch [1/2], Step [10300/15231], Loss: 0.2722\n",
            "Epoch [1/2], Step [10320/15231], Loss: 0.2815\n",
            "Epoch [1/2], Step [10340/15231], Loss: 0.3295\n",
            "Epoch [1/2], Step [10360/15231], Loss: 0.3130\n",
            "Epoch [1/2], Step [10380/15231], Loss: 0.2842\n",
            "Epoch [1/2], Step [10400/15231], Loss: 0.2692\n",
            "Epoch [1/2], Step [10420/15231], Loss: 0.3098\n",
            "Epoch [1/2], Step [10440/15231], Loss: 0.2835\n",
            "Epoch [1/2], Step [10460/15231], Loss: 0.2804\n",
            "Epoch [1/2], Step [10480/15231], Loss: 0.2720\n",
            "Epoch [1/2], Step [10500/15231], Loss: 0.3178\n",
            "Epoch [1/2], Step [10520/15231], Loss: 0.3062\n",
            "Epoch [1/2], Step [10540/15231], Loss: 0.2974\n",
            "Epoch [1/2], Step [10560/15231], Loss: 0.2964\n",
            "Epoch [1/2], Step [10580/15231], Loss: 0.2839\n",
            "Epoch [1/2], Step [10600/15231], Loss: 0.2836\n",
            "Epoch [1/2], Step [10620/15231], Loss: 0.2580\n",
            "Epoch [1/2], Step [10640/15231], Loss: 0.2996\n",
            "Epoch [1/2], Step [10660/15231], Loss: 0.2950\n",
            "Epoch [1/2], Step [10680/15231], Loss: 0.2976\n",
            "Epoch [1/2], Step [10700/15231], Loss: 0.3104\n",
            "Epoch [1/2], Step [10720/15231], Loss: 0.3233\n",
            "Epoch [1/2], Step [10740/15231], Loss: 0.3076\n",
            "Epoch [1/2], Step [10760/15231], Loss: 0.2818\n",
            "Epoch [1/2], Step [10780/15231], Loss: 0.3143\n",
            "Epoch [1/2], Step [10800/15231], Loss: 0.2772\n",
            "Epoch [1/2], Step [10820/15231], Loss: 0.2994\n",
            "Epoch [1/2], Step [10840/15231], Loss: 0.2706\n",
            "Epoch [1/2], Step [10860/15231], Loss: 0.2907\n",
            "Epoch [1/2], Step [10880/15231], Loss: 0.3013\n",
            "Epoch [1/2], Step [10900/15231], Loss: 0.2758\n",
            "Epoch [1/2], Step [10920/15231], Loss: 0.2777\n",
            "Epoch [1/2], Step [10940/15231], Loss: 0.2966\n",
            "Epoch [1/2], Step [10960/15231], Loss: 0.2423\n",
            "Epoch [1/2], Step [10980/15231], Loss: 0.2949\n",
            "Epoch [1/2], Step [11000/15231], Loss: 0.2618\n",
            "Epoch [1/2], Step [11020/15231], Loss: 0.2951\n",
            "Epoch [1/2], Step [11040/15231], Loss: 0.3100\n",
            "Epoch [1/2], Step [11060/15231], Loss: 0.2739\n",
            "Epoch [1/2], Step [11080/15231], Loss: 0.3321\n",
            "Epoch [1/2], Step [11100/15231], Loss: 0.3147\n",
            "Epoch [1/2], Step [11120/15231], Loss: 0.3013\n",
            "Epoch [1/2], Step [11140/15231], Loss: 0.2971\n",
            "Epoch [1/2], Step [11160/15231], Loss: 0.2846\n",
            "Epoch [1/2], Step [11180/15231], Loss: 0.2889\n",
            "Epoch [1/2], Step [11200/15231], Loss: 0.2600\n",
            "Epoch [1/2], Step [11220/15231], Loss: 0.2926\n",
            "Epoch [1/2], Step [11240/15231], Loss: 0.3147\n",
            "Epoch [1/2], Step [11260/15231], Loss: 0.2717\n",
            "Epoch [1/2], Step [11280/15231], Loss: 0.2744\n",
            "Epoch [1/2], Step [11300/15231], Loss: 0.2759\n",
            "Epoch [1/2], Step [11320/15231], Loss: 0.2732\n",
            "Epoch [1/2], Step [11340/15231], Loss: 0.2542\n",
            "Epoch [1/2], Step [11360/15231], Loss: 0.2774\n",
            "Epoch [1/2], Step [11380/15231], Loss: 0.2869\n",
            "Epoch [1/2], Step [11400/15231], Loss: 0.3118\n",
            "Epoch [1/2], Step [11420/15231], Loss: 0.2873\n",
            "Epoch [1/2], Step [11440/15231], Loss: 0.2859\n",
            "Epoch [1/2], Step [11460/15231], Loss: 0.3001\n",
            "Epoch [1/2], Step [11480/15231], Loss: 0.2895\n",
            "Epoch [1/2], Step [11500/15231], Loss: 0.2784\n",
            "Epoch [1/2], Step [11520/15231], Loss: 0.2671\n",
            "Epoch [1/2], Step [11540/15231], Loss: 0.3221\n",
            "Epoch [1/2], Step [11560/15231], Loss: 0.3008\n",
            "Epoch [1/2], Step [11580/15231], Loss: 0.2937\n",
            "Epoch [1/2], Step [11600/15231], Loss: 0.2837\n",
            "Epoch [1/2], Step [11620/15231], Loss: 0.2889\n",
            "Epoch [1/2], Step [11640/15231], Loss: 0.2507\n",
            "Epoch [1/2], Step [11660/15231], Loss: 0.2967\n",
            "Epoch [1/2], Step [11680/15231], Loss: 0.2553\n",
            "Epoch [1/2], Step [11700/15231], Loss: 0.2717\n",
            "Epoch [1/2], Step [11720/15231], Loss: 0.2907\n",
            "Epoch [1/2], Step [11740/15231], Loss: 0.2943\n",
            "Epoch [1/2], Step [11760/15231], Loss: 0.3026\n",
            "Epoch [1/2], Step [11780/15231], Loss: 0.2852\n",
            "Epoch [1/2], Step [11800/15231], Loss: 0.2919\n",
            "Epoch [1/2], Step [11820/15231], Loss: 0.3016\n",
            "Epoch [1/2], Step [11840/15231], Loss: 0.3050\n",
            "Epoch [1/2], Step [11860/15231], Loss: 0.2796\n",
            "Epoch [1/2], Step [11880/15231], Loss: 0.2814\n",
            "Epoch [1/2], Step [11900/15231], Loss: 0.2877\n",
            "Epoch [1/2], Step [11920/15231], Loss: 0.2998\n",
            "Epoch [1/2], Step [11940/15231], Loss: 0.2970\n",
            "Epoch [1/2], Step [11960/15231], Loss: 0.2673\n",
            "Epoch [1/2], Step [11980/15231], Loss: 0.2717\n",
            "Epoch [1/2], Step [12000/15231], Loss: 0.2717\n",
            "Epoch [1/2], Step [12020/15231], Loss: 0.2636\n",
            "Epoch [1/2], Step [12040/15231], Loss: 0.2956\n",
            "Epoch [1/2], Step [12060/15231], Loss: 0.2577\n",
            "Epoch [1/2], Step [12080/15231], Loss: 0.2999\n",
            "Epoch [1/2], Step [12100/15231], Loss: 0.2660\n",
            "Epoch [1/2], Step [12120/15231], Loss: 0.2904\n",
            "Epoch [1/2], Step [12140/15231], Loss: 0.3358\n",
            "Epoch [1/2], Step [12160/15231], Loss: 0.2726\n",
            "Epoch [1/2], Step [12180/15231], Loss: 0.2808\n",
            "Epoch [1/2], Step [12200/15231], Loss: 0.2790\n",
            "Epoch [1/2], Step [12220/15231], Loss: 0.2641\n",
            "Epoch [1/2], Step [12240/15231], Loss: 0.2699\n",
            "Epoch [1/2], Step [12260/15231], Loss: 0.2978\n",
            "Epoch [1/2], Step [12280/15231], Loss: 0.2992\n",
            "Epoch [1/2], Step [12300/15231], Loss: 0.2869\n",
            "Epoch [1/2], Step [12320/15231], Loss: 0.2455\n",
            "Epoch [1/2], Step [12340/15231], Loss: 0.2736\n",
            "Epoch [1/2], Step [12360/15231], Loss: 0.3102\n",
            "Epoch [1/2], Step [12380/15231], Loss: 0.2385\n",
            "Epoch [1/2], Step [12400/15231], Loss: 0.2774\n",
            "Epoch [1/2], Step [12420/15231], Loss: 0.2940\n",
            "Epoch [1/2], Step [12440/15231], Loss: 0.3279\n",
            "Epoch [1/2], Step [12460/15231], Loss: 0.3079\n",
            "Epoch [1/2], Step [12480/15231], Loss: 0.2547\n",
            "Epoch [1/2], Step [12500/15231], Loss: 0.2670\n",
            "Epoch [1/2], Step [12520/15231], Loss: 0.3157\n",
            "Epoch [1/2], Step [12540/15231], Loss: 0.2873\n",
            "Epoch [1/2], Step [12560/15231], Loss: 0.2759\n",
            "Epoch [1/2], Step [12580/15231], Loss: 0.2988\n",
            "Epoch [1/2], Step [12600/15231], Loss: 0.2738\n",
            "Epoch [1/2], Step [12620/15231], Loss: 0.2468\n",
            "Epoch [1/2], Step [12640/15231], Loss: 0.2546\n",
            "Epoch [1/2], Step [12660/15231], Loss: 0.2878\n",
            "Epoch [1/2], Step [12680/15231], Loss: 0.2665\n",
            "Epoch [1/2], Step [12700/15231], Loss: 0.2654\n",
            "Epoch [1/2], Step [12720/15231], Loss: 0.2986\n",
            "Epoch [1/2], Step [12740/15231], Loss: 0.2777\n",
            "Epoch [1/2], Step [12760/15231], Loss: 0.2661\n",
            "Epoch [1/2], Step [12780/15231], Loss: 0.2811\n",
            "Epoch [1/2], Step [12800/15231], Loss: 0.2744\n",
            "Epoch [1/2], Step [12820/15231], Loss: 0.2467\n",
            "Epoch [1/2], Step [12840/15231], Loss: 0.2685\n",
            "Epoch [1/2], Step [12860/15231], Loss: 0.3031\n",
            "Epoch [1/2], Step [12880/15231], Loss: 0.3004\n",
            "Epoch [1/2], Step [12900/15231], Loss: 0.2936\n",
            "Epoch [1/2], Step [12920/15231], Loss: 0.2474\n",
            "Epoch [1/2], Step [12940/15231], Loss: 0.2815\n",
            "Epoch [1/2], Step [12960/15231], Loss: 0.2738\n",
            "Epoch [1/2], Step [12980/15231], Loss: 0.2845\n",
            "Epoch [1/2], Step [13000/15231], Loss: 0.3067\n",
            "Epoch [1/2], Step [13020/15231], Loss: 0.2596\n",
            "Epoch [1/2], Step [13040/15231], Loss: 0.2572\n",
            "Epoch [1/2], Step [13060/15231], Loss: 0.3238\n",
            "Epoch [1/2], Step [13080/15231], Loss: 0.2539\n",
            "Epoch [1/2], Step [13100/15231], Loss: 0.2888\n",
            "Epoch [1/2], Step [13120/15231], Loss: 0.2745\n",
            "Epoch [1/2], Step [13140/15231], Loss: 0.2819\n",
            "Epoch [1/2], Step [13160/15231], Loss: 0.3029\n",
            "Epoch [1/2], Step [13180/15231], Loss: 0.2602\n",
            "Epoch [1/2], Step [13200/15231], Loss: 0.2857\n",
            "Epoch [1/2], Step [13220/15231], Loss: 0.2911\n",
            "Epoch [1/2], Step [13240/15231], Loss: 0.2627\n",
            "Epoch [1/2], Step [13260/15231], Loss: 0.2840\n",
            "Epoch [1/2], Step [13280/15231], Loss: 0.2715\n",
            "Epoch [1/2], Step [13300/15231], Loss: 0.2718\n",
            "Epoch [1/2], Step [13320/15231], Loss: 0.2859\n",
            "Epoch [1/2], Step [13340/15231], Loss: 0.2519\n",
            "Epoch [1/2], Step [13360/15231], Loss: 0.2801\n",
            "Epoch [1/2], Step [13380/15231], Loss: 0.2423\n",
            "Epoch [1/2], Step [13400/15231], Loss: 0.2545\n",
            "Epoch [1/2], Step [13420/15231], Loss: 0.2471\n",
            "Epoch [1/2], Step [13440/15231], Loss: 0.2798\n",
            "Epoch [1/2], Step [13460/15231], Loss: 0.2845\n",
            "Epoch [1/2], Step [13480/15231], Loss: 0.2876\n",
            "Epoch [1/2], Step [13500/15231], Loss: 0.2535\n",
            "Epoch [1/2], Step [13520/15231], Loss: 0.2651\n",
            "Epoch [1/2], Step [13540/15231], Loss: 0.3066\n",
            "Epoch [1/2], Step [13560/15231], Loss: 0.2810\n",
            "Epoch [1/2], Step [13580/15231], Loss: 0.2631\n",
            "Epoch [1/2], Step [13600/15231], Loss: 0.3365\n",
            "Epoch [1/2], Step [13620/15231], Loss: 0.2671\n",
            "Epoch [1/2], Step [13640/15231], Loss: 0.2990\n",
            "Epoch [1/2], Step [13660/15231], Loss: 0.2639\n",
            "Epoch [1/2], Step [13680/15231], Loss: 0.2839\n",
            "Epoch [1/2], Step [13700/15231], Loss: 0.3054\n",
            "Epoch [1/2], Step [13720/15231], Loss: 0.2778\n",
            "Epoch [1/2], Step [13740/15231], Loss: 0.2763\n",
            "Epoch [1/2], Step [13760/15231], Loss: 0.2625\n",
            "Epoch [1/2], Step [13780/15231], Loss: 0.2538\n",
            "Epoch [1/2], Step [13800/15231], Loss: 0.2614\n",
            "Epoch [1/2], Step [13820/15231], Loss: 0.2829\n",
            "Epoch [1/2], Step [13840/15231], Loss: 0.2684\n",
            "Epoch [1/2], Step [13860/15231], Loss: 0.2713\n",
            "Epoch [1/2], Step [13880/15231], Loss: 0.2813\n",
            "Epoch [1/2], Step [13900/15231], Loss: 0.2951\n",
            "Epoch [1/2], Step [13920/15231], Loss: 0.3266\n",
            "Epoch [1/2], Step [13940/15231], Loss: 0.2728\n",
            "Epoch [1/2], Step [13960/15231], Loss: 0.2878\n",
            "Epoch [1/2], Step [13980/15231], Loss: 0.2330\n",
            "Epoch [1/2], Step [14000/15231], Loss: 0.2795\n",
            "Epoch [1/2], Step [14020/15231], Loss: 0.2680\n",
            "Epoch [1/2], Step [14040/15231], Loss: 0.3030\n",
            "Epoch [1/2], Step [14060/15231], Loss: 0.2640\n",
            "Epoch [1/2], Step [14080/15231], Loss: 0.3030\n",
            "Epoch [1/2], Step [14100/15231], Loss: 0.2941\n",
            "Epoch [1/2], Step [14120/15231], Loss: 0.2434\n",
            "Epoch [1/2], Step [14140/15231], Loss: 0.2644\n",
            "Epoch [1/2], Step [14160/15231], Loss: 0.2459\n",
            "Epoch [1/2], Step [14180/15231], Loss: 0.2717\n",
            "Epoch [1/2], Step [14200/15231], Loss: 0.2550\n",
            "Epoch [1/2], Step [14220/15231], Loss: 0.2987\n",
            "Epoch [1/2], Step [14240/15231], Loss: 0.2807\n",
            "Epoch [1/2], Step [14260/15231], Loss: 0.2699\n",
            "Epoch [1/2], Step [14280/15231], Loss: 0.2570\n",
            "Epoch [1/2], Step [14300/15231], Loss: 0.2935\n",
            "Epoch [1/2], Step [14320/15231], Loss: 0.2935\n",
            "Epoch [1/2], Step [14340/15231], Loss: 0.3179\n",
            "Epoch [1/2], Step [14360/15231], Loss: 0.2972\n",
            "Epoch [1/2], Step [14380/15231], Loss: 0.2685\n",
            "Epoch [1/2], Step [14400/15231], Loss: 0.2462\n",
            "Epoch [1/2], Step [14420/15231], Loss: 0.2616\n",
            "Epoch [1/2], Step [14440/15231], Loss: 0.3015\n",
            "Epoch [1/2], Step [14460/15231], Loss: 0.2799\n",
            "Epoch [1/2], Step [14480/15231], Loss: 0.2773\n",
            "Epoch [1/2], Step [14500/15231], Loss: 0.2668\n",
            "Epoch [1/2], Step [14520/15231], Loss: 0.2612\n",
            "Epoch [1/2], Step [14540/15231], Loss: 0.2784\n",
            "Epoch [1/2], Step [14560/15231], Loss: 0.2778\n",
            "Epoch [1/2], Step [14580/15231], Loss: 0.2958\n",
            "Epoch [1/2], Step [14600/15231], Loss: 0.2739\n",
            "Epoch [1/2], Step [14620/15231], Loss: 0.2829\n",
            "Epoch [1/2], Step [14640/15231], Loss: 0.2800\n",
            "Epoch [1/2], Step [14660/15231], Loss: 0.3395\n",
            "Epoch [1/2], Step [14680/15231], Loss: 0.2777\n",
            "Epoch [1/2], Step [14700/15231], Loss: 0.2671\n",
            "Epoch [1/2], Step [14720/15231], Loss: 0.2507\n",
            "Epoch [1/2], Step [14740/15231], Loss: 0.2938\n",
            "Epoch [1/2], Step [14760/15231], Loss: 0.2317\n",
            "Epoch [1/2], Step [14780/15231], Loss: 0.2979\n",
            "Epoch [1/2], Step [14800/15231], Loss: 0.2668\n",
            "Epoch [1/2], Step [14820/15231], Loss: 0.3044\n",
            "Epoch [1/2], Step [14840/15231], Loss: 0.2727\n",
            "Epoch [1/2], Step [14860/15231], Loss: 0.2797\n",
            "Epoch [1/2], Step [14880/15231], Loss: 0.2384\n",
            "Epoch [1/2], Step [14900/15231], Loss: 0.2625\n",
            "Epoch [1/2], Step [14920/15231], Loss: 0.2233\n",
            "Epoch [1/2], Step [14940/15231], Loss: 0.2288\n",
            "Epoch [1/2], Step [14960/15231], Loss: 0.2821\n",
            "Epoch [1/2], Step [14980/15231], Loss: 0.2584\n",
            "Epoch [1/2], Step [15000/15231], Loss: 0.2722\n",
            "Epoch [1/2], Step [15020/15231], Loss: 0.2758\n",
            "Epoch [1/2], Step [15040/15231], Loss: 0.2710\n",
            "Epoch [1/2], Step [15060/15231], Loss: 0.2826\n",
            "Epoch [1/2], Step [15080/15231], Loss: 0.2915\n",
            "Epoch [1/2], Step [15100/15231], Loss: 0.2509\n",
            "Epoch [1/2], Step [15120/15231], Loss: 0.2633\n",
            "Epoch [1/2], Step [15140/15231], Loss: 0.2876\n",
            "Epoch [1/2], Step [15160/15231], Loss: 0.2892\n",
            "Epoch [1/2], Step [15180/15231], Loss: 0.2975\n",
            "Epoch [1/2], Step [15200/15231], Loss: 0.2807\n",
            "Epoch [1/2], Step [15220/15231], Loss: 0.3034\n",
            "Epoch [1/2] | Train Loss: 0.3355 | Train Acc: 88.39% | Test Loss: 0.9367 | Test Acc: 71.50%\n",
            "Epoch [2/2], Step [20/15231], Loss: 0.2218\n",
            "Epoch [2/2], Step [40/15231], Loss: 0.2904\n",
            "Epoch [2/2], Step [60/15231], Loss: 0.2523\n",
            "Epoch [2/2], Step [80/15231], Loss: 0.2510\n",
            "Epoch [2/2], Step [100/15231], Loss: 0.2600\n",
            "Epoch [2/2], Step [120/15231], Loss: 0.2884\n",
            "Epoch [2/2], Step [140/15231], Loss: 0.2903\n",
            "Epoch [2/2], Step [160/15231], Loss: 0.2554\n",
            "Epoch [2/2], Step [180/15231], Loss: 0.2617\n",
            "Epoch [2/2], Step [200/15231], Loss: 0.2261\n",
            "Epoch [2/2], Step [220/15231], Loss: 0.2619\n",
            "Epoch [2/2], Step [240/15231], Loss: 0.2416\n",
            "Epoch [2/2], Step [260/15231], Loss: 0.2557\n",
            "Epoch [2/2], Step [280/15231], Loss: 0.2897\n",
            "Epoch [2/2], Step [300/15231], Loss: 0.2537\n",
            "Epoch [2/2], Step [320/15231], Loss: 0.2533\n",
            "Epoch [2/2], Step [340/15231], Loss: 0.2585\n",
            "Epoch [2/2], Step [360/15231], Loss: 0.2845\n",
            "Epoch [2/2], Step [380/15231], Loss: 0.2663\n",
            "Epoch [2/2], Step [400/15231], Loss: 0.2923\n",
            "Epoch [2/2], Step [420/15231], Loss: 0.2650\n",
            "Epoch [2/2], Step [440/15231], Loss: 0.2800\n",
            "Epoch [2/2], Step [460/15231], Loss: 0.2720\n",
            "Epoch [2/2], Step [480/15231], Loss: 0.2682\n",
            "Epoch [2/2], Step [500/15231], Loss: 0.2462\n",
            "Epoch [2/2], Step [520/15231], Loss: 0.2430\n",
            "Epoch [2/2], Step [540/15231], Loss: 0.2806\n",
            "Epoch [2/2], Step [560/15231], Loss: 0.2907\n",
            "Epoch [2/2], Step [580/15231], Loss: 0.2903\n",
            "Epoch [2/2], Step [600/15231], Loss: 0.2574\n",
            "Epoch [2/2], Step [620/15231], Loss: 0.2794\n",
            "Epoch [2/2], Step [640/15231], Loss: 0.2360\n",
            "Epoch [2/2], Step [660/15231], Loss: 0.2796\n",
            "Epoch [2/2], Step [680/15231], Loss: 0.2776\n",
            "Epoch [2/2], Step [700/15231], Loss: 0.2503\n",
            "Epoch [2/2], Step [720/15231], Loss: 0.2711\n",
            "Epoch [2/2], Step [740/15231], Loss: 0.2606\n",
            "Epoch [2/2], Step [760/15231], Loss: 0.2466\n",
            "Epoch [2/2], Step [780/15231], Loss: 0.2470\n",
            "Epoch [2/2], Step [800/15231], Loss: 0.2709\n",
            "Epoch [2/2], Step [820/15231], Loss: 0.2683\n",
            "Epoch [2/2], Step [840/15231], Loss: 0.2623\n",
            "Epoch [2/2], Step [860/15231], Loss: 0.2813\n",
            "Epoch [2/2], Step [880/15231], Loss: 0.3156\n",
            "Epoch [2/2], Step [900/15231], Loss: 0.2812\n",
            "Epoch [2/2], Step [920/15231], Loss: 0.2694\n",
            "Epoch [2/2], Step [940/15231], Loss: 0.2603\n",
            "Epoch [2/2], Step [960/15231], Loss: 0.2822\n",
            "Epoch [2/2], Step [980/15231], Loss: 0.2963\n",
            "Epoch [2/2], Step [1000/15231], Loss: 0.2342\n",
            "Epoch [2/2], Step [1020/15231], Loss: 0.2692\n",
            "Epoch [2/2], Step [1040/15231], Loss: 0.2706\n",
            "Epoch [2/2], Step [1060/15231], Loss: 0.2320\n",
            "Epoch [2/2], Step [1080/15231], Loss: 0.2597\n",
            "Epoch [2/2], Step [1100/15231], Loss: 0.2514\n",
            "Epoch [2/2], Step [1120/15231], Loss: 0.2574\n",
            "Epoch [2/2], Step [1140/15231], Loss: 0.2916\n",
            "Epoch [2/2], Step [1160/15231], Loss: 0.2470\n",
            "Epoch [2/2], Step [1180/15231], Loss: 0.2834\n",
            "Epoch [2/2], Step [1200/15231], Loss: 0.2776\n",
            "Epoch [2/2], Step [1220/15231], Loss: 0.2400\n",
            "Epoch [2/2], Step [1240/15231], Loss: 0.2354\n",
            "Epoch [2/2], Step [1260/15231], Loss: 0.3038\n",
            "Epoch [2/2], Step [1280/15231], Loss: 0.2476\n",
            "Epoch [2/2], Step [1300/15231], Loss: 0.2943\n",
            "Epoch [2/2], Step [1320/15231], Loss: 0.2745\n",
            "Epoch [2/2], Step [1340/15231], Loss: 0.2705\n",
            "Epoch [2/2], Step [1360/15231], Loss: 0.2655\n",
            "Epoch [2/2], Step [1380/15231], Loss: 0.2702\n",
            "Epoch [2/2], Step [1400/15231], Loss: 0.2893\n",
            "Epoch [2/2], Step [1420/15231], Loss: 0.2633\n",
            "Epoch [2/2], Step [1440/15231], Loss: 0.2465\n",
            "Epoch [2/2], Step [1460/15231], Loss: 0.2523\n",
            "Epoch [2/2], Step [1480/15231], Loss: 0.2533\n",
            "Epoch [2/2], Step [1500/15231], Loss: 0.2718\n",
            "Epoch [2/2], Step [1520/15231], Loss: 0.2712\n",
            "Epoch [2/2], Step [1540/15231], Loss: 0.2333\n",
            "Epoch [2/2], Step [1560/15231], Loss: 0.2855\n",
            "Epoch [2/2], Step [1580/15231], Loss: 0.2789\n",
            "Epoch [2/2], Step [1600/15231], Loss: 0.2605\n",
            "Epoch [2/2], Step [1620/15231], Loss: 0.2958\n",
            "Epoch [2/2], Step [1640/15231], Loss: 0.2777\n",
            "Epoch [2/2], Step [1660/15231], Loss: 0.2414\n",
            "Epoch [2/2], Step [1680/15231], Loss: 0.2682\n",
            "Epoch [2/2], Step [1700/15231], Loss: 0.2802\n",
            "Epoch [2/2], Step [1720/15231], Loss: 0.2518\n",
            "Epoch [2/2], Step [1740/15231], Loss: 0.2632\n",
            "Epoch [2/2], Step [1760/15231], Loss: 0.2471\n",
            "Epoch [2/2], Step [1780/15231], Loss: 0.2477\n",
            "Epoch [2/2], Step [1800/15231], Loss: 0.2485\n",
            "Epoch [2/2], Step [1820/15231], Loss: 0.2671\n",
            "Epoch [2/2], Step [1840/15231], Loss: 0.2806\n",
            "Epoch [2/2], Step [1860/15231], Loss: 0.2929\n",
            "Epoch [2/2], Step [1880/15231], Loss: 0.2585\n",
            "Epoch [2/2], Step [1900/15231], Loss: 0.2454\n",
            "Epoch [2/2], Step [1920/15231], Loss: 0.2664\n",
            "Epoch [2/2], Step [1940/15231], Loss: 0.2573\n",
            "Epoch [2/2], Step [1960/15231], Loss: 0.2426\n",
            "Epoch [2/2], Step [1980/15231], Loss: 0.2797\n",
            "Epoch [2/2], Step [2000/15231], Loss: 0.2752\n",
            "Epoch [2/2], Step [2020/15231], Loss: 0.2635\n",
            "Epoch [2/2], Step [2040/15231], Loss: 0.2353\n",
            "Epoch [2/2], Step [2060/15231], Loss: 0.2558\n",
            "Epoch [2/2], Step [2080/15231], Loss: 0.2724\n",
            "Epoch [2/2], Step [2100/15231], Loss: 0.2558\n",
            "Epoch [2/2], Step [2120/15231], Loss: 0.2764\n",
            "Epoch [2/2], Step [2140/15231], Loss: 0.2509\n",
            "Epoch [2/2], Step [2160/15231], Loss: 0.2206\n",
            "Epoch [2/2], Step [2180/15231], Loss: 0.2281\n",
            "Epoch [2/2], Step [2200/15231], Loss: 0.2908\n",
            "Epoch [2/2], Step [2220/15231], Loss: 0.2416\n",
            "Epoch [2/2], Step [2240/15231], Loss: 0.2529\n",
            "Epoch [2/2], Step [2260/15231], Loss: 0.2694\n",
            "Epoch [2/2], Step [2280/15231], Loss: 0.3007\n",
            "Epoch [2/2], Step [2300/15231], Loss: 0.2752\n",
            "Epoch [2/2], Step [2320/15231], Loss: 0.2449\n",
            "Epoch [2/2], Step [2340/15231], Loss: 0.2631\n",
            "Epoch [2/2], Step [2360/15231], Loss: 0.2673\n",
            "Epoch [2/2], Step [2380/15231], Loss: 0.2511\n",
            "Epoch [2/2], Step [2400/15231], Loss: 0.2487\n",
            "Epoch [2/2], Step [2420/15231], Loss: 0.2482\n",
            "Epoch [2/2], Step [2440/15231], Loss: 0.2602\n",
            "Epoch [2/2], Step [2460/15231], Loss: 0.2528\n",
            "Epoch [2/2], Step [2480/15231], Loss: 0.2815\n",
            "Epoch [2/2], Step [2500/15231], Loss: 0.2431\n",
            "Epoch [2/2], Step [2520/15231], Loss: 0.2523\n",
            "Epoch [2/2], Step [2540/15231], Loss: 0.2979\n",
            "Epoch [2/2], Step [2560/15231], Loss: 0.2623\n",
            "Epoch [2/2], Step [2580/15231], Loss: 0.2730\n",
            "Epoch [2/2], Step [2600/15231], Loss: 0.2572\n",
            "Epoch [2/2], Step [2620/15231], Loss: 0.3054\n",
            "Epoch [2/2], Step [2640/15231], Loss: 0.2497\n",
            "Epoch [2/2], Step [2660/15231], Loss: 0.2434\n",
            "Epoch [2/2], Step [2680/15231], Loss: 0.2135\n",
            "Epoch [2/2], Step [2700/15231], Loss: 0.2543\n",
            "Epoch [2/2], Step [2720/15231], Loss: 0.2274\n",
            "Epoch [2/2], Step [2740/15231], Loss: 0.2525\n",
            "Epoch [2/2], Step [2760/15231], Loss: 0.2631\n",
            "Epoch [2/2], Step [2780/15231], Loss: 0.2335\n",
            "Epoch [2/2], Step [2800/15231], Loss: 0.2419\n",
            "Epoch [2/2], Step [2820/15231], Loss: 0.2518\n",
            "Epoch [2/2], Step [2840/15231], Loss: 0.2187\n",
            "Epoch [2/2], Step [2860/15231], Loss: 0.2742\n",
            "Epoch [2/2], Step [2880/15231], Loss: 0.2780\n",
            "Epoch [2/2], Step [2900/15231], Loss: 0.2743\n",
            "Epoch [2/2], Step [2920/15231], Loss: 0.2410\n",
            "Epoch [2/2], Step [2940/15231], Loss: 0.3152\n",
            "Epoch [2/2], Step [2960/15231], Loss: 0.2705\n",
            "Epoch [2/2], Step [2980/15231], Loss: 0.2476\n",
            "Epoch [2/2], Step [3000/15231], Loss: 0.2668\n",
            "Epoch [2/2], Step [3020/15231], Loss: 0.2743\n",
            "Epoch [2/2], Step [3040/15231], Loss: 0.2333\n",
            "Epoch [2/2], Step [3060/15231], Loss: 0.2646\n",
            "Epoch [2/2], Step [3080/15231], Loss: 0.2549\n",
            "Epoch [2/2], Step [3100/15231], Loss: 0.2295\n",
            "Epoch [2/2], Step [3120/15231], Loss: 0.2891\n",
            "Epoch [2/2], Step [3140/15231], Loss: 0.2592\n",
            "Epoch [2/2], Step [3160/15231], Loss: 0.2553\n",
            "Epoch [2/2], Step [3180/15231], Loss: 0.2294\n",
            "Epoch [2/2], Step [3200/15231], Loss: 0.2346\n",
            "Epoch [2/2], Step [3220/15231], Loss: 0.2734\n",
            "Epoch [2/2], Step [3240/15231], Loss: 0.2633\n",
            "Epoch [2/2], Step [3260/15231], Loss: 0.2821\n",
            "Epoch [2/2], Step [3280/15231], Loss: 0.2981\n",
            "Epoch [2/2], Step [3300/15231], Loss: 0.2723\n",
            "Epoch [2/2], Step [3320/15231], Loss: 0.2544\n",
            "Epoch [2/2], Step [3340/15231], Loss: 0.2651\n",
            "Epoch [2/2], Step [3360/15231], Loss: 0.2287\n",
            "Epoch [2/2], Step [3380/15231], Loss: 0.2175\n",
            "Epoch [2/2], Step [3400/15231], Loss: 0.2807\n",
            "Epoch [2/2], Step [3420/15231], Loss: 0.2586\n",
            "Epoch [2/2], Step [3440/15231], Loss: 0.2789\n",
            "Epoch [2/2], Step [3460/15231], Loss: 0.2985\n",
            "Epoch [2/2], Step [3480/15231], Loss: 0.2695\n",
            "Epoch [2/2], Step [3500/15231], Loss: 0.2645\n",
            "Epoch [2/2], Step [3520/15231], Loss: 0.2533\n",
            "Epoch [2/2], Step [3540/15231], Loss: 0.2613\n",
            "Epoch [2/2], Step [3560/15231], Loss: 0.2463\n",
            "Epoch [2/2], Step [3580/15231], Loss: 0.2874\n",
            "Epoch [2/2], Step [3600/15231], Loss: 0.2841\n",
            "Epoch [2/2], Step [3620/15231], Loss: 0.2564\n",
            "Epoch [2/2], Step [3640/15231], Loss: 0.2391\n",
            "Epoch [2/2], Step [3660/15231], Loss: 0.2701\n",
            "Epoch [2/2], Step [3680/15231], Loss: 0.2184\n",
            "Epoch [2/2], Step [3700/15231], Loss: 0.2819\n",
            "Epoch [2/2], Step [3720/15231], Loss: 0.2425\n",
            "Epoch [2/2], Step [3740/15231], Loss: 0.2919\n",
            "Epoch [2/2], Step [3760/15231], Loss: 0.2450\n",
            "Epoch [2/2], Step [3780/15231], Loss: 0.3013\n",
            "Epoch [2/2], Step [3800/15231], Loss: 0.2127\n",
            "Epoch [2/2], Step [3820/15231], Loss: 0.2567\n",
            "Epoch [2/2], Step [3840/15231], Loss: 0.2305\n",
            "Epoch [2/2], Step [3860/15231], Loss: 0.2401\n",
            "Epoch [2/2], Step [3880/15231], Loss: 0.2665\n",
            "Epoch [2/2], Step [3900/15231], Loss: 0.2491\n",
            "Epoch [2/2], Step [3920/15231], Loss: 0.2564\n",
            "Epoch [2/2], Step [3940/15231], Loss: 0.2268\n",
            "Epoch [2/2], Step [3960/15231], Loss: 0.2699\n",
            "Epoch [2/2], Step [3980/15231], Loss: 0.2807\n",
            "Epoch [2/2], Step [4000/15231], Loss: 0.2783\n",
            "Epoch [2/2], Step [4020/15231], Loss: 0.2569\n",
            "Epoch [2/2], Step [4040/15231], Loss: 0.2710\n",
            "Epoch [2/2], Step [4060/15231], Loss: 0.2599\n",
            "Epoch [2/2], Step [4080/15231], Loss: 0.3211\n",
            "Epoch [2/2], Step [4100/15231], Loss: 0.2602\n",
            "Epoch [2/2], Step [4120/15231], Loss: 0.2438\n",
            "Epoch [2/2], Step [4140/15231], Loss: 0.2076\n",
            "Epoch [2/2], Step [4160/15231], Loss: 0.2707\n",
            "Epoch [2/2], Step [4180/15231], Loss: 0.2561\n",
            "Epoch [2/2], Step [4200/15231], Loss: 0.2605\n",
            "Epoch [2/2], Step [4220/15231], Loss: 0.2497\n",
            "Epoch [2/2], Step [4240/15231], Loss: 0.2632\n",
            "Epoch [2/2], Step [4260/15231], Loss: 0.2417\n",
            "Epoch [2/2], Step [4280/15231], Loss: 0.2523\n",
            "Epoch [2/2], Step [4300/15231], Loss: 0.2539\n",
            "Epoch [2/2], Step [4320/15231], Loss: 0.2522\n",
            "Epoch [2/2], Step [4340/15231], Loss: 0.2447\n",
            "Epoch [2/2], Step [4360/15231], Loss: 0.2317\n",
            "Epoch [2/2], Step [4380/15231], Loss: 0.2167\n",
            "Epoch [2/2], Step [4400/15231], Loss: 0.2328\n",
            "Epoch [2/2], Step [4420/15231], Loss: 0.2649\n",
            "Epoch [2/2], Step [4440/15231], Loss: 0.2572\n",
            "Epoch [2/2], Step [4460/15231], Loss: 0.2478\n",
            "Epoch [2/2], Step [4480/15231], Loss: 0.2561\n",
            "Epoch [2/2], Step [4500/15231], Loss: 0.2526\n",
            "Epoch [2/2], Step [4520/15231], Loss: 0.2290\n",
            "Epoch [2/2], Step [4540/15231], Loss: 0.2421\n",
            "Epoch [2/2], Step [4560/15231], Loss: 0.2626\n",
            "Epoch [2/2], Step [4580/15231], Loss: 0.2648\n",
            "Epoch [2/2], Step [4600/15231], Loss: 0.2769\n",
            "Epoch [2/2], Step [4620/15231], Loss: 0.2592\n",
            "Epoch [2/2], Step [4640/15231], Loss: 0.2222\n",
            "Epoch [2/2], Step [4660/15231], Loss: 0.2282\n",
            "Epoch [2/2], Step [4680/15231], Loss: 0.2674\n",
            "Epoch [2/2], Step [4700/15231], Loss: 0.2210\n",
            "Epoch [2/2], Step [4720/15231], Loss: 0.2284\n",
            "Epoch [2/2], Step [4740/15231], Loss: 0.2617\n",
            "Epoch [2/2], Step [4760/15231], Loss: 0.2294\n",
            "Epoch [2/2], Step [4780/15231], Loss: 0.2467\n",
            "Epoch [2/2], Step [4800/15231], Loss: 0.2554\n",
            "Epoch [2/2], Step [4820/15231], Loss: 0.2518\n",
            "Epoch [2/2], Step [4840/15231], Loss: 0.2577\n",
            "Epoch [2/2], Step [4860/15231], Loss: 0.2259\n",
            "Epoch [2/2], Step [4880/15231], Loss: 0.2310\n",
            "Epoch [2/2], Step [4900/15231], Loss: 0.2384\n",
            "Epoch [2/2], Step [4920/15231], Loss: 0.2645\n",
            "Epoch [2/2], Step [4940/15231], Loss: 0.2400\n",
            "Epoch [2/2], Step [4960/15231], Loss: 0.2637\n",
            "Epoch [2/2], Step [4980/15231], Loss: 0.2423\n",
            "Epoch [2/2], Step [5000/15231], Loss: 0.2423\n",
            "Epoch [2/2], Step [5020/15231], Loss: 0.2364\n",
            "Epoch [2/2], Step [5040/15231], Loss: 0.2481\n",
            "Epoch [2/2], Step [5060/15231], Loss: 0.2608\n",
            "Epoch [2/2], Step [5080/15231], Loss: 0.2602\n",
            "Epoch [2/2], Step [5100/15231], Loss: 0.2602\n",
            "Epoch [2/2], Step [5120/15231], Loss: 0.2680\n",
            "Epoch [2/2], Step [5140/15231], Loss: 0.2470\n",
            "Epoch [2/2], Step [5160/15231], Loss: 0.2712\n",
            "Epoch [2/2], Step [5180/15231], Loss: 0.2588\n",
            "Epoch [2/2], Step [5200/15231], Loss: 0.2460\n",
            "Epoch [2/2], Step [5220/15231], Loss: 0.2907\n",
            "Epoch [2/2], Step [5240/15231], Loss: 0.2635\n",
            "Epoch [2/2], Step [5260/15231], Loss: 0.2671\n",
            "Epoch [2/2], Step [5280/15231], Loss: 0.2099\n",
            "Epoch [2/2], Step [5300/15231], Loss: 0.2707\n",
            "Epoch [2/2], Step [5320/15231], Loss: 0.2910\n",
            "Epoch [2/2], Step [5340/15231], Loss: 0.2182\n",
            "Epoch [2/2], Step [5360/15231], Loss: 0.2244\n",
            "Epoch [2/2], Step [5380/15231], Loss: 0.2245\n",
            "Epoch [2/2], Step [5400/15231], Loss: 0.2489\n",
            "Epoch [2/2], Step [5420/15231], Loss: 0.2584\n",
            "Epoch [2/2], Step [5440/15231], Loss: 0.2702\n",
            "Epoch [2/2], Step [5460/15231], Loss: 0.2521\n",
            "Epoch [2/2], Step [5480/15231], Loss: 0.2707\n",
            "Epoch [2/2], Step [5500/15231], Loss: 0.2626\n",
            "Epoch [2/2], Step [5520/15231], Loss: 0.2752\n",
            "Epoch [2/2], Step [5540/15231], Loss: 0.2375\n",
            "Epoch [2/2], Step [5560/15231], Loss: 0.2260\n",
            "Epoch [2/2], Step [5580/15231], Loss: 0.2233\n",
            "Epoch [2/2], Step [5600/15231], Loss: 0.2467\n",
            "Epoch [2/2], Step [5620/15231], Loss: 0.2404\n",
            "Epoch [2/2], Step [5640/15231], Loss: 0.2282\n",
            "Epoch [2/2], Step [5660/15231], Loss: 0.2402\n",
            "Epoch [2/2], Step [5680/15231], Loss: 0.2449\n",
            "Epoch [2/2], Step [5700/15231], Loss: 0.2675\n",
            "Epoch [2/2], Step [5720/15231], Loss: 0.2731\n",
            "Epoch [2/2], Step [5740/15231], Loss: 0.2436\n",
            "Epoch [2/2], Step [5760/15231], Loss: 0.2984\n",
            "Epoch [2/2], Step [5780/15231], Loss: 0.2473\n",
            "Epoch [2/2], Step [5800/15231], Loss: 0.2249\n",
            "Epoch [2/2], Step [5820/15231], Loss: 0.2830\n",
            "Epoch [2/2], Step [5840/15231], Loss: 0.2623\n",
            "Epoch [2/2], Step [5860/15231], Loss: 0.2533\n",
            "Epoch [2/2], Step [5880/15231], Loss: 0.2534\n",
            "Epoch [2/2], Step [5900/15231], Loss: 0.2553\n",
            "Epoch [2/2], Step [5920/15231], Loss: 0.2698\n",
            "Epoch [2/2], Step [5940/15231], Loss: 0.2313\n",
            "Epoch [2/2], Step [5960/15231], Loss: 0.2398\n",
            "Epoch [2/2], Step [5980/15231], Loss: 0.2195\n",
            "Epoch [2/2], Step [6000/15231], Loss: 0.2756\n",
            "Epoch [2/2], Step [6020/15231], Loss: 0.2559\n",
            "Epoch [2/2], Step [6040/15231], Loss: 0.2818\n",
            "Epoch [2/2], Step [6060/15231], Loss: 0.2299\n",
            "Epoch [2/2], Step [6080/15231], Loss: 0.2552\n",
            "Epoch [2/2], Step [6100/15231], Loss: 0.2554\n",
            "Epoch [2/2], Step [6120/15231], Loss: 0.2315\n",
            "Epoch [2/2], Step [6140/15231], Loss: 0.2563\n",
            "Epoch [2/2], Step [6160/15231], Loss: 0.2537\n",
            "Epoch [2/2], Step [6180/15231], Loss: 0.2415\n",
            "Epoch [2/2], Step [6200/15231], Loss: 0.2579\n",
            "Epoch [2/2], Step [6220/15231], Loss: 0.2783\n",
            "Epoch [2/2], Step [6240/15231], Loss: 0.2654\n",
            "Epoch [2/2], Step [6260/15231], Loss: 0.2585\n",
            "Epoch [2/2], Step [6280/15231], Loss: 0.2341\n",
            "Epoch [2/2], Step [6300/15231], Loss: 0.2476\n",
            "Epoch [2/2], Step [6320/15231], Loss: 0.2301\n",
            "Epoch [2/2], Step [6340/15231], Loss: 0.2395\n",
            "Epoch [2/2], Step [6360/15231], Loss: 0.2596\n",
            "Epoch [2/2], Step [6380/15231], Loss: 0.2712\n",
            "Epoch [2/2], Step [6400/15231], Loss: 0.2337\n",
            "Epoch [2/2], Step [6420/15231], Loss: 0.2385\n",
            "Epoch [2/2], Step [6440/15231], Loss: 0.2637\n",
            "Epoch [2/2], Step [6460/15231], Loss: 0.2274\n",
            "Epoch [2/2], Step [6480/15231], Loss: 0.2239\n",
            "Epoch [2/2], Step [6500/15231], Loss: 0.2395\n",
            "Epoch [2/2], Step [6520/15231], Loss: 0.2367\n",
            "Epoch [2/2], Step [6540/15231], Loss: 0.2467\n",
            "Epoch [2/2], Step [6560/15231], Loss: 0.2700\n",
            "Epoch [2/2], Step [6580/15231], Loss: 0.2252\n",
            "Epoch [2/2], Step [6600/15231], Loss: 0.2484\n",
            "Epoch [2/2], Step [6620/15231], Loss: 0.2258\n",
            "Epoch [2/2], Step [6640/15231], Loss: 0.2557\n",
            "Epoch [2/2], Step [6660/15231], Loss: 0.2658\n",
            "Epoch [2/2], Step [6680/15231], Loss: 0.2244\n",
            "Epoch [2/2], Step [6700/15231], Loss: 0.2643\n",
            "Epoch [2/2], Step [6720/15231], Loss: 0.2734\n",
            "Epoch [2/2], Step [6740/15231], Loss: 0.2463\n",
            "Epoch [2/2], Step [6760/15231], Loss: 0.2385\n",
            "Epoch [2/2], Step [6780/15231], Loss: 0.2782\n",
            "Epoch [2/2], Step [6800/15231], Loss: 0.2554\n",
            "Epoch [2/2], Step [6820/15231], Loss: 0.2455\n",
            "Epoch [2/2], Step [6840/15231], Loss: 0.2528\n",
            "Epoch [2/2], Step [6860/15231], Loss: 0.2398\n",
            "Epoch [2/2], Step [6880/15231], Loss: 0.2513\n",
            "Epoch [2/2], Step [6900/15231], Loss: 0.2175\n",
            "Epoch [2/2], Step [6920/15231], Loss: 0.2111\n",
            "Epoch [2/2], Step [6940/15231], Loss: 0.2401\n",
            "Epoch [2/2], Step [6960/15231], Loss: 0.2338\n",
            "Epoch [2/2], Step [6980/15231], Loss: 0.2236\n",
            "Epoch [2/2], Step [7000/15231], Loss: 0.2792\n",
            "Epoch [2/2], Step [7020/15231], Loss: 0.2477\n",
            "Epoch [2/2], Step [7040/15231], Loss: 0.2234\n",
            "Epoch [2/2], Step [7060/15231], Loss: 0.2390\n",
            "Epoch [2/2], Step [7080/15231], Loss: 0.2637\n",
            "Epoch [2/2], Step [7100/15231], Loss: 0.2420\n",
            "Epoch [2/2], Step [7120/15231], Loss: 0.2544\n",
            "Epoch [2/2], Step [7140/15231], Loss: 0.2619\n",
            "Epoch [2/2], Step [7160/15231], Loss: 0.2062\n",
            "Epoch [2/2], Step [7180/15231], Loss: 0.3115\n",
            "Epoch [2/2], Step [7200/15231], Loss: 0.2522\n",
            "Epoch [2/2], Step [7220/15231], Loss: 0.2278\n",
            "Epoch [2/2], Step [7240/15231], Loss: 0.2353\n",
            "Epoch [2/2], Step [7260/15231], Loss: 0.2347\n",
            "Epoch [2/2], Step [7280/15231], Loss: 0.2670\n",
            "Epoch [2/2], Step [7300/15231], Loss: 0.2373\n",
            "Epoch [2/2], Step [7320/15231], Loss: 0.2385\n",
            "Epoch [2/2], Step [7340/15231], Loss: 0.2735\n",
            "Epoch [2/2], Step [7360/15231], Loss: 0.2584\n",
            "Epoch [2/2], Step [7380/15231], Loss: 0.2724\n",
            "Epoch [2/2], Step [7400/15231], Loss: 0.2591\n",
            "Epoch [2/2], Step [7420/15231], Loss: 0.2288\n",
            "Epoch [2/2], Step [7440/15231], Loss: 0.2329\n",
            "Epoch [2/2], Step [7460/15231], Loss: 0.2494\n",
            "Epoch [2/2], Step [7480/15231], Loss: 0.2172\n",
            "Epoch [2/2], Step [7500/15231], Loss: 0.2729\n",
            "Epoch [2/2], Step [7520/15231], Loss: 0.2401\n",
            "Epoch [2/2], Step [7540/15231], Loss: 0.2735\n",
            "Epoch [2/2], Step [7560/15231], Loss: 0.2655\n",
            "Epoch [2/2], Step [7580/15231], Loss: 0.2427\n",
            "Epoch [2/2], Step [7600/15231], Loss: 0.2454\n",
            "Epoch [2/2], Step [7620/15231], Loss: 0.2470\n",
            "Epoch [2/2], Step [7640/15231], Loss: 0.2413\n",
            "Epoch [2/2], Step [7660/15231], Loss: 0.2518\n",
            "Epoch [2/2], Step [7680/15231], Loss: 0.2431\n",
            "Epoch [2/2], Step [7700/15231], Loss: 0.2334\n",
            "Epoch [2/2], Step [7720/15231], Loss: 0.2807\n",
            "Epoch [2/2], Step [7740/15231], Loss: 0.2462\n",
            "Epoch [2/2], Step [7760/15231], Loss: 0.2316\n",
            "Epoch [2/2], Step [7780/15231], Loss: 0.2622\n",
            "Epoch [2/2], Step [7800/15231], Loss: 0.2275\n",
            "Epoch [2/2], Step [7820/15231], Loss: 0.2587\n",
            "Epoch [2/2], Step [7840/15231], Loss: 0.2693\n",
            "Epoch [2/2], Step [7860/15231], Loss: 0.2630\n",
            "Epoch [2/2], Step [7880/15231], Loss: 0.2441\n",
            "Epoch [2/2], Step [7900/15231], Loss: 0.2728\n",
            "Epoch [2/2], Step [7920/15231], Loss: 0.2444\n",
            "Epoch [2/2], Step [7940/15231], Loss: 0.2441\n",
            "Epoch [2/2], Step [7960/15231], Loss: 0.2655\n",
            "Epoch [2/2], Step [7980/15231], Loss: 0.2107\n",
            "Epoch [2/2], Step [8000/15231], Loss: 0.2526\n",
            "Epoch [2/2], Step [8020/15231], Loss: 0.2491\n",
            "Epoch [2/2], Step [8040/15231], Loss: 0.2536\n",
            "Epoch [2/2], Step [8060/15231], Loss: 0.2514\n",
            "Epoch [2/2], Step [8080/15231], Loss: 0.2439\n",
            "Epoch [2/2], Step [8100/15231], Loss: 0.2578\n",
            "Epoch [2/2], Step [8120/15231], Loss: 0.2513\n",
            "Epoch [2/2], Step [8140/15231], Loss: 0.2593\n",
            "Epoch [2/2], Step [8160/15231], Loss: 0.2451\n",
            "Epoch [2/2], Step [8180/15231], Loss: 0.2722\n",
            "Epoch [2/2], Step [8200/15231], Loss: 0.2602\n",
            "Epoch [2/2], Step [8220/15231], Loss: 0.2257\n",
            "Epoch [2/2], Step [8240/15231], Loss: 0.2416\n",
            "Epoch [2/2], Step [8260/15231], Loss: 0.2445\n",
            "Epoch [2/2], Step [8280/15231], Loss: 0.2499\n",
            "Epoch [2/2], Step [8300/15231], Loss: 0.2563\n",
            "Epoch [2/2], Step [8320/15231], Loss: 0.2205\n",
            "Epoch [2/2], Step [8340/15231], Loss: 0.2712\n",
            "Epoch [2/2], Step [8360/15231], Loss: 0.2387\n",
            "Epoch [2/2], Step [8380/15231], Loss: 0.2328\n",
            "Epoch [2/2], Step [8400/15231], Loss: 0.2294\n",
            "Epoch [2/2], Step [8420/15231], Loss: 0.2569\n",
            "Epoch [2/2], Step [8440/15231], Loss: 0.2767\n",
            "Epoch [2/2], Step [8460/15231], Loss: 0.2776\n",
            "Epoch [2/2], Step [8480/15231], Loss: 0.2482\n",
            "Epoch [2/2], Step [8500/15231], Loss: 0.2655\n",
            "Epoch [2/2], Step [8520/15231], Loss: 0.2311\n",
            "Epoch [2/2], Step [8540/15231], Loss: 0.2199\n",
            "Epoch [2/2], Step [8560/15231], Loss: 0.2285\n",
            "Epoch [2/2], Step [8580/15231], Loss: 0.2106\n",
            "Epoch [2/2], Step [8600/15231], Loss: 0.2634\n",
            "Epoch [2/2], Step [8620/15231], Loss: 0.2440\n",
            "Epoch [2/2], Step [8640/15231], Loss: 0.2602\n",
            "Epoch [2/2], Step [8660/15231], Loss: 0.2614\n",
            "Epoch [2/2], Step [8680/15231], Loss: 0.2441\n",
            "Epoch [2/2], Step [8700/15231], Loss: 0.2380\n",
            "Epoch [2/2], Step [8720/15231], Loss: 0.2369\n",
            "Epoch [2/2], Step [8740/15231], Loss: 0.2613\n",
            "Epoch [2/2], Step [8760/15231], Loss: 0.2147\n",
            "Epoch [2/2], Step [8780/15231], Loss: 0.2568\n",
            "Epoch [2/2], Step [8800/15231], Loss: 0.2190\n",
            "Epoch [2/2], Step [8820/15231], Loss: 0.2278\n",
            "Epoch [2/2], Step [8840/15231], Loss: 0.2472\n",
            "Epoch [2/2], Step [8860/15231], Loss: 0.2423\n",
            "Epoch [2/2], Step [8880/15231], Loss: 0.2547\n",
            "Epoch [2/2], Step [8900/15231], Loss: 0.2286\n",
            "Epoch [2/2], Step [8920/15231], Loss: 0.2395\n",
            "Epoch [2/2], Step [8940/15231], Loss: 0.2551\n",
            "Epoch [2/2], Step [8960/15231], Loss: 0.2140\n",
            "Epoch [2/2], Step [8980/15231], Loss: 0.2349\n",
            "Epoch [2/2], Step [9000/15231], Loss: 0.2286\n",
            "Epoch [2/2], Step [9020/15231], Loss: 0.2300\n",
            "Epoch [2/2], Step [9040/15231], Loss: 0.2434\n",
            "Epoch [2/2], Step [9060/15231], Loss: 0.2702\n",
            "Epoch [2/2], Step [9080/15231], Loss: 0.2305\n",
            "Epoch [2/2], Step [9100/15231], Loss: 0.2202\n",
            "Epoch [2/2], Step [9120/15231], Loss: 0.2750\n",
            "Epoch [2/2], Step [9140/15231], Loss: 0.2573\n",
            "Epoch [2/2], Step [9160/15231], Loss: 0.2417\n",
            "Epoch [2/2], Step [9180/15231], Loss: 0.2568\n",
            "Epoch [2/2], Step [9200/15231], Loss: 0.2465\n",
            "Epoch [2/2], Step [9220/15231], Loss: 0.2372\n",
            "Epoch [2/2], Step [9240/15231], Loss: 0.2282\n",
            "Epoch [2/2], Step [9260/15231], Loss: 0.2303\n",
            "Epoch [2/2], Step [9280/15231], Loss: 0.2314\n",
            "Epoch [2/2], Step [9300/15231], Loss: 0.2400\n",
            "Epoch [2/2], Step [9320/15231], Loss: 0.2613\n",
            "Epoch [2/2], Step [9340/15231], Loss: 0.2346\n",
            "Epoch [2/2], Step [9360/15231], Loss: 0.2556\n",
            "Epoch [2/2], Step [9380/15231], Loss: 0.2470\n",
            "Epoch [2/2], Step [9400/15231], Loss: 0.2197\n",
            "Epoch [2/2], Step [9420/15231], Loss: 0.2019\n",
            "Epoch [2/2], Step [9440/15231], Loss: 0.2511\n",
            "Epoch [2/2], Step [9460/15231], Loss: 0.2269\n",
            "Epoch [2/2], Step [9480/15231], Loss: 0.2072\n",
            "Epoch [2/2], Step [9500/15231], Loss: 0.2394\n",
            "Epoch [2/2], Step [9520/15231], Loss: 0.2478\n",
            "Epoch [2/2], Step [9540/15231], Loss: 0.2388\n",
            "Epoch [2/2], Step [9560/15231], Loss: 0.2174\n",
            "Epoch [2/2], Step [9580/15231], Loss: 0.2672\n",
            "Epoch [2/2], Step [9600/15231], Loss: 0.2449\n",
            "Epoch [2/2], Step [9620/15231], Loss: 0.2256\n",
            "Epoch [2/2], Step [9640/15231], Loss: 0.2607\n",
            "Epoch [2/2], Step [9660/15231], Loss: 0.2235\n",
            "Epoch [2/2], Step [9680/15231], Loss: 0.2561\n",
            "Epoch [2/2], Step [9700/15231], Loss: 0.2284\n",
            "Epoch [2/2], Step [9720/15231], Loss: 0.2668\n",
            "Epoch [2/2], Step [9740/15231], Loss: 0.2436\n",
            "Epoch [2/2], Step [9760/15231], Loss: 0.2471\n",
            "Epoch [2/2], Step [9780/15231], Loss: 0.2521\n",
            "Epoch [2/2], Step [9800/15231], Loss: 0.2226\n",
            "Epoch [2/2], Step [9820/15231], Loss: 0.2158\n",
            "Epoch [2/2], Step [9840/15231], Loss: 0.2403\n",
            "Epoch [2/2], Step [9860/15231], Loss: 0.2583\n",
            "Epoch [2/2], Step [9880/15231], Loss: 0.2494\n",
            "Epoch [2/2], Step [9900/15231], Loss: 0.2425\n",
            "Epoch [2/2], Step [9920/15231], Loss: 0.2358\n",
            "Epoch [2/2], Step [9940/15231], Loss: 0.2375\n",
            "Epoch [2/2], Step [9960/15231], Loss: 0.2428\n",
            "Epoch [2/2], Step [9980/15231], Loss: 0.2523\n",
            "Epoch [2/2], Step [10000/15231], Loss: 0.2305\n",
            "Epoch [2/2], Step [10020/15231], Loss: 0.2657\n",
            "Epoch [2/2], Step [10040/15231], Loss: 0.2642\n",
            "Epoch [2/2], Step [10060/15231], Loss: 0.2140\n",
            "Epoch [2/2], Step [10080/15231], Loss: 0.2544\n",
            "Epoch [2/2], Step [10100/15231], Loss: 0.2445\n",
            "Epoch [2/2], Step [10120/15231], Loss: 0.2743\n",
            "Epoch [2/2], Step [10140/15231], Loss: 0.2132\n",
            "Epoch [2/2], Step [10160/15231], Loss: 0.2467\n",
            "Epoch [2/2], Step [10180/15231], Loss: 0.2535\n",
            "Epoch [2/2], Step [10200/15231], Loss: 0.2567\n",
            "Epoch [2/2], Step [10220/15231], Loss: 0.2367\n",
            "Epoch [2/2], Step [10240/15231], Loss: 0.2123\n",
            "Epoch [2/2], Step [10260/15231], Loss: 0.2345\n",
            "Epoch [2/2], Step [10280/15231], Loss: 0.2727\n",
            "Epoch [2/2], Step [10300/15231], Loss: 0.2186\n",
            "Epoch [2/2], Step [10320/15231], Loss: 0.2411\n",
            "Epoch [2/2], Step [10340/15231], Loss: 0.2278\n",
            "Epoch [2/2], Step [10360/15231], Loss: 0.2259\n",
            "Epoch [2/2], Step [10380/15231], Loss: 0.2375\n",
            "Epoch [2/2], Step [10400/15231], Loss: 0.2714\n",
            "Epoch [2/2], Step [10420/15231], Loss: 0.2359\n",
            "Epoch [2/2], Step [10440/15231], Loss: 0.2172\n",
            "Epoch [2/2], Step [10460/15231], Loss: 0.2218\n",
            "Epoch [2/2], Step [10480/15231], Loss: 0.2631\n",
            "Epoch [2/2], Step [10500/15231], Loss: 0.2332\n",
            "Epoch [2/2], Step [10520/15231], Loss: 0.2256\n",
            "Epoch [2/2], Step [10540/15231], Loss: 0.2247\n",
            "Epoch [2/2], Step [10560/15231], Loss: 0.2077\n",
            "Epoch [2/2], Step [10580/15231], Loss: 0.2767\n",
            "Epoch [2/2], Step [10600/15231], Loss: 0.2365\n",
            "Epoch [2/2], Step [10620/15231], Loss: 0.2473\n",
            "Epoch [2/2], Step [10640/15231], Loss: 0.2042\n",
            "Epoch [2/2], Step [10660/15231], Loss: 0.2223\n",
            "Epoch [2/2], Step [10680/15231], Loss: 0.2299\n",
            "Epoch [2/2], Step [10700/15231], Loss: 0.2266\n",
            "Epoch [2/2], Step [10720/15231], Loss: 0.2589\n",
            "Epoch [2/2], Step [10740/15231], Loss: 0.2316\n",
            "Epoch [2/2], Step [10760/15231], Loss: 0.2308\n",
            "Epoch [2/2], Step [10780/15231], Loss: 0.2652\n",
            "Epoch [2/2], Step [10800/15231], Loss: 0.2167\n",
            "Epoch [2/2], Step [10820/15231], Loss: 0.2316\n",
            "Epoch [2/2], Step [10840/15231], Loss: 0.2333\n",
            "Epoch [2/2], Step [10860/15231], Loss: 0.2494\n",
            "Epoch [2/2], Step [10880/15231], Loss: 0.2501\n",
            "Epoch [2/2], Step [10900/15231], Loss: 0.2524\n",
            "Epoch [2/2], Step [10920/15231], Loss: 0.2241\n",
            "Epoch [2/2], Step [10940/15231], Loss: 0.2291\n",
            "Epoch [2/2], Step [10960/15231], Loss: 0.2296\n",
            "Epoch [2/2], Step [10980/15231], Loss: 0.2597\n",
            "Epoch [2/2], Step [11000/15231], Loss: 0.1995\n",
            "Epoch [2/2], Step [11020/15231], Loss: 0.2287\n",
            "Epoch [2/2], Step [11040/15231], Loss: 0.2376\n",
            "Epoch [2/2], Step [11060/15231], Loss: 0.2292\n",
            "Epoch [2/2], Step [11080/15231], Loss: 0.2475\n",
            "Epoch [2/2], Step [11100/15231], Loss: 0.2308\n",
            "Epoch [2/2], Step [11120/15231], Loss: 0.2164\n",
            "Epoch [2/2], Step [11140/15231], Loss: 0.2283\n",
            "Epoch [2/2], Step [11160/15231], Loss: 0.2021\n",
            "Epoch [2/2], Step [11180/15231], Loss: 0.2163\n",
            "Epoch [2/2], Step [11200/15231], Loss: 0.2074\n",
            "Epoch [2/2], Step [11220/15231], Loss: 0.2464\n",
            "Epoch [2/2], Step [11240/15231], Loss: 0.2801\n",
            "Epoch [2/2], Step [11260/15231], Loss: 0.2665\n",
            "Epoch [2/2], Step [11280/15231], Loss: 0.2175\n",
            "Epoch [2/2], Step [11300/15231], Loss: 0.2438\n",
            "Epoch [2/2], Step [11320/15231], Loss: 0.2538\n",
            "Epoch [2/2], Step [11340/15231], Loss: 0.2417\n",
            "Epoch [2/2], Step [11360/15231], Loss: 0.2459\n",
            "Epoch [2/2], Step [11380/15231], Loss: 0.2544\n",
            "Epoch [2/2], Step [11400/15231], Loss: 0.2524\n",
            "Epoch [2/2], Step [11420/15231], Loss: 0.2734\n",
            "Epoch [2/2], Step [11440/15231], Loss: 0.2252\n",
            "Epoch [2/2], Step [11460/15231], Loss: 0.2194\n",
            "Epoch [2/2], Step [11480/15231], Loss: 0.2110\n",
            "Epoch [2/2], Step [11500/15231], Loss: 0.2449\n",
            "Epoch [2/2], Step [11520/15231], Loss: 0.2061\n",
            "Epoch [2/2], Step [11540/15231], Loss: 0.2414\n",
            "Epoch [2/2], Step [11560/15231], Loss: 0.2355\n",
            "Epoch [2/2], Step [11580/15231], Loss: 0.2208\n",
            "Epoch [2/2], Step [11600/15231], Loss: 0.2283\n",
            "Epoch [2/2], Step [11620/15231], Loss: 0.2605\n",
            "Epoch [2/2], Step [11640/15231], Loss: 0.2603\n",
            "Epoch [2/2], Step [11660/15231], Loss: 0.2433\n",
            "Epoch [2/2], Step [11680/15231], Loss: 0.2404\n",
            "Epoch [2/2], Step [11700/15231], Loss: 0.2733\n",
            "Epoch [2/2], Step [11720/15231], Loss: 0.2315\n",
            "Epoch [2/2], Step [11740/15231], Loss: 0.2816\n",
            "Epoch [2/2], Step [11760/15231], Loss: 0.2800\n",
            "Epoch [2/2], Step [11780/15231], Loss: 0.2408\n",
            "Epoch [2/2], Step [11800/15231], Loss: 0.2505\n",
            "Epoch [2/2], Step [11820/15231], Loss: 0.2349\n",
            "Epoch [2/2], Step [11840/15231], Loss: 0.2451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best architecture"
      ],
      "metadata": {
        "id": "OW0zauKLk4em"
      }
    }
  ]
}