{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasColas/CNN-VS-ViT/blob/main/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet"
      ],
      "metadata": {
        "id": "tD6I13Z_s6ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install medmnist"
      ],
      "metadata": {
        "id": "wNX1kWxxuk39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from medmnist import OCTMNIST  # Import the OCTMNIST dataset"
      ],
      "metadata": {
        "id": "dNe-jwnUuUwN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture"
      ],
      "metadata": {
        "id": "LxroXd-atMKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResidualBlock"
      ],
      "metadata": {
        "id": "47tSPB0PtFAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3,\n",
        "                 activation=nn.ReLU(inplace=True), use_batchnorm=True):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        padding = kernel_size // 2  # To keep spatial dimensions constant\n",
        "\n",
        "        # First convolution layer\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                               padding=padding, bias=not use_batchnorm)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels) if use_batchnorm else nn.Identity()\n",
        "\n",
        "        # Second convolution layer\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size,\n",
        "                               padding=padding, bias=not use_batchnorm)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels) if use_batchnorm else nn.Identity()\n",
        "\n",
        "        # Define the activation function\n",
        "        self.activation = activation\n",
        "\n",
        "        # If input and output channels differ, use a 1x1 conv to match dimensions\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.activation(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += shortcut\n",
        "        out = self.activation(out)\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jF2GPxJNs4zi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet"
      ],
      "metadata": {
        "id": "6sP9HxoVtO9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomResNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=3,         # Input channels (e.g., 3 for RGB images)\n",
        "                 num_blocks=4,          # Number of residual blocks\n",
        "                 base_channels=64,      # Number of channels for the first block\n",
        "                 kernel_size=3,         # Kernel size for convolutions\n",
        "                 activation=nn.ReLU(inplace=True),  # Activation function\n",
        "                 use_batchnorm=True,    # Whether to use BatchNorm\n",
        "                 num_classes=10         # Number of classes for final output\n",
        "                 ):\n",
        "        super(CustomResNet, self).__init__()\n",
        "\n",
        "        self.initial_conv = nn.Conv2d(in_channels, base_channels, kernel_size=kernel_size,\n",
        "                                      padding=kernel_size//2, bias=not use_batchnorm)\n",
        "        self.initial_bn = nn.BatchNorm2d(base_channels) if use_batchnorm else nn.Identity()\n",
        "        self.activation = activation\n",
        "\n",
        "        # Create a sequential container for residual blocks.\n",
        "        layers = []\n",
        "        # First block: input channels = base_channels, output channels = base_channels\n",
        "        for _ in range(num_blocks):\n",
        "            layers.append(ResidualBlock(base_channels, base_channels,\n",
        "                                        kernel_size=kernel_size,\n",
        "                                        activation=activation,\n",
        "                                        use_batchnorm=use_batchnorm))\n",
        "        self.residual_layers = nn.Sequential(*layers)\n",
        "\n",
        "        # Global average pooling and a final linear classifier.\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(base_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial_conv(x)\n",
        "        x = self.initial_bn(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.residual_layers(x)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "CGjqNFzitOow"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = CustomResNet(\n",
        "        in_channels=3,\n",
        "        num_blocks=5,               # You can choose how many blocks\n",
        "        base_channels=64,\n",
        "        kernel_size=3,              # Kernel size can be adjusted\n",
        "        activation=nn.LeakyReLU(0.1, inplace=True),  # You can choose any activation\n",
        "        use_batchnorm=True,         # Toggle batch normalization\n",
        "        num_classes=1000            # For example, for ImageNet classification\n",
        "    )\n",
        "\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "\n",
        "\n",
        "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "output = model(dummy_input)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvS7mOYitYeA",
        "outputId": "6bafe6e4-dcbf-42e0-d0c5-1a6b4c0f9193"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "CustomResNet(\n",
            "  (initial_conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (initial_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  (residual_layers): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (4): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "  )\n",
            "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=64, out_features=1000, bias=True)\n",
            ")\n",
            "Output shape: torch.Size([1, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "_YUhZm4xuNUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install medmnist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao_-qCJFuQK_",
        "outputId": "45a11d3f-d208-4296-91a3-432980beebc2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting medmnist\n",
            "  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from medmnist) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from medmnist) (4.67.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from medmnist) (11.1.0)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from medmnist) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from medmnist) (0.21.0+cu124)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->medmnist) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->medmnist) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (1.14.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (2025.3.13)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medmnist) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->medmnist) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->medmnist)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->medmnist)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->medmnist)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->medmnist)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->medmnist)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->medmnist)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->medmnist)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->medmnist) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->medmnist) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->medmnist) (3.0.2)\n",
            "Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=2ff4013ac652512ffb91d7d2082f09a0cf8af5e10eb12ce542289337eaeacde6\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, medmnist\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed fire-0.7.0 medmnist-3.0.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize for a single channel (grayscale)\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Download the OCTMNIST dataset.\n",
        "train_dataset = OCTMNIST(split='train', transform=transform, download=True)\n",
        "test_dataset  = OCTMNIST(split='test', transform=transform, download=True)\n",
        "\n",
        "print(\"Train dataset size:\", len(train_dataset))\n",
        "print(\"Test dataset size:\", len(test_dataset))\n",
        "\n",
        "# Create data loaders.\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Visualize a few training samples\n",
        "def imshow(img, title=None):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "imshow(torchvision.utils.make_grid(images[:8]), title=\"Sample OCTMNIST Images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "56d-4mfzufmN",
        "outputId": "5a5dbb6b-34dc-4a6e-efe2-04dbeec07ccd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 97477\n",
            "Test dataset size: 1000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAACDCAYAAAAtZnnAAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAYnxJREFUeJztfXm0XFWV/ldV79Xwqt6U4WUgA6NEIqAEiJFughBI0gQHcIGALSCNCglCALuF7p8Blg0CIiggDig4MCiKMkmQAIEGAmhAARFknhMyvrnqvVd1fn9kfTf77jr31q03J5xvrVpVde+5Zz57f3uf4caMMQYODg4ODg4ODsOE+EhnwMHBwcHBweGDBUc+HBwcHBwcHIYVjnw4ODg4ODg4DCsc+XBwcHBwcHAYVjjy4eDg4ODg4DCscOTDwcHBwcHBYVjhyIeDg4ODg4PDsMKRDwcHBwcHB4dhhSMfDg4ODg4ODsMKRz4cHEYQsVgM559//khnw8HBwWFY4ciHwzaPZ599Fp/73Ocwffp0pNNp7LDDDjj00ENx1VVXjXTWRgS9vb34/ve/j/322w/19fXI5XLYb7/98P3vfx+9vb3WZ4rFIq6//nocdNBBGDNmDFKpFHbccUecdNJJ+Mtf/gJgC1GK8lm5ciVef/117/+3vvUta5rHH388YrEYcrmc7/pBBx2EWCyGI444ouwZxvud73zHu7Zy5UrEYjH89re/9YWt1C/OP//8SOU56KCDAuv6hhtuQCwW8+rIwcEhGmpGOgMODgPBY489hk9+8pOYNm0aTjnlFEycOBFvvfUWHn/8cXzve9/D6aefPtJZHFZ0dnbi8MMPx0MPPYRFixbhxBNPRDwex/Lly3HGGWfgtttuw913341sNus9093djSOPPBLLly/HgQceiPPOOw9jxozB66+/jt/85jf4+c9/jjfffBO//OUvfWn94he/wH333Vd2/cMf/jC6u7sBAOl0GjfffDP+53/+pyyft99+O9LpdGBZ7rrrLqxevRqzZs2quh6i9IsjjzwSu+66q/dMR0cHTj31VHz2s5/FkUce6V2fMGFC1ek7ODhUgHFw2Ibxb//2b2b8+PFm06ZNZffWrl07/BmqEgDMsmXLBi2+L3/5ywaAueqqq8ruXX311QaA+epXv+q7vnjxYgPAXHHFFWXP9PX1mcsuu8y89dZbZff4nA2vvfaaAWCOPPJIA8D89a9/9d2/8cYbTW1trTniiCNMNpv13Zs7d66ZNm2aaW5uNkcccYQ13ssuu8y79uCDDxoA5tZbb/Wu9adfrFu3rur2uP766w0A8+c//znyMw4ODsa4aReHbRqvvPIKZs6ciaamprJ7LS0tvv/XX389Dj74YLS0tCCVSmGPPfbAtddeW/bcjjvuiEWLFmHlypXYd999kclksOeee2LlypUAgNtuuw177rkn0uk0Zs2ahaefftr3/IknnohcLodXX30V8+fPRzabxeTJk3HhhRfCRHiJ9DvvvIMvfelLmDBhAlKpFGbOnImf/exnFZ97++238dOf/hQHH3wwlixZUnZ/8eLF+OQnP4nrrrsOb7/9tvfMj370Ixx66KE488wzy55JJBI455xzMGXKlIrp2zBnzhzstNNOuOmmm3zXb7zxRixYsABjxoyxPldfX4+lS5fizjvvxFNPPVV1utX0i8EG2//NN9/EokWLkMvlsMMOO+Caa64BsGU66OCDD0Y2m8X06dPL6mbjxo0455xzsOeeeyKXy6GhoQELFy7E3/72t7K03njjDXzqU59CNptFS0sLli5dinvvvdeb/pJ44oknsGDBAjQ2NqKurg5z587Fo48+6gvT3t6OM888EzvuuCNSqRRaWlpw6KGH9qsNHBzC4MiHwzaN6dOnY/Xq1Xjuuecqhr322msxffp0nHfeebj88ssxdepUnHbaaZ5SkHj55Zdx3HHH4YgjjsDFF1+MTZs24YgjjsCNN96IpUuX4gtf+AIuuOACvPLKKzj66KNRKpV8zxeLRSxYsAATJkzApZdeilmzZmHZsmVYtmxZaB7Xrl2Lj3/841ixYgWWLFmC733ve9h1111x8skn48orrwx99p577kGxWMQXv/jFwDBf/OIX0dfXh+XLl3vP9PX14d///d9D4x4Ijj32WNxyyy0e8Vq/fj3+9Kc/4bjjjgt97owzzkBzc3O/FuRW0y+GAsViEQsXLsTUqVNx6aWXYscdd8SSJUtwww03YMGCBdh3331xySWXoL6+Hl/84hfx2muvec+++uqr+MMf/oBFixbhu9/9Lr7+9a/j2Wefxdy5c/Huu+964To7O3HwwQdjxYoV+NrXvob//u//xmOPPYb/+q//KsvPAw88gAMPPBBtbW1YtmwZLrroImzevBkHH3wwnnzySS/cV7/6VVx77bU46qij8IMf/ADnnHMOMpkM/vGPfwxthTl88DDSrhcHh4HgT3/6k0kkEiaRSJg5c+aY//zP/zT33nuv6enpKQvb1dVVdm3+/Plm55139l2bPn26AWAee+wx79q9995rAJhMJmPeeOMN7/qPfvQjA8A8+OCD3rUTTjjBADCnn366d61UKpnDDz/cJJNJs27dOu86lJv/5JNPNpMmTTLr16/35enzn/+8aWxstJaBOPPMMw0A8/TTTweGeeqppwwAc9ZZZxljjFm6dGnFZ4IQZdrlsssuM88995wBYP7v//7PGGPMNddcY3K5nOns7DQnnHCCddpl5syZxhhjLrjgAgPArF69uixewjbtUk2/IAZr2oXtf9FFF3nXNm3aZDKZjInFYuaWW27xrr/wwgtlaebzeVMsFn3pvPbaayaVSpkLL7zQu3b55ZcbAOYPf/iDd627u9vMmDHD1ydLpZLZbbfdzPz5802pVPLCdnV1mZ122skceuih3rXGxkazePHiyOV3cOgvnOfDYZvGoYceilWrVuFTn/oU/va3v+HSSy/F/PnzscMOO+COO+7whc1kMt7v1tZWrF+/HnPnzsWrr76K1tZWX9g99tgDc+bM8f7Pnj0bAHDwwQdj2rRpZddfffXVsrzJqY9YLIYlS5agp6cHK1assJbFGIPf/e53OOKII2CMwfr1673P/Pnz0draGur+bm9vB7BlyiIIvNfW1ub7DntmoJg5cyb22msv3HzzzQCAm266CZ/+9KdRV1dX8Vl6Py644IKq0qymXwwV/uM//sP73dTUhN133x3ZbBZHH320d3333XdHU1OTr/+kUinE41tEc7FYxIYNG5DL5bD77rv72n/58uXYYYcd8KlPfcq7lk6nccopp/jy8de//hUvvfQSjjvuOGzYsMHrU52dnTjkkEPw8MMPe567pqYmPPHEEz4Pi4PDUMCRD4dtHvvttx9uu+02bNq0CU8++STOPfdctLe343Of+xyef/55L9yjjz6KefPmIZvNoqmpCePHj8d5550HAGXkQxIMAGhsbAQATJ061Xp906ZNvuvxeBw777yz79qHPvQhAFu2i9qwbt06bN68GT/+8Y8xfvx43+ekk04CALz//vuB9UACQRJigyYoDQ0NFZ8ZDBx33HG49dZb8fLLL+Oxxx6rOOVCNDY24swzz8Qdd9xRtramEqL2i6FAOp3G+PHjfdcaGxsxZcoUxGKxsuuy/5RKJVxxxRXYbbfdkEqlMG7cOIwfPx7PPPOMr5++8cYb2GWXXcrikzt4AOCll14CAJxwwgll/eq6665DoVDw4r300kvx3HPPYerUqdh///1x/vnnW4m1g8NA4ciHw3aDZDKJ/fbbDxdddBGuvfZa9Pb24tZbbwWwZQHiIYccgvXr1+O73/0u7r77btx3331YunQpAJSt2UgkEtY0gq6bCAtJK4F5+MIXvoD77rvP+jnggAMCn//whz8MAHjmmWcCw/DeHnvsAQCYMWMGgC2LIIcSxx57LNavX49TTjkFY8eOxWGHHRb52TPOOANNTU1Vez+IsH4xVBhI/7noootw1lln4cADD8SvfvUr3Hvvvbjvvvswc+bMsn4aBXzmsssuC+xXPGvl6KOPxquvvoqrrroKkydPxmWXXYaZM2finnvuqTpdB4cwuHM+HLZL7LvvvgCA9957DwBw5513olAo4I477vB5NR588MEhSb9UKuHVV1/1vB0A8M9//hPAlt00NowfPx719fUoFouYN29e1WkuXLgQiUQCv/zlLwMXnf7iF79ATU0NFixY4HvmV7/61ZAuOp02bRoOOOAArFy5EqeeeipqaqKLHno/zj//fJxwwgkDyofuF6MRv/3tb/HJT34SP/3pT33XN2/ejHHjxnn/p0+fjueffx7GGJ/34+WXX/Y9t8suuwDY4uWK0q8mTZqE0047Daeddhref/997LPPPvjf//1fLFy4cCDFcnDwwXk+HLZpPPjgg1avwx//+EcAW+bUga0Wpwzb2tqK66+/fsjydvXVV3u/jTG4+uqrUVtbi0MOOcQaPpFI4KijjsLvfvc76y6NdevWhaY3depUnHTSSVixYoV1C/EPf/hDPPDAAzj55JO9rbNTp07FKaecgj/96U/WE2FLpRIuv/xyb2vuQPCtb30Ly5Yt69fBb2eeeSaamppw4YUXRgoftV+MRiQSibK833rrrXjnnXd81+bPn4933nnHt4Yln8/jJz/5iS/crFmzsMsuu+A73/kOOjo6ytJjvyoWi2XTjy0tLZg8eTIKhcKAyuTgoOE8Hw7bNE4//XR0dXXhs5/9LGbMmIGenh489thj+PWvf+0dDw4Ahx12GJLJJI444gh85StfQUdHB37yk5+gpaVlSKzgdDqN5cuX44QTTsDs2bNxzz334O6778Z5551XthZA4tvf/jYefPBBzJ49G6eccgr22GMPbNy4EU899RRWrFiBjRs3hqZ7xRVX4IUXXsBpp52G5cuXex6Oe++9F7fffjvmzp2Lyy+/3PfM5ZdfjldeeQVf+9rXcNttt2HRokVobm7Gm2++iVtvvRUvvPACPv/5zw+4TubOnYu5c+f269nGxkacccYZkadeovaL0YhFixbhwgsvxEknnYRPfOITePbZZ3HjjTeWrSH6yle+gquvvhrHHnsszjjjDEyaNAk33nijd2osvSHxeBzXXXcdFi5ciJkzZ+Kkk07CDjvsgHfeeQcPPvggGhoacOedd6K9vR1TpkzB5z73Oey9997I5XJYsWIF/vznP5f1GQeHAWOkttk4OAwG7rnnHvOlL33JzJgxw+RyOZNMJs2uu+5qTj/99LKTLO+44w6z1157mXQ6bXbccUdzySWXmJ/97GcGgHnttde8cNOnTzeHH354WVoAyrYh2rZ+cvvoK6+8Yg477DBTV1dnJkyYYJYtW1a2hRKWrZ1r1641ixcvNlOnTjW1tbVm4sSJ5pBDDjE//vGPI9VJoVAwV1xxhZk1a5bJZrOmrq7O7LPPPubKK68M3Gra19dnrrvuOvOv//qvprGx0dTW1prp06ebk046KXAbbtSttmGotNVWYtOmTaaxsTHSVttq+gUxmFttdZnCyqX7Wz6fN2effbaZNGmSyWQy5oADDjCrVq0yc+fONXPnzvU9++qrr5rDDz/cZDIZM378eHP22Web3/3udwaAefzxx31hn376aXPkkUeasWPHmlQqZaZPn26OPvpoc//99xtjtvSbr3/962bvvfc29fX1JpvNmr333tv84Ac/iFwfDg5RETNmEFbKOTg4eDjxxBPx29/+1uridnAYalx55ZVYunQp3n77beywww4jnR0HByvcmg8HBweHbRR8gR+Rz+fxox/9CLvttpsjHg6jGm7Nh4ODg8M2iiOPPBLTpk3DRz/6UbS2tuJXv/oVXnjhBdx4440jnTUHh1A48uHg4OCwjWL+/Pm47rrrcOONN6JYLGKPPfbALbfcgmOOOWaks+bgEAq35sPBwcHBwcFhWDFkaz6uueYa7Ljjjkin05g9e7bvzYkODg4ODg4OH1wMCfn49a9/jbPOOgvLli3DU089hb333hvz588PfS+Fg4ODg4ODwwcDQzLtMnv2bOy3337eCY+lUglTp07F6aefjm984xuhz5ZKJbz77ruor68ve2GSg4ODg4ODw+iEMQbt7e2YPHmy92bmIAz6gtOenh6sXr0a5557rnctHo9j3rx5WLVqVVn4QqHgO7r3nXfe8V565eDg4ODg4LBt4a233vJe4RCEQScf69evR7FYxIQJE3zXJ0yYgBdeeKEs/MUXX2w9Mnnp0qVIpVKDnT0HBwcHBweHIUChUMAVV1yB+vr6imFHfKvtueeei7POOsv739bWhqlTpyKVSiGVSsEYg0KhUPWrpMOmbEbjBp/a2lrU1tZ6//v6+tDb2+v9Hw15DqpTnTcZzog3biYSCdTV1SEej6O2thbGGOTzea9tjTHo6enxtXU1U29Gvd1ztKKmpsb3VldjjK8OZRmi/Lb9H4pwYXVb6V6pVEKhUPDKaRvXvFepDYdqLPS374TlJ5lM+sZ1b28venp6KqZXqYy2Z4dLRlSqp3g8jlQq5Xu3TDqdruiG39ZRKBR8MjuZTCKZTI5gjoYexWIR+Xw+VAcEYdDJx7hx45BIJLB27Vrf9bVr12LixIll4UkyglAoFPDEE09g8+bN3rWgQcYCx2Ixq6CWgq9UKsEYg2KxCACeEOSz8Xjc+w5KM2yw63syfgBe/PzMmDHD9/r1t956y3tdNj9hBEymJ8spFbJMUz8rFSB/x+PxsnwGpa0VRywW812rqanB5MmTsWjRIrS0tGCPPfZAZ2cnVqxYgba2NgBb2vrpp5/2/ss0bXmX+ZZljapgbfcq9S3mJSzOYrEY2la77rordtxxR+9/T08P8vm8r+/xk0gkEI/HUVNT432zPmU/5X9ZX3w+Fot58fC/blfel/1et79+Rn6YN1v9xGIxtLa24sknn0RXVxcAoKurC48//jhaW1u9sRjWX0lgOA7C6jcqiZDpMc+6H+i8EDrP8hn5+2Mf+xhmzJjhXX/55ZfxzDPPRMqjLT7mpVL4oQT7hUxTfjc3N+Nf/uVfkMlkAAC5XA6HHHIImpqaIsU/mAZEfwhef0nh448/jueff977P3PmTOyzzz4RchkdWt6xHWx9IqpREyXNoOfWrVuHFStWeOO6Ggw6+Ugmk5g1axbuv/9+fOYznwGwZaDef//9WLJkSdXxFYtFbNy4sex14rKzS4XHbymIpUCRipwfqdhtCkA+a0OU61o5amE+depU37NdXV1Yt26dNY+2zmATAkECtBL5YDo28mF7Vn5r8sEBUltbi1wuh1Qqhfr6ekyePBltbW0+D0CxWMTmzZuxcePGsrbU+dBtyW+tJHQ9Rb1uK4/Mj+1Z/u/r6wtVjnoutLe3F11dXb6yStKRSCRQLBaRSCS8MkqlKRWnFEJascoy6TqV5EOmrwmJ7Zu/a2try+qHn0KhUKa8N27ciA0bNqBYLPraURMo/pcGQ9B4rFaoMr1EIuF7Xrc/78v8yzzLOGXdd3Z2+p7r6OjAmjVrqsqj/AaCyYcOZ/vfH8Vu6+esDy0zWCc06pjfpqYmjB07tuq05PWg8RY1ziDZFTUPYTDGoK6uznetoaGh4rqHatK3GaJ6LDOcDK/jtdVHVEPaljc9NqJiSKZdzjrrLJxwwgnYd999sf/+++PKK69EZ2fngF9jbRuItv+8pis3yItAyxEoH9haiQSlHSVPNiUe9jyVmCYHQcqVZdYegGoHXSKR8LFq+VxYJw0jfMViEb29vWhvb0dXV5dVOVOocUpGxlmpLEH1yGcqDSBbnPyt6zaImFSThsSaNWvw97//vSzP+lvWg7bUg8JrD14QmdTXbaTDRsz1fVteY7EYamtrMWbMGJ+Sr6urQ6FQ8Pp5b2+vz6tRiTDaiGbYf9sYY7tKT5KWFTYjIsxbI/PWX8E82JB9V5M83te/w/qwJBv6Q3JIdHZ24v777/dNQVTqtzr9VCrlkVsZR9jz8noikfBNBTHvlfIi46mtrQ1sT2OMzzvPOpIkzAY9ViohqHwabOMgY8yGMBkqw8hwuq2rwZCQj2OOOQbr1q3DN7/5TaxZswYf/ehHsXz58rJFqNUgipIJe9ZGPrRABeAJH/kcUK7AouRJWyu642gBoJ+1EQ8JOUi0ktRhbHmyxWWLO+y5sGd1WYrFInp6erw5bxuo1Cox9rC6iwrb82FEVuepmjoJA71cQf0pSl8Pqi9dd1FJi4248Hkb+bCVXabV0NCAffbZx3PFx2IxpFIppNNpj3TEYjEUi0WfQNN9OYhg24wNW7/U9cR0dVm0t1FDp2v7VCP4hwpB41eTJhleXg8jIja5apNVvb29eOONN6zxVSKt/E6n00gmk6H9NSyOmpoaZDIZazmiEqFUKhVKJvXUA5VzGILyK+OwQbeffq6vrw/GGG+atRJ02wXJOh2up6dndJEPAFiyZEm/pllsCKsYGSaIsQZZK1LgyLlw2WkonHS8Nos4ioIKyru+Lhdl6umWIGJgI1a2tHVaNrIi6y0o70wjjAjwPhcmrV+/Hg0NDYGDMsgSCFKilfpFUNls34wjyuLmSvUSBn2f/U+XRQqYSkQkTDmEPSefjyoEbZ4Q2e9kfPytp6JSqRRmzJiBfD7veT+6urrQ19eHQqHgrZuxkSqiq6vLF7ZYLKKvr88jMPp5nfdSqYS+vj6vfuUUls0zKtfeMB5JnGzEY7QtOIzFtnhjpBING7thcWhjDoB16rO7uxvPPvusTznbCCq/beMfKFeIQURFP8//QcTB9mzYWAiKo6WlxbeupVAooL293VdHtqlDm4wH4BlrWtZIXabzIOPnmAiSK/oj9SPDFotFb6G4DlcqlZDP55HP5yvWjw0jvtslCmxCNEiJy2+tQIOYOVDumdACP8xC0N8yP/p/UBwaFGAczPr5sAERpFQlNJEJek6G12w7KC82AmaM8RRMd3d3v9myjlcKsaC2rUQ6KpVnMBEUN9ua0KQjiOCGwTZmwvpBmNCXz8p647iRAtRWv5lMxle+eDyOMWPGoK+vD/l8Hn19fejs7ERvby+6u7s9EhFEaAB461IYloJaCnJJcvXzvM9yhNULFTatX07T9PT0eHHI+uBHrmkaSkQhl/L3QPq4LJ9sH2nQSfT19eG9997zpiWCCCpQTkaI3t7ewLa0la2a8gWRDp2XsIXksVgMH/vYx3zko7e3F52dnT5lTWKsyYGNJBSLxTJZqTdKaFlLgsM0ODbCyIbtm3FzPNrCsd9zLFSLbYJ8VIsg0qCJhRQW+rkgIV0NgpR/lIYqlUqeMA2yrDVB0krCRlpsCLqv4wvKSxAhlEKdno+1a9eisbHRs3Zt9RxVkErFZ7MiZNio32HkViOs3sKe0fdpXVR6NkgoktiF5SEoXhtxjELGZNoU2HJLqQ6n3b+lUgmtra2e54Bh4vE4ksmkrz3pNaEg5f9sNou6ujpP0HIbK7/7+vq8rdvsa1KwxuNxj0yk02kkEgkkk0mfZ4OejlQqhZqaGi9cbW0t4vG4L/+ynvmdzWat9T6SCLKaK0GPEf6WCkn+1mFsxmFYOhpB03saQTIkilzRxEj+t9WbfFbfW716NZ588sky2aQ3Eej6C6srm4yT9yX5kPkNMkTCZDy/9foemedcLofdd9+9Xx6+bZZ82AZC2H+bsrF5KWyVb4s3St70s0GC3cbUbS4zWzlkGvKjPQNRhE0QEdB50PHaCAiFu3Rj9/X1oaOjAx0dHb7FtDrNoIEVRCQ1wsJF+ba1ky2fQeTS9t+WNxnWNg1VybLTbRtluqiSwLaVv5KS0G1tC2PzLHR3d6Onp6dshw0tKZaL3gWeJ8A4eYaGJB/88MyFmpqaMhIiF1zSi5FMJr11AbzGqc94fMs5FSQfNTU13nUp5G1KZCimXaqRRTZEIR9B/VQqOXk9SFaE5bnSYn6Zl6D+ZUunGqOgkn6oJAdkHvW9t956C6+++qovX5o86P6ip6/0FnobSZD3ST5ItiuVIYicyHLJ6Srdx8eMGYNdd93VWieVsM2SD8LW6GFhNWxMndclKq3Aj5quzcUXFmeY0tPxhrHbSvFrQhFGvMLIlS1N6Zrr7u5GPp8PdGEGET/t7eBnoILYlkYQodFlCvpfbZ4oXKMITZuwlERVxmlLJ+g+28O2K8YWXgtpY4zvgCWN3t5eX/44X9zd3e3FEbR7huNTCtREIuEdxCfzkkwmPc9EX18fUqkUenp60NXVhd7eXi+O3t5eJBIJZDIZ1NbWIpvNora2FvX19T7Phty1wnRk37VZiVqJjHaEEfEopESGJRGN+lwUVEtqqiH++hkbAdf9MChu3dbsd0HpadKhv/WYsOVXj1f5fKlUsq5zsemNID0k612TnaC2joptgnzYOl6Q5W2zRit1OpuyDIrLZhVHsQy1IJefoAaMosSCOlIU4qHTikI49LeN8OitfJI90zING8i6w8tyBa2B6S9kH6nUXzTBsPUD5r3aARlG6PQ1/Ywm0EHCMygtHYckQkGExRa3zXvDeGzTQj09Pd50k4wvjPjIctHbIQ9Qk4t3eTYK14XIOu3r6/Msy9raWqRSKSSTSdTV1XmekKCt5tJtrvOm22KwyHF/EWb1yvs2QyjMOAqTmQMhGjbIeg26H/Y/Sr5sckDrljCZZUtXTqcG5UH3KdsuK60jZBxy6lnGqRePyrh0vw0zMjiOguTBQNp6myAfYbBVpu5IYUoliHzoeXXNIMMswaj5Dgsfi23xFmiGrMsYdi1qXsIUnFZiUZQkw5VKJe/wLFqq3d3d6OrqQmdnZ9mg0WnIQa+JVVShqAWIrBfb8zYrwwZb3okwL4YtXZvCCvpdDXTZw+qCv3XeZN3r/OjnbMraNj6BLeOpsbERqVTK8yRwUSGnRoLOEGBcFNYyX3LcMu9yTUcstmU7Lz0e9fX1qK2tRTqd9p0jQbIUpNi0QtRjxLa1fjRAGgKVUI3xEvaMMVuO0o+yKyJMHgaR/UrPEmFrc6KM16jbVmV63E2liZyUcVJe22ST7XpQf5PPSDJjK09YWQmb/pTfAyHX2zz5ACq7xImga7qBqDzCmD/Tsf2vZG1UCs88yO1suiNpC9tGUMJcYpWshUp5DiMsUjkUCgVvzpyWALdn6R0efNam7GwEMyoRClNgGlHnlsPiDos/iDDZ2tb2CYKelw3KX1jd6P4uyZ/Op46XJ7HqsgYRxXg8jlwuh2Qy6S0OZZxcr8F1Glo4a+OA+ZLTJDJtLiTljhUqhZqaGs/TkUqlfGfM6FNXbZDh5PSMPNJ+KMhHFIUbdl327zAlHKaYw2Scrc/ZvFw6riB5WOl/lDh03qohH0SYPA0yzCQ5tZGQoPzbDBhbGfVUpQ7P9VDVIIpxouV0f7BNkA8b+7IpnyjxaGhLQDdmVEXUH8iOqfMmLT+bK47P8btSZwgSDrpeg56xpaGVIuuLHyoEzq3zkKBSqYSOjg4r+bAJLp3fIAuhUrmjhtHbBwcaf3+hLYxqiU5UQsk4bMSDv3X6tjYI2nFDJa3TIyHgCyTr6+thjPE8IFxoym9el9sT9bZa7bHQC/vkuJZEo6enB7FYzCNBuryaUDAOrhWR1yQJGm1v5q5EqIDKUyhSDui6slnbsVjMe5liGJEIIx/Sy9Vf6PJUQzyAyltt9T3KcJm2jYDIPEl9Q28g82hri6DtrrK+aMRKVGuU6HxqHdAfbHPkg5CWzUChBVIQk+yvt0AjCuuWVpVNYNhYaNActa2jVMqrJnlBzF7niZYfp4z4O5vNejsISD5sg8KWb11vMr3+9oGgegiy7KPGx3wNFLpsNmKphY0tL1H+A3aXchDJZNpynGiCoZ/R26pJPthH5Mp+Pk/S0dnZiUKh4B0qRs8IP0yHyiFIAWpywHTk9I6MSz5Lz4g2TBoaGrzttDblMtrIB4AyRa5liR7zNoub4fTUVxBJ4CJeiTAiYvs/EPIRZEgGyQHbGLYdDRCWP1s/kv1HfmzXSqWS75AxOTZ4TS40lX1aEg/bGjmtC4KMrSACquPoD0Y9+QgSfKywagqvLUi5xU8KQIaVedAdL4hABIUJWq0syYWEnHaxXec2QLkynxYYzx/YsGGDN38umbleqc/yaiFTyeUsv/k867K2thaZTAZjxoxBMpn0jtUulUoYP368d9SxTcDohVc6DZlHm+AMymvYNZmGXufQH4QN2KCwlfIfRiKiEIwoZQ6CVLiSYPK/Fu5auDU3N/s8jMViEW1tbejr6ysjH/r8Fp79kUgkkM1mPbJAT4htLNl+22QJ05JjR/YpeV+WlfKHJJvpyMOjSqUSxo4dO6QERCumKNAKj3JQnuLKNtZvQ2Zfkd4neqVoqdfV1VnzE6WP6vvaCKoEm6HFMoeF1WFs6YXJBFv8zc3N2GmnnXykTPcP7d3WBD5oHVXY4m75X/d523eYDgO2TunyuvY29hejnnwAA/N82FxF8h4HFoWbPpGQFSy3CtoaNKyhgfK1BLaG1HmTeZFESZ5JIL9pndXW1qKrqwsdHR3W+MMGpvT4SAtIh5PxSEFN4pFMJjFmzBjsueeeSCaT3sLCjo4OjBs3DrlczluMpeMLWvWt82CzzmzP6DyHgc8Phveimjzo/lyNcBiM/Nj6g6xbSSr54QFdtbW13hkYMn+S7HILK1EqlbB582bk83lPscmzNTgWZZpSKcozNmz9k+1oU468LtNkWeQY0jtopPJhvJs2bfK2jvO0VnnOSCaTQWNj46C210DB9mS5WK98eVsymfTOUKGskVuPWU6unZFH4hcKBeRyuVAvg/4fZrgRQWuxohiBMmyQhS/rht9BBCToOX1v7Nix3lo3eunYP3jGDU/35RHmkoywnSRsu6xs5QzSL0HkI+y+9JxIosT27y+2GfJRKpU860S7iG3uKmOMdyIi45AsX3s82Mj6uFsqwqAzDMI6vRaG2gVn+yYymQzGjRvn/ZeWJudP0+m0Vxau3CcJyefz6OzsRFdXF9ra2rw3yspDkYIsVnZYhiV5SKfTPvezFOK0DrmLoK6uDo2NjWhoaPDyToGdTCatJ5xKMqi9L1ow6Tl8m0vS5lGwec1s5DaIRNraTIerZtEqANTW1iKXy5Xli3EG9XWmpZWqFhC6TW2WHn/brF/+Zr8jEZBnavA7rD/rfsbDvyTBYT+S8fb29pYtJtXtoddosR51Pdi8NrLPyakbCfYH3qdHkeeHsP65eJZeGRsRtimPMMUb1Fb6iG256FV+67KyLGxHST441vkGWPlhm8t+BsBX7q6uLuRyuUClWcmYsBk0NsLAa3JaQXt0dNvKcLId2Le4vZoyThNOLoAmYeju7vbannJQgvHI9UrZbNbrHyQj8hUD8luuGdFjlvUry2zrR7b60/Wox5QkJHLM6rClUmlArw/YJsiHFBwcLHKxIiuY1+V59lKYSEtKrn4Htio0PlsoFKzMLsjalt86LOAX+joe2/qSXC6HKVOmeP9JKqSCTqfT3mBIJpMYP348kskkstksent7UVtbi87OTrz55pvo7OxER0eHb8EehTs7mN7iyHqnchw7diyam5s9y0ivpOY5CalUCvX19d4327BYLCIe37LzhQNXCyO5XkR+5HSMrGvOdbIuo3gFbMRDCj3ZJ2wCU04L2MJVeteBzmM6nUZTU1OZl8nWR2wKpVgsorW11XeSpz7RU56tYpvKkgKYW0+phKT1q8kG20vmzaYE9W4YY7Zuv9SClVY2vRHS0yHHtJwakKRC16NsM0mubG1NQiwPIyuVSj7LngtUZT3a5I02OHQfCOuHsq/alCvzKBUUF3RLOUc5wTbl2ALgLQJn3bGt0+k0MpmMT7H29vYilUohl8t59cY0mJfe3l60trZadz6x3+n60Iak7AtyKsgmK7TnRvYBeRqtJBOyT/A3+3d9fT2SySQaGxs9bxv1DIlmT08PNm3ahK6uLrz//vtob29He3s78vm8lXDX19d73iGdtvSUMe729nYUCgVs3rwZPT09Pu+1LLeNNABb+6GUZyyDTTZKY1wbcFrW2gw9aXBUi1FPPrioq6enx2OmmpHqhpADVM9rSq+HVFg2BWdjlDaGKP9LaAavn5FrMYIspKC4+TwFAwCvA1MoUeDU1dXBGINsNotEIuGxaiksZX7lupdYLIZsNuu9R4PkguSDQrpYLJadkyDLynRI6jZu3Fim2I0xnivS5l2Q7UEECa4ghHk7gupa97VK4W2EJQzJZBINDQ2++G3WfZBiL5W2vOdEkg4Ka/YFXmf9y3aPxWKey53WMP9LlzshiTTJepCS5LcsI+uI22m1tUuSSmJJ8iHHJ+/ZPlq5yPHPsmiLTU4RsT/Lb+nZkPWqCV1U8hFkccq8yLDyvvZeSSJFucc+UCwWfcfVU95oJc/nSKxYZulRY1+VBpAkbT09Pejp6bGWlYRGGjaybLJ8tnFJgiHbUU4RSW8WP5KE6f4o618SKUmGZN0A8NbuZDIZ5PN51NXVobOz01sQLV8qB2zZMs6XJErPuezn7E/UBSRKbD8aF3qtSJAnKcio4DWbJ9k2lWMjwbZ2HQhGPflIJBJoaWmxLtrSg1JaAlSGpVLJs+C08OKH4aUyZdwUUnLAagvFpnw0OEBkmtLC0kJKX7NNjZBRd3d3e0pFDs6mpibU1NR43oru7m50d3d70zDyfAVjTNmcOxVQc3MzGhoa0NzcjObmZq8+mT7jkoOKBGfTpk1e/BxsdXV1qK+v97xUslydnZ1ob2/3ueClAqokrGweKVvYoGfZ1lqB6d9BfYHPBxEb3peoq6tDS0uLF38Ua1grr3Q67bOG5Vigd6xYLJa5ipmXxsZG1NXVlXkSNEGX3i45ZaLrSf/PZrMYP368N56M2fJul87OzrK60t4d6WmSip7x6DzotpUKRk4VkezaxrLtW6bNetMkRCsH23Stbj/dlpII8VsbSIxHegfk1JFco0alIxd9y/Uy0jqW53Fw7Yo0KGR7kFRKWdTV1VXW/+PxuG8bNcPLvi37ohx/jIfeCXrmMpkM6urqkMlkPKOKbWprf/6X0yt6IaUmuQRJTEtLC9LptOetIOng+h6Njo4Ob1F10Es0KbOZRjabhTHGWxNHbzWnqbu6ujyZL73Yum/pqSdJSvUniGjouGS+B0o8gG2AfJANStYs70k2J1m1dD9Jhqm34/FDC05u3WMaEmFKS/63CS/Z2AQ7hY4jnU57rxznnDcAjyCxU7BjcvDJQSUFK0kFSRmAMuLDZznA6eWQ7lmGl+5YDgKbhSbf1krhlkgk0NHRgZqamjJFTGvGJpBlmwTVv6zLMPKhUYlsBBGQIGVny4MtfoaxkU/50f3G5hnSY0D2Y/YJuXBQxqnfZaLzLwVyENkLg37BGomSfucLxzO/bfmRfVr3BVlf/C/d84xX5kHXtSY2uh50fct1VJUsU+khYny2/DNspfqWeZVKQhJ7lpMv8QO2EjxJVAnWl1zky4Xt9fX1Po8Bp3FIaGkwyDwmEgk0NDR4edKeI6B8rR2fl6SR0yn0zKXTaU9Wyb5gq3vWu+wftrblGKA8o+chkUigra0N+Xze2wJOQsC4gmSLbFc9pknouOtLErBSqeRrJ45ZOTXIe3K86/6kCa4kI7Z+L+tMfkvocdAfbBPkgyvlaU2z4GR12iXKgcDnpZLVlgqwVfizYWXaQYsytaLR0wFaCMqdHVJAEPr5pqYmNDc3o6OjA5s3b/a9Lry1tbWsA+sFgsViEe3t7QC2LkzjGzu5upplZV3Qssjlckin0541LOf9WU8cgHreWZIiWgeyTvhyr3Xr1iGVSiGbzXplj8fjqKur81Z+y3axzWXyGTmwgkhGGLO3KVAdRl6zDW7bs7Z86PsAPEuG92ykI+jwKw2tuEg6ZP+QZEfXo23uVxNnLdSl0A8qsxSsDJPP573j9yVkf5Dfup2oHGUe9TimUqbSokDX9arLJr0LtrQZN72HtoXqNlLJvDCvJGCyjzOcnFqT7nO2oRy/erqB/VLWCRfH5nI5z5MsDTNCLhrPZrNIp9PIZrO+NR9y+prygLJJ9jtgS9+bPHmyF0aTD5sSlAQnFot561bYFnqaTXqUpR5gO9GTK+WFrc1jsZhn7NFQImll2Wz9NGjKz+a5o2zkmg/ZL7iuj9dYZrY5DchkMunJDbnbiPUry8bn5NubpSxg2W1eED2utQzS5LoabBPkQ1oukjTIxWW06Mkg5SphKSCla00LTs41S2as13xowWqbRwT882j9YYZk+5zC4CmQhULBm0Ki8OKbE6XrkwQgHo97HZP5ZYeWrzMvlUre/KncakerQgpqQgpM7cJlB5UHC7FO2D42V6R0JdqswCiKXg8eDRthkG2l05TKj2E1mZH5i0KANLSSk0KYCjOMfGilKfOgBZ+NPMvnpPdBllcKJy1kqSh02oRtEa72mNksUZkXTTS1+14qWjn+ZX4pnINIHp+zeSw0JDnWBCaorajU2f9p+erxIKca5TfvkcgDW40wTicwD/QqsY7pCZVTpCybJDhcn8GPND5kXWpiKg1DXXdS2cl2lXUkv7Vi1wfQ6WkjST4Yrxy3NhKg07UZI2GkWo9ZykyC6+TkNCfbuVAooKenB52dnV6+5TjU//V4lkYF9R/LJn/L6etK5EOvx2E+NOkYDK8HsA2QDwDeojdWqBR29HxoQQZsXeQoCYfsaNJtSEHGuXEunJIDmf/5vMyDnJ9m2lJh2wR4GIOkB4KdhztHmE/msaurC2vWrEFXVxfWrVuH7u5urFu3DrFYDOPGjfO5JJlPHtRUU1PjW1TGNR60cjKZjLejplQqobu72+ukmtxJS0iSRbppGQddsxS6msxwHYB2D2uSJ+stTClXgo3ls6215S3DSSEs51dt5EN/67aWVi5QPm2j+42tDHLhmiQZQYqR0LsNJIG0KVyGkeVmvwo658MY4x0qJ/NMb16QZ0PXh1zbAMA3RRqkBKmE6Drnf/ZD6QkKIhAyz/rbRl5s45vg+h65Vsomc6RM0QtK5VQHw1L50XKngpNKhXXQ19fn7WDj2glOY9DLwXvZbNZXTyQucucIpyB4fgUNHNmWso9Lkqeh+wI/UsbLqS0NSd7kGjZJ6oMMJrkYWcoZ7SEO+m5ubvaRj8mTJ2PSpEnelJckHjwGYcOGDSgUCt46PK7J6u7u9vqPrSxyRxE9X+xD0oCTC8nlVJqeetHEndelXGH96iUN/cU2QT50hRBaSEilJ93KrHzd4ST5YEXyWbleQg50Qg542Sl4XS/4k+5gKjjJ0jWYh3w+j46ODs+jo61iuhONMR5Z4fZF7balEGNdsKwsGzso13jIXQ5yUFMQSdIhyYe0dDjXLy0VLcQJ5o3PBFmbQQpBElNeC1Kg+ll9TeYpKP2w+IPKaAPrnOlpJRykzGwCVS/o00pRuvZt3gQbAdLQBIVtT8Gr88u+peOTZMlWbp2e7Ds0JjT5tJFROe4l6aXiBOBTSPrZSt9BpMXW9iT3crG23jkmy6jbiOMY2LoYXpJfm7JkHcpzWnhIHI2Muro6z+PJupVkQnpUZPk1+Whvb/fKGNSGQPn7Rmz9zjb2ZBjdd+Uz0hCSpMJGPmyyPGi6RMt06S3XYL1or1YikfCIOgloTU2N75yPrq4uz+vNZyTpY/61N0wuEmd/k0cqaLki69O2wLxUKnkzDjQi+Sx1bn8x6smHMcZ7BbsejHqQsQORCdICyOVy3mprwF/ZksHbyIL8SCHJxuQ8IjsuG1curCoWt+4y4HbYzs5Ob/GS7ZS47u5udHR0YMOGDXjnnXeQTqfR0NDgW/FdV1cHYMv6kFKphKamJhSLW96F0dPTg82bN3vTKVRwJFzSEySJAeB35bHzyTKTIFCASc+U7NwkNnoetlgseqviJRKJBMaMGeNZZ6VSyXfeiiRrQQKK/UP3oaBvG5mQQj0sHvk7jOTIvNkg171IZW7zAsjfUgiTvNhO8pTknfWqFRz7hEaQEpXpc+yQxOp+xT4o4+d9uZg6aIqTaVDQy7Mf5NquoLqSdSAXa8s0bK71IOJpq5Mgz5K2DHO5HCZOnOgjKVrO2AwV6ZqXHhFJRrimQm7vJLmpqalBQ0ODj2g0NTUhm81676dhfmiJM2/S28R0Ojo6vPFSKm09ZKy7uxu5XK7sOH0pt3U92+ra5l1gOLm4Xh6SJpWn/NAI4lZvbcgyX9KroOWIMca3voLeDDme9PbtjRs3+taJ8AA37tbJZDLe+T4yHZKOrq4uvP322z4PnzyHSi4UlnmWZeNZOYxb9iMta2QdSqOB5EPqNMalPZrVYNSTD8DvQrNZR+wkcp8213/09fWhrq7O24posxwlG5cMXwsEG/nQbFkqBsms2TnJTjmoyXilu07mj8RFCgFaKFIJy47JvOp5U2k5yYGlBabsmNKDZFOKsjMyH7L+5CFNUgF0dnZ6W6FlutzVRPehJJm2eVAtxPS1sN9hSlV+y2e1BR4EhrHFoUFLRwtMW15sFp60vLTHTQtxbV0zj9qK0e2pr+l8SOEkhZy0NIPaShIAKfj4kfUi88syynaRYcLaUfZ5Wxl1+2mSoolzEPmwQZMfKXMkWdFp2/Jrq08+S2XKBaTZbLZsmoWGCWWN9Liy/9BgYbzycDhJPqgU9bEI8Xjcm76V5dB1YqtPbQzK8klPN8eBLV7Wi17Po/sdp+IoZ4JIsW5HORYlWCdEbW2tRwalbpAEUuqvdDrtHVLGtpByURoUkmTLfErvjR4b0mNt06uMRxISWZ9STvUHo558xONx77RMgpUilaxNOcrBIwe4fCOm3J8tGTXDs0G5ZRTw74LR7iytfNlI3LFD9yT3ijP+5uZmX7nJOGOxGNrb273zL5gmt75p8sEOGovFvHMbZH1It7smTLJ+paDT3h+eBSAVmly9TQHGQaXdh319fWhra0Mul8OECRO8aRauipeH87S3t3seI547QAKj8yUFTBC010Be12TU9hwAn7KXdUahJr0YmtTpuABg/fr1+Oc//2nNg+1bC0aZtsyDFI6yD9iIplZs8mObirCROq3YpXu7sbERkyZN8pFsm1KV8WtlQ88HvSVyfAV5jmx9O4y8yvFkqxcbybDVFePSJEWu7dAkXip/uYNBHhMAbD0CX8oqAD7FlEwmMXbsWMTjWw5ppOdUej5kWbu6upDP571p3o6ODi9vPPWUddvT0+Nbk2DM1pf8cSGlrNfa2lpMmTKlbH2RrHP90VtyKV/keJfPSyNRHgTHNunu7vbkpyYVUrFLGcJ6pveARE2ebhuLxTzvryYfbW1t3jlH9CDTa5xOp31rMOiV5hQ6jc1p06YB2HqYH+Ur5SHP/ZAeYtkvOzo6fGe1sH71oW2S/FInkYzpaRepG4PaNApGPfmgEqWlbxMA0m0k2ZxNKJKMsMKC5ll1hWpFRUhhpoWJtuwkUzTG+HYwaM8HOyIbnh/J+CWLleSDTJgdTM5LcoDwuiRPNmEKbF2vYZuKYmclkZNbvlheDhoyev7n2hNdz3zOGOMJSUki5XSOXoCmlYJGmNUlFbS2fLVVbXtepquJh74vwS3JmnTY+py0stgXbFadJMdSqDOc7JOVyIfuD/q3LpMen8YYz/MYpT2C0tTKXP+WLueweG351m2m20/KHn4q5dkGKdS195F9WXowdd+WdaBJIf/L6dFEIuF5Oujy1/KCcXAamNM3rAdN7kgEZLo6v7pupAfVpqxkf9b1yXLr3VSSROo8yLVs2tMhF7ED/tOXWf/AVuOM6+144qjcMcW6LJVKHpEgaIyx/PRys31pRNIDLM89YjtS/mmSLY9+l2sVpZeR6WpiR/0hZQPjkeXRz9iI+Ha95qOmpgY77bQTampqyiwBwM6a5cE49Dbw7IhSqeTNp1Hgs8PxGdkAmsUD/sFkG2j81sJONjZfO03GnMvlfPFks1m0tLRg/fr1nreAHZlxkdEyv9LqN8agra0NQPmJkfKdHOyI2lpnfFL46c4dBNk5acXx7aW5XM5nyWil1draiu7ubk9AUWg2NDR4g8QY4zvxj54YxinzKAmJbitCDjASI0lq5ZoX6U3Q7cz4K3lfdN0VCgVvbtgmmG0ETVtv0oqTZdJuUWkNynBaKdja2KaQdR0EEaxMJlORjIURGlk2SQ4plG0C1qbodNy6zBq6jNKbA/iVgI7DFh/f3yHzKJWI3BmhlacmlIxftmksFvMWnvNdJTRiWHcbN270vBfylFoabRxDBNdy0LigDJH1wmdoFMiy9/b24vXXX/cW+9rIO8eYPAiOhlSxWPSmiCQxk2e5yPBy0TXvS4+TJjqUcdJra4zx9Ae9FHr9FNuBC/QnTJiAMWPGlLU186bX6bAeuFOMa0Fk/uRaQspq9gG5cFXWg/xmG0ujXMp66W2j14ptradhSUAkad2uyUcstnX/tGT+mplJsILp1pIdFYCvk8qpBy7mBLZaQtK9axPIUYUOG0x7Z7QyI6TnQ5ILmQ+tBCkw2Ek4iKTHgN+SjOhtnlKoaS+Htrb4jK0tKIxY17bBEQVaULO8bBvp+ZILU7Ub0dZmlZS9thZ0fTE+3T6VyiMhXepBedFxylXu8lsqZBmO92x9SJbTVge2PNvyZxsLsoyVyhV2TVp/Yc/JcoSRj7By2O6FGRVBpEmTNd6j1SvzqL0gklzJ+ORYkEfVa0IkF6bLNR00CDi1STJBhUglqz1klCc8TVlb/VI26LYulba+lM0m6wD/25Q5rmmkSKWvvULMnxzvtiPc6dnQ9SiJi/TaUskyjPR4aA8TAF+dybbWnjJ+y80SzKfN20QvNuucZITfmmTLcUJvjNY10ouhp11k/5SeeVkWvWYxSDZUwqgnH8ZstXK5F5rzW/IALABeOFYo38zJTsv5NskYpXDgG2GBrYqTlvi6devKBLdsMC30taBgx5VeB0lEeKIowd0snG7Sg0lOlcj7wNYtePyvFaVNsFRqAz5rIx8yDZv1TGtOd2JNQOT8NAUBrRG5vgfYujiYZxIQdGvyI70i7B+2XRkcrAC8+W0958t5Wb1zg3mTJ+1qQSTrxfaeIl2nss5tbUQFxX5gI0BaGdtIFes9KM+6P9uUuVSOMh35bTtQjm2ky2yrM50m70sBKfOjjRNb/wT8L2nT9WMrt40UaBkQRj7oNaBHgvP/lANS4eq1HjoeaSUbY7z3fTA/JByMR75JmnKVZIJp08MsF4hLI0x7NjTxsRkVxWKxzKOp60t6PuT0GY0XvW5P1oMk3pT1UubIPEnPhVTAMu+8z7wy33pMUobzGf1+F/ZPKbPkwl151gu3NNMrRnLD3ZRsZ+o8+Y4iSUB0GWjA6nEh+yyJBNcDSeOYbS7lttxxw3z0B9sE+ZCH5sgjegH4hA+VjI2Vc1DKipduPulhAbZ2SJIXphNm3RE2occOoS1NPXgJaQXIhmdcUihpJWojF9Ka4jWbwLSVI0jIMh4p0G0KQ3f0IK9HLLb1rY5UWFJYSg+GrF+y9Hg87s3L8gA1qWCZV0l+ZBswPnkeiiQfHMh6WzXj4ip2zu0G1YvekifbQ7ebFniMS163eWFkH7dBu/y1krMp3zCCafOa6H5jK7Pu9zZIa85GEHScUSDlg86zbrewPh40noLAcS0/ctsoFYEkxHL6gGB/lFMwbENJHAD4lIUke9ojKOWH9DBIRaQtdxtJtnm5qGR1vbFv0GiU3gY+x3zpOmD/lcpPTv3oBaSyPmU+tQdZ9ldb+2uyzTzrcutxI70MNnnI/DFNrStIWNg/5Ho+7fmUU0PSQCe0ESm9IuyPnDXQOoieDykr+4NRTz6KxSLeffddb2WvXEBTV1fnW6jZ1dXlWczGGO+AFTJ5eeAKG0Wug6ArkojH4573gQ2trRI2np7Lk9ZwsVj0dm9o9yIbTiukVCqF5uZmZDIZnyCQAkAu6gS2DhQ5yJmXMOuP0B00zMrV37a4ZJq09uW7XnQaicSWN0fGYjHvsJ22tjZv7pTtI60vupjZrkxXboXmGyJJYDs6OryzVqS7lP2gsbHRUwzSyiAx0ich6rUg0qLS7nRjjO/V8sBWgRmkoG3X9PWg9ghqU6lobBa6jq+/AoZxRCEZtufkt+1+mLKPkvdKdR6Wb20U2NLV8adSKTQ2NvqUHS1rhud4p/LUil4uEqcnToIGGmWhzh9JCMeQnjIg+dCkNKyuJfmwrZPjQlZdRzLNIDIZhdQxrFTm2ljS63X4HeSl1F4aSQb4vDFbdwrJ8um4GF56Y6T+CCqjbHtNEOQOHE0CGBffT6N3CukyMl7plSf50OWS9UKZ2x+MevJhjPFeXUwXvJx7lwVnB5eLn2pqapDP51EoFHzWhvRySLejrGgeosVpGsk2pQeFYWUnYGPzt5wSkeSHQkcqKeaJaWuWLBWGZu1ycAVZikEkpBL5kGEqkRBNPqSVEAbWnz4Yje2iX8JFSybIQpUWPkkaFb1cPc8w7BuafNjWfGgCq8+ikS5XAD7CpOsqyDsQBFt929q6mjgGK2w16G+82nMxFGlEedZGPsKelx4OaZUDWy1zkgG59oBxyXFPC15OvfF5eRgWIdd+BZ3NoxWjVopB4117PuR9OU77i7DxofPJcDJPOl9BSt/m9ZLp2BS89KiHwUaEguqXkMRPekDogaAco4dCEiN+s6/oKTkpm23kQ/YjmbaUcUFr1aKgKvJx8cUX47bbbsMLL7yATCaDT3ziE7jkkkuw++67e2Hy+TzOPvts3HLLLSgUCpg/fz5+8IMfYMKECf3KYLFYxLp167zzHowxnpDv7Oz0zWeRpFDZS1JBd7h+wQ5XMnOrkrRAa2pqvJe0SYYo5/oko5TnhsgBQGUq09dn7vOESoKslvPCcp+7tNRlpw9iz5VIh0aYxVgt+dDkSEILlL6+Prz++uveQTwkYKlUChMnTgQAzyPC/esdHR3e7iUuIJYkQBI9tl02m/UOapNCkeGlFcE2lgNN14selJqcaEtp/PjxZes++uMZGMznhxpDQV4GI86BeHT6g+7ubmzYsKGMOEnPB+WJnjZgOPZpuuDliyKLxSI6Ojq8k07ls3JqUnszgsiH9hJoxSXj4G+bJ4hj1IYobSDHkY5bf9vIh6xjPQWt45Nxac9FUF5t5MNGuLQhqfMo49Ntz+sECYXNeGX70YCVXnNNZGTdSBJCo4/eamnI8X8ul/NOpa4WVZGPhx56CIsXL8Z+++2Hvr4+nHfeeTjssMPw/PPPews1ly5dirvvvhu33norGhsbsWTJEhx55JF49NFHq84csHWldEdHh9cQVP78zUrlimoqat0oPPWU7iSeM2EjHyQ5HOx61bBeA6KnRAjZ8eV8L70qJCB62iUW828Z1paNjUgEEYFqyQfjGCzPh7YebL+BLW3d1taGQqHgTamxDjiI2HZsN54fIt3UMm2SU9Y9sHUHCNtP7zyQg1ILZ+ZTfst6lqRDej6kp6ypqSm0TvsL2deGQtkPBEOVn4HEOxL1xG2r0mJmXmSfk7JIkw/KKgDe/DuwlXxQBkqSAfhfQGbzVGgCou8xvJSrYcqfIIEP83xUkknai6jjj/It0woiHmHXbGsnwvIfRMSC6lXnz1aXhPSqy/Ql8WJbsb/INLWck2lL44mLYXlPe5MpX/uDqsjH8uXLff9vuOEGtLS0YPXq1TjwwAPR2tqKn/70p7jppptw8MEHAwCuv/56fPjDH8bjjz+Oj3/841VnkKxZzouywuQUS6lU8h2SI12V0pMh1wjIxYNyfkuSj+7ubt9eb6kEga3TAPoNkrpBmEdOBXE1NvORzWZ9BIR7vrmbgxa/Jh+MWyKMndu+wzqPTZDY/tvisBEjCZ3vUmnLOR+dnZ1oa2tDIpHApk2bvBMA5eI8YMv8+fjx4zF27Fivbtrb2z2iyvaU63hkWrZFV3pw2awKKRiCym3zfDC8XhXP/AxEEWpS1x+LfqCKvFLcg6noByuu4fZ85PN5tLa2lvU7uRgR8G/BlNOosdjWY9O11avXl8nzkAD/tItWfGEkRIYhkQ9T9DaFy11nLIcN8roOo6cVbKhEPrQHUhoRQUQgKH+2e1qecZEv7+sxquuX4WxGm5RNMg7bOhZdFhJUW3w6PwTrWu7ikR4P5mUgU2kDWvPBg5F4sMrq1avR29uLefPmeWFmzJiBadOmYdWqVVbywcWgBA/GkuBA0rs/qNCpeOjxkNspZSPzrBAqJjJCOScmyQe9G2R3tJDlwkamrb+DyAd/S7cap2EkSIi4LY95kHHbXHa2DsrrzLP81uE0hpJ86A5vjCk7f4BrdfhNj4jeeSLjIAGV02FBQtGW3yDyIS0SIHgbLMPS0pTuXra5rR4GqlB1W9vyVA3R3BYw0nmuVlEVi0XfYYHa3S3jldYq49TKic8aU37Ql94dBvhJN9PRclL/l+XU4ygK+WC59Q4wWz2FeWejkmpbnoJkkS5/JQISBNvYoj4I8yDr6SxbvPy2EQY9VabbS+ojG7mQdWCbWpPGrk1/DMRo6jf5KJVKOPPMM3HAAQfgIx/5CABgzZo13tsSJSZMmIA1a9ZY47n44otxwQUXBKZjzFbPh9xTHI/HvUWIkjiQeMi1GDYWLbdkykVgkrSQZFAJxuNxdHR0+BYdAvaFm5oY8ToP2unq6vLtkkgkEr5zPuhtyWazaGpq8k7Kk51IdgD5X3s0KhEPW53bflcTRg8uWzgbGZBMG4BXX/RA8YV0fFkWSQjbkbuTuC2bC5WDDjrTYB8Atp4oKT9y0AYJCzllJJ9luvp14yz7YCnSIEEWJrwHI/0oVmmla9VgpJ8nbOMpqK65gC+IfMixqz0Uuu/JXWwyvE0hSDe8Jh/8DvrocIxPltUWl7yvd9SwDPI76BrTCxt3NgQZF0FEq5IREHaP9SpBz4cmEGEERxorMm5tEMlntWdMlpX1Xol8aOIi86rjl2nYvLhR0W/ysXjxYjz33HN45JFH+p04AJx77rk466yzvP9tbW2YOnWq95+MnkqIZIC/jdm6nZXf9H6QtckGlQ0o97FLsiDJB9OSA113BrnTgdYL0+d2NzaYXIsiO9j48eN99SJ3XlDRBnV+zfCDBGEU4mGLM+x+0H9eq5RfDT1YWF887ld6rLhgVJ6vQWJCAsJ2kS8j065KmQ+5tcwmvKSS0NMx7Bu5XM6bprOt+dBerkpCr1roNo7S9oOZfjUYSQIRdSwMZh60jAkjH3qOXoaXyk56NWx9yUYkopAPfT+ovGFhZbn12pUo3xqVPCe2fOlwkkBVW+4w8mEjfNLzIRW3LZ4wL4cMo0mM/K8NUNv0MtPXfc22s4ZxBLUPZWl/0C/ysWTJEtx11114+OGHMWXKFO/6xIkT0dPTg82bN/u8H2vXrvV2K2hwN0MQjPHPF8ptRfIsDTkNIxd/suIlgZC7FqTngQyd6UrPhFZIfJYKhqcUcj5WTwXpxtcdS0498X48Hvfea8J3wTAPssPZOr2Oy9Yxw+rc9rvaa9XGbcxWLxeFsV7oq0lle3u7jyTW19cjl8t59SLP/9Bz2ra5X8A+7cK2trF/XQ6uC9LeMfZX2yFjQ4mRIBWjHVGt56hxRa3jUqlkXRjN/iXjk7IIgG8M6D4oZYGc3gtSPjrPNnJiky1afkQhHjQi9PqAwSAgUdoxSObYppQqlaMa8kGD2EY+guLQho2U29KQ4T2p82x506/IkGnJvNjuSwNf51PG319UJQWNMTj99NPx+9//HitXrsROO+3kuz9r1izU1tbi/vvvx1FHHQUAePHFF/Hmm29izpw5/c4kT8ej5coGkd4K275p3SCSeMiKluRDd0Bdudqlyd0YUnhoT4pcFMt45TfLKMHGl8esB8FGPrRylIMjjIBUsmqi5iFKWBsb5+FIPFaeZE5ae3JtDiEFsNxeyy3LerBJAiI/nO7S3jIpEGx1Z6snST74n+tAhhKSIEe5vi1jMMoynORMe96Yf8o2m+eD0H0SsCs0Lf+08tEelijfOn7b/7Bn6JFmnsO+9e+w/1HbP4xMRS1DteTD5u0J8mbIOGRc7BPyno006LVB8r4kICyHbRG97o9RZP+weT4WL16Mm266Cbfffjvq6+u9dRyNjY3IZDJobGzEySefjLPOOgtjxoxBQ0MDTj/9dMyZM6dfO10IPc0iPRg2Rh+kcOVHL9y0VTTXCTCMBpWhnA6SuzGYV879yY4YRIo0kskk6uvrkclkfG4wfutOJ+ujkiURpJwqXQu7rqGFXCWwTDzDIJ/P+6azZNmk54nfnZ2d3i4iubOJYcLKzW/bSYW6PDK8HNRSQOipNZaNnjJbHgYTQSQzjHwOFWxCfziV/miBJAWaeOv/QeSDsJFiGQ/TsRlmtn4sfwcpHpsC1Pdt17RRZ5PRLIP81tdt16LkhfUZltfBNLAA/zkfmjRqaL0lZbwkBAzL/Ng8HzIdTUCDDFBJPpi+NKaD6mEgcqQq8nHttdcCAA466CDf9euvvx4nnngiAOCKK65APB7HUUcd5TtkbCCQlSk7kW2OU3dIXfG2/2GWLBuPnci23YvkIRaL+aYJZFzyADLb8zZyA8DbhsujvRmfFE5hpKESAamEsAFXaTBWIjdBCklPndkEpTw+XQ7s7u5utLW1eYez8aA2OQ0iiYEWClpI2gSnLb+6D4aRD9uC0yj1WS1s1ljQ9ZEiA/1NczDJ00DjqqYMur/ovmYLF5ROUD+Tz9ss4zDyGZaelilhZbRd03HbiEdUxRZVnsky2xbiDiVIDgD7YlCJIO+GrHMtCwFUJB9BfSBI5zF9vb5ShgmSK9Wi6mmXSkin07jmmmtwzTXX9DtTtnTl4h2tjGTH1vNWOt82BmqLU6YrCY/sIDI9eQqmXKMg4632FLhYLObzfMiXBNnmctlJbINMWxtRMZBONliWhE0gym2IclqOZZNHrsvFwoT0lmiGbyO3Mm1df9rzodeIyHDSGupPXYRhJMhD1LRHMm+jCZrQhpEPm1HFe7Y49X0bKQ6KYyDliRJfmNVfKe7+5sn2OyqBGgxQTmu5q9tBXtfkg9flkRAS2rsl46+GJEjioY0qvfZIxjli53wMF8IWzGiL1cacgyyGoP8yPt0AkqECKFtMyukC/e4WoDr3FdknyUddXZ33plapgGWnsZU3yNqweYiGAjayU62lGNbGkpDS85RMJr320me3SKKmT5CV5yXItq/Uf3SZSD71dBH7sF7fM9C6D3o+SNDq64OpiKJY1BrVWlL9IdGjDTbCoNd8BClOXVe252xjpdr8Rf0flYBESTNqm1bq81EJWrVenLD+HZSmjRQEpSFluU1G2/oMvzUGQvi0/Atq/+2efFCZa4teN5J2LTEMKzJs9a4WxBzQtJql18ImCCTBkHHJRV5yEY9WbGHTLul02jvjRE+9yDIw/jB3u0ZQpxpMwW7LS5BglAt0WSbmJ2wwMT4SB3n2h1xMzLiCBq4cbLbFyUD5Ow50OSXBkWHYP23re4aKgAzX80D1JKK/aQ+WsgOGf/FtUL+zjd9KBEBes8mSIMUZpd5sbv+g8ti+beGipFuJpAblLyg/UeKtFE7LjUp5C3tWxmFrH9aT1BXyWelJAYJf9cDn+LEZUjovWg5q7341cjwKthnyYSMOem88YJ9iYcVq74kMo93jMi1ayYxfs0KGoQCRjUbvBHdtyLzIBg8jH5lMBul02vdOG1lW5l8KL1tdjBaECSi5yFe2DdtaH1nO5+RglLuf5CAmbHUhSQHv6/NAeE9uwbZZJ9z6LckHiSzvDyYGIgAGOx/9uReE/pKZ0Q4pPyS0YrEpUJvLXhJm3V8Bu4yLWq9hciMq8eC9gRAAW/5t5RlMYqrj0WlWynOU+PS9sDSiePtt4cPCSrlpM9wrlS9Ib0XBNkE+5IDSFaJdjgwjO6FWFLYG0OzOlo5+1uY+kwJEfvQK90qNxrhqa2vR1NSEbDbrEQ8+K5Ug4FeWuhxBgzUsfRtsAyNsQOp2k4RQo6amBpMmTUJdXZ3vnBS9oErmT9cz25nPxmKxQDJjW5Mh49aeD22NBK2e11sptWcuqO2HQtFGUR7DgTCLsppn+hMmCCNByCsp4SDlIGVK0DgOI/VA+GnD1cBGkKK0ZTVp2sJqIyCMpOnrYTKqkkcoSh3bZJrWB2HTLWFp2vKqw1Uqc9h9LUP1/aEyArYJ8mF7RwEQfSDHYjGf58IWrtLL2eQppTqcjFMvziGivNfAlq9MJoMJEyagqanJW5sgT0mlV8aYrVMOzEPYGpMoCBu8UQV3UBy2KZTa2lrssssu3uu3e3t70d7e7r3WWb6pU5Is+Vt6LfR6HKapt+fKtTmSKNrIhRZ6Mv4gi00Kf1tfq1Yw2zBUijSoTFHDyntBlt5IYjDTjyqkoyi5sPvVKjLeH6yyVmvIDEb6UQhGlPC2vjtUJFfKEO2VDopTyhjtVdXP2IhHULmjlkdft3nNBgvbBPkA7I0StdP0x90YlLbtni1eWyNqRRzGqDUb5dkQqVTKd9S6zZIOOs8iKK2gsEFsHoCVDAY9KxU+CVQikUAmk/ERpFQqhb333huJRMJ7HwtJB8lIoVDw3o/Bb/lCQZ6rIregyY8UCLxPzwjbWZ92K8saZTAy7SBXd3+VTxAGWzCE9XNbP670bLVpRcFglHmwCV818UVVEvp+NVZ6tXmKikpyZaj6d388BdXEEQXVtjHlQJDxrMHxZZsu1s/LOMIISDXlqGRARY0nCrYZ8gHYCUhQOP09UGFVjbURxmjD4rOBaxgSiQTq6+vR09ODdDrtbS81xvjWgcRiW3Z8MI2gurBZApppawKkLfiwjie9DDxllKeHJhIJpFIpNDY2+shHNpvFokWL0NLS4k27kITk83mPhPT09KC1tRXd3d1obW1FV1eX9ybczZs3o62tzSMkPJq/p6cH3d3d6Onp8YgMXz2ez+c9ElJTU4OGhgYAW3cyaQ+K3kqrdzWRGNnWAQ2VC9Nh9GF7bufBkKlDCRsJG+78SkOHaVc6OVRO0YYZdra0bN+Vwke5HpaX7Z58UPBL4hGFBQdZa/1BJc9HpeuVPB+29DhdREU9bdo05HI57xXznZ2dXhi5hVOvAdHkwZY3mSZ/B52oyI8+K0Ouo6CXo66uziMfXDxL8pFMJstO+pRvGqYFUCptfXlcJpNBX18fstksenp6MGbMGI9ccJqmq6vL84SEkQ9JQvQL5VifvC6ncfS5Hrp+JTGR4aK0u0P/MBL1Wim94VR2I9GnbJ7bwcwH2zRqnGFen5EgSjrvtvIElS2o7DbDMawdBqq3KmG7Jx+EzZMRxa092J6P/jSczfNBBE15UEnX1NRg+vTp+MQnPoHW1lasXbsWnZ2d2LBhg+eF6OvrQ3t7u08x5vN5n7IM2hMuyYQkF+l02ndiK7ev8mWAqVQKmUzG+yaZSCQS3jtpxo8f7529wdNa6QFpa2vDI488go6ODl9e5C6RVCoVac6TH5IETrPQQ9Tb24t8Pu+RD/3NI/A7OzvxxhtveKSlt7cXHR0d6Ovr876lx4RTP0zLGONtjdanD/IzkBXiDnaMRkI3WHkarYQ1TN4OZvz9CTOaPB9Sb0RdAlBpu7T+rdMNSqNa3UUDcLDIisQ2QT6CGGTYuonBHqyV1mhUuh4kQIIYrlRS8XgcuVwOU6ZMwZgxY9Dc3Ix8Po9NmzZ596kc5erqQqHgxSO/dXp6eoAfEgZe51ZfvjeFR5fX1tZ6RIXHwJOkNDQ0eO9XkdMufHGcnHbR6ylsaytsnhhZZ/ScyNNEU6kU+vr6kMlk0Nvbi2w26yMPnHbp6elBPp9HNpv1SItcd0Ly0dnZ6fump4XxsH7pMWE8ci3IYGIkBKvDVlQ75gdiXUZVYENJVoba4xEl3Wqvj8T4GEiasu9oT0eluLWRXA05DPO0BIXvb9uPevJhc2nbiId+Rn4PZd6i3rMpiLAy0O3PNRMTJ0701iLE43HPUufCzVKphO7ubu8+AN80Ab0CNstbkw9NQmR++a29JXr9A6dL5M4chmOaJAsSLK+uO9sbim15lMel63bQ/YL9Su4SYh55nfVKckcS0tvbi7a2Nm+dSaFQQEdHBwqFAt555x20tbWhq6sLhUIB7e3t6O7u9jwvXJOjyzgQbOsEZKDW/Uh5B6qRAZXCV5PmaGhr23ji92C3RTUejqBwQ11vQcalzZiy6TDtzQ8zWOV3EGKx4HeGRcVQ1tmoJx+JRAItLS2+OfRKlRFmKfQX1S64CXOX2f7X19f7/nd0dODtt9/2lCmnE7Rij8fj6O7u9nk6qDy1x0NPvzAfQdMuUdi1bR0IrzOPwFZCJO9v3rzZdzxvX18f1qxZ4zuEi/kMIx86X9W0t64feniYZqlU8jwYcrpFbvtlubjlua6uDqVSyTsOv7a21iMevb29qKur8+Whvr4eU6ZMiZznoUBUd3B/n2tubvbqFdhypsvEiRORSqWqzGlw+iOBMEUbi8XKxnV9fT122GGHAaVZrSwaKgQR+7Fjx/ramjJ8oG1dKQ9h16qVC/1BLpfz/a+vr8fkyZOtebDVnZapNsPbpgPD+oPN2AxDJb2lMWbMmKrfWebFbUbCXAhBW1sbGhsb8Y1vfAOpVMpTqtv7PDmnMuT/ZDLpC6O9P4StQ9tQ6f5A3YSV0tRhSqWSr225zkSfTzLS0EJA/9e7f2wLfeV9rp0hOMUzEAyVYA2zvKKEJbhVXPbVfD6/3Y/rZDLpG9fy7dbDheEW8ba2/iDIcE5TE2zrkSbIlTAQvSDfLA5s2en37W9/G62trZ6nPgij3vNBhfRBw0gIqZGGMcabOtqWUS150sTzg4BYLIZMJjPS2Rh2fFDb+oMowz+IbV0NRpeJ6eDg4ODg4LDdY9R5PugCKhQKI5wTBwcHBwcHh6ig3o4y1Tfq1ny8/fbbmDp16khnw8HBwcHBwaEfeOuttyouoh915KNUKuHFF1/EHnvsgbfeeqviohWH4UVbWxumTp3q2maUwbXL6IVrm9EJ1y6DD2MM2tvbMXny5Ipr30bdtEs8Hve2ojU0NLhOMUrh2mZ0wrXL6IVrm9EJ1y6Di8bGxkjh3IJTBwcHBwcHh2GFIx8ODg4ODg4Ow4pRST5SqRSWLVs2JCfiOQwMrm1GJ1y7jF64thmdcO0yshh1C04dHBwcHBwctm+MSs+Hg4ODg4ODw/YLRz4cHBwcHBwchhWOfDg4ODg4ODgMKxz5cHBwcHBwcBhWOPLh4ODg4ODgMKwYleTjmmuuwY477oh0Oo3Zs2fjySefHOksfaBw/vnnIxaL+T4zZszw7ufzeSxevBhjx45FLpfDUUcdhbVr145gjrdfPPzwwzjiiCMwefJkxGIx/OEPf/DdN8bgm9/8JiZNmoRMJoN58+bhpZde8oXZuHEjjj/+eDQ0NKCpqQknn3wyOjo6hrEU2x8qtcuJJ55YNoYWLFjgC+PaZfBx8cUXY7/99kN9fT1aWlrwmc98Bi+++KIvTBT59eabb+Lwww9HXV0dWlpa8PWvfx19fX3DWZTtHqOOfPz617/GWWedhWXLluGpp57C3nvvjfnz5+P9998f6ax9oDBz5ky899573ueRRx7x7i1duhR33nknbr31Vjz00EN49913ceSRR45gbrdfdHZ2Yu+998Y111xjvX/ppZfi+9//Pn74wx/iiSeeQDabxfz585HP570wxx9/PP7+97/jvvvuw1133YWHH34YX/7yl4erCNslKrULACxYsMA3hm6++Wbffdcug4+HHnoIixcvxuOPP4777rsPvb29OOyww9DZ2emFqSS/isUiDj/8cPT09OCxxx7Dz3/+c9xwww345je/ORJF2n5hRhn2339/s3jxYu9/sVg0kydPNhdffPEI5uqDhWXLlpm9997bem/z5s2mtrbW3Hrrrd61f/zjHwaAWbVq1TDl8IMJAOb3v/+9979UKpmJEyeayy67zLu2efNmk0qlzM0332yMMeb55583AMyf//xnL8w999xjYrGYeeedd4Yt79szdLsYY8wJJ5xgPv3pTwc+49plePD+++8bAOahhx4yxkSTX3/84x9NPB43a9as8cJce+21pqGhwRQKheEtwHaMUeX56OnpwerVqzFv3jzvWjwex7x587Bq1aoRzNkHDy+99BImT56MnXfeGccffzzefPNNAMDq1avR29vra6MZM2Zg2rRpro2GGa+99hrWrFnja4vGxkbMnj3ba4tVq1ahqakJ++67rxdm3rx5iMfjeOKJJ4Y9zx8krFy5Ei0tLdh9991x6qmnYsOGDd491y7Dg9bWVgDAmDFjAESTX6tWrcKee+6JCRMmeGHmz5+PtrY2/P3vfx/G3G/fGFXkY/369SgWi75GB4AJEyZgzZo1I5SrDx5mz56NG264AcuXL8e1116L1157Df/6r/+K9vZ2rFmzBslkEk1NTb5nXBsNP1jfYeNlzZo1aGlp8d2vqanBmDFjXHsNIRYsWIBf/OIXuP/++3HJJZfgoYcewsKFC1EsFgG4dhkOlEolnHnmmTjggAPwkY98BAAiya81a9ZYxxTvOQwOakY6Aw6jDwsXLvR+77XXXpg9ezamT5+O3/zmN8hkMiOYMweHbQOf//znvd977rkn9tprL+yyyy5YuXIlDjnkkBHM2QcHixcvxnPPPedbr+YwejCqPB/jxo1DIpEoW3m8du1aTJw4cYRy5dDU1IQPfehDePnllzFx4kT09PRg8+bNvjCujYYfrO+w8TJx4sSyxdp9fX3YuHGja69hxM4774xx48bh5ZdfBuDaZaixZMkS3HXXXXjwwQcxZcoU73oU+TVx4kTrmOI9h8HBqCIfyWQSs2bNwv333+9dK5VKuP/++zFnzpwRzNkHGx0dHXjllVcwadIkzJo1C7W1tb42evHFF/Hmm2+6Nhpm7LTTTpg4caKvLdra2vDEE094bTFnzhxs3rwZq1ev9sI88MADKJVKmD179rDn+YOKt99+Gxs2bMCkSZMAuHYZKhhjsGTJEvz+97/HAw88gJ122sl3P4r8mjNnDp599lkfObzvvvvQ0NCAPfbYY3gK8kHASK941bjllltMKpUyN9xwg3n++efNl7/8ZdPU1ORbeewwtDj77LPNypUrzWuvvWYeffRRM2/ePDNu3Djz/vvvG2OM+epXv2qmTZtmHnjgAfOXv/zFzJkzx8yZM2eEc719or293Tz99NPm6aefNgDMd7/7XfP000+bN954wxhjzLe//W3T1NRkbr/9dvPMM8+YT3/602annXYy3d3dXhwLFiwwH/vYx8wTTzxhHnnkEbPbbruZY489dqSKtF0grF3a29vNOeecY1atWmVee+01s2LFCrPPPvuY3XbbzeTzeS8O1y6Dj1NPPdU0NjaalStXmvfee8/7dHV1eWEqya++vj7zkY98xBx22GHmr3/9q1m+fLkZP368Offcc0eiSNstRh35MMaYq666ykybNs0kk0mz//77m8cff3yks/SBwjHHHGMmTZpkksmk2WGHHcwxxxxjXn75Ze9+d3e3Oe2000xzc7Opq6szn/3sZ8177703gjnefvHggw8aAGWfE044wRizZbvt//t//89MmDDBpFIpc8ghh5gXX3zRF8eGDRvMsccea3K5nGloaDAnnXSSaW9vH4HSbD8Ia5euri5z2GGHmfHjx5va2lozffp0c8opp5QZUK5dBh+2NgFgrr/+ei9MFPn1+uuvm4ULF5pMJmPGjRtnzj77bNPb2zvMpdm+ETPGmOH2tjg4ODg4ODh8cDGq1nw4ODg4ODg4bP9w5MPBwcHBwcFhWOHIh4ODg4ODg8OwwpEPBwcHBwcHh2GFIx8ODg4ODg4OwwpHPhwcHBwcHByGFY58ODg4ODg4OAwrHPlwcHBwcHBwGFY48uHg4ODg4OAwrHDkw8HBwcHBwWFY4ciHg4ODg4ODw7Di/wOwfapqiSOfCgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CustomResNet(\n",
        "    in_channels=1,\n",
        "    num_blocks=5,               # adjust number of blocks as desired\n",
        "    base_channels=64,\n",
        "    kernel_size=3,\n",
        "    activation=nn.LeakyReLU(0.1, inplace=True),  # choose your activation function\n",
        "    use_batchnorm=True,\n",
        "    num_classes=4               # OCTMNIST has 4 classes\n",
        ")\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 5  # adjust as necessary\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 20 == 19:  # print every 20 mini-batches\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/20:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "# Evaluation on test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.squeeze().to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRBseAzTu7U7",
        "outputId": "87eb86ae-3c46-4e38-bd38-c5f296bade67"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomResNet(\n",
            "  (initial_conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (initial_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "  (residual_layers): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (2): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (3): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "    (4): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      (shortcut): Identity()\n",
            "    )\n",
            "  )\n",
            "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n",
            "Epoch [1/5], Step [20/1524], Loss: 0.9820\n",
            "Epoch [1/5], Step [40/1524], Loss: 0.7545\n",
            "Epoch [1/5], Step [60/1524], Loss: 0.7776\n",
            "Epoch [1/5], Step [80/1524], Loss: 0.7208\n",
            "Epoch [1/5], Step [100/1524], Loss: 0.6921\n",
            "Epoch [1/5], Step [120/1524], Loss: 0.6211\n",
            "Epoch [1/5], Step [140/1524], Loss: 0.6242\n",
            "Epoch [1/5], Step [160/1524], Loss: 0.6119\n",
            "Epoch [1/5], Step [180/1524], Loss: 0.6272\n",
            "Epoch [1/5], Step [200/1524], Loss: 0.5823\n",
            "Epoch [1/5], Step [220/1524], Loss: 0.5832\n",
            "Epoch [1/5], Step [240/1524], Loss: 0.5438\n",
            "Epoch [1/5], Step [260/1524], Loss: 0.5417\n",
            "Epoch [1/5], Step [280/1524], Loss: 0.5433\n",
            "Epoch [1/5], Step [300/1524], Loss: 0.5134\n",
            "Epoch [1/5], Step [320/1524], Loss: 0.5574\n",
            "Epoch [1/5], Step [340/1524], Loss: 0.5189\n",
            "Epoch [1/5], Step [360/1524], Loss: 0.5195\n",
            "Epoch [1/5], Step [380/1524], Loss: 0.4882\n",
            "Epoch [1/5], Step [400/1524], Loss: 0.5696\n",
            "Epoch [1/5], Step [420/1524], Loss: 0.4952\n",
            "Epoch [1/5], Step [440/1524], Loss: 0.4925\n",
            "Epoch [1/5], Step [460/1524], Loss: 0.5114\n",
            "Epoch [1/5], Step [480/1524], Loss: 0.5073\n",
            "Epoch [1/5], Step [500/1524], Loss: 0.4545\n",
            "Epoch [1/5], Step [520/1524], Loss: 0.4752\n",
            "Epoch [1/5], Step [540/1524], Loss: 0.4511\n",
            "Epoch [1/5], Step [560/1524], Loss: 0.4408\n",
            "Epoch [1/5], Step [580/1524], Loss: 0.4763\n",
            "Epoch [1/5], Step [600/1524], Loss: 0.4921\n",
            "Epoch [1/5], Step [620/1524], Loss: 0.4517\n",
            "Epoch [1/5], Step [640/1524], Loss: 0.4592\n",
            "Epoch [1/5], Step [660/1524], Loss: 0.4493\n",
            "Epoch [1/5], Step [680/1524], Loss: 0.4572\n",
            "Epoch [1/5], Step [700/1524], Loss: 0.4463\n",
            "Epoch [1/5], Step [720/1524], Loss: 0.4434\n",
            "Epoch [1/5], Step [740/1524], Loss: 0.4408\n",
            "Epoch [1/5], Step [760/1524], Loss: 0.4433\n",
            "Epoch [1/5], Step [780/1524], Loss: 0.3980\n",
            "Epoch [1/5], Step [800/1524], Loss: 0.4026\n",
            "Epoch [1/5], Step [820/1524], Loss: 0.3937\n",
            "Epoch [1/5], Step [840/1524], Loss: 0.3869\n",
            "Epoch [1/5], Step [860/1524], Loss: 0.4023\n",
            "Epoch [1/5], Step [880/1524], Loss: 0.3886\n",
            "Epoch [1/5], Step [900/1524], Loss: 0.3812\n",
            "Epoch [1/5], Step [920/1524], Loss: 0.3929\n",
            "Epoch [1/5], Step [940/1524], Loss: 0.4276\n",
            "Epoch [1/5], Step [960/1524], Loss: 0.3894\n",
            "Epoch [1/5], Step [980/1524], Loss: 0.3878\n",
            "Epoch [1/5], Step [1000/1524], Loss: 0.4151\n",
            "Epoch [1/5], Step [1020/1524], Loss: 0.4301\n",
            "Epoch [1/5], Step [1040/1524], Loss: 0.3753\n",
            "Epoch [1/5], Step [1060/1524], Loss: 0.3723\n",
            "Epoch [1/5], Step [1080/1524], Loss: 0.3702\n",
            "Epoch [1/5], Step [1100/1524], Loss: 0.3588\n",
            "Epoch [1/5], Step [1120/1524], Loss: 0.3267\n",
            "Epoch [1/5], Step [1140/1524], Loss: 0.4106\n",
            "Epoch [1/5], Step [1160/1524], Loss: 0.3642\n",
            "Epoch [1/5], Step [1180/1524], Loss: 0.3947\n",
            "Epoch [1/5], Step [1200/1524], Loss: 0.4039\n",
            "Epoch [1/5], Step [1220/1524], Loss: 0.3585\n",
            "Epoch [1/5], Step [1240/1524], Loss: 0.3790\n",
            "Epoch [1/5], Step [1260/1524], Loss: 0.3396\n",
            "Epoch [1/5], Step [1280/1524], Loss: 0.3252\n",
            "Epoch [1/5], Step [1300/1524], Loss: 0.3718\n",
            "Epoch [1/5], Step [1320/1524], Loss: 0.3777\n",
            "Epoch [1/5], Step [1340/1524], Loss: 0.4118\n",
            "Epoch [1/5], Step [1360/1524], Loss: 0.3209\n",
            "Epoch [1/5], Step [1380/1524], Loss: 0.3549\n",
            "Epoch [1/5], Step [1400/1524], Loss: 0.3611\n",
            "Epoch [1/5], Step [1420/1524], Loss: 0.3539\n",
            "Epoch [1/5], Step [1440/1524], Loss: 0.3668\n",
            "Epoch [1/5], Step [1460/1524], Loss: 0.3336\n",
            "Epoch [1/5], Step [1480/1524], Loss: 0.3997\n",
            "Epoch [1/5], Step [1500/1524], Loss: 0.3508\n",
            "Epoch [1/5], Step [1520/1524], Loss: 0.3739\n",
            "Epoch [2/5], Step [20/1524], Loss: 0.3818\n",
            "Epoch [2/5], Step [40/1524], Loss: 0.3387\n",
            "Epoch [2/5], Step [60/1524], Loss: 0.2972\n",
            "Epoch [2/5], Step [80/1524], Loss: 0.3676\n",
            "Epoch [2/5], Step [100/1524], Loss: 0.3363\n",
            "Epoch [2/5], Step [120/1524], Loss: 0.3277\n",
            "Epoch [2/5], Step [140/1524], Loss: 0.3452\n",
            "Epoch [2/5], Step [160/1524], Loss: 0.3281\n",
            "Epoch [2/5], Step [180/1524], Loss: 0.3278\n",
            "Epoch [2/5], Step [200/1524], Loss: 0.3318\n",
            "Epoch [2/5], Step [220/1524], Loss: 0.3626\n",
            "Epoch [2/5], Step [240/1524], Loss: 0.2792\n",
            "Epoch [2/5], Step [260/1524], Loss: 0.2888\n",
            "Epoch [2/5], Step [280/1524], Loss: 0.3654\n",
            "Epoch [2/5], Step [300/1524], Loss: 0.3184\n",
            "Epoch [2/5], Step [320/1524], Loss: 0.3083\n",
            "Epoch [2/5], Step [340/1524], Loss: 0.3074\n",
            "Epoch [2/5], Step [360/1524], Loss: 0.3114\n",
            "Epoch [2/5], Step [380/1524], Loss: 0.3439\n",
            "Epoch [2/5], Step [400/1524], Loss: 0.3183\n",
            "Epoch [2/5], Step [420/1524], Loss: 0.3403\n",
            "Epoch [2/5], Step [440/1524], Loss: 0.3226\n",
            "Epoch [2/5], Step [460/1524], Loss: 0.3049\n",
            "Epoch [2/5], Step [480/1524], Loss: 0.3455\n",
            "Epoch [2/5], Step [500/1524], Loss: 0.3349\n",
            "Epoch [2/5], Step [520/1524], Loss: 0.3399\n",
            "Epoch [2/5], Step [540/1524], Loss: 0.2916\n",
            "Epoch [2/5], Step [560/1524], Loss: 0.3064\n",
            "Epoch [2/5], Step [580/1524], Loss: 0.3259\n",
            "Epoch [2/5], Step [600/1524], Loss: 0.2953\n",
            "Epoch [2/5], Step [620/1524], Loss: 0.3223\n",
            "Epoch [2/5], Step [640/1524], Loss: 0.3462\n",
            "Epoch [2/5], Step [660/1524], Loss: 0.3355\n",
            "Epoch [2/5], Step [680/1524], Loss: 0.3122\n",
            "Epoch [2/5], Step [700/1524], Loss: 0.2844\n",
            "Epoch [2/5], Step [720/1524], Loss: 0.3271\n",
            "Epoch [2/5], Step [740/1524], Loss: 0.3219\n",
            "Epoch [2/5], Step [760/1524], Loss: 0.3181\n",
            "Epoch [2/5], Step [780/1524], Loss: 0.3162\n",
            "Epoch [2/5], Step [800/1524], Loss: 0.3239\n",
            "Epoch [2/5], Step [820/1524], Loss: 0.2774\n",
            "Epoch [2/5], Step [840/1524], Loss: 0.3283\n",
            "Epoch [2/5], Step [860/1524], Loss: 0.3158\n",
            "Epoch [2/5], Step [880/1524], Loss: 0.2972\n",
            "Epoch [2/5], Step [900/1524], Loss: 0.2919\n",
            "Epoch [2/5], Step [920/1524], Loss: 0.3024\n",
            "Epoch [2/5], Step [940/1524], Loss: 0.3196\n",
            "Epoch [2/5], Step [960/1524], Loss: 0.3454\n",
            "Epoch [2/5], Step [980/1524], Loss: 0.3172\n",
            "Epoch [2/5], Step [1000/1524], Loss: 0.3096\n",
            "Epoch [2/5], Step [1020/1524], Loss: 0.3038\n",
            "Epoch [2/5], Step [1040/1524], Loss: 0.3565\n",
            "Epoch [2/5], Step [1060/1524], Loss: 0.2914\n",
            "Epoch [2/5], Step [1080/1524], Loss: 0.3352\n",
            "Epoch [2/5], Step [1100/1524], Loss: 0.2872\n",
            "Epoch [2/5], Step [1120/1524], Loss: 0.2999\n",
            "Epoch [2/5], Step [1140/1524], Loss: 0.2945\n",
            "Epoch [2/5], Step [1160/1524], Loss: 0.3317\n",
            "Epoch [2/5], Step [1180/1524], Loss: 0.3591\n",
            "Epoch [2/5], Step [1200/1524], Loss: 0.3045\n",
            "Epoch [2/5], Step [1220/1524], Loss: 0.3292\n",
            "Epoch [2/5], Step [1240/1524], Loss: 0.2931\n",
            "Epoch [2/5], Step [1260/1524], Loss: 0.3129\n",
            "Epoch [2/5], Step [1280/1524], Loss: 0.3198\n",
            "Epoch [2/5], Step [1300/1524], Loss: 0.2969\n",
            "Epoch [2/5], Step [1320/1524], Loss: 0.3065\n",
            "Epoch [2/5], Step [1340/1524], Loss: 0.2858\n",
            "Epoch [2/5], Step [1360/1524], Loss: 0.2955\n",
            "Epoch [2/5], Step [1380/1524], Loss: 0.2926\n",
            "Epoch [2/5], Step [1400/1524], Loss: 0.3154\n",
            "Epoch [2/5], Step [1420/1524], Loss: 0.2781\n",
            "Epoch [2/5], Step [1440/1524], Loss: 0.3104\n",
            "Epoch [2/5], Step [1460/1524], Loss: 0.2964\n",
            "Epoch [2/5], Step [1480/1524], Loss: 0.3054\n",
            "Epoch [2/5], Step [1500/1524], Loss: 0.2943\n",
            "Epoch [2/5], Step [1520/1524], Loss: 0.3602\n",
            "Epoch [3/5], Step [20/1524], Loss: 0.2627\n",
            "Epoch [3/5], Step [40/1524], Loss: 0.2789\n",
            "Epoch [3/5], Step [60/1524], Loss: 0.3255\n",
            "Epoch [3/5], Step [80/1524], Loss: 0.3213\n",
            "Epoch [3/5], Step [100/1524], Loss: 0.2991\n",
            "Epoch [3/5], Step [120/1524], Loss: 0.2793\n",
            "Epoch [3/5], Step [140/1524], Loss: 0.3053\n",
            "Epoch [3/5], Step [160/1524], Loss: 0.3080\n",
            "Epoch [3/5], Step [180/1524], Loss: 0.2851\n",
            "Epoch [3/5], Step [200/1524], Loss: 0.3318\n",
            "Epoch [3/5], Step [220/1524], Loss: 0.2576\n",
            "Epoch [3/5], Step [240/1524], Loss: 0.2493\n",
            "Epoch [3/5], Step [260/1524], Loss: 0.3001\n",
            "Epoch [3/5], Step [280/1524], Loss: 0.3058\n",
            "Epoch [3/5], Step [300/1524], Loss: 0.2792\n",
            "Epoch [3/5], Step [320/1524], Loss: 0.3160\n",
            "Epoch [3/5], Step [340/1524], Loss: 0.2568\n",
            "Epoch [3/5], Step [360/1524], Loss: 0.2962\n",
            "Epoch [3/5], Step [380/1524], Loss: 0.2993\n",
            "Epoch [3/5], Step [400/1524], Loss: 0.2987\n",
            "Epoch [3/5], Step [420/1524], Loss: 0.2778\n",
            "Epoch [3/5], Step [440/1524], Loss: 0.2911\n",
            "Epoch [3/5], Step [460/1524], Loss: 0.2818\n",
            "Epoch [3/5], Step [480/1524], Loss: 0.2643\n",
            "Epoch [3/5], Step [500/1524], Loss: 0.2871\n",
            "Epoch [3/5], Step [520/1524], Loss: 0.2750\n",
            "Epoch [3/5], Step [540/1524], Loss: 0.2953\n",
            "Epoch [3/5], Step [560/1524], Loss: 0.2810\n",
            "Epoch [3/5], Step [580/1524], Loss: 0.2970\n",
            "Epoch [3/5], Step [600/1524], Loss: 0.2885\n",
            "Epoch [3/5], Step [620/1524], Loss: 0.2592\n",
            "Epoch [3/5], Step [640/1524], Loss: 0.2933\n",
            "Epoch [3/5], Step [660/1524], Loss: 0.2741\n",
            "Epoch [3/5], Step [680/1524], Loss: 0.2505\n",
            "Epoch [3/5], Step [700/1524], Loss: 0.2494\n",
            "Epoch [3/5], Step [720/1524], Loss: 0.2959\n",
            "Epoch [3/5], Step [740/1524], Loss: 0.2652\n",
            "Epoch [3/5], Step [760/1524], Loss: 0.2704\n",
            "Epoch [3/5], Step [780/1524], Loss: 0.2611\n",
            "Epoch [3/5], Step [800/1524], Loss: 0.2784\n",
            "Epoch [3/5], Step [820/1524], Loss: 0.2956\n",
            "Epoch [3/5], Step [840/1524], Loss: 0.3502\n",
            "Epoch [3/5], Step [860/1524], Loss: 0.2980\n",
            "Epoch [3/5], Step [880/1524], Loss: 0.2991\n",
            "Epoch [3/5], Step [900/1524], Loss: 0.2950\n",
            "Epoch [3/5], Step [920/1524], Loss: 0.2398\n",
            "Epoch [3/5], Step [940/1524], Loss: 0.2881\n",
            "Epoch [3/5], Step [960/1524], Loss: 0.2965\n",
            "Epoch [3/5], Step [980/1524], Loss: 0.3225\n",
            "Epoch [3/5], Step [1000/1524], Loss: 0.2792\n",
            "Epoch [3/5], Step [1020/1524], Loss: 0.3052\n",
            "Epoch [3/5], Step [1040/1524], Loss: 0.2928\n",
            "Epoch [3/5], Step [1060/1524], Loss: 0.2755\n",
            "Epoch [3/5], Step [1080/1524], Loss: 0.3037\n",
            "Epoch [3/5], Step [1100/1524], Loss: 0.2704\n",
            "Epoch [3/5], Step [1120/1524], Loss: 0.3036\n",
            "Epoch [3/5], Step [1140/1524], Loss: 0.3426\n",
            "Epoch [3/5], Step [1160/1524], Loss: 0.2778\n",
            "Epoch [3/5], Step [1180/1524], Loss: 0.3062\n",
            "Epoch [3/5], Step [1200/1524], Loss: 0.2733\n",
            "Epoch [3/5], Step [1220/1524], Loss: 0.2685\n",
            "Epoch [3/5], Step [1240/1524], Loss: 0.2917\n",
            "Epoch [3/5], Step [1260/1524], Loss: 0.2823\n",
            "Epoch [3/5], Step [1280/1524], Loss: 0.2691\n",
            "Epoch [3/5], Step [1300/1524], Loss: 0.2950\n",
            "Epoch [3/5], Step [1320/1524], Loss: 0.3167\n",
            "Epoch [3/5], Step [1340/1524], Loss: 0.2899\n",
            "Epoch [3/5], Step [1360/1524], Loss: 0.2620\n",
            "Epoch [3/5], Step [1380/1524], Loss: 0.3063\n",
            "Epoch [3/5], Step [1400/1524], Loss: 0.3054\n",
            "Epoch [3/5], Step [1420/1524], Loss: 0.2848\n",
            "Epoch [3/5], Step [1440/1524], Loss: 0.3009\n",
            "Epoch [3/5], Step [1460/1524], Loss: 0.2903\n",
            "Epoch [3/5], Step [1480/1524], Loss: 0.2797\n",
            "Epoch [3/5], Step [1500/1524], Loss: 0.3053\n",
            "Epoch [3/5], Step [1520/1524], Loss: 0.2828\n",
            "Epoch [4/5], Step [20/1524], Loss: 0.2948\n",
            "Epoch [4/5], Step [40/1524], Loss: 0.2738\n",
            "Epoch [4/5], Step [60/1524], Loss: 0.2828\n",
            "Epoch [4/5], Step [80/1524], Loss: 0.2891\n",
            "Epoch [4/5], Step [100/1524], Loss: 0.2818\n",
            "Epoch [4/5], Step [120/1524], Loss: 0.2987\n",
            "Epoch [4/5], Step [140/1524], Loss: 0.2773\n",
            "Epoch [4/5], Step [160/1524], Loss: 0.2683\n",
            "Epoch [4/5], Step [180/1524], Loss: 0.3010\n",
            "Epoch [4/5], Step [200/1524], Loss: 0.2987\n",
            "Epoch [4/5], Step [220/1524], Loss: 0.2431\n",
            "Epoch [4/5], Step [240/1524], Loss: 0.3111\n",
            "Epoch [4/5], Step [260/1524], Loss: 0.2471\n",
            "Epoch [4/5], Step [280/1524], Loss: 0.2630\n",
            "Epoch [4/5], Step [300/1524], Loss: 0.2912\n",
            "Epoch [4/5], Step [320/1524], Loss: 0.2342\n",
            "Epoch [4/5], Step [340/1524], Loss: 0.2677\n",
            "Epoch [4/5], Step [360/1524], Loss: 0.2556\n",
            "Epoch [4/5], Step [380/1524], Loss: 0.2746\n",
            "Epoch [4/5], Step [400/1524], Loss: 0.2937\n",
            "Epoch [4/5], Step [420/1524], Loss: 0.2860\n",
            "Epoch [4/5], Step [440/1524], Loss: 0.2903\n",
            "Epoch [4/5], Step [460/1524], Loss: 0.2639\n",
            "Epoch [4/5], Step [480/1524], Loss: 0.2955\n",
            "Epoch [4/5], Step [500/1524], Loss: 0.3016\n",
            "Epoch [4/5], Step [520/1524], Loss: 0.2198\n",
            "Epoch [4/5], Step [540/1524], Loss: 0.2951\n",
            "Epoch [4/5], Step [560/1524], Loss: 0.2740\n",
            "Epoch [4/5], Step [580/1524], Loss: 0.2597\n",
            "Epoch [4/5], Step [600/1524], Loss: 0.3210\n",
            "Epoch [4/5], Step [620/1524], Loss: 0.2682\n",
            "Epoch [4/5], Step [640/1524], Loss: 0.2831\n",
            "Epoch [4/5], Step [660/1524], Loss: 0.3104\n",
            "Epoch [4/5], Step [680/1524], Loss: 0.2834\n",
            "Epoch [4/5], Step [700/1524], Loss: 0.2494\n",
            "Epoch [4/5], Step [720/1524], Loss: 0.2592\n",
            "Epoch [4/5], Step [740/1524], Loss: 0.2865\n",
            "Epoch [4/5], Step [760/1524], Loss: 0.3073\n",
            "Epoch [4/5], Step [780/1524], Loss: 0.3121\n",
            "Epoch [4/5], Step [800/1524], Loss: 0.2633\n",
            "Epoch [4/5], Step [820/1524], Loss: 0.2601\n",
            "Epoch [4/5], Step [840/1524], Loss: 0.2558\n",
            "Epoch [4/5], Step [860/1524], Loss: 0.2826\n",
            "Epoch [4/5], Step [880/1524], Loss: 0.2774\n",
            "Epoch [4/5], Step [900/1524], Loss: 0.2375\n",
            "Epoch [4/5], Step [920/1524], Loss: 0.2717\n",
            "Epoch [4/5], Step [940/1524], Loss: 0.2584\n",
            "Epoch [4/5], Step [960/1524], Loss: 0.2458\n",
            "Epoch [4/5], Step [980/1524], Loss: 0.2694\n",
            "Epoch [4/5], Step [1000/1524], Loss: 0.2566\n",
            "Epoch [4/5], Step [1020/1524], Loss: 0.2851\n",
            "Epoch [4/5], Step [1040/1524], Loss: 0.2648\n",
            "Epoch [4/5], Step [1060/1524], Loss: 0.2420\n",
            "Epoch [4/5], Step [1080/1524], Loss: 0.2405\n",
            "Epoch [4/5], Step [1100/1524], Loss: 0.2865\n",
            "Epoch [4/5], Step [1120/1524], Loss: 0.2387\n",
            "Epoch [4/5], Step [1140/1524], Loss: 0.2957\n",
            "Epoch [4/5], Step [1160/1524], Loss: 0.2529\n",
            "Epoch [4/5], Step [1180/1524], Loss: 0.2712\n",
            "Epoch [4/5], Step [1200/1524], Loss: 0.2638\n",
            "Epoch [4/5], Step [1220/1524], Loss: 0.2689\n",
            "Epoch [4/5], Step [1240/1524], Loss: 0.2517\n",
            "Epoch [4/5], Step [1260/1524], Loss: 0.2428\n",
            "Epoch [4/5], Step [1280/1524], Loss: 0.2547\n",
            "Epoch [4/5], Step [1300/1524], Loss: 0.2975\n",
            "Epoch [4/5], Step [1320/1524], Loss: 0.2753\n",
            "Epoch [4/5], Step [1340/1524], Loss: 0.2513\n",
            "Epoch [4/5], Step [1360/1524], Loss: 0.2441\n",
            "Epoch [4/5], Step [1380/1524], Loss: 0.2559\n",
            "Epoch [4/5], Step [1400/1524], Loss: 0.2658\n",
            "Epoch [4/5], Step [1420/1524], Loss: 0.2523\n",
            "Epoch [4/5], Step [1440/1524], Loss: 0.2761\n",
            "Epoch [4/5], Step [1460/1524], Loss: 0.2989\n",
            "Epoch [4/5], Step [1480/1524], Loss: 0.2717\n",
            "Epoch [4/5], Step [1500/1524], Loss: 0.2449\n",
            "Epoch [4/5], Step [1520/1524], Loss: 0.2702\n",
            "Epoch [5/5], Step [20/1524], Loss: 0.2599\n",
            "Epoch [5/5], Step [40/1524], Loss: 0.2739\n",
            "Epoch [5/5], Step [60/1524], Loss: 0.2740\n",
            "Epoch [5/5], Step [80/1524], Loss: 0.2735\n",
            "Epoch [5/5], Step [100/1524], Loss: 0.2745\n",
            "Epoch [5/5], Step [120/1524], Loss: 0.2738\n",
            "Epoch [5/5], Step [140/1524], Loss: 0.2778\n",
            "Epoch [5/5], Step [160/1524], Loss: 0.2702\n",
            "Epoch [5/5], Step [180/1524], Loss: 0.2475\n",
            "Epoch [5/5], Step [200/1524], Loss: 0.2654\n",
            "Epoch [5/5], Step [220/1524], Loss: 0.2335\n",
            "Epoch [5/5], Step [240/1524], Loss: 0.2508\n",
            "Epoch [5/5], Step [260/1524], Loss: 0.2479\n",
            "Epoch [5/5], Step [280/1524], Loss: 0.2321\n",
            "Epoch [5/5], Step [300/1524], Loss: 0.3003\n",
            "Epoch [5/5], Step [320/1524], Loss: 0.2553\n",
            "Epoch [5/5], Step [340/1524], Loss: 0.2414\n",
            "Epoch [5/5], Step [360/1524], Loss: 0.3066\n",
            "Epoch [5/5], Step [380/1524], Loss: 0.2741\n",
            "Epoch [5/5], Step [400/1524], Loss: 0.2528\n",
            "Epoch [5/5], Step [420/1524], Loss: 0.2405\n",
            "Epoch [5/5], Step [440/1524], Loss: 0.2637\n",
            "Epoch [5/5], Step [460/1524], Loss: 0.2684\n",
            "Epoch [5/5], Step [480/1524], Loss: 0.2856\n",
            "Epoch [5/5], Step [500/1524], Loss: 0.2399\n",
            "Epoch [5/5], Step [520/1524], Loss: 0.3148\n",
            "Epoch [5/5], Step [540/1524], Loss: 0.2638\n",
            "Epoch [5/5], Step [560/1524], Loss: 0.2654\n",
            "Epoch [5/5], Step [580/1524], Loss: 0.2752\n",
            "Epoch [5/5], Step [600/1524], Loss: 0.2544\n",
            "Epoch [5/5], Step [620/1524], Loss: 0.2663\n",
            "Epoch [5/5], Step [640/1524], Loss: 0.2763\n",
            "Epoch [5/5], Step [660/1524], Loss: 0.2521\n",
            "Epoch [5/5], Step [680/1524], Loss: 0.2719\n",
            "Epoch [5/5], Step [700/1524], Loss: 0.2471\n",
            "Epoch [5/5], Step [720/1524], Loss: 0.2735\n",
            "Epoch [5/5], Step [740/1524], Loss: 0.2624\n",
            "Epoch [5/5], Step [760/1524], Loss: 0.2379\n",
            "Epoch [5/5], Step [780/1524], Loss: 0.2469\n",
            "Epoch [5/5], Step [800/1524], Loss: 0.2621\n",
            "Epoch [5/5], Step [820/1524], Loss: 0.2301\n",
            "Epoch [5/5], Step [840/1524], Loss: 0.2757\n",
            "Epoch [5/5], Step [860/1524], Loss: 0.2376\n",
            "Epoch [5/5], Step [880/1524], Loss: 0.2568\n",
            "Epoch [5/5], Step [900/1524], Loss: 0.2441\n",
            "Epoch [5/5], Step [920/1524], Loss: 0.2410\n",
            "Epoch [5/5], Step [940/1524], Loss: 0.2543\n",
            "Epoch [5/5], Step [960/1524], Loss: 0.2665\n",
            "Epoch [5/5], Step [980/1524], Loss: 0.2585\n",
            "Epoch [5/5], Step [1000/1524], Loss: 0.2476\n",
            "Epoch [5/5], Step [1020/1524], Loss: 0.2447\n",
            "Epoch [5/5], Step [1040/1524], Loss: 0.2742\n",
            "Epoch [5/5], Step [1060/1524], Loss: 0.2630\n",
            "Epoch [5/5], Step [1080/1524], Loss: 0.2407\n",
            "Epoch [5/5], Step [1100/1524], Loss: 0.2860\n",
            "Epoch [5/5], Step [1120/1524], Loss: 0.2523\n",
            "Epoch [5/5], Step [1140/1524], Loss: 0.2557\n",
            "Epoch [5/5], Step [1160/1524], Loss: 0.3032\n",
            "Epoch [5/5], Step [1180/1524], Loss: 0.2632\n",
            "Epoch [5/5], Step [1200/1524], Loss: 0.2893\n",
            "Epoch [5/5], Step [1220/1524], Loss: 0.2526\n",
            "Epoch [5/5], Step [1240/1524], Loss: 0.2425\n",
            "Epoch [5/5], Step [1260/1524], Loss: 0.2380\n",
            "Epoch [5/5], Step [1280/1524], Loss: 0.2596\n",
            "Epoch [5/5], Step [1300/1524], Loss: 0.2448\n",
            "Epoch [5/5], Step [1320/1524], Loss: 0.2409\n",
            "Epoch [5/5], Step [1340/1524], Loss: 0.2237\n",
            "Epoch [5/5], Step [1360/1524], Loss: 0.2492\n",
            "Epoch [5/5], Step [1380/1524], Loss: 0.2534\n",
            "Epoch [5/5], Step [1400/1524], Loss: 0.2832\n",
            "Epoch [5/5], Step [1420/1524], Loss: 0.2345\n",
            "Epoch [5/5], Step [1440/1524], Loss: 0.2789\n",
            "Epoch [5/5], Step [1460/1524], Loss: 0.2716\n",
            "Epoch [5/5], Step [1480/1524], Loss: 0.2396\n",
            "Epoch [5/5], Step [1500/1524], Loss: 0.2279\n",
            "Epoch [5/5], Step [1520/1524], Loss: 0.2281\n",
            "Test Accuracy: 76.30%\n"
          ]
        }
      ]
    }
  ]
}